{"version":2,"kind":"Article","sha256":"ae77dda8973f2a950282115dc44296c93c0097fccd31799f9e4ca6fb9eaee61a","slug":"performance-optimization","location":"/03-scientific-computing-with-python/03-advanced-scientific-computing/11-performance-optimization.md","dependencies":[],"frontmatter":{"title":"Chapter 7: Performance Optimization","content_includes_title":false,"authors":[{"nameParsed":{"literal":"Anna Rosen","given":"Anna","family":"Rosen"},"name":"Anna Rosen","orcid":"0000-0003-4423-0660","email":"alrosen@sdsu.edu","affiliations":["San Diego State University"],"id":"contributors-myst-generated-uid-0","corresponding":true}],"license":{"content":{"id":"CC-BY-4.0","url":"https://creativecommons.org/licenses/by/4.0/","name":"Creative Commons Attribution 4.0 International","free":true,"CC":true},"code":{"id":"MIT","url":"https://opensource.org/licenses/MIT","name":"MIT License","free":true,"osi":true}},"github":"https://github.com/astrobytes-edu/astr596-modeling-universe","subject":"Modeling the Universe","venue":{"title":"ASTR 596 - Fall 2025","url":"https://www.anna-rosen.com"},"keywords":["computational astrophysics","python","numerical methods","machine learning","monte carlo","neural networks","radiative transfer","bayesian inference","JAX"],"affiliations":[{"id":"San Diego State University","name":"San Diego State University"}],"numbering":{"title":{"offset":2}},"edit_url":"https://github.com/astrobytes-edu/astr596-modeling-universe/blob/main/03-scientific-computing-with-python/03-advanced-scientific-computing/11-performance-optimization.md","exports":[{"format":"md","filename":"11-performance-optimization.md","url":"/11-performance-optim-6ac20b7d0d72dc4f06a6275eef975f93.md"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"heading","depth":2,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Learning Objectives","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"oc00oxn1yf"}],"identifier":"learning-objectives","label":"Learning Objectives","html_id":"learning-objectives","implicit":true,"key":"zamlSFsx3a"},{"type":"paragraph","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"By the end of this chapter, you will:","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"Lm2pFyvxKm"}],"key":"aeyiAy8q1a"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":5,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Profile code to identify bottlenecks scientifically","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"nhXUAqOSFJ"}],"key":"jabR97iiZg"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"Vectorize computations for 10-100x speedups","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"JadGjitnbt"}],"key":"QB3KHVZpqC"},{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"Use Numba JIT compilation for near-C performance","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"LN235CiDMJ"}],"key":"LFFRgkSc3b"},{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"Implement parallel processing for multi-core systems","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"EO04m9PrEr"}],"key":"Uk7cmo475E"},{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"Optimize memory usage for large astronomical datasets","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"puImf71YoM"}],"key":"SDDE971qXB"},{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"Know when to optimize and when to use existing solutions","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"apwSD0EOEW"}],"key":"sMYBUo4Vlb"}],"key":"eyAlPVjW4B"},{"type":"heading","depth":2,"position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"7.1 Profiling: Measure Before Optimizing","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"IXo8tWa2Rg"}],"identifier":"id-7-1-profiling-measure-before-optimizing","label":"7.1 Profiling: Measure Before Optimizing","html_id":"id-7-1-profiling-measure-before-optimizing","implicit":true,"key":"nrDb0IMbGc"},{"type":"heading","depth":3,"position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"text","value":"The Golden Rule of Optimization","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"gFI9HAg8TV"}],"identifier":"the-golden-rule-of-optimization","label":"The Golden Rule of Optimization","html_id":"the-golden-rule-of-optimization","implicit":true,"key":"TLv5OJzTjX"},{"type":"code","lang":"python","value":"import time\nimport numpy as np\nimport cProfile\nimport pstats\nfrom line_profiler import LineProfiler\n\ndef demonstrate_premature_optimization():\n    \"\"\"\n    Knuth's famous quote: \"Premature optimization is the root of all evil\"\n    \n    Profile FIRST, optimize SECOND.\n    \"\"\"\n    \n    # Version 1: Readable but \"inefficient\"?\n    def calculate_distances_readable(coords):\n        n = len(coords)\n        distances = []\n        for i in range(n):\n            for j in range(i+1, n):\n                dist = np.sqrt((coords[i][0] - coords[j][0])**2 + \n                              (coords[i][1] - coords[j][1])**2)\n                distances.append(dist)\n        return distances\n    \n    # Version 2: \"Optimized\" but actually slower!\n    def calculate_distances_clever(coords):\n        n = len(coords)\n        distances = []\n        for i in range(n):\n            for j in range(i+1, n):\n                # \"Optimize\" by avoiding sqrt for comparison\n                dist_sq = (coords[i][0] - coords[j][0])**2 + \\\n                         (coords[i][1] - coords[j][1])**2\n                # But then we need sqrt anyway...\n                distances.append(np.sqrt(dist_sq))\n        return distances\n    \n    # Version 3: Actually optimized\n    def calculate_distances_vectorized(coords):\n        coords = np.array(coords)\n        diff = coords[:, np.newaxis, :] - coords[np.newaxis, :, :]\n        dist_matrix = np.sqrt(np.sum(diff**2, axis=2))\n        return dist_matrix[np.triu_indices_from(dist_matrix, k=1)]\n    \n    # Test with realistic data\n    n_stars = 100\n    coords = [(np.random.uniform(0, 100), np.random.uniform(0, 100)) \n               for _ in range(n_stars)]\n    \n    # Time each version\n    start = time.perf_counter()\n    d1 = calculate_distances_readable(coords)\n    t1 = time.perf_counter() - start\n    \n    start = time.perf_counter()\n    d2 = calculate_distances_clever(coords)\n    t2 = time.perf_counter() - start\n    \n    start = time.perf_counter()\n    d3 = calculate_distances_vectorized(coords)\n    t3 = time.perf_counter() - start\n    \n    print(f\"Readable version:   {t1*1000:.2f} ms\")\n    print(f\"'Clever' version:   {t2*1000:.2f} ms (no improvement!)\")\n    print(f\"Vectorized version: {t3*1000:.2f} ms ({t1/t3:.1f}x faster)\")\n\ndemonstrate_premature_optimization()","position":{"start":{"line":16,"column":1},"end":{"line":84,"column":1}},"key":"uC1XUueh70"},{"type":"heading","depth":3,"position":{"start":{"line":86,"column":1},"end":{"line":86,"column":1}},"children":[{"type":"text","value":"Using cProfile for Function-Level Profiling","position":{"start":{"line":86,"column":1},"end":{"line":86,"column":1}},"key":"dQW7q3KmG7"}],"identifier":"using-cprofile-for-function-level-profiling","label":"Using cProfile for Function-Level Profiling","html_id":"using-cprofile-for-function-level-profiling","implicit":true,"key":"phm1krQIE5"},{"type":"code","lang":"python","value":"def profile_spectrum_analysis():\n    \"\"\"Profile a realistic spectrum analysis pipeline.\"\"\"\n    \n    def load_spectrum(n_points=10000):\n        \"\"\"Simulate loading spectrum data.\"\"\"\n        wavelengths = np.linspace(4000, 7000, n_points)\n        fluxes = np.random.randn(n_points) + 100\n        # Add some spectral lines\n        for line_center in [4861, 6563]:  # H-beta, H-alpha\n            fluxes += 50 * np.exp(-(wavelengths - line_center)**2 / 25)\n        return wavelengths, fluxes\n    \n    def remove_continuum(wavelengths, fluxes, window=100):\n        \"\"\"Remove continuum using median filter.\"\"\"\n        from scipy.ndimage import median_filter\n        continuum = median_filter(fluxes, size=window)\n        return fluxes - continuum\n    \n    def find_lines(wavelengths, fluxes, threshold=3):\n        \"\"\"Find emission lines.\"\"\"\n        from scipy.signal import find_peaks\n        std = np.std(fluxes)\n        peaks, properties = find_peaks(fluxes, height=threshold*std)\n        return wavelengths[peaks], fluxes[peaks]\n    \n    def measure_redshift(line_wavelengths, rest_wavelength=6563):\n        \"\"\"Calculate redshift from lines.\"\"\"\n        if len(line_wavelengths) == 0:\n            return 0\n        # Find closest to expected H-alpha\n        closest_idx = np.argmin(np.abs(line_wavelengths - rest_wavelength))\n        observed = line_wavelengths[closest_idx]\n        return (observed - rest_wavelength) / rest_wavelength\n    \n    # Profile the complete pipeline\n    profiler = cProfile.Profile()\n    profiler.enable()\n    \n    # Run analysis\n    wavelengths, fluxes = load_spectrum(50000)\n    normalized = remove_continuum(wavelengths, fluxes)\n    line_waves, line_fluxes = find_lines(wavelengths, normalized)\n    redshift = measure_redshift(line_waves)\n    \n    profiler.disable()\n    \n    # Print statistics\n    stats = pstats.Stats(profiler)\n    stats.sort_stats('cumulative')\n    print(\"\\nTop 10 functions by cumulative time:\")\n    stats.print_stats(10)\n    \n    return redshift\n\n# z = profile_spectrum_analysis()  # Uncomment to see profiling","position":{"start":{"line":88,"column":1},"end":{"line":144,"column":1}},"key":"qc2cK1dTnt"},{"type":"heading","depth":3,"position":{"start":{"line":146,"column":1},"end":{"line":146,"column":1}},"children":[{"type":"text","value":"Line-by-Line Profiling","position":{"start":{"line":146,"column":1},"end":{"line":146,"column":1}},"key":"gh4kW2yaTd"}],"identifier":"line-by-line-profiling","label":"Line-by-Line Profiling","html_id":"line-by-line-profiling","implicit":true,"key":"Gj4ihrX4ij"},{"type":"code","lang":"python","value":"# Use line_profiler for detailed analysis\n# Install: pip install line_profiler\n\ndef detailed_profile_example():\n    \"\"\"Example of line-by-line profiling.\"\"\"\n    \n    # Decorate function with @profile (added by line_profiler)\n    # Run with: kernprof -l -v script.py\n    \n    def process_image(image):  # Add @profile decorator\n        \"\"\"Process CCD image - which lines are slow?\"\"\"\n        # Line 1: Median filter for cosmic ray removal\n        from scipy.ndimage import median_filter\n        cleaned = median_filter(image, size=3)\n        \n        # Line 2: Calculate statistics\n        mean = np.mean(cleaned)\n        std = np.std(cleaned)\n        \n        # Line 3: Find sources above threshold\n        threshold = mean + 5 * std\n        sources = cleaned > threshold\n        \n        # Line 4: Label connected regions\n        from scipy.ndimage import label\n        labeled, num_sources = label(sources)\n        \n        # Line 5: Calculate properties for each source\n        source_properties = []\n        for i in range(1, num_sources + 1):\n            mask = labeled == i\n            flux = np.sum(cleaned[mask])\n            centroid = np.mean(np.argwhere(mask), axis=0)\n            source_properties.append({'flux': flux, 'centroid': centroid})\n        \n        return source_properties\n    \n    # Simulate CCD image\n    image = np.random.randn(1024, 1024) + 100\n    # Add some \"stars\"\n    for _ in range(50):\n        x, y = np.random.randint(10, 1014, 2)\n        image[x-2:x+3, y-2:y+3] += 1000\n    \n    # This would show time spent on each line\n    # sources = process_image(image)\n    \n    print(\"Line profiler example ready - add @profile decorator and run with kernprof\")\n\ndetailed_profile_example()","position":{"start":{"line":148,"column":1},"end":{"line":199,"column":1}},"key":"apcWuKsGxg"},{"type":"heading","depth":2,"position":{"start":{"line":201,"column":1},"end":{"line":201,"column":1}},"children":[{"type":"text","value":"7.2 Vectorization: The NumPy Way","position":{"start":{"line":201,"column":1},"end":{"line":201,"column":1}},"key":"A663Awe2Zb"}],"identifier":"id-7-2-vectorization-the-numpy-way","label":"7.2 Vectorization: The NumPy Way","html_id":"id-7-2-vectorization-the-numpy-way","implicit":true,"key":"cPohJ0EPy5"},{"type":"heading","depth":3,"position":{"start":{"line":203,"column":1},"end":{"line":203,"column":1}},"children":[{"type":"text","value":"Understanding Why Vectorization is Fast","position":{"start":{"line":203,"column":1},"end":{"line":203,"column":1}},"key":"JvWQcDWHwE"}],"identifier":"understanding-why-vectorization-is-fast","label":"Understanding Why Vectorization is Fast","html_id":"understanding-why-vectorization-is-fast","implicit":true,"key":"IBfm1BZOzo"},{"type":"code","lang":"python","value":"def vectorization_explanation():\n    \"\"\"Why is NumPy so much faster than pure Python loops?\"\"\"\n    \n    print(\"Why Vectorization Works:\\n\")\n    print(\"1. Python loops have overhead:\")\n    print(\"   - Type checking each iteration\")\n    print(\"   - Function calls for each operation\")\n    print(\"   - Memory allocation for intermediate results\")\n    \n    print(\"\\n2. NumPy operations:\")\n    print(\"   - Implemented in optimized C\")\n    print(\"   - Use CPU vector instructions (SIMD)\")\n    print(\"   - Better memory access patterns\")\n    print(\"   - No Python overhead in inner loop\")\n    \n    # Demonstration\n    n = 1_000_000\n    \n    # Python list operations\n    python_list = list(range(n))\n    start = time.perf_counter()\n    python_result = [x**2 for x in python_list]\n    python_time = time.perf_counter() - start\n    \n    # NumPy operations\n    numpy_array = np.arange(n)\n    start = time.perf_counter()\n    numpy_result = numpy_array**2\n    numpy_time = time.perf_counter() - start\n    \n    print(f\"\\nSquaring {n:,} numbers:\")\n    print(f\"Python list: {python_time*1000:.1f} ms\")\n    print(f\"NumPy array: {numpy_time*1000:.1f} ms\")\n    print(f\"Speedup: {python_time/numpy_time:.1f}x\")\n\nvectorization_explanation()","position":{"start":{"line":205,"column":1},"end":{"line":242,"column":1}},"key":"LKIcr8ctnc"},{"type":"heading","depth":3,"position":{"start":{"line":244,"column":1},"end":{"line":244,"column":1}},"children":[{"type":"text","value":"Vectorization Patterns for Astronomy","position":{"start":{"line":244,"column":1},"end":{"line":244,"column":1}},"key":"M8xVYG5vph"}],"identifier":"vectorization-patterns-for-astronomy","label":"Vectorization Patterns for Astronomy","html_id":"vectorization-patterns-for-astronomy","implicit":true,"key":"MZ3W8JpCJH"},{"type":"code","lang":"python","value":"class VectorizedAstronomyCalculations:\n    \"\"\"Common vectorization patterns in astronomy.\"\"\"\n    \n    @staticmethod\n    def angular_distance_matrix(ra, dec):\n        \"\"\"\n        Calculate all pairwise angular distances.\n        Vectorized haversine formula.\n        \"\"\"\n        # Convert to radians\n        ra_rad = np.radians(ra)\n        dec_rad = np.radians(dec)\n        \n        # Use broadcasting to compute all pairs\n        # Shape: (n, 1) - (1, n) = (n, n)\n        dra = ra_rad[:, np.newaxis] - ra_rad[np.newaxis, :]\n        ddec = dec_rad[:, np.newaxis] - dec_rad[np.newaxis, :]\n        \n        # Haversine formula\n        a = np.sin(ddec/2)**2 + \\\n            np.cos(dec_rad[:, np.newaxis]) * np.cos(dec_rad[np.newaxis, :]) * \\\n            np.sin(dra/2)**2\n        \n        c = 2 * np.arcsin(np.sqrt(a))\n        return np.degrees(c)\n    \n    @staticmethod\n    def extinction_correction_vectorized(magnitudes, colors, R_v=3.1):\n        \"\"\"\n        Apply extinction correction to many stars at once.\n        \n        Instead of looping over stars, process all simultaneously.\n        \"\"\"\n        # Ensure arrays\n        magnitudes = np.asarray(magnitudes)\n        colors = np.asarray(colors)\n        \n        # Cardelli extinction law coefficients (simplified)\n        a_v = 0.574 * colors  # Simplified color-extinction relation\n        \n        # Apply to all magnitudes at once\n        corrected = magnitudes - R_v * a_v\n        \n        # Handle edge cases with numpy\n        corrected = np.where(colors < 0, magnitudes, corrected)  # No correction for blue colors\n        \n        return corrected\n    \n    @staticmethod\n    def phase_fold_vectorized(times, fluxes, period):\n        \"\"\"\n        Phase fold a light curve - vectorized version.\n        \"\"\"\n        # Calculate phase for all times at once\n        phases = (times % period) / period\n        \n        # Sort by phase (vectorized)\n        sort_idx = np.argsort(phases)\n        \n        return phases[sort_idx], fluxes[sort_idx]\n    \n    @staticmethod\n    def match_catalogs_vectorized(ra1, dec1, ra2, dec2, max_sep=1.0):\n        \"\"\"\n        Cross-match catalogs using vectorized operations.\n        \n        More memory intensive but much faster than loops.\n        \"\"\"\n        # Convert max separation to radians\n        max_sep_rad = np.radians(max_sep / 3600)  # arcsec to radians\n        \n        # Use KDTree for efficient spatial matching\n        from scipy.spatial import cKDTree\n        \n        # Convert to Cartesian for KDTree\n        def radec_to_xyz(ra, dec):\n            ra_rad = np.radians(ra)\n            dec_rad = np.radians(dec)\n            x = np.cos(dec_rad) * np.cos(ra_rad)\n            y = np.cos(dec_rad) * np.sin(ra_rad)\n            z = np.sin(dec_rad)\n            return np.column_stack([x, y, z])\n        \n        xyz1 = radec_to_xyz(ra1, dec1)\n        xyz2 = radec_to_xyz(ra2, dec2)\n        \n        # Build tree and query\n        tree = cKDTree(xyz2)\n        \n        # Vectorized query for all points at once\n        # Convert angular separation to 3D distance\n        max_3d_dist = 2 * np.sin(max_sep_rad / 2)\n        \n        distances, indices = tree.query(xyz1, distance_upper_bound=max_3d_dist)\n        \n        # Valid matches where distance is within bound\n        valid = distances < max_3d_dist\n        \n        matches = []\n        for i, (is_valid, idx) in enumerate(zip(valid, indices)):\n            if is_valid:\n                matches.append((i, idx))\n        \n        return matches\n\n# Test vectorized calculations\ncalc = VectorizedAstronomyCalculations()\n\n# Generate test data\nn_stars = 1000\nra = np.random.uniform(0, 360, n_stars)\ndec = np.random.uniform(-30, 30, n_stars)\n\n# Time angular distance calculation\nstart = time.perf_counter()\ndist_matrix = calc.angular_distance_matrix(ra, dec)\nprint(f\"Angular distance matrix for {n_stars} stars: {time.perf_counter() - start:.3f}s\")\n\n# Test extinction correction\nmagnitudes = np.random.uniform(10, 15, n_stars)\ncolors = np.random.uniform(-0.5, 2.0, n_stars)\ncorrected = calc.extinction_correction_vectorized(magnitudes, colors)\nprint(f\"Corrected {n_stars} magnitudes (vectorized)\")","position":{"start":{"line":246,"column":1},"end":{"line":370,"column":1}},"key":"tnNlgcvgr9"},{"type":"heading","depth":2,"position":{"start":{"line":372,"column":1},"end":{"line":372,"column":1}},"children":[{"type":"text","value":"7.3 Numba: JIT Compilation for Python","position":{"start":{"line":372,"column":1},"end":{"line":372,"column":1}},"key":"SO0z6Y1K90"}],"identifier":"id-7-3-numba-jit-compilation-for-python","label":"7.3 Numba: JIT Compilation for Python","html_id":"id-7-3-numba-jit-compilation-for-python","implicit":true,"key":"ssE53rDopF"},{"type":"heading","depth":3,"position":{"start":{"line":374,"column":1},"end":{"line":374,"column":1}},"children":[{"type":"text","value":"Basic Numba Usage","position":{"start":{"line":374,"column":1},"end":{"line":374,"column":1}},"key":"Q8aWd95eDI"}],"identifier":"basic-numba-usage","label":"Basic Numba Usage","html_id":"basic-numba-usage","implicit":true,"key":"CAdBvsnabS"},{"type":"code","lang":"python","value":"from numba import jit, njit, prange, vectorize\nimport numba\n\n@njit  # No Python mode, faster\ndef orbital_integration_numba(x0, v0, mass, dt, n_steps):\n    \"\"\"\n    Orbital integration with Numba acceleration.\n    \n    Compare this to pure Python version!\n    \"\"\"\n    G = 6.67430e-11\n    \n    # Pre-allocate arrays\n    positions = np.zeros((n_steps, 3))\n    velocities = np.zeros((n_steps, 3))\n    \n    positions[0] = x0\n    velocities[0] = v0\n    \n    for i in range(1, n_steps):\n        # Current state\n        r = positions[i-1]\n        v = velocities[i-1]\n        \n        # Calculate acceleration\n        r_mag = np.sqrt(r[0]**2 + r[1]**2 + r[2]**2)\n        a = -G * mass * r / r_mag**3\n        \n        # Leapfrog integration\n        v_half = v + 0.5 * dt * a\n        positions[i] = r + dt * v_half\n        \n        # Update acceleration at new position\n        r_new = positions[i]\n        r_mag_new = np.sqrt(r_new[0]**2 + r_new[1]**2 + r_new[2]**2)\n        a_new = -G * mass * r_new / r_mag_new**3\n        \n        velocities[i] = v_half + 0.5 * dt * a_new\n    \n    return positions, velocities\n\ndef compare_integration_performance():\n    \"\"\"Compare Python vs Numba performance.\"\"\"\n    \n    # Pure Python version (simplified)\n    def orbital_integration_python(x0, v0, mass, dt, n_steps):\n        G = 6.67430e-11\n        positions = []\n        velocities = []\n        \n        r = np.array(x0)\n        v = np.array(v0)\n        \n        for _ in range(n_steps):\n            positions.append(r.copy())\n            velocities.append(v.copy())\n            \n            r_mag = np.linalg.norm(r)\n            a = -G * mass * r / r_mag**3\n            \n            v = v + dt * a\n            r = r + dt * v\n        \n        return np.array(positions), np.array(velocities)\n    \n    # Test parameters\n    x0 = np.array([1.496e11, 0.0, 0.0])  # 1 AU\n    v0 = np.array([0.0, 29780.0, 0.0])   # Earth orbital velocity\n    mass = 1.989e30  # Solar mass\n    dt = 3600.0  # 1 hour\n    n_steps = 10000\n    \n    # Time Python version\n    start = time.perf_counter()\n    pos_py, vel_py = orbital_integration_python(x0, v0, mass, dt, n_steps)\n    python_time = time.perf_counter() - start\n    \n    # Time Numba version (first call includes compilation)\n    start = time.perf_counter()\n    pos_nb, vel_nb = orbital_integration_numba(x0, v0, mass, dt, n_steps)\n    numba_time_with_compile = time.perf_counter() - start\n    \n    # Time Numba version again (already compiled)\n    start = time.perf_counter()\n    pos_nb, vel_nb = orbital_integration_numba(x0, v0, mass, dt, n_steps)\n    numba_time = time.perf_counter() - start\n    \n    print(f\"Integration of {n_steps} steps:\")\n    print(f\"  Pure Python: {python_time:.3f}s\")\n    print(f\"  Numba (with compilation): {numba_time_with_compile:.3f}s\")\n    print(f\"  Numba (compiled): {numba_time:.3f}s\")\n    print(f\"  Speedup: {python_time/numba_time:.1f}x\")\n\ncompare_integration_performance()","position":{"start":{"line":376,"column":1},"end":{"line":471,"column":1}},"key":"X3zkz1fRfR"},{"type":"heading","depth":3,"position":{"start":{"line":473,"column":1},"end":{"line":473,"column":1}},"children":[{"type":"text","value":"Parallel Computing with Numba","position":{"start":{"line":473,"column":1},"end":{"line":473,"column":1}},"key":"S2mJu0bFCl"}],"identifier":"parallel-computing-with-numba","label":"Parallel Computing with Numba","html_id":"parallel-computing-with-numba","implicit":true,"key":"pYFElKbNaU"},{"type":"code","lang":"python","value":"@njit(parallel=True)\ndef monte_carlo_pi_parallel(n_samples):\n    \"\"\"\n    Parallel Monte Carlo calculation of π.\n    \n    Demonstrates Numba's automatic parallelization.\n    \"\"\"\n    count = 0\n    \n    # prange automatically parallelizes this loop\n    for i in prange(n_samples):\n        x = np.random.random()\n        y = np.random.random()\n        \n        if x*x + y*y <= 1.0:\n            count += 1\n    \n    return 4.0 * count / n_samples\n\n@njit(parallel=True)\ndef process_many_spectra(spectra, noise_threshold=3.0):\n    \"\"\"\n    Process multiple spectra in parallel.\n    \n    Each spectrum is processed independently - perfect for parallelization.\n    \"\"\"\n    n_spectra, n_points = spectra.shape\n    results = np.zeros(n_spectra)\n    \n    # Process each spectrum in parallel\n    for i in prange(n_spectra):\n        spectrum = spectra[i]\n        \n        # Calculate statistics\n        mean = np.mean(spectrum)\n        std = np.std(spectrum)\n        \n        # Count significant peaks\n        threshold = mean + noise_threshold * std\n        n_peaks = 0\n        for j in range(1, n_points - 1):\n            if spectrum[j] > threshold:\n                if spectrum[j] > spectrum[j-1] and spectrum[j] > spectrum[j+1]:\n                    n_peaks += 1\n        \n        results[i] = n_peaks\n    \n    return results\n\n# Test parallel performance\ndef test_parallel_speedup():\n    \"\"\"Demonstrate parallel speedup with Numba.\"\"\"\n    \n    # Monte Carlo test\n    n_samples = 10_000_000\n    \n    start = time.perf_counter()\n    pi_estimate = monte_carlo_pi_parallel(n_samples)\n    parallel_time = time.perf_counter() - start\n    \n    print(f\"π estimate: {pi_estimate:.6f}\")\n    print(f\"Time with {numba.config.NUMBA_NUM_THREADS} threads: {parallel_time:.3f}s\")\n    \n    # Spectra processing test\n    n_spectra = 1000\n    n_points = 2048\n    spectra = np.random.randn(n_spectra, n_points) + 100\n    \n    start = time.perf_counter()\n    peak_counts = process_many_spectra(spectra)\n    spec_time = time.perf_counter() - start\n    \n    print(f\"\\nProcessed {n_spectra} spectra in {spec_time:.3f}s\")\n    print(f\"Average peaks per spectrum: {np.mean(peak_counts):.1f}\")\n\ntest_parallel_speedup()","position":{"start":{"line":475,"column":1},"end":{"line":552,"column":1}},"key":"X7K6cRfdam"},{"type":"heading","depth":3,"position":{"start":{"line":554,"column":1},"end":{"line":554,"column":1}},"children":[{"type":"text","value":"Custom NumPy UFuncs with Numba","position":{"start":{"line":554,"column":1},"end":{"line":554,"column":1}},"key":"Z0WtYXh2gm"}],"identifier":"custom-numpy-ufuncs-with-numba","label":"Custom NumPy UFuncs with Numba","html_id":"custom-numpy-ufuncs-with-numba","implicit":true,"key":"j6gk6nInQA"},{"type":"code","lang":"python","value":"@vectorize(['float64(float64, float64)'], nopython=True)\ndef magnitude_addition(mag1, mag2):\n    \"\"\"\n    Vectorized function to add astronomical magnitudes.\n    \n    Works element-wise on arrays, just like NumPy functions.\n    \"\"\"\n    flux1 = 10**(-0.4 * mag1)\n    flux2 = 10**(-0.4 * mag2)\n    total_flux = flux1 + flux2\n    return -2.5 * np.log10(total_flux)\n\n@vectorize(['float64(float64, float64, float64)'], nopython=True)\ndef planck_function_fast(wavelength, temperature, scale):\n    \"\"\"\n    Fast Planck function evaluation.\n    \"\"\"\n    h = 6.62607015e-34\n    c = 299792458.0\n    k = 1.380649e-23\n    \n    # Wavelength in meters\n    lam = wavelength * 1e-9\n    \n    # Planck function\n    numerator = 2 * h * c**2 / lam**5\n    x = h * c / (lam * k * temperature)\n    \n    # Avoid overflow\n    if x > 700:\n        return 0.0\n    \n    denominator = np.exp(x) - 1\n    return scale * numerator / denominator\n\n# Test custom ufuncs\ndef test_custom_ufuncs():\n    \"\"\"Test our custom vectorized functions.\"\"\"\n    \n    # Test magnitude addition\n    mag_array1 = np.array([10.0, 11.0, 12.0, 13.0])\n    mag_array2 = np.array([10.5, 10.5, 10.5, 10.5])\n    \n    combined = magnitude_addition(mag_array1, mag_array2)\n    print(\"Combined magnitudes:\")\n    for m1, m2, mc in zip(mag_array1, mag_array2, combined):\n        print(f\"  {m1:.1f} + {m2:.1f} = {mc:.2f}\")\n    \n    # Test Planck function\n    wavelengths = np.linspace(100, 3000, 1000)  # nm\n    temps = np.array([3000, 5778, 10000])  # K\n    \n    print(f\"\\nPlanck function evaluated at {len(wavelengths)} wavelengths\")\n    \n    for temp in temps:\n        start = time.perf_counter()\n        spectrum = planck_function_fast(wavelengths, temp, 1.0)\n        elapsed = time.perf_counter() - start\n        peak_idx = np.argmax(spectrum)\n        print(f\"  T={temp}K: Peak at {wavelengths[peak_idx]:.0f}nm, \"\n              f\"computed in {elapsed*1000:.2f}ms\")\n\ntest_custom_ufuncs()","position":{"start":{"line":556,"column":1},"end":{"line":620,"column":1}},"key":"yPZFYaN8fq"},{"type":"heading","depth":2,"position":{"start":{"line":622,"column":1},"end":{"line":622,"column":1}},"children":[{"type":"text","value":"7.4 Multiprocessing for CPU-Bound Tasks","position":{"start":{"line":622,"column":1},"end":{"line":622,"column":1}},"key":"S0YIeVqVkG"}],"identifier":"id-7-4-multiprocessing-for-cpu-bound-tasks","label":"7.4 Multiprocessing for CPU-Bound Tasks","html_id":"id-7-4-multiprocessing-for-cpu-bound-tasks","implicit":true,"key":"S83EpczMP1"},{"type":"heading","depth":3,"position":{"start":{"line":624,"column":1},"end":{"line":624,"column":1}},"children":[{"type":"text","value":"Process Pool for Parallel Data Processing","position":{"start":{"line":624,"column":1},"end":{"line":624,"column":1}},"key":"uMpAE1hXa1"}],"identifier":"process-pool-for-parallel-data-processing","label":"Process Pool for Parallel Data Processing","html_id":"process-pool-for-parallel-data-processing","implicit":true,"key":"KiI3gTivQY"},{"type":"code","lang":"python","value":"from multiprocessing import Pool, cpu_count\nimport multiprocessing as mp\n\ndef process_single_observation(args):\n    \"\"\"\n    Process a single observation file.\n    \n    This function runs in a separate process.\n    \"\"\"\n    filename, params = args\n    \n    # Simulate loading and processing\n    np.random.seed(hash(filename) % 2**32)  # Reproducible randomness\n    \n    # \"Load\" data\n    data = np.random.randn(1024, 1024) + 1000\n    \n    # Apply calibration\n    dark = np.random.randn(1024, 1024) * 10\n    flat = np.ones((1024, 1024)) + np.random.randn(1024, 1024) * 0.1\n    \n    calibrated = (data - dark) / flat\n    \n    # Extract photometry\n    sources = []\n    threshold = np.mean(calibrated) + 5 * np.std(calibrated)\n    \n    # Find bright pixels (simplified source detection)\n    bright_pixels = np.argwhere(calibrated > threshold)\n    \n    if len(bright_pixels) > 0:\n        # Group into sources (simplified)\n        n_sources = min(10, len(bright_pixels))\n        for i in range(n_sources):\n            y, x = bright_pixels[i]\n            flux = calibrated[y, x]\n            sources.append({\n                'file': filename,\n                'x': x,\n                'y': y,\n                'flux': flux\n            })\n    \n    return sources\n\ndef parallel_pipeline(filenames, n_processes=None):\n    \"\"\"\n    Process multiple observations in parallel.\n    \"\"\"\n    if n_processes is None:\n        n_processes = cpu_count()\n    \n    print(f\"Processing {len(filenames)} files with {n_processes} processes\")\n    \n    # Prepare arguments\n    args = [(f, {'threshold': 5.0}) for f in filenames]\n    \n    # Process in parallel\n    start = time.perf_counter()\n    \n    with Pool(n_processes) as pool:\n        # map applies function to each item in parallel\n        results = pool.map(process_single_observation, args)\n    \n    elapsed = time.perf_counter() - start\n    \n    # Flatten results\n    all_sources = []\n    for source_list in results:\n        all_sources.extend(source_list)\n    \n    print(f\"Found {len(all_sources)} sources in {elapsed:.2f}s\")\n    print(f\"Processing rate: {len(filenames)/elapsed:.1f} files/second\")\n    \n    return all_sources\n\n# Test parallel processing\ndef test_multiprocessing():\n    \"\"\"Compare serial vs parallel processing.\"\"\"\n    \n    # Generate fake filenames\n    n_files = 50\n    filenames = [f\"observation_{i:04d}.fits\" for i in range(n_files)]\n    \n    # Serial processing\n    print(\"Serial processing:\")\n    start = time.perf_counter()\n    serial_results = []\n    for filename in filenames:\n        sources = process_single_observation((filename, {}))\n        serial_results.extend(sources)\n    serial_time = time.perf_counter() - start\n    print(f\"  Time: {serial_time:.2f}s\")\n    \n    # Parallel processing\n    print(\"\\nParallel processing:\")\n    parallel_results = parallel_pipeline(filenames)\n    \n    # Note: Parallel might be slower for small tasks due to overhead!\n    # But scales better for real work\n\ntest_multiprocessing()","position":{"start":{"line":626,"column":1},"end":{"line":729,"column":1}},"key":"vP1Gd5F1Ky"},{"type":"heading","depth":3,"position":{"start":{"line":731,"column":1},"end":{"line":731,"column":1}},"children":[{"type":"text","value":"Shared Memory for Large Arrays","position":{"start":{"line":731,"column":1},"end":{"line":731,"column":1}},"key":"k6IxtIUCTg"}],"identifier":"shared-memory-for-large-arrays","label":"Shared Memory for Large Arrays","html_id":"shared-memory-for-large-arrays","implicit":true,"key":"F8G525fSUM"},{"type":"code","lang":"python","value":"def shared_memory_example():\n    \"\"\"\n    Use shared memory to avoid copying large arrays between processes.\n    \"\"\"\n    from multiprocessing import shared_memory\n    \n    def process_shared_chunk(args):\n        \"\"\"Process a chunk of shared array.\"\"\"\n        shm_name, shape, dtype, start_idx, end_idx = args\n        \n        # Attach to existing shared memory\n        existing_shm = shared_memory.SharedMemory(name=shm_name)\n        \n        # Create numpy array from shared memory\n        array = np.ndarray(shape, dtype=dtype, buffer=existing_shm.buf)\n        \n        # Process the chunk (example: apply median filter)\n        from scipy.ndimage import median_filter\n        chunk = array[start_idx:end_idx]\n        filtered = median_filter(chunk, size=3)\n        \n        # Write back to shared memory\n        array[start_idx:end_idx] = filtered\n        \n        # Clean up\n        existing_shm.close()\n        \n        return f\"Processed rows {start_idx}-{end_idx}\"\n    \n    # Create large array in shared memory\n    size = (4096, 4096)\n    dtype = np.float64\n    \n    # Create shared memory\n    shm = shared_memory.SharedMemory(\n        create=True, \n        size=np.prod(size) * np.dtype(dtype).itemsize\n    )\n    \n    # Create numpy array backed by shared memory\n    shared_array = np.ndarray(size, dtype=dtype, buffer=shm.buf)\n    \n    # Initialize with data\n    shared_array[:] = np.random.randn(*size) + 1000\n    \n    print(f\"Created shared array: {shared_array.shape}, \"\n          f\"size: {shared_array.nbytes / 1e6:.1f} MB\")\n    \n    # Process in parallel without copying\n    n_processes = 4\n    chunk_size = size[0] // n_processes\n    \n    args = []\n    for i in range(n_processes):\n        start = i * chunk_size\n        end = start + chunk_size if i < n_processes - 1 else size[0]\n        args.append((shm.name, size, dtype, start, end))\n    \n    with Pool(n_processes) as pool:\n        results = pool.map(process_shared_chunk, args)\n    \n    print(\"Processing complete:\", results)\n    \n    # Clean up shared memory\n    shm.close()\n    shm.unlink()\n\n# shared_memory_example()  # Uncomment to run\nprint(\"Shared memory example ready\")","position":{"start":{"line":733,"column":1},"end":{"line":803,"column":1}},"key":"MR9uOuQgKw"},{"type":"heading","depth":2,"position":{"start":{"line":805,"column":1},"end":{"line":805,"column":1}},"children":[{"type":"text","value":"7.5 Memory Optimization","position":{"start":{"line":805,"column":1},"end":{"line":805,"column":1}},"key":"d0heJ0b4UG"}],"identifier":"id-7-5-memory-optimization","label":"7.5 Memory Optimization","html_id":"id-7-5-memory-optimization","implicit":true,"key":"XhgVRvGMGx"},{"type":"heading","depth":3,"position":{"start":{"line":807,"column":1},"end":{"line":807,"column":1}},"children":[{"type":"text","value":"Memory Profiling and Optimization","position":{"start":{"line":807,"column":1},"end":{"line":807,"column":1}},"key":"Oz7ppZSTuZ"}],"identifier":"memory-profiling-and-optimization","label":"Memory Profiling and Optimization","html_id":"memory-profiling-and-optimization","implicit":true,"key":"z11EIndEdq"},{"type":"code","lang":"python","value":"def memory_optimization_techniques():\n    \"\"\"Demonstrate memory optimization strategies.\"\"\"\n    \n    print(\"Memory Optimization Techniques:\\n\")\n    \n    # 1. Use appropriate dtypes\n    print(\"1. Choose appropriate data types:\")\n    \n    # Bad: Using float64 when not needed\n    large_array_64 = np.random.randn(10000, 10000)  # 800 MB\n    \n    # Good: Use float32 if precision allows\n    large_array_32 = np.random.randn(10000, 10000).astype(np.float32)  # 400 MB\n    \n    # Better: Use int16 for counts\n    count_array = np.random.randint(0, 1000, (10000, 10000), dtype=np.int16)  # 200 MB\n    \n    print(f\"  float64: {large_array_64.nbytes / 1e6:.1f} MB\")\n    print(f\"  float32: {large_array_32.nbytes / 1e6:.1f} MB\")\n    print(f\"  int16:   {count_array.nbytes / 1e6:.1f} MB\")\n    \n    # Clean up\n    del large_array_64, large_array_32, count_array\n    \n    # 2. Use views instead of copies\n    print(\"\\n2. Use views instead of copies:\")\n    \n    spectrum = np.random.randn(100000)\n    \n    # Bad: Creates a copy\n    blue_region_copy = spectrum[10000:20000].copy()\n    \n    # Good: Creates a view (no memory duplication)\n    blue_region_view = spectrum[10000:20000]\n    \n    print(f\"  Original: {spectrum.nbytes / 1e3:.1f} KB\")\n    print(f\"  Copy uses additional: {blue_region_copy.nbytes / 1e3:.1f} KB\")\n    print(f\"  View uses: 0 KB additional\")\n    \n    # 3. Generator expressions for large datasets\n    print(\"\\n3. Use generators for streaming data:\")\n    \n    def load_observations_generator(n_files):\n        \"\"\"Generator: loads one at a time.\"\"\"\n        for i in range(n_files):\n            # Simulate loading\n            yield np.random.randn(1024, 1024)\n    \n    def load_observations_list(n_files):\n        \"\"\"List: loads all into memory.\"\"\"\n        return [np.random.randn(1024, 1024) for i in range(n_files)]\n    \n    # Generator uses constant memory\n    gen = load_observations_generator(100)\n    print(f\"  Generator object size: {sys.getsizeof(gen)} bytes\")\n    \n    # List would use ~800 MB!\n    # observations = load_observations_list(100)  # Don't run this!\n    \n    # 4. Memory mapping for large files\n    print(\"\\n4. Memory-mapped files for large datasets:\")\n    \n    # Create a memory-mapped array\n    filename = 'large_data.dat'\n    shape = (10000, 10000)\n    \n    # Write\n    fp = np.memmap(filename, dtype='float32', mode='w+', shape=shape)\n    fp[:] = np.random.randn(*shape)\n    del fp  # Flush to disk\n    \n    # Read without loading into RAM\n    data = np.memmap(filename, dtype='float32', mode='r', shape=shape)\n    print(f\"  Memory-mapped array: {shape}, accessing without loading all\")\n    \n    # Only accessed parts are loaded\n    small_section = data[0:100, 0:100]\n    print(f\"  Accessed section: {small_section.shape}\")\n    \n    # Clean up\n    del data\n    import os\n    os.remove(filename)\n\nmemory_optimization_techniques()","position":{"start":{"line":809,"column":1},"end":{"line":895,"column":1}},"key":"DwzdPw2x9H"},{"type":"heading","depth":3,"position":{"start":{"line":897,"column":1},"end":{"line":897,"column":1}},"children":[{"type":"text","value":"Chunked Processing for Large Datasets","position":{"start":{"line":897,"column":1},"end":{"line":897,"column":1}},"key":"XBxxAsnDNl"}],"identifier":"chunked-processing-for-large-datasets","label":"Chunked Processing for Large Datasets","html_id":"chunked-processing-for-large-datasets","implicit":true,"key":"yfoUBZ6sMY"},{"type":"code","lang":"python","value":"class ChunkedProcessor:\n    \"\"\"Process large datasets in manageable chunks.\"\"\"\n    \n    def __init__(self, chunk_size=1000):\n        self.chunk_size = chunk_size\n    \n    def process_large_catalog(self, filename, n_total):\n        \"\"\"\n        Process a large catalog without loading it all.\n        \n        Simulates reading from a huge file.\n        \"\"\"\n        results = []\n        \n        for chunk_start in range(0, n_total, self.chunk_size):\n            chunk_end = min(chunk_start + self.chunk_size, n_total)\n            \n            # \"Load\" just this chunk\n            chunk_data = self._load_chunk(filename, chunk_start, chunk_end)\n            \n            # Process the chunk\n            chunk_results = self._process_chunk(chunk_data)\n            \n            # Store results (or write to disk)\n            results.extend(chunk_results)\n            \n            # Clear chunk from memory\n            del chunk_data\n            \n            if chunk_start % (self.chunk_size * 10) == 0:\n                print(f\"  Processed {chunk_start}/{n_total} objects\")\n        \n        return results\n    \n    def _load_chunk(self, filename, start, end):\n        \"\"\"Simulate loading a chunk of data.\"\"\"\n        n_objects = end - start\n        return {\n            'ra': np.random.uniform(0, 360, n_objects),\n            'dec': np.random.uniform(-90, 90, n_objects),\n            'mag': np.random.uniform(10, 20, n_objects)\n        }\n    \n    def _process_chunk(self, chunk):\n        \"\"\"Process a chunk of objects.\"\"\"\n        # Example: Select bright objects\n        mask = chunk['mag'] < 15\n        \n        bright_objects = []\n        for i in np.where(mask)[0]:\n            bright_objects.append({\n                'ra': chunk['ra'][i],\n                'dec': chunk['dec'][i],\n                'mag': chunk['mag'][i]\n            })\n        \n        return bright_objects\n    \n    def streaming_statistics(self, data_generator):\n        \"\"\"\n        Calculate statistics on streaming data without storing it all.\n        \n        Uses Welford's algorithm for numerical stability.\n        \"\"\"\n        n = 0\n        mean = 0\n        M2 = 0\n        min_val = float('inf')\n        max_val = float('-inf')\n        \n        for chunk in data_generator:\n            for value in chunk:\n                n += 1\n                delta = value - mean\n                mean += delta / n\n                delta2 = value - mean\n                M2 += delta * delta2\n                \n                min_val = min(min_val, value)\n                max_val = max(max_val, value)\n        \n        variance = M2 / (n - 1) if n > 1 else 0\n        std = np.sqrt(variance)\n        \n        return {\n            'count': n,\n            'mean': mean,\n            'std': std,\n            'min': min_val,\n            'max': max_val\n        }\n\n# Test chunked processing\nprocessor = ChunkedProcessor(chunk_size=5000)\n\nprint(\"Processing large catalog in chunks:\")\nresults = processor.process_large_catalog(\"huge_catalog.dat\", n_total=100000)\nprint(f\"Found {len(results)} bright objects\")\n\n# Test streaming statistics\ndef data_stream():\n    \"\"\"Generate data chunks.\"\"\"\n    for _ in range(100):\n        yield np.random.randn(1000) * 10 + 50\n\nprint(\"\\nCalculating statistics on streaming data:\")\nstats = processor.streaming_statistics(data_stream())\nprint(f\"Statistics: mean={stats['mean']:.2f}, std={stats['std']:.2f}\")","position":{"start":{"line":899,"column":1},"end":{"line":1008,"column":1}},"key":"FJxZkBfgZn"},{"type":"heading","depth":2,"position":{"start":{"line":1010,"column":1},"end":{"line":1010,"column":1}},"children":[{"type":"text","value":"Try It Yourself","position":{"start":{"line":1010,"column":1},"end":{"line":1010,"column":1}},"key":"KE7hj3jdi4"}],"identifier":"try-it-yourself","label":"Try It Yourself","html_id":"try-it-yourself","implicit":true,"key":"xLfotLbEmS"},{"type":"heading","depth":3,"position":{"start":{"line":1012,"column":1},"end":{"line":1012,"column":1}},"children":[{"type":"text","value":"Exercise 7.1: Optimize Light Curve Analysis","position":{"start":{"line":1012,"column":1},"end":{"line":1012,"column":1}},"key":"B7CX20DuRX"}],"identifier":"exercise-7-1-optimize-light-curve-analysis","label":"Exercise 7.1: Optimize Light Curve Analysis","html_id":"exercise-7-1-optimize-light-curve-analysis","implicit":true,"key":"MTVvdy6Ll1"},{"type":"paragraph","position":{"start":{"line":1013,"column":1},"end":{"line":1013,"column":1}},"children":[{"type":"text","value":"Profile and optimize this light curve analysis code.","position":{"start":{"line":1013,"column":1},"end":{"line":1013,"column":1}},"key":"LzEd2dQfht"}],"key":"vH9wtblHM7"},{"type":"code","lang":"python","value":"def analyze_light_curves_slow(times, fluxes, periods_to_test):\n    \"\"\"\n    Slow light curve period analysis.\n    \n    Your task:\n    1. Profile to find bottlenecks\n    2. Vectorize the period folding\n    3. Add Numba acceleration\n    4. Parallelize across multiple light curves\n    \"\"\"\n    best_periods = []\n    \n    for time, flux in zip(times, fluxes):\n        chi2_values = []\n        \n        for period in periods_to_test:\n            # Phase fold\n            phases = []\n            for t in time:\n                phase = (t % period) / period\n                phases.append(phase)\n            \n            # Sort by phase\n            sorted_indices = sorted(range(len(phases)), key=lambda i: phases[i])\n            sorted_flux = [flux[i] for i in sorted_indices]\n            \n            # Calculate chi-squared (simplified)\n            mean_flux = sum(sorted_flux) / len(sorted_flux)\n            chi2 = 0\n            for f in sorted_flux:\n                chi2 += (f - mean_flux) ** 2\n            \n            chi2_values.append(chi2)\n        \n        # Find best period (minimum chi2)\n        best_idx = chi2_values.index(min(chi2_values))\n        best_periods.append(periods_to_test[best_idx])\n    \n    return best_periods\n\n# Your optimized version:\ndef analyze_light_curves_fast(times, fluxes, periods_to_test):\n    \"\"\"Your optimized implementation.\"\"\"\n    # Your code here\n    pass\n\n# Test data\nn_curves = 100\nn_points = 1000\nn_periods = 100\n\ntimes = [np.sort(np.random.uniform(0, 100, n_points)) for _ in range(n_curves)]\nfluxes = [np.random.randn(n_points) + 100 for _ in range(n_curves)]\nperiods = np.linspace(0.5, 10, n_periods)\n\n# Compare performance\n# slow_results = analyze_light_curves_slow(times[:5], fluxes[:5], periods)\n# fast_results = analyze_light_curves_fast(times, fluxes, periods)","position":{"start":{"line":1015,"column":1},"end":{"line":1074,"column":1}},"key":"gqNGWd2Pa8"},{"type":"heading","depth":3,"position":{"start":{"line":1076,"column":1},"end":{"line":1076,"column":1}},"children":[{"type":"text","value":"Exercise 7.2: Memory-Efficient Spectrum Stack","position":{"start":{"line":1076,"column":1},"end":{"line":1076,"column":1}},"key":"CNlzOdj9ko"}],"identifier":"exercise-7-2-memory-efficient-spectrum-stack","label":"Exercise 7.2: Memory-Efficient Spectrum Stack","html_id":"exercise-7-2-memory-efficient-spectrum-stack","implicit":true,"key":"R757Ievb2r"},{"type":"paragraph","position":{"start":{"line":1077,"column":1},"end":{"line":1077,"column":1}},"children":[{"type":"text","value":"Implement memory-efficient processing of many spectra.","position":{"start":{"line":1077,"column":1},"end":{"line":1077,"column":1}},"key":"F6BfzxZAVM"}],"key":"bjEYw9A00T"},{"type":"code","lang":"python","value":"class SpectrumStack:\n    \"\"\"\n    Handle thousands of spectra efficiently.\n    \n    Requirements:\n    - Load spectra on demand, not all at once\n    - Compute median spectrum without loading all data\n    - Find outlier spectra using streaming statistics\n    - Support both in-memory and memory-mapped modes\n    \"\"\"\n    \n    def __init__(self, mode='memory-mapped'):\n        self.mode = mode\n        # Your code here\n        pass\n    \n    def add_spectrum(self, wavelengths, flux):\n        \"\"\"Add a spectrum to the stack.\"\"\"\n        # Your code here\n        pass\n    \n    def compute_median_spectrum(self):\n        \"\"\"\n        Compute median spectrum across all spectra.\n        \n        Challenge: Do this without loading all spectra at once!\n        \"\"\"\n        # Your code here\n        pass\n    \n    def find_outliers(self, threshold=5):\n        \"\"\"\n        Find spectra that deviate significantly from median.\n        \n        Use streaming algorithm to avoid loading all data.\n        \"\"\"\n        # Your code here\n        pass\n    \n    def coadd_spectra(self, weights=None):\n        \"\"\"Coadd all spectra with optional weights.\"\"\"\n        # Your code here\n        pass\n\n# Test your implementation\nstack = SpectrumStack()\n\n# Add many spectra\nfor i in range(1000):\n    wavelengths = np.linspace(4000, 7000, 3000)\n    flux = np.random.randn(3000) + 100\n    if i % 100 == 0:  # Add some outliers\n        flux += 50\n    stack.add_spectrum(wavelengths, flux)\n\n# median = stack.compute_median_spectrum()\n# outliers = stack.find_outliers()\n# print(f\"Found {len(outliers)} outlier spectra\")","position":{"start":{"line":1079,"column":1},"end":{"line":1138,"column":1}},"key":"l9EO8hmaVQ"},{"type":"heading","depth":3,"position":{"start":{"line":1140,"column":1},"end":{"line":1140,"column":1}},"children":[{"type":"text","value":"Exercise 7.3: Parallel Monte Carlo Simulation","position":{"start":{"line":1140,"column":1},"end":{"line":1140,"column":1}},"key":"b7h2wzJV2g"}],"identifier":"exercise-7-3-parallel-monte-carlo-simulation","label":"Exercise 7.3: Parallel Monte Carlo Simulation","html_id":"exercise-7-3-parallel-monte-carlo-simulation","implicit":true,"key":"e4RfUCGV3F"},{"type":"paragraph","position":{"start":{"line":1141,"column":1},"end":{"line":1141,"column":1}},"children":[{"type":"text","value":"Implement parallel Monte Carlo for stellar population synthesis.","position":{"start":{"line":1141,"column":1},"end":{"line":1141,"column":1}},"key":"u8jJE69j7B"}],"key":"A8K6SyiAD4"},{"type":"code","lang":"python","value":"def stellar_population_monte_carlo(n_stars, n_realizations, imf_params):\n    \"\"\"\n    Monte Carlo simulation of stellar populations.\n    \n    Your task:\n    1. Implement IMF sampling\n    2. Add stellar evolution tracks\n    3. Parallelize across realizations\n    4. Use Numba for the inner loops\n    5. Optimize memory usage\n    \n    Should return statistics about the population.\n    \"\"\"\n    # Your code here\n    pass\n\n# Target performance:\n# - 10,000 stars per population\n# - 1,000 realizations\n# - Complete in < 10 seconds\n\n# imf_params = {'alpha': -2.35, 'min_mass': 0.08, 'max_mass': 120}\n# results = stellar_population_monte_carlo(10000, 1000, imf_params)","position":{"start":{"line":1143,"column":1},"end":{"line":1167,"column":1}},"key":"NzBBayJ16K"},{"type":"heading","depth":2,"position":{"start":{"line":1169,"column":1},"end":{"line":1169,"column":1}},"children":[{"type":"text","value":"Key Takeaways","position":{"start":{"line":1169,"column":1},"end":{"line":1169,"column":1}},"key":"kkh1E3ybQK"}],"identifier":"key-takeaways","label":"Key Takeaways","html_id":"key-takeaways","implicit":true,"key":"k2C6iuI2Es"},{"type":"paragraph","position":{"start":{"line":1171,"column":1},"end":{"line":1178,"column":1}},"children":[{"type":"text","value":"✅ ","position":{"start":{"line":1171,"column":1},"end":{"line":1171,"column":1}},"key":"wt7AO7b4Pc"},{"type":"strong","position":{"start":{"line":1171,"column":1},"end":{"line":1171,"column":1}},"children":[{"type":"text","value":"Profile before optimizing","position":{"start":{"line":1171,"column":1},"end":{"line":1171,"column":1}},"key":"i6gtD89PwV"}],"key":"VaWVYlUOfU"},{"type":"text","value":" - Use cProfile and line_profiler to find real bottlenecks","position":{"start":{"line":1171,"column":1},"end":{"line":1171,"column":1}},"key":"oBvbZOrV1n"},{"type":"break","position":{"start":{"line":1171,"column":1},"end":{"line":1171,"column":1}},"key":"DHbAUlLLJW"},{"type":"text","value":"✅ ","position":{"start":{"line":1171,"column":1},"end":{"line":1171,"column":1}},"key":"vjFBLrs9M7"},{"type":"strong","position":{"start":{"line":1171,"column":1},"end":{"line":1171,"column":1}},"children":[{"type":"text","value":"Vectorization first","position":{"start":{"line":1171,"column":1},"end":{"line":1171,"column":1}},"key":"cg7fRo7vUD"}],"key":"vcTxgKuZeU"},{"type":"text","value":" - NumPy operations are usually fast enough","position":{"start":{"line":1171,"column":1},"end":{"line":1171,"column":1}},"key":"tn1YMPIRy5"},{"type":"break","position":{"start":{"line":1171,"column":1},"end":{"line":1171,"column":1}},"key":"Cm13Rt3rvh"},{"type":"text","value":"✅ ","position":{"start":{"line":1171,"column":1},"end":{"line":1171,"column":1}},"key":"SKlX2q56Bh"},{"type":"strong","position":{"start":{"line":1171,"column":1},"end":{"line":1171,"column":1}},"children":[{"type":"text","value":"Numba for loops you can’t vectorize","position":{"start":{"line":1171,"column":1},"end":{"line":1171,"column":1}},"key":"cNgMRNNu5r"}],"key":"IcD6G6k9Ef"},{"type":"text","value":" - Near C-speed with minimal changes","position":{"start":{"line":1171,"column":1},"end":{"line":1171,"column":1}},"key":"N62kxhrcsZ"},{"type":"break","position":{"start":{"line":1171,"column":1},"end":{"line":1171,"column":1}},"key":"EN4emVw76y"},{"type":"text","value":"✅ ","position":{"start":{"line":1171,"column":1},"end":{"line":1171,"column":1}},"key":"uOUP40xIio"},{"type":"strong","position":{"start":{"line":1171,"column":1},"end":{"line":1171,"column":1}},"children":[{"type":"text","value":"Parallelize embarrassingly parallel problems","position":{"start":{"line":1171,"column":1},"end":{"line":1171,"column":1}},"key":"syHCBtDdKV"}],"key":"cs72VtydBm"},{"type":"text","value":" - Multiple files, independent calculations","position":{"start":{"line":1171,"column":1},"end":{"line":1171,"column":1}},"key":"LBv8zoCUZ1"},{"type":"break","position":{"start":{"line":1171,"column":1},"end":{"line":1171,"column":1}},"key":"vdDxtfv8l3"},{"type":"text","value":"✅ ","position":{"start":{"line":1171,"column":1},"end":{"line":1171,"column":1}},"key":"g72wVHVACC"},{"type":"strong","position":{"start":{"line":1171,"column":1},"end":{"line":1171,"column":1}},"children":[{"type":"text","value":"Choose the right dtype","position":{"start":{"line":1171,"column":1},"end":{"line":1171,"column":1}},"key":"y2XqXYJe1W"}],"key":"V3jya228G7"},{"type":"text","value":" - float32 vs float64 can halve memory usage","position":{"start":{"line":1171,"column":1},"end":{"line":1171,"column":1}},"key":"fbZVTOJx8r"},{"type":"break","position":{"start":{"line":1171,"column":1},"end":{"line":1171,"column":1}},"key":"EIc0sSNHjb"},{"type":"text","value":"✅ ","position":{"start":{"line":1171,"column":1},"end":{"line":1171,"column":1}},"key":"F8wN3z5nUR"},{"type":"strong","position":{"start":{"line":1171,"column":1},"end":{"line":1171,"column":1}},"children":[{"type":"text","value":"Stream large datasets","position":{"start":{"line":1171,"column":1},"end":{"line":1171,"column":1}},"key":"ZMYFtCPSiV"}],"key":"tjLZYgEY4D"},{"type":"text","value":" - Process in chunks, use generators","position":{"start":{"line":1171,"column":1},"end":{"line":1171,"column":1}},"key":"DIYwoBdlmw"},{"type":"break","position":{"start":{"line":1171,"column":1},"end":{"line":1171,"column":1}},"key":"J00hTBm83P"},{"type":"text","value":"✅ ","position":{"start":{"line":1171,"column":1},"end":{"line":1171,"column":1}},"key":"n92cxZXu8V"},{"type":"strong","position":{"start":{"line":1171,"column":1},"end":{"line":1171,"column":1}},"children":[{"type":"text","value":"Memory-map huge files","position":{"start":{"line":1171,"column":1},"end":{"line":1171,"column":1}},"key":"jcBMJuI75D"}],"key":"tlQ3Bye1vP"},{"type":"text","value":" - Access TB-sized files without loading to RAM","position":{"start":{"line":1171,"column":1},"end":{"line":1171,"column":1}},"key":"BK8IpOW3lF"},{"type":"break","position":{"start":{"line":1171,"column":1},"end":{"line":1171,"column":1}},"key":"cKdDDcmxme"},{"type":"text","value":"✅ ","position":{"start":{"line":1171,"column":1},"end":{"line":1171,"column":1}},"key":"ZYW79Gsk4Q"},{"type":"strong","position":{"start":{"line":1171,"column":1},"end":{"line":1171,"column":1}},"children":[{"type":"text","value":"Know when to stop","position":{"start":{"line":1171,"column":1},"end":{"line":1171,"column":1}},"key":"wFHtk8s7fS"}],"key":"QoHO4DCqgC"},{"type":"text","value":" - “Fast enough” is often better than “fastest possible”","position":{"start":{"line":1171,"column":1},"end":{"line":1171,"column":1}},"key":"K8OvA9BVxr"}],"key":"SYZkOckadN"},{"type":"heading","depth":2,"position":{"start":{"line":1180,"column":1},"end":{"line":1180,"column":1}},"children":[{"type":"text","value":"Next Chapter Preview","position":{"start":{"line":1180,"column":1},"end":{"line":1180,"column":1}},"key":"EwjmWbZGt6"}],"identifier":"next-chapter-preview","label":"Next Chapter Preview","html_id":"next-chapter-preview","implicit":true,"key":"DyIlirxyUs"},{"type":"paragraph","position":{"start":{"line":1181,"column":1},"end":{"line":1181,"column":1}},"children":[{"type":"text","value":"Chapter 8 will provide an optional sampler of advanced Python topics including async programming, metaclasses, descriptors, and advanced decorators - choose what’s relevant for your projects!","position":{"start":{"line":1181,"column":1},"end":{"line":1181,"column":1}},"key":"rLDV0sU7ye"}],"key":"j8TyicvZD8"}],"key":"oF2HWZZjec"}],"key":"FqprE2amGy"},"references":{"cite":{"order":[],"data":{}}}}