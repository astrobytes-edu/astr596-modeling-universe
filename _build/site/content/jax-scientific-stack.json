{"version":2,"kind":"Article","sha256":"3127ec69673987107d97ccbd4b19dd4c5ba4efa6360c6daf0624d3c36ea52634","slug":"jax-scientific-stack","location":"/03-scientific-computing-with-python/05-modern-approaches-jax-ecosystem/02-jax_scientific_stack.md","dependencies":[],"frontmatter":{"title":"JAX Scientific Computing Stack: Equinox, Diffrax, and Optimistix","content_includes_title":false,"authors":[{"nameParsed":{"literal":"Anna Rosen","given":"Anna","family":"Rosen"},"name":"Anna Rosen","orcid":"0000-0003-4423-0660","email":"alrosen@sdsu.edu","affiliations":["San Diego State University"],"id":"contributors-myst-generated-uid-0","corresponding":true}],"license":{"content":{"id":"CC-BY-4.0","url":"https://creativecommons.org/licenses/by/4.0/","name":"Creative Commons Attribution 4.0 International","free":true,"CC":true},"code":{"id":"MIT","url":"https://opensource.org/licenses/MIT","name":"MIT License","free":true,"osi":true}},"github":"https://github.com/astrobytes-edu/astr596-modeling-universe","subject":"Modeling the Universe","venue":{"title":"ASTR 596 - Fall 2025","url":"https://www.anna-rosen.com"},"keywords":["computational astrophysics","python","numerical methods","machine learning","monte carlo","neural networks","radiative transfer","bayesian inference","JAX"],"affiliations":[{"id":"San Diego State University","name":"San Diego State University"}],"numbering":{"title":{"offset":2}},"edit_url":"https://github.com/astrobytes-edu/astr596-modeling-universe/blob/main/03-scientific-computing-with-python/05-modern-approaches-jax-ecosystem/02-jax_scientific_stack.md","exports":[{"format":"md","filename":"02-jax_scientific_stack.md","url":"/02-jax_scientific_st-7e29fc4c311065d61611e416e82f927f.md"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"heading","depth":2,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Learning Objectives","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"rcOO4JlDpB"}],"identifier":"learning-objectives","label":"Learning Objectives","html_id":"learning-objectives","implicit":true,"key":"p0cDTWU3q7"},{"type":"paragraph","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"By the end of this chapter, you will:","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"Lqx9aJXk6d"}],"key":"dcnUpmGsWl"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":5,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Build neural networks and scientific models with Equinox","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"ueUhO78vOy"}],"key":"Y8z1mIyhZ2"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"Solve ODEs/SDEs with Diffrax’s advanced solvers","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"RdjEpnBU1l"}],"key":"UJxWv5jnGq"},{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"Implement root-finding and optimization with Optimistix","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"jqaV2s484I"}],"key":"ftudIYtHXw"},{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"Use jaxtyping for runtime type checking","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"UgdQ3bhxxz"}],"key":"CvmqbebbG8"},{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"Combine these libraries for complex astronomical simulations","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"FSy7MQiG1H"}],"key":"upFm0yDuah"}],"key":"Vzrwy2y5xN"},{"type":"heading","depth":2,"position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"Equinox: Neural Networks as PyTrees","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"UEBAdn0YDE"}],"identifier":"equinox-neural-networks-as-pytrees","label":"Equinox: Neural Networks as PyTrees","html_id":"equinox-neural-networks-as-pytrees","implicit":true,"key":"fDSfYnpzLm"},{"type":"heading","depth":3,"position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"Introduction to Equinox","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"ObnRmOV65Z"}],"identifier":"introduction-to-equinox","label":"Introduction to Equinox","html_id":"introduction-to-equinox","implicit":true,"key":"UzKTBnoH1z"},{"type":"code","lang":"python","value":"import jax\nimport jax.numpy as jnp\nfrom jax import grad, jit, vmap, random\nimport equinox as eqx\nfrom jaxtyping import Array, Float, Int, PyTree, jaxtyped\nfrom typing import Optional\nimport matplotlib.pyplot as plt\nimport time\n\ndef equinox_fundamentals():\n    \"\"\"Learn Equinox's approach to neural networks and models.\"\"\"\n    \n    print(\"EQUINOX: NEURAL NETWORKS AS PYTREES\")\n    print(\"=\" * 50)\n    \n    # 1. Basic neural network\n    print(\"\\n1. SIMPLE NEURAL NETWORK:\")\n    \n    class StellarClassifier(eqx.Module):\n        \"\"\"Classify stellar types from spectra.\"\"\"\n        layers: list\n        dropout: eqx.nn.Dropout\n        \n        def __init__(self, input_dim: int, hidden_dim: int, output_dim: int, key):\n            keys = random.split(key, 4)\n            \n            self.layers = [\n                eqx.nn.Linear(input_dim, hidden_dim, key=keys[0]),\n                eqx.nn.Linear(hidden_dim, hidden_dim, key=keys[1]),\n                eqx.nn.Linear(hidden_dim, output_dim, key=keys[2])\n            ]\n            self.dropout = eqx.nn.Dropout(p=0.1)\n        \n        def __call__(self, x: Float[Array, \"wavelengths\"], *, key: Optional[random.PRNGKey] = None) -> Float[Array, \"classes\"]:\n            # First hidden layer\n            x = self.layers[0](x)\n            x = jax.nn.relu(x)\n            \n            # Dropout during training\n            if key is not None:\n                x = self.dropout(x, key=key)\n            \n            # Second hidden layer\n            x = self.layers[1](x)\n            x = jax.nn.relu(x)\n            \n            # Output layer\n            x = self.layers[2](x)\n            return jax.nn.log_softmax(x)\n    \n    # Initialize model\n    key = random.PRNGKey(0)\n    model = StellarClassifier(input_dim=1000, hidden_dim=128, output_dim=7, key=key)\n    \n    # Test forward pass\n    spectrum = random.normal(key, (1000,))\n    logits = model(spectrum)\n    print(f\"  Input shape: {spectrum.shape}\")\n    print(f\"  Output shape: {logits.shape}\")\n    print(f\"  Predicted class: {jnp.argmax(logits)}\")\n    \n    # 2. Model manipulation as PyTrees\n    print(\"\\n2. PYTREE OPERATIONS:\")\n    \n    # Get model parameters\n    params, static = eqx.partition(model, eqx.is_array)\n    \n    # Count parameters\n    num_params = sum(x.size for x in jax.tree_util.tree_leaves(params))\n    print(f\"  Total parameters: {num_params:,}\")\n    \n    # Modify parameters\n    def add_noise(params, key, noise_scale=0.01):\n        \"\"\"Add noise to parameters.\"\"\"\n        leaves, treedef = jax.tree_util.tree_flatten(params)\n        keys = random.split(key, len(leaves))\n        \n        noisy_leaves = [\n            leaf + noise_scale * random.normal(k, leaf.shape)\n            for leaf, k in zip(leaves, keys)\n        ]\n        \n        return jax.tree_util.tree_unflatten(treedef, noisy_leaves)\n    \n    noisy_params = add_noise(params, key)\n    noisy_model = eqx.combine(noisy_params, static)\n    \n    # 3. Advanced model with custom layers\n    print(\"\\n3. CUSTOM LAYERS AND MODULES:\")\n    \n    class SpectralConvolution(eqx.Module):\n        \"\"\"1D convolution for spectral features.\"\"\"\n        weight: Float[Array, \"out_channels in_channels kernel_size\"]\n        bias: Optional[Float[Array, \"out_channels\"]]\n        \n        def __init__(self, in_channels, out_channels, kernel_size, *, key, use_bias=True):\n            wkey, bkey = random.split(key)\n            \n            # He initialization\n            lim = jnp.sqrt(2.0 / (in_channels * kernel_size))\n            self.weight = random.uniform(\n                wkey, \n                (out_channels, in_channels, kernel_size),\n                minval=-lim, maxval=lim\n            )\n            \n            if use_bias:\n                self.bias = jnp.zeros((out_channels,))\n            else:\n                self.bias = None\n        \n        def __call__(self, x: Float[Array, \"batch channels length\"]) -> Float[Array, \"batch out_channels new_length\"]:\n            # Apply convolution\n            out = jax.lax.conv_general_dilated(\n                x, self.weight, \n                window_strides=(1,),\n                padding='SAME'\n            )\n            \n            if self.bias is not None:\n                out = out + self.bias[None, :, None]\n            \n            return out\n    \n    # Use custom layer\n    conv_layer = SpectralConvolution(1, 16, kernel_size=5, key=key)\n    test_input = random.normal(key, (32, 1, 100))  # batch, channels, length\n    output = conv_layer(test_input)\n    print(f\"  Conv input: {test_input.shape}\")\n    print(f\"  Conv output: {output.shape}\")\n    \n    # 4. Filtering and freezing\n    print(\"\\n4. FILTERING AND FREEZING PARAMETERS:\")\n    \n    # Filter specific layers\n    def get_linear_params(model):\n        \"\"\"Extract only Linear layer parameters.\"\"\"\n        return eqx.filter(model, lambda x: isinstance(x, eqx.nn.Linear))\n    \n    linear_params = get_linear_params(model)\n    \n    # Freeze layers\n    @eqx.filter_jit\n    def forward_with_frozen_first_layer(model, x):\n        \"\"\"Forward pass with first layer frozen.\"\"\"\n        # Partition into first layer and rest\n        first_layer = model.layers[0]\n        other_layers = model.layers[1:]\n        \n        # Stop gradient on first layer\n        x = jax.lax.stop_gradient(first_layer(x))\n        x = jax.nn.relu(x)\n        \n        # Continue with gradients\n        for layer in other_layers[:-1]:\n            x = layer(x)\n            x = jax.nn.relu(x)\n        \n        x = other_layers[-1](x)\n        return x\n    \n    print(\"  Filtered and frozen layer operations configured\")\n\nequinox_fundamentals()","position":{"start":{"line":15,"column":1},"end":{"line":180,"column":1}},"key":"tlvY7x1xOE"},{"type":"heading","depth":3,"position":{"start":{"line":182,"column":1},"end":{"line":182,"column":1}},"children":[{"type":"text","value":"Training with Equinox","position":{"start":{"line":182,"column":1},"end":{"line":182,"column":1}},"key":"ws7uECUsQu"}],"identifier":"training-with-equinox","label":"Training with Equinox","html_id":"training-with-equinox","implicit":true,"key":"EQ74gAcDpJ"},{"type":"code","lang":"python","value":"def equinox_training():\n    \"\"\"Training loops and optimization with Equinox.\"\"\"\n    \n    print(\"\\nTRAINING WITH EQUINOX\")\n    print(\"=\" * 50)\n    \n    import optax\n    \n    # 1. Define model for photometric redshift\n    print(\"\\n1. PHOTOMETRIC REDSHIFT MODEL:\")\n    \n    class PhotoZNet(eqx.Module):\n        \"\"\"Estimate redshift from photometry.\"\"\"\n        encoder: eqx.nn.Sequential\n        decoder: eqx.nn.Sequential\n        \n        def __init__(self, n_bands: int, key):\n            key1, key2 = random.split(key)\n            \n            self.encoder = eqx.nn.Sequential([\n                eqx.nn.Linear(n_bands, 64, key=key1),\n                eqx.nn.Lambda(jax.nn.relu),\n                eqx.nn.Linear(64, 32, key=key2),\n                eqx.nn.Lambda(jax.nn.relu),\n            ])\n            \n            self.decoder = eqx.nn.Sequential([\n                eqx.nn.Linear(32, 16, key=key1),\n                eqx.nn.Lambda(jax.nn.relu),\n                eqx.nn.Linear(16, 1, key=key2),  # Single redshift value\n            ])\n        \n        def __call__(self, x: Float[Array, \"n_bands\"]) -> Float[Array, \"1\"]:\n            features = self.encoder(x)\n            return self.decoder(features).squeeze()\n    \n    # Initialize\n    key = random.PRNGKey(42)\n    model = PhotoZNet(n_bands=5, key=key)\n    \n    # 2. Loss functions\n    print(\"\\n2. LOSS FUNCTIONS:\")\n    \n    @eqx.filter_jit\n    def loss_fn(model, x, y_true):\n        \"\"\"MSE loss with outlier robustness.\"\"\"\n        y_pred = vmap(model)(x)\n        \n        # Huber loss for robustness\n        delta = 0.1\n        residual = jnp.abs(y_pred - y_true)\n        \n        loss = jnp.where(\n            residual < delta,\n            0.5 * residual ** 2,\n            delta * residual - 0.5 * delta ** 2\n        )\n        \n        return jnp.mean(loss)\n    \n    # Generate synthetic data\n    key, subkey = random.split(key)\n    n_samples = 1000\n    X_train = random.normal(subkey, (n_samples, 5))\n    z_true = random.uniform(subkey, (n_samples,), minval=0, maxval=3)\n    \n    # 3. Training loop\n    print(\"\\n3. TRAINING LOOP:\")\n    \n    # Optimizer\n    optimizer = optax.adam(learning_rate=1e-3)\n    opt_state = optimizer.init(eqx.filter(model, eqx.is_array))\n    \n    @eqx.filter_jit\n    def train_step(model, opt_state, x_batch, y_batch):\n        \"\"\"Single training step.\"\"\"\n        # Compute loss and gradients\n        loss, grads = eqx.filter_value_and_grad(loss_fn)(model, x_batch, y_batch)\n        \n        # Update parameters\n        updates, opt_state = optimizer.update(grads, opt_state)\n        model = eqx.apply_updates(model, updates)\n        \n        return model, opt_state, loss\n    \n    # Training\n    batch_size = 32\n    n_epochs = 10\n    \n    for epoch in range(n_epochs):\n        # Shuffle data\n        key, subkey = random.split(key)\n        perm = random.permutation(subkey, n_samples)\n        X_shuffled = X_train[perm]\n        z_shuffled = z_true[perm]\n        \n        # Mini-batches\n        epoch_loss = 0.0\n        n_batches = n_samples // batch_size\n        \n        for i in range(n_batches):\n            start = i * batch_size\n            end = start + batch_size\n            \n            x_batch = X_shuffled[start:end]\n            y_batch = z_shuffled[start:end]\n            \n            model, opt_state, loss = train_step(model, opt_state, x_batch, y_batch)\n            epoch_loss += loss\n        \n        if epoch % 2 == 0:\n            print(f\"  Epoch {epoch}: Loss = {epoch_loss/n_batches:.4f}\")\n    \n    # 4. Model evaluation\n    print(\"\\n4. MODEL EVALUATION:\")\n    \n    @eqx.filter_jit\n    def evaluate(model, x, y_true):\n        \"\"\"Evaluate model performance.\"\"\"\n        y_pred = vmap(model)(x)\n        \n        # Metrics\n        mse = jnp.mean((y_pred - y_true) ** 2)\n        mae = jnp.mean(jnp.abs(y_pred - y_true))\n        \n        # Outlier fraction (|Δz| > 0.15)\n        outliers = jnp.sum(jnp.abs(y_pred - y_true) > 0.15) / len(y_true)\n        \n        return {'mse': mse, 'mae': mae, 'outlier_frac': outliers}\n    \n    # Test set\n    X_test = random.normal(random.PRNGKey(123), (200, 5))\n    z_test = random.uniform(random.PRNGKey(124), (200,), minval=0, maxval=3)\n    \n    metrics = evaluate(model, X_test, z_test)\n    print(f\"  Test MSE: {metrics['mse']:.4f}\")\n    print(f\"  Test MAE: {metrics['mae']:.4f}\")\n    print(f\"  Outlier fraction: {metrics['outlier_frac']:.2%}\")\n\nequinox_training()","position":{"start":{"line":184,"column":1},"end":{"line":325,"column":1}},"key":"AXZ5bE6SDk"},{"type":"heading","depth":2,"position":{"start":{"line":327,"column":1},"end":{"line":327,"column":1}},"children":[{"type":"text","value":"Diffrax: Advanced ODE/SDE Solvers","position":{"start":{"line":327,"column":1},"end":{"line":327,"column":1}},"key":"reEV5ne3lr"}],"identifier":"diffrax-advanced-ode-sde-solvers","label":"Diffrax: Advanced ODE/SDE Solvers","html_id":"diffrax-advanced-ode-sde-solvers","implicit":true,"key":"hK0aRcYWfS"},{"type":"heading","depth":3,"position":{"start":{"line":329,"column":1},"end":{"line":329,"column":1}},"children":[{"type":"text","value":"Solving ODEs with Diffrax","position":{"start":{"line":329,"column":1},"end":{"line":329,"column":1}},"key":"JgLuIrtWCl"}],"identifier":"solving-odes-with-diffrax","label":"Solving ODEs with Diffrax","html_id":"solving-odes-with-diffrax","implicit":true,"key":"o7buajr46J"},{"type":"code","lang":"python","value":"import diffrax\n\ndef diffrax_ode_solvers():\n    \"\"\"Advanced ODE solving for astronomical systems.\"\"\"\n    \n    print(\"\\nDIFFRAX: ADVANCED ODE SOLVERS\")\n    print(\"=\" * 50)\n    \n    # 1. Basic ODE: Binary star system\n    print(\"\\n1. BINARY STAR SYSTEM:\")\n    \n    def binary_system(t, y, args):\n        \"\"\"Binary star dynamics with tidal effects.\"\"\"\n        # y = [r1, v1, r2, v2] (6D each)\n        r1, v1, r2, v2 = y[0:3], y[3:6], y[6:9], y[9:12]\n        m1, m2 = args['m1'], args['m2']\n        \n        # Gravitational forces\n        r = r2 - r1\n        r_norm = jnp.linalg.norm(r)\n        f_grav = r / (r_norm**3 + 1e-10)\n        \n        # Tidal forces (simplified)\n        a1 = m2 * f_grav\n        a2 = -m1 * f_grav\n        \n        return jnp.concatenate([v1, a1, v2, a2])\n    \n    # Initial conditions\n    y0 = jnp.array([\n        1.0, 0.0, 0.0,    # r1\n        0.0, 0.3, 0.0,    # v1\n        -1.0, 0.0, 0.0,   # r2\n        0.0, -0.3, 0.0    # v2\n    ])\n    \n    # Solve with different methods\n    args = {'m1': 1.0, 'm2': 0.8}\n    t0, t1 = 0.0, 100.0\n    dt0 = 0.01\n    \n    # Dopri5 (adaptive RK4/5)\n    solver = diffrax.Dopri5()\n    term = diffrax.ODETerm(binary_system)\n    saveat = diffrax.SaveAt(ts=jnp.linspace(t0, t1, 1000))\n    \n    sol = diffrax.diffeqsolve(\n        term, solver, t0, t1, dt0, y0,\n        args=args, saveat=saveat,\n        stepsize_controller=diffrax.PIDController(rtol=1e-8, atol=1e-10)\n    )\n    \n    print(f\"  Solved with {sol.stats['num_steps']} steps\")\n    print(f\"  Final positions: r1={sol.ys[-1, 0:3]}, r2={sol.ys[-1, 6:9]}\")\n    \n    # 2. Stiff ODEs: Chemical evolution\n    print(\"\\n2. STIFF SYSTEM - CHEMICAL NETWORK:\")\n    \n    def chemical_network(t, y, args):\n        \"\"\"Simplified CNO cycle in stellar interior.\"\"\"\n        # y = [C12, N14, O16]\n        C12, N14, O16 = y\n        T = args['temperature']  # in 10^7 K\n        \n        # Reaction rates (simplified)\n        k1 = 1e-2 * jnp.exp(-15.0 / T)  # C12 + p -> N14\n        k2 = 1e-3 * jnp.exp(-20.0 / T)  # N14 + p -> O16\n        k3 = 1e-4 * jnp.exp(-25.0 / T)  # O16 + p -> C12\n        \n        dC12_dt = -k1 * C12 + k3 * O16\n        dN14_dt = k1 * C12 - k2 * N14\n        dO16_dt = k2 * N14 - k3 * O16\n        \n        return jnp.array([dC12_dt, dN14_dt, dO16_dt])\n    \n    # Use implicit solver for stiff system\n    y0_chem = jnp.array([1.0, 0.0, 0.0])  # Start with pure C12\n    \n    solver_stiff = diffrax.Kvaerno5()  # Implicit solver\n    term_stiff = diffrax.ODETerm(chemical_network)\n    \n    sol_stiff = diffrax.diffeqsolve(\n        term_stiff, solver_stiff, 0.0, 1e10, 1e6, y0_chem,\n        args={'temperature': 2.0},  # 20 million K\n        max_steps=10000\n    )\n    \n    print(f\"  Final abundances: C12={sol_stiff.ys[-1, 0]:.3f}, \" +\n          f\"N14={sol_stiff.ys[-1, 1]:.3f}, O16={sol_stiff.ys[-1, 2]:.3f}\")\n    \n    # 3. Event detection\n    print(\"\\n3. EVENT DETECTION - PERICENTER PASSAGE:\")\n    \n    def detect_pericenter(state, **kwargs):\n        \"\"\"Detect when binary reaches pericenter.\"\"\"\n        r1, v1, r2, v2 = state[0:3], state[3:6], state[6:9], state[9:12]\n        separation = jnp.linalg.norm(r2 - r1)\n        return separation - 0.5  # Trigger when separation < 0.5\n    \n    event = diffrax.DiscreteTerminatingEvent(detect_pericenter)\n    \n    sol_event = diffrax.diffeqsolve(\n        term, solver, t0, t1, dt0, y0,\n        args=args,\n        discrete_terminating_event=event\n    )\n    \n    if sol_event.event_mask:\n        print(f\"  Pericenter reached at t={sol_event.ts[-1]:.2f}\")\n    \n    # 4. Sensitivity analysis\n    print(\"\\n4. SENSITIVITY ANALYSIS:\")\n    \n    def binary_with_sensitivity(t, y, args):\n        \"\"\"Binary system with parameter sensitivity.\"\"\"\n        return binary_system(t, y, args)\n    \n    # Gradient with respect to initial conditions\n    @jit\n    def trajectory_loss(y0, args):\n        \"\"\"Loss based on final state.\"\"\"\n        sol = diffrax.diffeqsolve(\n            term, solver, t0, 10.0, dt0, y0,\n            args=args\n        )\n        return jnp.sum(sol.ys[-1] ** 2)\n    \n    grad_fn = grad(trajectory_loss)\n    sensitivity = grad_fn(y0, args)\n    print(f\"  Sensitivity of final state to initial conditions:\")\n    print(f\"  Max gradient: {jnp.max(jnp.abs(sensitivity)):.3e}\")\n\ndiffrax_ode_solvers()","position":{"start":{"line":331,"column":1},"end":{"line":465,"column":1}},"key":"VSG5LBribu"},{"type":"heading","depth":3,"position":{"start":{"line":467,"column":1},"end":{"line":467,"column":1}},"children":[{"type":"text","value":"Stochastic Differential Equations","position":{"start":{"line":467,"column":1},"end":{"line":467,"column":1}},"key":"qLVvRYVksK"}],"identifier":"stochastic-differential-equations","label":"Stochastic Differential Equations","html_id":"stochastic-differential-equations","implicit":true,"key":"t9uZw91PpL"},{"type":"code","lang":"python","value":"def diffrax_sde_solvers():\n    \"\"\"Solve SDEs for stochastic astronomical processes.\"\"\"\n    \n    print(\"\\nSTOCHASTIC DIFFERENTIAL EQUATIONS\")\n    print(\"=\" * 50)\n    \n    # 1. Brownian motion in globular cluster\n    print(\"\\n1. STELLAR BROWNIAN MOTION:\")\n    \n    def drift(t, y, args):\n        \"\"\"Drift term: gravitational force.\"\"\"\n        # Simplified central potential\n        r = jnp.linalg.norm(y[:3])\n        force = -y[:3] / (r**3 + 0.1)\n        return jnp.concatenate([y[3:6], force])\n    \n    def diffusion(t, y, args):\n        \"\"\"Diffusion term: random kicks from encounters.\"\"\"\n        sigma = args['sigma']\n        # Only velocity gets random kicks\n        return jnp.concatenate([\n            jnp.zeros(3),\n            jnp.eye(3) * sigma\n        ])\n    \n    # Setup SDE\n    key = random.PRNGKey(0)\n    \n    brownian_motion = diffrax.VirtualBrownianTree(\n        t0=0.0, t1=100.0, tol=1e-3, shape=(3,), key=key\n    )\n    \n    drift_term = diffrax.ODETerm(drift)\n    diffusion_term = diffrax.ControlTerm(diffusion, brownian_motion)\n    terms = diffrax.MultiTerm(drift_term, diffusion_term)\n    \n    solver_sde = diffrax.Euler()  # Euler-Maruyama for SDEs\n    \n    y0_sde = jnp.array([1.0, 0.0, 0.0, 0.0, 1.0, 0.0])  # position, velocity\n    \n    # Solve SDE\n    sol_sde = diffrax.diffeqsolve(\n        terms, solver_sde, 0.0, 100.0, 0.01, y0_sde,\n        args={'sigma': 0.01},\n        saveat=diffrax.SaveAt(ts=jnp.linspace(0, 100, 1000))\n    )\n    \n    positions = sol_sde.ys[:, :3]\n    print(f\"  Final position: {positions[-1]}\")\n    print(f\"  RMS displacement: {jnp.sqrt(jnp.mean(jnp.sum(positions**2, axis=1))):.3f}\")\n    \n    # 2. Stochastic accretion\n    print(\"\\n2. STOCHASTIC ACCRETION DISK:\")\n    \n    def accretion_drift(t, y, args):\n        \"\"\"Mean accretion rate.\"\"\"\n        M, mdot = y\n        alpha = args['alpha']  # Viscosity parameter\n        \n        # Simplified accretion model\n        dM_dt = mdot\n        dmdot_dt = -alpha * mdot  # Decay of accretion\n        \n        return jnp.array([dM_dt, dmdot_dt])\n    \n    def accretion_noise(t, y, args):\n        \"\"\"Turbulent fluctuations.\"\"\"\n        M, mdot = y\n        beta = args['beta']\n        \n        # Noise proportional to accretion rate\n        return jnp.array([[0.0], [beta * jnp.sqrt(jnp.abs(mdot))]])\n    \n    # Setup\n    key, subkey = random.split(key)\n    brownian_1d = diffrax.VirtualBrownianTree(\n        t0=0.0, t1=10.0, tol=1e-3, shape=(1,), key=subkey\n    )\n    \n    drift_acc = diffrax.ODETerm(accretion_drift)\n    diffusion_acc = diffrax.ControlTerm(accretion_noise, brownian_1d)\n    terms_acc = diffrax.MultiTerm(drift_acc, diffusion_acc)\n    \n    y0_acc = jnp.array([1.0, 0.1])  # Initial mass and accretion rate\n    \n    sol_acc = diffrax.diffeqsolve(\n        terms_acc, solver_sde, 0.0, 10.0, 0.001, y0_acc,\n        args={'alpha': 0.1, 'beta': 0.05}\n    )\n    \n    print(f\"  Final mass: {sol_acc.ys[-1, 0]:.3f}\")\n    print(f\"  Final accretion rate: {sol_acc.ys[-1, 1]:.4f}\")\n    \n    # 3. Jump diffusion for flares\n    print(\"\\n3. JUMP DIFFUSION - STELLAR FLARES:\")\n    \n    class FlareProcess(eqx.Module):\n        \"\"\"Jump process for stellar flares.\"\"\"\n        rate: float = 0.1  # Flare rate\n        \n        def __call__(self, t, y, args, key):\n            \"\"\"Generate jump if flare occurs.\"\"\"\n            subkey1, subkey2 = random.split(key)\n            \n            # Poisson process for flare occurrence\n            occurs = random.uniform(subkey1) < self.rate * 0.01  # dt = 0.01\n            \n            # Flare amplitude (log-normal distribution)\n            amplitude = jnp.where(\n                occurs,\n                jnp.exp(random.normal(subkey2) * 0.5),\n                0.0\n            )\n            \n            return jnp.array([amplitude])\n    \n    # Combined drift-diffusion-jump process\n    def luminosity_drift(t, y, args):\n        \"\"\"Decay of luminosity.\"\"\"\n        return -0.1 * y  # Exponential decay\n    \n    # Solve with jumps (simplified - Diffrax doesn't have built-in jump diffusion)\n    # This would require custom implementation\n    \n    print(\"  Jump diffusion setup complete (requires custom implementation)\")\n\ndiffrax_sde_solvers()","position":{"start":{"line":469,"column":1},"end":{"line":597,"column":1}},"key":"gkvOnELBDN"},{"type":"heading","depth":2,"position":{"start":{"line":599,"column":1},"end":{"line":599,"column":1}},"children":[{"type":"text","value":"Optimistix: Root Finding and Optimization","position":{"start":{"line":599,"column":1},"end":{"line":599,"column":1}},"key":"XMdIDPZfqh"}],"identifier":"optimistix-root-finding-and-optimization","label":"Optimistix: Root Finding and Optimization","html_id":"optimistix-root-finding-and-optimization","implicit":true,"key":"Yg0QHiszDL"},{"type":"heading","depth":3,"position":{"start":{"line":601,"column":1},"end":{"line":601,"column":1}},"children":[{"type":"text","value":"Nonlinear Solvers","position":{"start":{"line":601,"column":1},"end":{"line":601,"column":1}},"key":"NwvMl8frMD"}],"identifier":"nonlinear-solvers","label":"Nonlinear Solvers","html_id":"nonlinear-solvers","implicit":true,"key":"osfVY11ra0"},{"type":"code","lang":"python","value":"import optimistix as optx\n\ndef optimistix_solvers():\n    \"\"\"Root finding and optimization for astronomical problems.\"\"\"\n    \n    print(\"\\nOPTIMISTIX: ROOT FINDING & OPTIMIZATION\")\n    print(\"=\" * 50)\n    \n    # 1. Root finding: Kepler's equation\n    print(\"\\n1. KEPLER'S EQUATION:\")\n    \n    @jit\n    def kepler_equation(E, M_and_e):\n        \"\"\"Kepler's equation: M = E - e*sin(E)\"\"\"\n        M, e = M_and_e\n        return E - e * jnp.sin(E) - M\n    \n    # Solve for different mean anomalies\n    M_values = jnp.linspace(0, 2*jnp.pi, 10)\n    e = 0.5  # Eccentricity\n    \n    E_solutions = []\n    for M in M_values:\n        solver = optx.Newton(rtol=1e-8, atol=1e-10)\n        sol = optx.root_find(\n            lambda E: kepler_equation(E, (M, e)),\n            solver,\n            M,  # Initial guess\n            max_steps=100\n        )\n        E_solutions.append(sol.value)\n    \n    E_solutions = jnp.array(E_solutions)\n    print(f\"  Solved {len(E_solutions)} values\")\n    print(f\"  Max residual: {max(abs(kepler_equation(E, (M, e))) for E, M in zip(E_solutions, M_values)):.2e}\")\n    \n    # 2. Least squares: Orbit fitting\n    print(\"\\n2. ORBIT FITTING WITH LEAST SQUARES:\")\n    \n    def orbit_residuals(params, data):\n        \"\"\"Residuals for orbit fitting.\"\"\"\n        a, e, i, omega, Omega, T0 = params\n        times, ra, dec = data\n        \n        # Compute predicted positions (simplified)\n        n = 2 * jnp.pi / (a ** 1.5)  # Mean motion\n        M = n * (times - T0)\n        \n        # Would solve Kepler's equation here for each M\n        # For simplicity, use small eccentricity approximation\n        E = M + e * jnp.sin(M)\n        \n        # True anomaly (simplified)\n        f = E + 2 * e * jnp.sin(E)\n        \n        # Predicted RA/Dec (very simplified!)\n        ra_pred = a * jnp.cos(f + omega)\n        dec_pred = a * jnp.sin(f + omega) * jnp.sin(i)\n        \n        return jnp.concatenate([ra - ra_pred, dec - dec_pred])\n    \n    # Synthetic observations\n    key = random.PRNGKey(42)\n    n_obs = 20\n    times = jnp.linspace(0, 10, n_obs)\n    true_params = jnp.array([1.0, 0.3, 0.5, 0.2, 0.1, 0.0])\n    \n    # Generate noisy observations\n    noise_key = random.split(key, 2)\n    ra_obs = true_params[0] * jnp.cos(2*jnp.pi/true_params[0]**1.5 * times) + \\\n              0.01 * random.normal(noise_key[0], (n_obs,))\n    dec_obs = true_params[0] * jnp.sin(2*jnp.pi/true_params[0]**1.5 * times) * 0.5 + \\\n               0.01 * random.normal(noise_key[1], (n_obs,))\n    \n    data = (times, ra_obs, dec_obs)\n    \n    # Least squares solver\n    solver_ls = optx.LevenbergMarquardt(rtol=1e-6, atol=1e-8)\n    initial_guess = jnp.array([1.1, 0.2, 0.4, 0.3, 0.2, 0.1])\n    \n    sol_ls = optx.least_squares(\n        lambda p: orbit_residuals(p, data),\n        solver_ls,\n        initial_guess,\n        max_steps=100\n    )\n    \n    print(f\"  Converged in {sol_ls.stats['num_steps']} steps\")\n    print(f\"  True params: {true_params}\")\n    print(f\"  Fitted params: {sol_ls.value}\")\n    \n    # 3. Minimization: Maximum likelihood\n    print(\"\\n3. MAXIMUM LIKELIHOOD OPTIMIZATION:\")\n    \n    def negative_log_likelihood(params, data):\n        \"\"\"Negative log likelihood for power law fit.\"\"\"\n        alpha, x_min = params\n        x_data = data\n        \n        # Power law likelihood\n        if alpha <= 1.0 or x_min <= 0:\n            return jnp.inf\n        \n        norm = (alpha - 1) / x_min * (x_min / jnp.maximum(x_data, x_min)) ** alpha\n        log_like = jnp.sum(jnp.log(norm))\n        \n        return -log_like\n    \n    # Generate power law distributed data (e.g., mass function)\n    key, subkey = random.split(key)\n    alpha_true = 2.35  # Salpeter IMF\n    x_min_true = 0.1\n    n_stars = 1000\n    \n    # Inverse transform sampling\n    u = random.uniform(subkey, (n_stars,))\n    masses = x_min_true * (1 - u) ** (-1/(alpha_true - 1))\n    \n    # Optimize\n    solver_opt = optx.BFGS(rtol=1e-6, atol=1e-8)\n    initial = jnp.array([2.0, 0.15])\n    \n    sol_opt = optx.minimise(\n        lambda p: negative_log_likelihood(p, masses),\n        solver_opt,\n        initial,\n        max_steps=100\n    )\n    \n    print(f\"  True parameters: α={alpha_true}, x_min={x_min_true}\")\n    print(f\"  MLE estimates: α={sol_opt.value[0]:.3f}, x_min={sol_opt.value[1]:.3f}\")\n    \n    # 4. Fixed point iteration\n    print(\"\\n4. FIXED POINT - STELLAR STRUCTURE:\")\n    \n    def stellar_structure_iteration(y, args):\n        \"\"\"\n        Fixed point iteration for stellar structure.\n        Simplified Lane-Emden equation.\n        \"\"\"\n        rho, T = y\n        gamma = args['gamma']\n        \n        # Hydrostatic equilibrium + energy transport\n        rho_new = T ** (1/(gamma - 1))\n        T_new = rho_new ** (gamma - 1)\n        \n        # Add some nonlinearity\n        rho_new = 0.9 * rho_new + 0.1 * rho\n        T_new = 0.9 * T_new + 0.1 * T\n        \n        return jnp.array([rho_new, T_new])\n    \n    solver_fp = optx.FixedPointIteration(rtol=1e-6, atol=1e-8)\n    initial_structure = jnp.array([1.0, 1.0])\n    \n    sol_fp = optx.fixed_point(\n        lambda y: stellar_structure_iteration(y, {'gamma': 5/3}),\n        solver_fp,\n        initial_structure,\n        max_steps=100\n    )\n    \n    print(f\"  Converged to: ρ={sol_fp.value[0]:.3f}, T={sol_fp.value[1]:.3f}\")\n\noptimistix_solvers()","position":{"start":{"line":603,"column":1},"end":{"line":770,"column":1}},"key":"OrXDNi2c8X"},{"type":"heading","depth":2,"position":{"start":{"line":772,"column":1},"end":{"line":772,"column":1}},"children":[{"type":"text","value":"Lineax: Linear Algebra","position":{"start":{"line":772,"column":1},"end":{"line":772,"column":1}},"key":"hHRlJF4rSM"}],"identifier":"lineax-linear-algebra","label":"Lineax: Linear Algebra","html_id":"lineax-linear-algebra","implicit":true,"key":"rayli8Kiue"},{"type":"heading","depth":3,"position":{"start":{"line":774,"column":1},"end":{"line":774,"column":1}},"children":[{"type":"text","value":"Linear System Solvers","position":{"start":{"line":774,"column":1},"end":{"line":774,"column":1}},"key":"iCafWGytRp"}],"identifier":"linear-system-solvers","label":"Linear System Solvers","html_id":"linear-system-solvers","implicit":true,"key":"rnslC6fxvx"},{"type":"code","lang":"python","value":"import lineax as lx\n\ndef lineax_solvers():\n    \"\"\"Advanced linear algebra for astronomical applications.\"\"\"\n    \n    print(\"\\nLINEAX: LINEAR ALGEBRA SOLVERS\")\n    print(\"=\" * 50)\n    \n    # 1. Basic linear solve\n    print(\"\\n1. BASIC LINEAR SYSTEM:\")\n    \n    # Poisson equation for gravitational potential\n    def create_poisson_system(n):\n        \"\"\"Create discrete Poisson equation.\"\"\"\n        # -∇²φ = 4πGρ\n        # Finite difference discretization\n        h = 1.0 / n\n        \n        # Tridiagonal matrix (1D simplification)\n        A = (\n            -2 * jnp.eye(n) +\n            jnp.eye(n, k=1) +\n            jnp.eye(n, k=-1)\n        ) / h**2\n        \n        # Random density distribution\n        key = random.PRNGKey(0)\n        rho = random.uniform(key, (n,))\n        b = 4 * jnp.pi * rho\n        \n        return A, b\n    \n    A, b = create_poisson_system(100)\n    \n    # Direct solve\n    solver = lx.LU()\n    sol = lx.linear_solve(A, b, solver)\n    \n    print(f\"  Solved {A.shape[0]}x{A.shape[0]} system\")\n    print(f\"  Residual norm: {jnp.linalg.norm(A @ sol.value - b):.2e}\")\n    \n    # 2. Iterative solvers for large systems\n    print(\"\\n2. ITERATIVE SOLVERS:\")\n    \n    # Large sparse system (PSF deconvolution)\n    def psf_convolution_operator(x, psf_kernel):\n        \"\"\"Apply PSF convolution.\"\"\"\n        # Simplified: just blur with kernel\n        return jax.scipy.signal.convolve(x, psf_kernel, mode='same')\n    \n    # Create blurred image problem\n    n_pixels = 1000\n    key = random.PRNGKey(1)\n    \n    # PSF kernel (Gaussian)\n    x_kernel = jnp.arange(-5, 6)\n    psf_kernel = jnp.exp(-x_kernel**2 / 2) / jnp.sqrt(2 * jnp.pi)\n    \n    # True image (point sources)\n    true_image = jnp.zeros(n_pixels)\n    source_positions = random.choice(key, n_pixels, (10,), replace=False)\n    true_image = true_image.at[source_positions].set(1.0)\n    \n    # Blurred observation\n    blurred = psf_convolution_operator(true_image, psf_kernel)\n    noise = 0.01 * random.normal(key, (n_pixels,))\n    observed = blurred + noise\n    \n    # Define linear operator\n    psf_op = lx.FunctionLinearOperator(\n        lambda x: psf_convolution_operator(x, psf_kernel),\n        observed.shape\n    )\n    \n    # Conjugate gradient solver\n    solver_cg = lx.CG(rtol=1e-5, atol=1e-7)\n    sol_cg = lx.linear_solve(psf_op, observed, solver_cg)\n    \n    print(f\"  Deconvolved image with CG\")\n    print(f\"  Iterations: {sol_cg.stats['num_steps']}\")\n    \n    # 3. Matrix decompositions\n    print(\"\\n3. MATRIX DECOMPOSITIONS:\")\n    \n    # Covariance matrix for galaxy clustering\n    def create_covariance_matrix(n_galaxies, correlation_length=0.1):\n        \"\"\"Create spatial covariance matrix.\"\"\"\n        # Positions\n        key = random.PRNGKey(2)\n        positions = random.uniform(key, (n_galaxies, 2))\n        \n        # Distance matrix\n        dist = jnp.sqrt(\n            ((positions[:, None, :] - positions[None, :, :]) ** 2).sum(axis=2)\n        )\n        \n        # Exponential covariance\n        C = jnp.exp(-dist / correlation_length)\n        \n        return C, positions\n    \n    C, positions = create_covariance_matrix(50)\n    \n    # Eigendecomposition for PCA\n    eigenvalues, eigenvectors = jnp.linalg.eigh(C)\n    \n    print(f\"  Covariance matrix: {C.shape}\")\n    print(f\"  Top 5 eigenvalues: {eigenvalues[-5:]}\")\n    print(f\"  Explained variance (top 10): {jnp.sum(eigenvalues[-10:]) / jnp.sum(eigenvalues):.1%}\")\n    \n    # 4. Regularized solutions\n    print(\"\\n4. REGULARIZED INVERSE PROBLEMS:\")\n    \n    def tikhonov_solve(A, b, alpha=0.01):\n        \"\"\"Tikhonov regularization.\"\"\"\n        # Solve (A^T A + alpha I) x = A^T b\n        n = A.shape[1]\n        A_reg = A.T @ A + alpha * jnp.eye(n)\n        b_reg = A.T @ b\n        \n        solver = lx.LU()\n        sol = lx.linear_solve(A_reg, b_reg, solver)\n        \n        return sol.value\n    \n    # Ill-conditioned problem\n    A_ill = random.normal(random.PRNGKey(3), (100, 50))\n    # Make it ill-conditioned\n    U, S, Vt = jnp.linalg.svd(A_ill, full_matrices=False)\n    S = S.at[:10].set(S[:10] * 1e-6)  # Small singular values\n    A_ill = U @ jnp.diag(S) @ Vt\n    \n    x_true = random.normal(random.PRNGKey(4), (50,))\n    b_ill = A_ill @ x_true + 0.01 * random.normal(random.PRNGKey(5), (100,))\n    \n    # Compare regularized vs non-regularized\n    x_reg = tikhonov_solve(A_ill, b_ill, alpha=0.1)\n    \n    print(f\"  Condition number: {jnp.linalg.cond(A_ill):.2e}\")\n    print(f\"  Regularized solution error: {jnp.linalg.norm(x_reg - x_true) / jnp.linalg.norm(x_true):.3f}\")\n\nlineax_solvers()","position":{"start":{"line":776,"column":1},"end":{"line":919,"column":1}},"key":"X0Il4UPpkD"},{"type":"heading","depth":2,"position":{"start":{"line":921,"column":1},"end":{"line":921,"column":1}},"children":[{"type":"text","value":"jaxtyping: Runtime Type Checking","position":{"start":{"line":921,"column":1},"end":{"line":921,"column":1}},"key":"JX1jUfXR6d"}],"identifier":"jaxtyping-runtime-type-checking","label":"jaxtyping: Runtime Type Checking","html_id":"jaxtyping-runtime-type-checking","implicit":true,"key":"GdkHU2QNNz"},{"type":"heading","depth":3,"position":{"start":{"line":923,"column":1},"end":{"line":923,"column":1}},"children":[{"type":"text","value":"Type-Safe Scientific Computing","position":{"start":{"line":923,"column":1},"end":{"line":923,"column":1}},"key":"kWLG0VjBKR"}],"identifier":"type-safe-scientific-computing","label":"Type-Safe Scientific Computing","html_id":"type-safe-scientific-computing","implicit":true,"key":"FDC3cv5XwP"},{"type":"code","lang":"python","value":"from jaxtyping import Float, Int, Bool, Complex, PyTree, Shaped, jaxtyped\nfrom typeguard import typechecked as typechecker\n\ndef jaxtyping_examples():\n    \"\"\"Type checking for safer scientific code.\"\"\"\n    \n    print(\"\\nJAXTYPING: TYPE-SAFE SCIENTIFIC COMPUTING\")\n    print(\"=\" * 50)\n    \n    # 1. Basic type annotations\n    print(\"\\n1. BASIC TYPE ANNOTATIONS:\")\n    \n    @jaxtyped(typechecker=typechecker)\n    def process_spectrum(\n        wavelengths: Float[Array, \"n_wavelengths\"],\n        flux: Float[Array, \"n_wavelengths\"],\n        errors: Optional[Float[Array, \"n_wavelengths\"]] = None\n    ) -> tuple[Float[Array, \"\"], Float[Array, \"\"]]:\n        \"\"\"Process spectrum with type checking.\"\"\"\n        \n        # Compute signal-to-noise\n        if errors is not None:\n            snr = flux / errors\n            median_snr = jnp.median(snr)\n        else:\n            median_snr = jnp.nan\n        \n        # Compute equivalent width\n        ew = jnp.trapz(1 - flux, wavelengths)\n        \n        return median_snr, ew\n    \n    # Test with correct types\n    wl = jnp.linspace(4000, 7000, 1000)\n    flux = 1.0 - 0.1 * jnp.exp(-(wl - 5500)**2 / 100**2)\n    errors = 0.01 * jnp.ones_like(flux)\n    \n    snr, ew = process_spectrum(wl, flux, errors)\n    print(f\"  SNR: {snr:.1f}, EW: {ew:.1f} Å\")\n    \n    # 2. Complex shape relationships\n    print(\"\\n2. COMPLEX SHAPE ANNOTATIONS:\")\n    \n    @jaxtyped(typechecker=typechecker)\n    def n_body_step(\n        positions: Float[Array, \"n_bodies 3\"],\n        velocities: Float[Array, \"n_bodies 3\"],\n        masses: Float[Array, \"n_bodies\"],\n        dt: float\n    ) -> tuple[Float[Array, \"n_bodies 3\"], Float[Array, \"n_bodies 3\"]]:\n        \"\"\"N-body integration step with shape checking.\"\"\"\n        \n        n = positions.shape[0]\n        forces = jnp.zeros_like(positions)\n        \n        # Compute forces\n        for i in range(n):\n            for j in range(n):\n                if i != j:\n                    r_ij = positions[j] - positions[i]\n                    r3 = jnp.linalg.norm(r_ij) ** 3\n                    forces = forces.at[i].add(masses[j] * r_ij / r3)\n        \n        # Update\n        accelerations = forces\n        new_velocities = velocities + accelerations * dt\n        new_positions = positions + new_velocities * dt\n        \n        return new_positions, new_velocities\n    \n    # Test\n    n = 3\n    pos = random.normal(random.PRNGKey(0), (n, 3))\n    vel = random.normal(random.PRNGKey(1), (n, 3)) * 0.1\n    m = jnp.ones(n)\n    \n    new_pos, new_vel = n_body_step(pos, vel, m, 0.01)\n    print(f\"  Updated {n} bodies\")\n    \n    # 3. PyTree annotations\n    print(\"\\n3. PYTREE TYPE ANNOTATIONS:\")\n    \n    @jaxtyped(typechecker=typechecker)\n    class GalaxyModel(eqx.Module):\n        \"\"\"Type-annotated galaxy model.\"\"\"\n        \n        disk_params: dict[str, Float[Array, \"...\"]]\n        bulge_params: dict[str, Float[Array, \"...\"]]\n        n_stars: Int[Array, \"\"]\n        \n        def __init__(self, n_stars: int):\n            self.disk_params = {\n                'scale_radius': jnp.array(5.0),\n                'scale_height': jnp.array(0.5),\n                'mass': jnp.array(1e10)\n            }\n            self.bulge_params = {\n                'effective_radius': jnp.array(1.0),\n                'sersic_index': jnp.array(4.0),\n                'mass': jnp.array(1e9)\n            }\n            self.n_stars = jnp.array(n_stars)\n        \n        @jaxtyped(typechecker=typechecker)\n        def density_profile(\n            self, \n            r: Float[Array, \"n_points\"],\n            component: str = 'disk'\n        ) -> Float[Array, \"n_points\"]:\n            \"\"\"Compute density profile.\"\"\"\n            \n            if component == 'disk':\n                r_d = self.disk_params['scale_radius']\n                return jnp.exp(-r / r_d) / r_d\n            else:\n                r_e = self.bulge_params['effective_radius']\n                n = self.bulge_params['sersic_index']\n                return jnp.exp(-7.67 * ((r / r_e) ** (1/n) - 1))\n    \n    galaxy = GalaxyModel(n_stars=10000)\n    radii = jnp.linspace(0.1, 20, 100)\n    density = galaxy.density_profile(radii, 'disk')\n    print(f\"  Disk density computed at {len(radii)} points\")\n    \n    # 4. Dimension variables\n    print(\"\\n4. DIMENSION VARIABLES:\")\n    \n    from jaxtyping import Float32, Float64\n    \n    @jaxtyped(typechecker=typechecker)\n    def mixed_precision_computation(\n        high_precision: Float64[Array, \"n m\"],\n        low_precision: Float32[Array, \"n m\"]\n    ) -> Float64[Array, \"n n\"]:\n        \"\"\"Mixed precision matrix computation.\"\"\"\n        \n        # Upcast low precision\n        low_as_high = low_precision.astype(jnp.float64)\n        \n        # Compute in high precision\n        result = high_precision @ low_as_high.T\n        \n        return result\n    \n    # Test mixed precision\n    hp = jnp.array(random.normal(random.PRNGKey(6), (10, 5)), dtype=jnp.float64)\n    lp = jnp.array(random.normal(random.PRNGKey(7), (10, 5)), dtype=jnp.float32)\n    \n    result = mixed_precision_computation(hp, lp)\n    print(f\"  Mixed precision result: {result.dtype}, shape {result.shape}\")\n\njaxtyping_examples()","position":{"start":{"line":925,"column":1},"end":{"line":1078,"column":1}},"key":"lr1zIwjf6C"},{"type":"heading","depth":2,"position":{"start":{"line":1080,"column":1},"end":{"line":1080,"column":1}},"children":[{"type":"text","value":"Integration Example: Complete Scientific Workflow","position":{"start":{"line":1080,"column":1},"end":{"line":1080,"column":1}},"key":"MCGaf9CX1K"}],"identifier":"integration-example-complete-scientific-workflow","label":"Integration Example: Complete Scientific Workflow","html_id":"integration-example-complete-scientific-workflow","implicit":true,"key":"acLNWTjarQ"},{"type":"heading","depth":3,"position":{"start":{"line":1082,"column":1},"end":{"line":1082,"column":1}},"children":[{"type":"text","value":"Combining All Libraries","position":{"start":{"line":1082,"column":1},"end":{"line":1082,"column":1}},"key":"YAytWNvl70"}],"identifier":"combining-all-libraries","label":"Combining All Libraries","html_id":"combining-all-libraries","implicit":true,"key":"bY0oAAmF1K"},{"type":"code","lang":"python","value":"def complete_scientific_workflow():\n    \"\"\"\n    Complete workflow combining Equinox, Diffrax, Optimistix, and Lineax.\n    Example: Fitting a dynamical model to observations.\n    \"\"\"\n    \n    print(\"\\nCOMPLETE SCIENTIFIC WORKFLOW\")\n    print(\"=\" * 50)\n    \n    # 1. Define the dynamical model with Equinox\n    class DynamicalModel(eqx.Module):\n        \"\"\"Neural ODE for galaxy dynamics.\"\"\"\n        \n        potential_net: eqx.nn.MLP\n        \n        def __init__(self, key):\n            self.potential_net = eqx.nn.MLP(\n                in_size=3,  # x, y, z\n                out_size=1,  # potential\n                width_size=64,\n                depth=3,\n                activation=jax.nn.tanh,\n                key=key\n            )\n        \n        def potential(self, position: Float[Array, \"3\"]) -> Float[Array, \"\"]:\n            \"\"\"Gravitational potential.\"\"\"\n            return self.potential_net(position).squeeze()\n        \n        def dynamics(self, t, state, args):\n            \"\"\"Hamiltonian dynamics.\"\"\"\n            q, p = state[:3], state[3:]\n            \n            # Gradient of potential\n            dV_dq = grad(self.potential)(q)\n            \n            dq_dt = p  # dq/dt = p\n            dp_dt = -dV_dq  # dp/dt = -∇V\n            \n            return jnp.concatenate([dq_dt, dp_dt])\n    \n    # 2. Generate synthetic observations\n    key = random.PRNGKey(42)\n    key, model_key, data_key = random.split(key, 3)\n    \n    true_model = DynamicalModel(model_key)\n    \n    # Integrate orbits with Diffrax\n    def integrate_orbit(model, initial_state, t_obs):\n        \"\"\"Integrate orbit and return positions at observation times.\"\"\"\n        \n        term = diffrax.ODETerm(model.dynamics)\n        solver = diffrax.Dopri5()\n        saveat = diffrax.SaveAt(ts=t_obs)\n        \n        sol = diffrax.diffeqsolve(\n            term, solver, t_obs[0], t_obs[-1], 0.01, initial_state,\n            saveat=saveat\n        )\n        \n        return sol.ys[:, :3]  # Return positions only\n    \n    # Generate observations\n    n_orbits = 5\n    n_obs_per_orbit = 20\n    \n    observations = []\n    initial_states = []\n    \n    for i in range(n_orbits):\n        key, subkey = random.split(key)\n        \n        # Random initial condition\n        q0 = random.normal(subkey, (3,)) * 2\n        p0 = random.normal(subkey, (3,)) * 0.5\n        initial = jnp.concatenate([q0, p0])\n        initial_states.append(initial)\n        \n        # Observation times\n        t_obs = jnp.linspace(0, 10, n_obs_per_orbit)\n        \n        # Integrate and add noise\n        true_positions = integrate_orbit(true_model, initial, t_obs)\n        noise = 0.05 * random.normal(subkey, true_positions.shape)\n        observed = true_positions + noise\n        \n        observations.append(observed)\n    \n    print(f\"  Generated {n_orbits} orbits with {n_obs_per_orbit} observations each\")\n    \n    # 3. Define loss function\n    @eqx.filter_jit\n    def loss_function(model, initial_states, observations, t_obs):\n        \"\"\"Loss for orbit fitting.\"\"\"\n        \n        total_loss = 0.0\n        \n        for initial, observed in zip(initial_states, observations):\n            predicted = integrate_orbit(model, initial, t_obs)\n            residuals = predicted - observed\n            total_loss += jnp.sum(residuals ** 2)\n        \n        return total_loss / len(observations)\n    \n    # 4. Optimize with Optimistix\n    print(\"\\n  Fitting model to observations...\")\n    \n    # Initialize model to fit\n    key, fit_key = random.split(key)\n    fitted_model = DynamicalModel(fit_key)\n    \n    t_obs = jnp.linspace(0, 10, n_obs_per_orbit)\n    \n    # Use gradient descent\n    import optax\n    \n    optimizer = optax.adam(learning_rate=1e-3)\n    opt_state = optimizer.init(eqx.filter(fitted_model, eqx.is_array))\n    \n    # Training loop\n    n_epochs = 50\n    \n    for epoch in range(n_epochs):\n        loss, grads = eqx.filter_value_and_grad(loss_function)(\n            fitted_model, initial_states, observations, t_obs\n        )\n        \n        updates, opt_state = optimizer.update(grads, opt_state)\n        fitted_model = eqx.apply_updates(fitted_model, updates)\n        \n        if epoch % 10 == 0:\n            print(f\"    Epoch {epoch}: Loss = {loss:.4f}\")\n    \n    # 5. Analyze results with Lineax\n    print(\"\\n  Analyzing fitted model...\")\n    \n    # Compute Hessian at minimum for uncertainty estimation\n    def potential_at_points(model, points):\n        \"\"\"Evaluate potential at multiple points.\"\"\"\n        return vmap(model.potential)(points)\n    \n    # Sample points\n    test_points = random.normal(random.PRNGKey(99), (100, 3))\n    \n    # Compare potentials\n    true_pot = potential_at_points(true_model, test_points)\n    fitted_pot = potential_at_points(fitted_model, test_points)\n    \n    error = jnp.sqrt(jnp.mean((true_pot - fitted_pot) ** 2))\n    print(f\"  RMS potential error: {error:.4f}\")\n    \n    # Compute correlation\n    correlation = jnp.corrcoef(true_pot, fitted_pot)[0, 1]\n    print(f\"  Potential correlation: {correlation:.3f}\")\n    \n    # Visualize one orbit\n    initial_test = initial_states[0]\n    t_plot = jnp.linspace(0, 20, 200)\n    \n    true_orbit = integrate_orbit(true_model, initial_test, t_plot)\n    fitted_orbit = integrate_orbit(fitted_model, initial_test, t_plot)\n    \n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n    \n    # True model\n    axes[0].plot(true_orbit[:, 0], true_orbit[:, 1], 'b-', label='True')\n    axes[0].plot(fitted_orbit[:, 0], fitted_orbit[:, 1], 'r--', label='Fitted')\n    axes[0].set_xlabel('x')\n    axes[0].set_ylabel('y')\n    axes[0].set_title('Orbit Comparison')\n    axes[0].legend()\n    axes[0].set_aspect('equal')\n    \n    # Potential contours\n    x = y = jnp.linspace(-3, 3, 50)\n    X, Y = jnp.meshgrid(x, y)\n    Z = jnp.zeros_like(X)\n    \n    points_grid = jnp.stack([X.ravel(), Y.ravel(), Z.ravel()], axis=1)\n    true_pot_grid = potential_at_points(true_model, points_grid).reshape(X.shape)\n    fitted_pot_grid = potential_at_points(fitted_model, points_grid).reshape(X.shape)\n    \n    axes[1].contour(X, Y, true_pot_grid, levels=10, colors='blue', alpha=0.5, label='True')\n    axes[1].contour(X, Y, fitted_pot_grid, levels=10, colors='red', alpha=0.5, linestyles='--', label='Fitted')\n    axes[1].set_xlabel('x')\n    axes[1].set_ylabel('y')\n    axes[1].set_title('Potential Contours (z=0)')\n    axes[1].set_aspect('equal')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return fitted_model\n\n# Run complete workflow\nfitted_model = complete_scientific_workflow()","position":{"start":{"line":1084,"column":1},"end":{"line":1281,"column":1}},"key":"mSVjTd6JJF"},{"type":"heading","depth":2,"position":{"start":{"line":1283,"column":1},"end":{"line":1283,"column":1}},"children":[{"type":"text","value":"Key Takeaways","position":{"start":{"line":1283,"column":1},"end":{"line":1283,"column":1}},"key":"vcOBOntU2P"}],"identifier":"key-takeaways","label":"Key Takeaways","html_id":"key-takeaways","implicit":true,"key":"SDHXLAJ1Pu"},{"type":"paragraph","position":{"start":{"line":1285,"column":1},"end":{"line":1290,"column":1}},"children":[{"type":"text","value":"✅ ","position":{"start":{"line":1285,"column":1},"end":{"line":1285,"column":1}},"key":"kVTD1p96CT"},{"type":"strong","position":{"start":{"line":1285,"column":1},"end":{"line":1285,"column":1}},"children":[{"type":"text","value":"Equinox","position":{"start":{"line":1285,"column":1},"end":{"line":1285,"column":1}},"key":"PDHXFhkQJW"}],"key":"aMMZ36e7YF"},{"type":"text","value":" - Neural networks and models as PyTrees, perfect for scientific ML","position":{"start":{"line":1285,"column":1},"end":{"line":1285,"column":1}},"key":"jIyEpxTIbf"},{"type":"break","position":{"start":{"line":1285,"column":1},"end":{"line":1285,"column":1}},"key":"lnD91EE4SN"},{"type":"text","value":"✅ ","position":{"start":{"line":1285,"column":1},"end":{"line":1285,"column":1}},"key":"o2PIz23K4A"},{"type":"strong","position":{"start":{"line":1285,"column":1},"end":{"line":1285,"column":1}},"children":[{"type":"text","value":"Diffrax","position":{"start":{"line":1285,"column":1},"end":{"line":1285,"column":1}},"key":"lGiwIOy1Qr"}],"key":"kATCOrhDyu"},{"type":"text","value":" - State-of-the-art ODE/SDE solvers with automatic differentiation","position":{"start":{"line":1285,"column":1},"end":{"line":1285,"column":1}},"key":"GCkCQGE99d"},{"type":"break","position":{"start":{"line":1285,"column":1},"end":{"line":1285,"column":1}},"key":"xpNDp4rVHu"},{"type":"text","value":"✅ ","position":{"start":{"line":1285,"column":1},"end":{"line":1285,"column":1}},"key":"dd5dl1NANe"},{"type":"strong","position":{"start":{"line":1285,"column":1},"end":{"line":1285,"column":1}},"children":[{"type":"text","value":"Optimistix","position":{"start":{"line":1285,"column":1},"end":{"line":1285,"column":1}},"key":"P2LrffuE6r"}],"key":"Ucc4miOI4g"},{"type":"text","value":" - Root finding, optimization, and fixed-point solvers","position":{"start":{"line":1285,"column":1},"end":{"line":1285,"column":1}},"key":"ernRY44EnW"},{"type":"break","position":{"start":{"line":1285,"column":1},"end":{"line":1285,"column":1}},"key":"LgSNGiooEK"},{"type":"text","value":"✅ ","position":{"start":{"line":1285,"column":1},"end":{"line":1285,"column":1}},"key":"xhpHocwC7v"},{"type":"strong","position":{"start":{"line":1285,"column":1},"end":{"line":1285,"column":1}},"children":[{"type":"text","value":"Lineax","position":{"start":{"line":1285,"column":1},"end":{"line":1285,"column":1}},"key":"XFKT2URmwv"}],"key":"JjJqFUyHNx"},{"type":"text","value":" - Linear algebra solvers with multiple backends","position":{"start":{"line":1285,"column":1},"end":{"line":1285,"column":1}},"key":"XGVSoEihaD"},{"type":"break","position":{"start":{"line":1285,"column":1},"end":{"line":1285,"column":1}},"key":"uyDW1fsSYT"},{"type":"text","value":"✅ ","position":{"start":{"line":1285,"column":1},"end":{"line":1285,"column":1}},"key":"GyxwQwApyK"},{"type":"strong","position":{"start":{"line":1285,"column":1},"end":{"line":1285,"column":1}},"children":[{"type":"text","value":"jaxtyping","position":{"start":{"line":1285,"column":1},"end":{"line":1285,"column":1}},"key":"ORUFJOiMhr"}],"key":"myULNGiSdl"},{"type":"text","value":" - Runtime type checking for safer scientific code","position":{"start":{"line":1285,"column":1},"end":{"line":1285,"column":1}},"key":"buSOdYXIBY"},{"type":"break","position":{"start":{"line":1285,"column":1},"end":{"line":1285,"column":1}},"key":"jYERAjF59E"},{"type":"text","value":"✅ ","position":{"start":{"line":1285,"column":1},"end":{"line":1285,"column":1}},"key":"cgujAgwBuD"},{"type":"strong","position":{"start":{"line":1285,"column":1},"end":{"line":1285,"column":1}},"children":[{"type":"text","value":"Integration","position":{"start":{"line":1285,"column":1},"end":{"line":1285,"column":1}},"key":"fWSgro6Bjw"}],"key":"wK8njR2K0x"},{"type":"text","value":" - These libraries work seamlessly together for complex workflows","position":{"start":{"line":1285,"column":1},"end":{"line":1285,"column":1}},"key":"iH3gZPqiQA"}],"key":"q6c8Ax16Xh"},{"type":"heading","depth":2,"position":{"start":{"line":1292,"column":1},"end":{"line":1292,"column":1}},"children":[{"type":"text","value":"Next Chapter Preview","position":{"start":{"line":1292,"column":1},"end":{"line":1292,"column":1}},"key":"zgXr6kZTkR"}],"identifier":"next-chapter-preview","label":"Next Chapter Preview","html_id":"next-chapter-preview","implicit":true,"key":"TXSBHTlzXa"},{"type":"paragraph","position":{"start":{"line":1293,"column":1},"end":{"line":1293,"column":1}},"children":[{"type":"text","value":"Deep Learning Stack: Flax for large-scale neural networks, Optax for optimization, and Orbax for checkpointing.","position":{"start":{"line":1293,"column":1},"end":{"line":1293,"column":1}},"key":"x7lROD2wiU"}],"key":"H0felErgcx"}],"key":"up1Rr8OKSR"}],"key":"POyHkQYkEE"},"references":{"cite":{"order":[],"data":{}}}}