{"version":2,"kind":"Article","sha256":"c9a6361d913d3849028c021d3aac08fb96f7d2ceaed7bbe409b1d036ccaa69b0","slug":"jax-advanced-chapter","location":"/03-scientific-computing-with-python/05-modern-approaches-jax-ecosystem/05-jax_advanced_chapter.md","dependencies":[],"frontmatter":{"title":"JAX Advanced Patterns: Control Flow and Optimization","content_includes_title":false,"authors":[{"nameParsed":{"literal":"Anna Rosen","given":"Anna","family":"Rosen"},"name":"Anna Rosen","orcid":"0000-0003-4423-0660","email":"alrosen@sdsu.edu","affiliations":["San Diego State University"],"id":"contributors-myst-generated-uid-0","corresponding":true}],"license":{"content":{"id":"CC-BY-4.0","url":"https://creativecommons.org/licenses/by/4.0/","name":"Creative Commons Attribution 4.0 International","free":true,"CC":true},"code":{"id":"MIT","url":"https://opensource.org/licenses/MIT","name":"MIT License","free":true,"osi":true}},"github":"https://github.com/astrobytes-edu/astr596-modeling-universe","subject":"Modeling the Universe","venue":{"title":"ASTR 596 - Fall 2025","url":"https://www.anna-rosen.com"},"keywords":["computational astrophysics","python","numerical methods","machine learning","monte carlo","neural networks","radiative transfer","bayesian inference","JAX"],"affiliations":[{"id":"San Diego State University","name":"San Diego State University"}],"numbering":{"title":{"offset":2}},"edit_url":"https://github.com/astrobytes-edu/astr596-modeling-universe/blob/main/03-scientific-computing-with-python/05-modern-approaches-jax-ecosystem/05-jax_advanced_chapter.md","exports":[{"format":"md","filename":"05-jax_advanced_chapter.md","url":"/05-jax_advanced_chap-6912ef420c265b23906d0bd3078a4fcb.md"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"heading","depth":2,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Learning Objectives","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"OlIoagtQ9Z"}],"identifier":"learning-objectives","label":"Learning Objectives","html_id":"learning-objectives","implicit":true,"key":"rpIenBkxO2"},{"type":"paragraph","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"By the end of this chapter, you will:","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"YUfgzqan2t"}],"key":"J2NdIqSAeU"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":5,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Master JAX control flow primitives (cond, scan, while_loop)","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"Ov9EvhxsmS"}],"key":"Fu6f9kFmjN"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"Implement custom derivatives and VJP rules","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"Hl2iUiteDF"}],"key":"KI8nkSsBZ1"},{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"Optimize memory and performance for large-scale problems","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"lsyOw3S6P1"}],"key":"co6YS5J4bL"},{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"Use sharding for multi-GPU computations","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"Pyf8Jbz9hU"}],"key":"ruRpD8wxU7"},{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"Debug and profile JAX programs effectively","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"ZEo2A90iOA"}],"key":"vGwAwCgEuO"},{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"Build production-ready astronomical simulations","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"dmpHpleufY"}],"key":"TjQNkPEanJ"}],"key":"nKmp4hJy2l"},{"type":"heading","depth":2,"position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"Control Flow in JAX","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"UaFnVl5GFt"}],"identifier":"control-flow-in-jax","label":"Control Flow in JAX","html_id":"control-flow-in-jax","implicit":true,"key":"Wk7uVRkEG0"},{"type":"heading","depth":3,"position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"text","value":"Conditional Execution with lax.cond","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"Y2xiGUZQxc"}],"identifier":"conditional-execution-with-lax-cond","label":"Conditional Execution with lax.cond","html_id":"conditional-execution-with-lax-cond","implicit":true,"key":"WD64jSkGMB"},{"type":"code","lang":"python","value":"import jax\nimport jax.numpy as jnp\nfrom jax import grad, jit, vmap, pmap\nfrom jax.lax import cond, scan, while_loop, fori_loop, switch\nfrom jax import random, custom_vjp, custom_jvp\nimport time\nimport matplotlib.pyplot as plt\n\ndef jax_control_flow():\n    \"\"\"Master JAX's functional control flow primitives.\"\"\"\n    \n    print(\"JAX CONTROL FLOW PRIMITIVES\")\n    print(\"=\" * 50)\n    \n    # 1. Conditional execution with cond\n    print(\"\\n1. CONDITIONAL EXECUTION:\")\n    \n    @jit\n    def stellar_evolution_step(age, mass):\n        \"\"\"\n        Evolve star based on evolutionary phase.\n        Different physics for different phases.\n        \"\"\"\n        \n        def main_sequence(args):\n            age, mass = args\n            # Main sequence evolution\n            luminosity = mass ** 3.5\n            radius = mass ** 0.8\n            return luminosity, radius\n        \n        def giant_branch(args):\n            age, mass = args\n            # Red giant evolution\n            luminosity = mass ** 2.5 * (age / 10.0)\n            radius = mass ** 0.5 * (age / 10.0) ** 0.3\n            return luminosity, radius\n        \n        def white_dwarf(args):\n            age, mass = args\n            # White dwarf cooling\n            luminosity = 0.001 * jnp.exp(-(age - 12.0) / 2.0)\n            radius = 0.01\n            return luminosity, radius\n        \n        # Multi-way conditional\n        ms_turnoff = 10.0 / mass ** 2.5  # Simplified\n        \n        # Nested conditions\n        return cond(\n            age < ms_turnoff,\n            main_sequence,\n            lambda args: cond(\n                age < ms_turnoff * 1.2,\n                giant_branch,\n                white_dwarf,\n                args\n            ),\n            (age, mass)\n        )\n    \n    # Test different evolutionary phases\n    ages = jnp.array([0.1, 5.0, 11.0, 15.0])\n    mass = 1.0\n    \n    for age in ages:\n        L, R = stellar_evolution_step(age, mass)\n        print(f\"  Age {age:.1f} Gyr: L={L:.3f} L☉, R={R:.3f} R☉\")\n    \n    # 2. Switch for multiple branches\n    print(\"\\n2. SWITCH STATEMENT:\")\n    \n    @jit\n    def process_observation(obs_type, data):\n        \"\"\"Process different observation types.\"\"\"\n        \n        def process_photometry(data):\n            # Magnitude calculation\n            return -2.5 * jnp.log10(data) + 25.0\n        \n        def process_spectroscopy(data):\n            # Continuum normalization\n            continuum = jnp.median(data)\n            return data / continuum\n        \n        def process_imaging(data):\n            # Background subtraction\n            background = jnp.percentile(data, 10)\n            return data - background\n        \n        branches = [process_photometry, process_spectroscopy, process_imaging]\n        \n        return switch(obs_type, branches, data)\n    \n    # Different observation types\n    data = jnp.array([100.0, 150.0, 200.0])\n    \n    for obs_type in range(3):\n        result = process_observation(obs_type, data)\n        print(f\"  Type {obs_type}: {result[0]:.3f}\")\n    \n    # 3. Gradient through conditionals\n    print(\"\\n3. GRADIENTS THROUGH CONDITIONALS:\")\n    \n    @jit\n    def piecewise_potential(r):\n        \"\"\"Piecewise gravitational potential.\"\"\"\n        \n        def inner_region(r):\n            # Constant density sphere\n            return -1.5 + 0.5 * r**2\n        \n        def outer_region(r):\n            # Point mass\n            return -1.0 / r\n        \n        return cond(r < 1.0, inner_region, outer_region, r)\n    \n    # Gradient (force) is continuous at boundary!\n    grad_potential = grad(piecewise_potential)\n    \n    radii = jnp.array([0.5, 0.99, 1.0, 1.01, 2.0])\n    for r in radii:\n        force = -grad_potential(r)\n        print(f\"  r={r:.2f}: F={force:.4f}\")\n\njax_control_flow()","position":{"start":{"line":16,"column":1},"end":{"line":144,"column":1}},"key":"r7kb8f1oki"},{"type":"heading","depth":3,"position":{"start":{"line":146,"column":1},"end":{"line":146,"column":1}},"children":[{"type":"text","value":"Loops with scan and fori_loop","position":{"start":{"line":146,"column":1},"end":{"line":146,"column":1}},"key":"ZG2977lJKx"}],"identifier":"loops-with-scan-and-fori-loop","label":"Loops with scan and fori_loop","html_id":"loops-with-scan-and-fori-loop","implicit":true,"key":"x68l922AjK"},{"type":"code","lang":"python","value":"def jax_loops():\n    \"\"\"Efficient loops in JAX using scan and fori_loop.\"\"\"\n    \n    print(\"\\nLOOPS IN JAX\")\n    print(\"=\" * 50)\n    \n    # 1. scan for carrying state\n    print(\"\\n1. SCAN FOR SEQUENTIAL COMPUTATIONS:\")\n    \n    @jit\n    def runge_kutta_4(dynamics, initial_state, t_span, dt):\n        \"\"\"\n        RK4 integration using scan.\n        \n        Parameters\n        ----------\n        dynamics : callable\n            dy/dt = dynamics(y, t)\n        initial_state : array\n            Initial conditions\n        t_span : tuple\n            (t_start, t_end)\n        dt : float\n            Time step\n        \"\"\"\n        \n        def rk4_step(carry, t):\n            y = carry\n            \n            k1 = dynamics(y, t)\n            k2 = dynamics(y + 0.5 * dt * k1, t + 0.5 * dt)\n            k3 = dynamics(y + 0.5 * dt * k2, t + 0.5 * dt)\n            k4 = dynamics(y + dt * k3, t + dt)\n            \n            y_new = y + (dt / 6.0) * (k1 + 2*k2 + 2*k3 + k4)\n            \n            return y_new, y_new  # carry and output\n        \n        t_start, t_end = t_span\n        n_steps = int((t_end - t_start) / dt)\n        times = jnp.linspace(t_start, t_end, n_steps)\n        \n        _, trajectory = scan(rk4_step, initial_state, times)\n        \n        return times, trajectory\n    \n    # Kepler problem\n    def kepler_dynamics(state, t):\n        \"\"\"Kepler two-body dynamics.\"\"\"\n        r, v = state[:2], state[2:]\n        r_norm = jnp.linalg.norm(r)\n        a = -r / r_norm**3\n        return jnp.concatenate([v, a])\n    \n    initial = jnp.array([1.0, 0.0, 0.0, 1.0])\n    times, trajectory = runge_kutta_4(kepler_dynamics, initial, (0, 10), 0.01)\n    \n    print(f\"  Integrated {len(times)} steps\")\n    print(f\"  Final position: {trajectory[-1, :2]}\")\n    \n    # Check energy conservation\n    def energy(state):\n        r, v = state[:2], state[2:]\n        return 0.5 * jnp.sum(v**2) - 1.0 / jnp.linalg.norm(r)\n    \n    initial_energy = energy(trajectory[0])\n    final_energy = energy(trajectory[-1])\n    print(f\"  Energy drift: {abs(final_energy - initial_energy):.2e}\")\n    \n    # 2. fori_loop for fixed iterations\n    print(\"\\n2. FORI_LOOP FOR FIXED ITERATIONS:\")\n    \n    @jit\n    def jacobi_iteration(A, b, x0, n_iterations):\n        \"\"\"Solve Ax = b using Jacobi iteration.\"\"\"\n        \n        D = jnp.diag(jnp.diag(A))  # Diagonal part\n        R = A - D  # Off-diagonal part\n        \n        def iteration(i, x):\n            return jnp.linalg.solve(D, b - R @ x)\n        \n        return fori_loop(0, n_iterations, iteration, x0)\n    \n    # Test system\n    A = jnp.array([[4.0, -1.0, 0.0],\n                   [-1.0, 4.0, -1.0],\n                   [0.0, -1.0, 3.0]])\n    b = jnp.array([15.0, 10.0, 10.0])\n    x0 = jnp.zeros(3)\n    \n    solution = jacobi_iteration(A, b, x0, 50)\n    print(f\"  Solution: {solution}\")\n    print(f\"  Residual: {jnp.linalg.norm(A @ solution - b):.2e}\")\n    \n    # 3. while_loop for adaptive algorithms\n    print(\"\\n3. WHILE_LOOP FOR ADAPTIVE ALGORITHMS:\")\n    \n    @jit\n    def adaptive_integration(f, x0, x1, tol=1e-6, max_depth=10):\n        \"\"\"Adaptive Simpson's integration.\"\"\"\n        \n        def simpson(a, b):\n            \"\"\"Simpson's rule on [a, b].\"\"\"\n            mid = (a + b) / 2\n            return (b - a) / 6 * (f(a) + 4*f(mid) + f(b))\n        \n        def should_refine(carry):\n            a, b, depth, _ = carry\n            return (depth < max_depth)\n        \n        def refine_step(carry):\n            a, b, depth, integral = carry\n            mid = (a + b) / 2\n            \n            whole = simpson(a, b)\n            left = simpson(a, mid)\n            right = simpson(mid, b)\n            \n            error = abs(whole - (left + right))\n            \n            # Simplified: just refine if error is large\n            # In practice, would accumulate integral properly\n            return (a, b, depth + 1, left + right)\n        \n        initial_carry = (x0, x1, 0, simpson(x0, x1))\n        final_carry = while_loop(should_refine, refine_step, initial_carry)\n        \n        return final_carry[3]\n    \n    # Test function\n    def test_func(x):\n        return jnp.sin(x) ** 2\n    \n    result = adaptive_integration(test_func, 0.0, jnp.pi)\n    print(f\"  ∫sin²(x)dx from 0 to π = {result:.6f}\")\n    print(f\"  Expected: {jnp.pi/2:.6f}\")\n    \n    # 4. Nested loops\n    print(\"\\n4. NESTED LOOPS WITH SCAN:\")\n    \n    @jit\n    def double_pendulum_poincare(initial_conditions, n_periods, points_per_period):\n        \"\"\"\n        Compute Poincaré section of double pendulum.\n        Nested loop: outer for periods, inner for integration.\n        \"\"\"\n        \n        def dynamics(state, t):\n            # Simplified double pendulum dynamics\n            theta1, theta2, p1, p2 = state\n            \n            # Just for demonstration (not accurate physics)\n            dtheta1 = p1\n            dtheta2 = p2\n            dp1 = -jnp.sin(theta1) - 0.1 * jnp.sin(theta1 - theta2)\n            dp2 = -jnp.sin(theta2) + 0.1 * jnp.sin(theta1 - theta2)\n            \n            return jnp.array([dtheta1, dtheta2, dp1, dp2])\n        \n        def integrate_period(carry, _):\n            state = carry\n            \n            # Inner loop: integrate for one period\n            def step(s, _):\n                new_s = s + 0.01 * dynamics(s, 0)\n                return new_s, new_s\n            \n            final_state, trajectory = scan(\n                step, state, None, length=points_per_period\n            )\n            \n            # Return final state and Poincaré point\n            return final_state, final_state[:2]  # Only positions\n        \n        # Outer loop: collect Poincaré points\n        _, poincare_points = scan(\n            integrate_period, initial_conditions, None, length=n_periods\n        )\n        \n        return poincare_points\n    \n    initial = jnp.array([0.1, 0.1, 0.0, 0.0])\n    poincare = double_pendulum_poincare(initial, 100, 100)\n    \n    print(f\"  Generated {len(poincare)} Poincaré points\")\n    print(f\"  Phase space bounds: θ₁∈[{poincare[:, 0].min():.2f}, {poincare[:, 0].max():.2f}]\")\n\njax_loops()","position":{"start":{"line":148,"column":1},"end":{"line":338,"column":1}},"key":"yrTcjT7vLp"},{"type":"heading","depth":2,"position":{"start":{"line":340,"column":1},"end":{"line":340,"column":1}},"children":[{"type":"text","value":"Custom Derivatives","position":{"start":{"line":340,"column":1},"end":{"line":340,"column":1}},"key":"shc69dCap9"}],"identifier":"custom-derivatives","label":"Custom Derivatives","html_id":"custom-derivatives","implicit":true,"key":"LxVWhHCJok"},{"type":"heading","depth":3,"position":{"start":{"line":342,"column":1},"end":{"line":342,"column":1}},"children":[{"type":"text","value":"Defining Custom VJP Rules","position":{"start":{"line":342,"column":1},"end":{"line":342,"column":1}},"key":"OXXy2lFYCv"}],"identifier":"defining-custom-vjp-rules","label":"Defining Custom VJP Rules","html_id":"defining-custom-vjp-rules","implicit":true,"key":"ifbkkuYzMC"},{"type":"code","lang":"python","value":"def custom_derivatives():\n    \"\"\"Define custom derivatives for specialized functions.\"\"\"\n    \n    print(\"\\nCUSTOM DERIVATIVES IN JAX\")\n    print(\"=\" * 50)\n    \n    # 1. Custom VJP for numerical stability\n    print(\"\\n1. STABLE SOFTPLUS WITH CUSTOM VJP:\")\n    \n    @custom_vjp\n    def stable_softplus(x):\n        \"\"\"Softplus with numerical stability.\"\"\"\n        return jnp.log1p(jnp.exp(-jnp.abs(x))) + jnp.maximum(x, 0)\n    \n    def stable_softplus_fwd(x):\n        \"\"\"Forward pass: compute value and save residuals.\"\"\"\n        y = stable_softplus(x)\n        return y, (x,)  # Save x for backward pass\n    \n    def stable_softplus_bwd(res, g):\n        \"\"\"Backward pass: compute VJP.\"\"\"\n        x, = res\n        # Derivative of softplus is sigmoid\n        sigmoid_x = 1 / (1 + jnp.exp(-x))\n        return (g * sigmoid_x,)\n    \n    stable_softplus.defvjp(stable_softplus_fwd, stable_softplus_bwd)\n    \n    # Test gradient\n    x_test = jnp.array([-100.0, -1.0, 0.0, 1.0, 100.0])\n    grad_fn = grad(lambda x: jnp.sum(stable_softplus(x)))\n    grads = grad_fn(x_test)\n    \n    print(f\"  x: {x_test}\")\n    print(f\"  softplus(x): {stable_softplus(x_test)}\")\n    print(f\"  gradients: {grads}\")\n    \n    # 2. Custom derivative for interpolation\n    print(\"\\n2. DIFFERENTIABLE INTERPOLATION:\")\n    \n    @custom_vjp\n    def interp1d(x, xp, fp):\n        \"\"\"Linear interpolation with custom gradient.\"\"\"\n        return jnp.interp(x, xp, fp)\n    \n    def interp1d_fwd(x, xp, fp):\n        y = jnp.interp(x, xp, fp)\n        return y, (x, xp, fp)\n    \n    def interp1d_bwd(res, g):\n        x, xp, fp = res\n        \n        # Find surrounding points\n        idx = jnp.searchsorted(xp, x)\n        idx = jnp.clip(idx, 1, len(xp) - 1)\n        \n        # Linear interpolation gradient\n        x0, x1 = xp[idx - 1], xp[idx]\n        f0, f1 = fp[idx - 1], fp[idx]\n        \n        # Gradient w.r.t. x\n        dfdx = (f1 - f0) / (x1 - x0)\n        \n        # Gradients w.r.t. xp and fp (simplified)\n        # In practice, these would be more complex\n        dxp = jnp.zeros_like(xp)\n        dfp = jnp.zeros_like(fp)\n        \n        # Weight for linear interpolation\n        alpha = (x - x0) / (x1 - x0)\n        dfp = dfp.at[idx - 1].add(g * (1 - alpha))\n        dfp = dfp.at[idx].add(g * alpha)\n        \n        return g * dfdx, dxp, dfp\n    \n    interp1d.defvjp(interp1d_fwd, interp1d_bwd)\n    \n    # Test interpolation gradient\n    xp = jnp.array([0.0, 1.0, 2.0, 3.0])\n    fp = jnp.array([0.0, 1.0, 0.5, 2.0])\n    \n    def loss(x):\n        return interp1d(x, xp, fp) ** 2\n    \n    x_test = 1.5\n    value = interp1d(x_test, xp, fp)\n    gradient = grad(loss)(x_test)\n    \n    print(f\"  Interpolated value at x={x_test}: {value:.3f}\")\n    print(f\"  Gradient: {gradient:.3f}\")\n    \n    # 3. Custom JVP for forward-mode AD\n    print(\"\\n3. CUSTOM JVP FOR SPECIAL FUNCTIONS:\")\n    \n    @custom_jvp\n    def safe_log(x):\n        \"\"\"Logarithm with safe gradient at x=0.\"\"\"\n        return jnp.log(jnp.maximum(x, 1e-10))\n    \n    @safe_log.defjvp\n    def safe_log_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        \n        # Primal computation\n        y = safe_log(x)\n        \n        # Tangent computation (forward-mode derivative)\n        # Use safe derivative\n        y_dot = x_dot / jnp.maximum(x, 1e-10)\n        \n        return y, y_dot\n    \n    # Test near zero\n    x_vals = jnp.array([1e-15, 1e-10, 0.1, 1.0])\n    grad_safe_log = grad(lambda x: jnp.sum(safe_log(x)))\n    grads = grad_safe_log(x_vals)\n    \n    print(f\"  x: {x_vals}\")\n    print(f\"  safe_log(x): {safe_log(x_vals)}\")\n    print(f\"  gradients: {grads}\")\n    \n    # 4. Physics-informed custom derivatives\n    print(\"\\n4. PHYSICS-INFORMED DERIVATIVES:\")\n    \n    @custom_vjp\n    def gravitational_lensing_deflection(source_pos, lens_mass, lens_pos):\n        \"\"\"\n        Gravitational lensing deflection angle.\n        Custom derivative ensures correct physics.\n        \"\"\"\n        # Vector from lens to source\n        r = source_pos - lens_pos\n        r_norm = jnp.linalg.norm(r)\n        \n        # Einstein radius effect (simplified)\n        deflection = lens_mass * r / (r_norm**2 + 0.01)  # Softening\n        \n        return deflection\n    \n    def lensing_fwd(source_pos, lens_mass, lens_pos):\n        deflection = gravitational_lensing_deflection(source_pos, lens_mass, lens_pos)\n        return deflection, (source_pos, lens_mass, lens_pos)\n    \n    def lensing_bwd(res, g):\n        source_pos, lens_mass, lens_pos = res\n        r = source_pos - lens_pos\n        r_norm = jnp.linalg.norm(r)\n        \n        # Gradients from physics\n        # ∂α/∂source_pos\n        d_source = g * lens_mass * (\n            jnp.eye(2) / (r_norm**2 + 0.01) - \n            2 * jnp.outer(r, r) / (r_norm**2 + 0.01)**2\n        )\n        \n        # ∂α/∂lens_mass\n        d_mass = jnp.dot(g, r / (r_norm**2 + 0.01))\n        \n        # ∂α/∂lens_pos\n        d_lens = -d_source  # Newton's third law!\n        \n        return d_source, d_mass, d_lens\n    \n    gravitational_lensing_deflection.defvjp(lensing_fwd, lensing_bwd)\n    \n    # Test lensing gradients\n    source = jnp.array([1.0, 0.5])\n    mass = 1.0\n    lens = jnp.array([0.0, 0.0])\n    \n    def lensing_potential(s):\n        deflection = gravitational_lensing_deflection(s, mass, lens)\n        return jnp.sum(deflection**2)\n    \n    grad_lensing = grad(lensing_potential)\n    gradient = grad_lensing(source)\n    \n    print(f\"  Source position: {source}\")\n    print(f\"  Deflection gradient: {gradient}\")\n\ncustom_derivatives()","position":{"start":{"line":344,"column":1},"end":{"line":527,"column":1}},"key":"CtZOc8mdBe"},{"type":"heading","depth":2,"position":{"start":{"line":529,"column":1},"end":{"line":529,"column":1}},"children":[{"type":"text","value":"Memory and Performance Optimization","position":{"start":{"line":529,"column":1},"end":{"line":529,"column":1}},"key":"taVCMvdqEE"}],"identifier":"memory-and-performance-optimization","label":"Memory and Performance Optimization","html_id":"memory-and-performance-optimization","implicit":true,"key":"keP8peYXqd"},{"type":"heading","depth":3,"position":{"start":{"line":531,"column":1},"end":{"line":531,"column":1}},"children":[{"type":"text","value":"Checkpointing and Memory Management","position":{"start":{"line":531,"column":1},"end":{"line":531,"column":1}},"key":"mdfRYKzgNh"}],"identifier":"checkpointing-and-memory-management","label":"Checkpointing and Memory Management","html_id":"checkpointing-and-memory-management","implicit":true,"key":"i8IGcl5aJB"},{"type":"code","lang":"python","value":"def memory_optimization():\n    \"\"\"Optimize memory usage in JAX computations.\"\"\"\n    \n    print(\"\\nMEMORY OPTIMIZATION IN JAX\")\n    print(\"=\" * 50)\n    \n    # 1. Gradient checkpointing\n    print(\"\\n1. GRADIENT CHECKPOINTING:\")\n    \n    from jax.experimental import checkpoint\n    \n    def deep_network(x, n_layers=50):\n        \"\"\"Deep network that would use lots of memory.\"\"\"\n        \n        @checkpoint  # Don't store","position":{"start":{"line":533,"column":1},"end":{"line":548,"column":1}},"key":"yV2gPzCnzc"}],"key":"MSb1fMQdHG"}],"key":"JdMCMxIRsn"},"references":{"cite":{"order":[],"data":{}}}}