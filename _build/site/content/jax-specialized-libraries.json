{"version":2,"kind":"Article","sha256":"3afc1257f005b048989503460d5d0dfd213fc6fb058f2420468912f20d708fa0","slug":"jax-specialized-libraries","location":"/03-scientific-computing-with-python/05-modern-approaches-jax-ecosystem/04-jax_specialized_libraries.md","dependencies":[],"frontmatter":{"title":"JAX Specialized Libraries: BlackJAX, Numpyro, and Domain-Specific Tools","content_includes_title":false,"authors":[{"nameParsed":{"literal":"Anna Rosen","given":"Anna","family":"Rosen"},"name":"Anna Rosen","orcid":"0000-0003-4423-0660","email":"alrosen@sdsu.edu","affiliations":["San Diego State University"],"id":"contributors-myst-generated-uid-0","corresponding":true}],"license":{"content":{"id":"CC-BY-4.0","url":"https://creativecommons.org/licenses/by/4.0/","name":"Creative Commons Attribution 4.0 International","free":true,"CC":true},"code":{"id":"MIT","url":"https://opensource.org/licenses/MIT","name":"MIT License","free":true,"osi":true}},"github":"https://github.com/astrobytes-edu/astr596-modeling-universe","subject":"Modeling the Universe","venue":{"title":"ASTR 596 - Fall 2025","url":"https://www.anna-rosen.com"},"keywords":["computational astrophysics","python","numerical methods","machine learning","monte carlo","neural networks","radiative transfer","bayesian inference","JAX"],"affiliations":[{"id":"San Diego State University","name":"San Diego State University"}],"numbering":{"title":{"offset":2}},"edit_url":"https://github.com/astrobytes-edu/astr596-modeling-universe/blob/main/03-scientific-computing-with-python/05-modern-approaches-jax-ecosystem/04-jax_specialized_libraries.md","exports":[{"format":"md","filename":"04-jax_specialized_libraries.md","url":"/04-jax_specialized_l-ba0d0ca2b388b1a768d8e5d861540639.md"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"heading","depth":2,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Learning Objectives","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"bPcvwnoKZ5"}],"identifier":"learning-objectives","label":"Learning Objectives","html_id":"learning-objectives","implicit":true,"key":"TtRDbq45xC"},{"type":"paragraph","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"By the end of this chapter, you will:","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"rvGD5K2SP6"}],"key":"udftPbEtqy"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":5,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Implement MCMC samplers with BlackJAX","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"xRHrugxIj8"}],"key":"YoklmKApZp"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"Build probabilistic models with Numpyro","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"tlN6HHNL8W"}],"key":"cd7s5rRevO"},{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"Use JAXopt for constrained optimization","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"kyXL115v9Z"}],"key":"feYe4JCnRA"},{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"Apply ChemJAX for molecular dynamics","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"SbsnIa9Hrd"}],"key":"moeYUykI71"},{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"Leverage domain-specific JAX libraries for astronomy","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"SfVH5E5Y62"}],"key":"fCmWUHvU1D"},{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"Build custom JAX-based tools for your research","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"g46ZiVlkJk"}],"key":"KgRMuY9Hld"}],"key":"ZyweCappZQ"},{"type":"heading","depth":2,"position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"BlackJAX: Modern MCMC Sampling","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"zV8EYMeO9t"}],"identifier":"blackjax-modern-mcmc-sampling","label":"BlackJAX: Modern MCMC Sampling","html_id":"blackjax-modern-mcmc-sampling","implicit":true,"key":"mlSsjaxnJW"},{"type":"heading","depth":3,"position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"text","value":"Introduction to BlackJAX","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"E6KupRRRDU"}],"identifier":"introduction-to-blackjax","label":"Introduction to BlackJAX","html_id":"introduction-to-blackjax","implicit":true,"key":"wCiSZaMqTb"},{"type":"code","lang":"python","value":"import jax\nimport jax.numpy as jnp\nfrom jax import random, grad, jit, vmap\nimport blackjax\nimport blackjax.smc as smc\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom typing import NamedTuple, Callable\n\ndef blackjax_fundamentals():\n    \"\"\"Learn BlackJAX for MCMC sampling in astronomy.\"\"\"\n    \n    print(\"BLACKJAX: MODERN MCMC SAMPLING\")\n    print(\"=\" * 50)\n    \n    # 1. Basic HMC sampling\n    print(\"\\n1. HAMILTONIAN MONTE CARLO:\")\n    \n    # Define a cosmological likelihood\n    def log_likelihood(theta):\n        \"\"\"Log likelihood for cosmological parameters.\"\"\"\n        omega_m, h0 = theta\n        \n        # Mock SNe Ia data\n        z_data = jnp.array([0.01, 0.05, 0.1, 0.5, 1.0])\n        mu_obs = jnp.array([33.0, 36.0, 38.0, 42.0, 44.0])\n        mu_err = jnp.array([0.1, 0.15, 0.2, 0.25, 0.3])\n        \n        # Theoretical distance modulus (simplified)\n        def luminosity_distance(z, om, h):\n            # Simplified for flat universe\n            c = 3e5  # km/s\n            dL = c * z * (1 + z/2 * (1 - om))  # Taylor expansion\n            return dL / h\n        \n        dL = vmap(lambda z: luminosity_distance(z, omega_m, h0))(z_data)\n        mu_theory = 5 * jnp.log10(dL) + 25\n        \n        # Chi-squared\n        chi2 = jnp.sum(((mu_obs - mu_theory) / mu_err) ** 2)\n        return -0.5 * chi2\n    \n    def log_prior(theta):\n        \"\"\"Log prior for cosmological parameters.\"\"\"\n        omega_m, h0 = theta\n        \n        # Uniform priors\n        if 0 < omega_m < 1 and 50 < h0 < 100:\n            return 0.0\n        return -jnp.inf\n    \n    def log_prob(theta):\n        \"\"\"Log posterior probability.\"\"\"\n        lp = log_prior(theta)\n        if jnp.isfinite(lp):\n            return lp + log_likelihood(theta)\n        return lp\n    \n    # Initialize HMC\n    key = random.PRNGKey(0)\n    initial_position = jnp.array([0.3, 70.0])\n    \n    # Build HMC kernel\n    inv_mass_matrix = jnp.array([0.01, 1.0])  # Diagonal mass matrix\n    num_integration_steps = 10\n    step_size = 0.01\n    \n    hmc = blackjax.hmc(\n        log_prob,\n        step_size=step_size,\n        inverse_mass_matrix=inv_mass_matrix,\n        num_integration_steps=num_integration_steps\n    )\n    \n    # Initialize state\n    state = hmc.init(initial_position)\n    \n    # Run sampling\n    def inference_loop(rng_key, kernel, initial_state, num_samples):\n        \"\"\"MCMC sampling loop.\"\"\"\n        \n        @jit\n        def one_step(state, rng_key):\n            state, info = kernel(rng_key, state)\n            return state, (state, info)\n        \n        keys = random.split(rng_key, num_samples)\n        _, (states, infos) = jax.lax.scan(one_step, initial_state, keys)\n        \n        return states, infos\n    \n    key, sample_key = random.split(key)\n    states, infos = inference_loop(sample_key, hmc.step, state, 1000)\n    \n    samples = states.position\n    print(f\"  Sampled {len(samples)} points\")\n    print(f\"  Mean Ωₘ = {jnp.mean(samples[:, 0]):.3f} ± {jnp.std(samples[:, 0]):.3f}\")\n    print(f\"  Mean H₀ = {jnp.mean(samples[:, 1]):.1f} ± {jnp.std(samples[:, 1]):.1f}\")\n    print(f\"  Acceptance rate: {jnp.mean(infos.is_accepted):.2%}\")\n    \n    # 2. NUTS (No U-Turn Sampler)\n    print(\"\\n2. NUTS SAMPLER:\")\n    \n    nuts = blackjax.nuts(log_prob, step_size=step_size)\n    state = nuts.init(initial_position)\n    \n    key, sample_key = random.split(key)\n    states, infos = inference_loop(sample_key, nuts.step, state, 500)\n    \n    print(f\"  NUTS: {len(states.position)} samples\")\n    print(f\"  Mean tree depth: {jnp.mean(infos.num_trajectory_expansions):.1f}\")\n    \n    # 3. Adaptive sampling\n    print(\"\\n3. ADAPTIVE SAMPLING:\")\n    \n    # Window adaptation for step size and mass matrix\n    def run_adaptive_sampling(kernel_factory, num_chains=4):\n        \"\"\"Run parallel chains with adaptation.\"\"\"\n        \n        key = random.PRNGKey(42)\n        keys = random.split(key, num_chains + 1)\n        \n        # Initialize multiple chains\n        initial_positions = jnp.array([\n            [0.25 + 0.1 * random.normal(keys[i], ()), \n             65.0 + 5.0 * random.normal(keys[i+1], ())]\n            for i in range(num_chains)\n        ])\n        \n        # Warmup with window adaptation\n        warmup = blackjax.window_adaptation(\n            blackjax.nuts,\n            log_prob,\n            num_steps=500,\n            initial_step_size=0.01,\n            target_acceptance_rate=0.8\n        )\n        \n        # Run warmup\n        (last_states, parameters), _ = warmup.run(keys[-1], initial_positions[0])\n        \n        print(f\"  Adapted step size: {parameters['step_size']:.4f}\")\n        \n        return last_states, parameters\n    \n    adapted_state, adapted_params = run_adaptive_sampling(blackjax.nuts)\n    \n    # 4. Sequential Monte Carlo\n    print(\"\\n4. SEQUENTIAL MONTE CARLO:\")\n    \n    # Temperature schedule for tempering\n    def tempered_log_prob(theta, beta):\n        \"\"\"Tempered posterior for SMC.\"\"\"\n        return beta * log_prob(theta)\n    \n    # SMC sampler\n    def run_smc(num_particles=100):\n        \"\"\"Run SMC sampler.\"\"\"\n        key = random.PRNGKey(123)\n        \n        # Initial particles from prior\n        init_key, smc_key = random.split(key)\n        initial_particles = jnp.array([\n            [random.uniform(init_key, (), minval=0.1, maxval=0.5),\n             random.uniform(init_key, (), minval=60, maxval=80)]\n            for _ in range(num_particles)\n        ])\n        \n        # Temperature schedule\n        betas = jnp.linspace(0, 1, 10)\n        \n        print(f\"  SMC with {num_particles} particles, {len(betas)} temperatures\")\n        \n        # Would run full SMC here\n        return initial_particles\n    \n    smc_particles = run_smc()\n\nblackjax_fundamentals()","position":{"start":{"line":16,"column":1},"end":{"line":196,"column":1}},"key":"ogWchXcizx"},{"type":"heading","depth":3,"position":{"start":{"line":198,"column":1},"end":{"line":198,"column":1}},"children":[{"type":"text","value":"Advanced Sampling Techniques","position":{"start":{"line":198,"column":1},"end":{"line":198,"column":1}},"key":"mP6Ub4xTH7"}],"identifier":"advanced-sampling-techniques","label":"Advanced Sampling Techniques","html_id":"advanced-sampling-techniques","implicit":true,"key":"YmG7CrNtnV"},{"type":"code","lang":"python","value":"def advanced_blackjax():\n    \"\"\"Advanced sampling techniques with BlackJAX.\"\"\"\n    \n    print(\"\\nADVANCED BLACKJAX TECHNIQUES\")\n    print(\"=\" * 50)\n    \n    # 1. Riemannian HMC\n    print(\"\\n1. RIEMANNIAN HMC:\")\n    \n    def log_prob_with_metric(theta):\n        \"\"\"Log probability with position-dependent metric.\"\"\"\n        # Stellar population synthesis parameters\n        age, metallicity = theta\n        \n        # Mock observables\n        color_obs = 0.8\n        magnitude_obs = -2.0\n        \n        # Model predictions (simplified)\n        color_model = 0.5 + 0.1 * age - 0.2 * metallicity\n        mag_model = -3.0 + 0.2 * age + 0.3 * metallicity\n        \n        # Position-dependent uncertainties\n        sigma_color = 0.1 * (1 + 0.1 * jnp.abs(age))\n        sigma_mag = 0.2 * (1 + 0.05 * jnp.abs(metallicity))\n        \n        log_like = -0.5 * (\n            ((color_obs - color_model) / sigma_color) ** 2 +\n            ((magnitude_obs - mag_model) / sigma_mag) ** 2\n        )\n        \n        # Priors\n        log_prior = -0.5 * (age ** 2 / 100 + metallicity ** 2 / 4)\n        \n        return log_like + log_prior\n    \n    # Metric tensor (Fisher information)\n    def metric_fn(theta):\n        \"\"\"Compute metric tensor at position.\"\"\"\n        hess = jax.hessian(log_prob_with_metric)(theta)\n        return -hess  # Fisher information\n    \n    # Would implement full Riemannian HMC here\n    print(\"  Riemannian HMC configured for curved parameter space\")\n    \n    # 2. Parallel tempering\n    print(\"\\n2. PARALLEL TEMPERING:\")\n    \n    def parallel_tempering(log_prob, num_chains=4, num_samples=1000):\n        \"\"\"Parallel tempering MCMC.\"\"\"\n        \n        # Temperature ladder\n        betas = jnp.array([1.0, 0.5, 0.25, 0.1])\n        \n        def tempered_log_prob(theta, beta):\n            return beta * log_prob(theta)\n        \n        # Initialize chains at different temperatures\n        key = random.PRNGKey(42)\n        keys = random.split(key, num_chains + 1)\n        \n        initial_positions = jnp.array([\n            [0.3 + 0.05 * i, 70.0 + 2.0 * i] \n            for i in range(num_chains)\n        ])\n        \n        # Build kernels for each temperature\n        kernels = []\n        states = []\n        for i, beta in enumerate(betas):\n            kernel = blackjax.hmc(\n                lambda theta: tempered_log_prob(theta, beta),\n                step_size=0.01 / jnp.sqrt(beta),  # Adjust step size\n                inverse_mass_matrix=jnp.ones(2),\n                num_integration_steps=10\n            )\n            kernels.append(kernel)\n            states.append(kernel.init(initial_positions[i]))\n        \n        # Sampling with swaps\n        @jit\n        def swap_step(states, key):\n            \"\"\"Propose and accept/reject swaps.\"\"\"\n            swap_key, accept_key = random.split(key)\n            \n            # Random pair to swap\n            pair = random.choice(swap_key, num_chains - 1)\n            \n            # Compute swap acceptance probability\n            theta_i = states[pair].position\n            theta_j = states[pair + 1].position\n            \n            log_prob_i = tempered_log_prob(theta_i, betas[pair])\n            log_prob_j = tempered_log_prob(theta_j, betas[pair + 1])\n            \n            log_prob_i_swap = tempered_log_prob(theta_j, betas[pair])\n            log_prob_j_swap = tempered_log_prob(theta_i, betas[pair + 1])\n            \n            log_alpha = (log_prob_i_swap + log_prob_j_swap - \n                        log_prob_i - log_prob_j)\n            \n            # Accept/reject\n            accept = random.uniform(accept_key) < jnp.exp(log_alpha)\n            \n            # Swap if accepted (simplified - would use lax.cond)\n            return states, accept\n        \n        print(f\"  Parallel tempering with {num_chains} chains\")\n        print(f\"  Temperatures: {1/betas}\")\n        \n        return states\n    \n    # Example cosmology log prob\n    def cosmo_log_prob(theta):\n        omega_m, h0 = theta\n        if 0 < omega_m < 1 and 50 < h0 < 100:\n            return -0.5 * ((omega_m - 0.3)**2 / 0.01 + (h0 - 70)**2 / 25)\n        return -jnp.inf\n    \n    pt_states = parallel_tempering(cosmo_log_prob)\n    \n    # 3. Ensemble samplers\n    print(\"\\n3. ENSEMBLE SAMPLERS:\")\n    \n    def affine_invariant_ensemble_sampler(log_prob, num_walkers=32):\n        \"\"\"Affine invariant ensemble sampler (like emcee).\"\"\"\n        \n        key = random.PRNGKey(99)\n        ndim = 2\n        \n        # Initialize walkers\n        initial_positions = random.normal(key, (num_walkers, ndim))\n        \n        @jit\n        def stretch_move(positions, key):\n            \"\"\"Stretch move for ensemble sampler.\"\"\"\n            n_walkers = len(positions)\n            keys = random.split(key, n_walkers + 2)\n            \n            # Random pairs\n            pairs = random.choice(keys[0], n_walkers, (n_walkers,))\n            \n            # Stretch factors\n            a = 2.0  # Stretch scale\n            z = ((a - 1) * random.uniform(keys[1], (n_walkers,)) + 1) ** 2 / a\n            \n            # Propose new positions\n            proposals = positions[pairs] + z[:, None] * (positions - positions[pairs])\n            \n            # Compute acceptance\n            log_probs_old = vmap(log_prob)(positions)\n            log_probs_new = vmap(log_prob)(proposals)\n            \n            log_alpha = (ndim - 1) * jnp.log(z) + log_probs_new - log_probs_old\n            \n            # Accept/reject\n            accept = random.uniform(keys[2], (n_walkers,)) < jnp.exp(log_alpha)\n            \n            new_positions = jnp.where(accept[:, None], proposals, positions)\n            \n            return new_positions, accept\n        \n        # Run ensemble\n        positions = initial_positions\n        for i in range(100):\n            key, step_key = random.split(key)\n            positions, accept = stretch_move(positions, step_key)\n        \n        print(f\"  Ensemble sampler with {num_walkers} walkers\")\n        print(f\"  Mean position: {jnp.mean(positions, axis=0)}\")\n        \n        return positions\n    \n    ensemble_samples = affine_invariant_ensemble_sampler(cosmo_log_prob)\n\nadvanced_blackjax()","position":{"start":{"line":200,"column":1},"end":{"line":377,"column":1}},"key":"fnXx8oBPCh"},{"type":"heading","depth":2,"position":{"start":{"line":379,"column":1},"end":{"line":379,"column":1}},"children":[{"type":"text","value":"Numpyro: Probabilistic Programming","position":{"start":{"line":379,"column":1},"end":{"line":379,"column":1}},"key":"C3xre5Il8w"}],"identifier":"numpyro-probabilistic-programming","label":"Numpyro: Probabilistic Programming","html_id":"numpyro-probabilistic-programming","implicit":true,"key":"FsCjK3YvpI"},{"type":"heading","depth":3,"position":{"start":{"line":381,"column":1},"end":{"line":381,"column":1}},"children":[{"type":"text","value":"Building Probabilistic Models","position":{"start":{"line":381,"column":1},"end":{"line":381,"column":1}},"key":"oGlYARbw4E"}],"identifier":"building-probabilistic-models","label":"Building Probabilistic Models","html_id":"building-probabilistic-models","implicit":true,"key":"wjDs56ecsA"},{"type":"code","lang":"python","value":"import numpyro\nimport numpyro.distributions as dist\nfrom numpyro.infer import MCMC, NUTS, SVI, Trace_ELBO\nfrom numpyro.infer.autoguide import AutoNormal\n\ndef numpyro_fundamentals():\n    \"\"\"Probabilistic programming with Numpyro.\"\"\"\n    \n    print(\"\\nNUMPYRO: PROBABILISTIC PROGRAMMING\")\n    print(\"=\" * 50)\n    \n    # 1. Basic hierarchical model\n    print(\"\\n1. HIERARCHICAL MODEL - CEPHEID CALIBRATION:\")\n    \n    def cepheid_model(periods, magnitudes=None):\n        \"\"\"\n        Hierarchical model for Cepheid period-luminosity relation.\n        Different galaxies have different zero points.\n        \"\"\"\n        n_obs = len(periods)\n        \n        # Hyperpriors for P-L relation\n        alpha = numpyro.sample('alpha', dist.Normal(-2.5, 0.5))\n        beta = numpyro.sample('beta', dist.Normal(-3.0, 0.5))\n        \n        # Intrinsic scatter\n        sigma_int = numpyro.sample('sigma_int', dist.HalfNormal(0.1))\n        \n        # Galaxy-specific zero points (distance moduli)\n        # Assume we have galaxy IDs\n        n_galaxies = 5\n        galaxy_ids = jnp.array([i % n_galaxies for i in range(n_obs)])\n        \n        with numpyro.plate('galaxies', n_galaxies):\n            mu_gal = numpyro.sample('mu_gal', dist.Normal(30.0, 2.0))\n        \n        # Expected magnitude\n        log_P = jnp.log10(periods)\n        M_expected = alpha * log_P + beta  # Absolute magnitude\n        m_expected = M_expected + mu_gal[galaxy_ids]  # Apparent magnitude\n        \n        # Observational uncertainty\n        sigma_obs = 0.05\n        sigma_total = jnp.sqrt(sigma_int**2 + sigma_obs**2)\n        \n        # Likelihood\n        with numpyro.plate('observations', n_obs):\n            numpyro.sample('magnitudes', \n                          dist.Normal(m_expected, sigma_total),\n                          obs=magnitudes)\n    \n    # Generate synthetic data\n    key = random.PRNGKey(0)\n    n_cepheids = 100\n    true_alpha = -2.43\n    true_beta = -3.05\n    \n    periods = jnp.exp(random.uniform(key, (n_cepheids,), minval=0, maxval=3))\n    galaxy_ids = jnp.array([i % 5 for i in range(n_cepheids)])\n    true_mu = jnp.array([29.5, 30.0, 30.5, 31.0, 31.5])\n    \n    log_P = jnp.log10(periods)\n    M_true = true_alpha * log_P + true_beta\n    m_true = M_true + true_mu[galaxy_ids]\n    \n    key, noise_key = random.split(key)\n    observed_mags = m_true + 0.05 * random.normal(noise_key, (n_cepheids,))\n    \n    # Run MCMC\n    nuts_kernel = NUTS(cepheid_model)\n    mcmc = MCMC(nuts_kernel, num_warmup=500, num_samples=1000)\n    \n    key, run_key = random.split(key)\n    mcmc.run(run_key, periods, magnitudes=observed_mags)\n    \n    samples = mcmc.get_samples()\n    print(f\"  α = {jnp.mean(samples['alpha']):.3f} ± {jnp.std(samples['alpha']):.3f}\")\n    print(f\"  β = {jnp.mean(samples['beta']):.3f} ± {jnp.std(samples['beta']):.3f}\")\n    \n    # 2. Mixture models\n    print(\"\\n2. MIXTURE MODEL - STELLAR POPULATIONS:\")\n    \n    def stellar_population_mixture(colors=None, magnitudes=None):\n        \"\"\"\n        Mixture model for stellar populations.\n        Main sequence, giants, and white dwarfs.\n        \"\"\"\n        n_obs = len(colors) if colors is not None else 100\n        \n        # Mixture weights\n        weights = numpyro.sample('weights', dist.Dirichlet(jnp.ones(3)))\n        \n        # Component parameters\n        with numpyro.plate('components', 3):\n            mu_color = numpyro.sample('mu_color', dist.Normal(0.0, 2.0))\n            mu_mag = numpyro.sample('mu_mag', dist.Normal(0.0, 5.0))\n            sigma_color = numpyro.sample('sigma_color', dist.HalfNormal(0.5))\n            sigma_mag = numpyro.sample('sigma_mag', dist.HalfNormal(1.0))\n        \n        # Mixture assignment\n        with numpyro.plate('stars', n_obs):\n            assignment = numpyro.sample('assignment', dist.Categorical(weights))\n            \n            # Observed color and magnitude\n            numpyro.sample('colors',\n                          dist.Normal(mu_color[assignment], sigma_color[assignment]),\n                          obs=colors)\n            numpyro.sample('magnitudes',\n                          dist.Normal(mu_mag[assignment], sigma_mag[assignment]),\n                          obs=magnitudes)\n    \n    # Generate synthetic CMD data\n    key, data_key = random.split(key)\n    n_stars = 200\n    \n    # True mixture\n    true_weights = jnp.array([0.7, 0.2, 0.1])  # MS, Giants, WDs\n    true_mu_color = jnp.array([0.5, 1.5, -0.5])\n    true_mu_mag = jnp.array([5.0, 0.0, 10.0])\n    \n    assignments = random.categorical(data_key, jnp.log(true_weights), shape=(n_stars,))\n    \n    keys = random.split(data_key, 3)\n    obs_colors = true_mu_color[assignments] + 0.2 * random.normal(keys[0], (n_stars,))\n    obs_mags = true_mu_mag[assignments] + 0.5 * random.normal(keys[1], (n_stars,))\n    \n    print(\"  Mixture model for CMD analysis configured\")\n    \n    # 3. Time series model\n    print(\"\\n3. TIME SERIES - QUASAR VARIABILITY:\")\n    \n    def quasar_drw_model(times, fluxes=None):\n        \"\"\"\n        Damped Random Walk model for quasar variability.\n        \"\"\"\n        n_obs = len(times)\n        dt = jnp.diff(times)\n        \n        # DRW parameters\n        tau = numpyro.sample('tau', dist.LogNormal(jnp.log(100), 0.5))\n        sigma = numpyro.sample('sigma', dist.HalfNormal(0.1))\n        \n        # Mean flux\n        mean_flux = numpyro.sample('mean_flux', dist.Normal(0, 1))\n        \n        # Initial state\n        flux_0 = numpyro.sample('flux_0', dist.Normal(mean_flux, sigma))\n        \n        # Evolution\n        def transition(carry, dt):\n            flux_prev = carry\n            \n            # DRW transition\n            decay = jnp.exp(-dt / tau)\n            noise_var = sigma**2 * (1 - decay**2)\n            \n            flux_next = numpyro.sample('flux_next',\n                                       dist.Normal(mean_flux + decay * (flux_prev - mean_flux),\n                                                  jnp.sqrt(noise_var)))\n            return flux_next, flux_next\n        \n        # Scan through time series\n        _, flux_trajectory = jax.lax.scan(transition, flux_0, dt)\n        flux_trajectory = jnp.concatenate([jnp.array([flux_0]), flux_trajectory])\n        \n        # Observational uncertainty\n        obs_error = 0.01\n        \n        with numpyro.plate('observations', n_obs):\n            numpyro.sample('fluxes',\n                          dist.Normal(flux_trajectory, obs_error),\n                          obs=fluxes)\n    \n    print(\"  DRW model for quasar variability created\")\n\nnumpyro_fundamentals()","position":{"start":{"line":383,"column":1},"end":{"line":560,"column":1}},"key":"d7f6ZtiPJH"},{"type":"heading","depth":3,"position":{"start":{"line":562,"column":1},"end":{"line":562,"column":1}},"children":[{"type":"text","value":"Variational Inference with Numpyro","position":{"start":{"line":562,"column":1},"end":{"line":562,"column":1}},"key":"S1IkDwvpfs"}],"identifier":"variational-inference-with-numpyro","label":"Variational Inference with Numpyro","html_id":"variational-inference-with-numpyro","implicit":true,"key":"nfCOxNmlsE"},{"type":"code","lang":"python","value":"def numpyro_variational_inference():\n    \"\"\"Variational inference for large-scale problems.\"\"\"\n    \n    print(\"\\nVARIATIONAL INFERENCE WITH NUMPYRO\")\n    print(\"=\" * 50)\n    \n    # 1. SVI for galaxy clustering\n    print(\"\\n1. SVI FOR GALAXY CLUSTERING:\")\n    \n    def galaxy_clustering_model(positions, redshifts=None):\n        \"\"\"\n        Hierarchical model for galaxy clustering.\n        \"\"\"\n        n_galaxies = len(positions)\n        \n        # Cosmological parameters\n        omega_m = numpyro.sample('omega_m', dist.Uniform(0.1, 0.5))\n        sigma_8 = numpyro.sample('sigma_8', dist.Uniform(0.5, 1.0))\n        \n        # Bias parameters for different galaxy types\n        n_types = 3\n        with numpyro.plate('galaxy_types', n_types):\n            bias = numpyro.sample('bias', dist.LogNormal(0, 0.5))\n        \n        # Galaxy type assignments (latent)\n        with numpyro.plate('galaxies', n_galaxies):\n            galaxy_type = numpyro.sample('galaxy_type',\n                                         dist.Categorical(jnp.ones(n_types) / n_types))\n            \n            # Simplified clustering likelihood\n            # In reality, would use correlation function\n            clustering_strength = bias[galaxy_type] * sigma_8\n            \n            # Observed redshift (with peculiar velocities)\n            if redshifts is not None:\n                z_cosmo = jnp.linalg.norm(positions, axis=1) / 3000  # Hubble flow\n                z_pec = clustering_strength * random.normal(random.PRNGKey(0), (n_galaxies,)) * 0.001\n                \n                numpyro.sample('redshifts',\n                              dist.Normal(z_cosmo + z_pec, 0.0001),\n                              obs=redshifts)\n    \n    # Generate mock data\n    key = random.PRNGKey(42)\n    n_gal = 1000\n    positions = random.normal(key, (n_gal, 3)) * 100\n    redshifts = jnp.linalg.norm(positions, axis=1) / 3000 + \\\n                0.001 * random.normal(key, (n_gal,))\n    \n    # Setup SVI\n    guide = AutoNormal(galaxy_clustering_model)\n    optimizer = numpyro.optim.Adam(step_size=0.01)\n    svi = SVI(galaxy_clustering_model, guide, optimizer, loss=Trace_ELBO())\n    \n    # Run optimization\n    svi_result = svi.run(random.PRNGKey(0), 1000, positions, redshifts)\n    \n    params = svi_result.params\n    print(f\"  SVI converged with loss: {svi_result.losses[-1]:.2f}\")\n    \n    # 2. Custom guides\n    print(\"\\n2. CUSTOM VARIATIONAL GUIDES:\")\n    \n    def custom_guide(positions, redshifts=None):\n        \"\"\"Custom guide for more control.\"\"\"\n        n_galaxies = len(positions)\n        \n        # Cosmological parameters with custom distributions\n        omega_m_loc = numpyro.param('omega_m_loc', 0.3)\n        omega_m_scale = numpyro.param('omega_m_scale', 0.01,\n                                      constraint=dist.constraints.positive)\n        omega_m = numpyro.sample('omega_m', dist.Normal(omega_m_loc, omega_m_scale))\n        \n        sigma_8_loc = numpyro.param('sigma_8_loc', 0.8)\n        sigma_8_scale = numpyro.param('sigma_8_scale', 0.01,\n                                      constraint=dist.constraints.positive)\n        sigma_8 = numpyro.sample('sigma_8', dist.Normal(sigma_8_loc, sigma_8_scale))\n        \n        # Galaxy bias parameters\n        with numpyro.plate('galaxy_types', 3):\n            bias_loc = numpyro.param('bias_loc', jnp.ones(3))\n            bias_scale = numpyro.param('bias_scale', 0.1 * jnp.ones(3),\n                                       constraint=dist.constraints.positive)\n            numpyro.sample('bias', dist.LogNormal(bias_loc, bias_scale))\n        \n        # Galaxy type assignments (discrete - use Gumbel-softmax)\n        with numpyro.plate('galaxies', n_galaxies):\n            type_logits = numpyro.param('type_logits', jnp.zeros((n_galaxies, 3)))\n            numpyro.sample('galaxy_type', dist.CategoricalLogits(type_logits))\n    \n    print(\"  Custom variational guide configured\")\n    \n    # 3. Normalizing flows\n    print(\"\\n3. NORMALIZING FLOWS FOR COMPLEX POSTERIORS:\")\n    \n    from numpyro.infer.autoguide import AutoIAFNormal\n    \n    def complex_posterior_model(data=None):\n        \"\"\"Model with complex, multimodal posterior.\"\"\"\n        # Bimodal distribution\n        mode = numpyro.sample('mode', dist.Bernoulli(0.5))\n        \n        # Different parameters for each mode\n        with numpyro.handlers.mask(mask=mode):\n            theta1 = numpyro.sample('theta1', dist.Normal(2.0, 0.5))\n        \n        with numpyro.handlers.mask(mask=~mode):\n            theta2 = numpyro.sample('theta2', dist.Normal(-2.0, 0.5))\n        \n        # Combine\n        theta = jnp.where(mode, theta1, theta2)\n        \n        # Likelihood\n        numpyro.sample('data', dist.Normal(theta, 0.1), obs=data)\n    \n    # Use IAF (Inverse Autoregressive Flow)\n    flow_guide = AutoIAFNormal(complex_posterior_model, num_flows=3)\n    \n    print(\"  Normalizing flow guide created for complex posteriors\")\n\nnumpyro_variational_inference()","position":{"start":{"line":564,"column":1},"end":{"line":686,"column":1}},"key":"qfmiR9pUei"},{"type":"heading","depth":2,"position":{"start":{"line":688,"column":1},"end":{"line":688,"column":1}},"children":[{"type":"text","value":"JAXopt: Constrained Optimization","position":{"start":{"line":688,"column":1},"end":{"line":688,"column":1}},"key":"sz9HdqDAAe"}],"identifier":"jaxopt-constrained-optimization","label":"JAXopt: Constrained Optimization","html_id":"jaxopt-constrained-optimization","implicit":true,"key":"dEnNAVILtP"},{"type":"heading","depth":3,"position":{"start":{"line":690,"column":1},"end":{"line":690,"column":1}},"children":[{"type":"text","value":"Optimization with Constraints","position":{"start":{"line":690,"column":1},"end":{"line":690,"column":1}},"key":"XOwNDRmB9x"}],"identifier":"optimization-with-constraints","label":"Optimization with Constraints","html_id":"optimization-with-constraints","implicit":true,"key":"ISp4WCivbY"},{"type":"code","lang":"python","value":"import jaxopt\n\ndef jaxopt_optimization():\n    \"\"\"Constrained optimization for astronomical problems.\"\"\"\n    \n    print(\"\\nJAXOPT: CONSTRAINED OPTIMIZATION\")\n    print(\"=\" * 50)\n    \n    # 1. Constrained least squares\n    print(\"\\n1. CONSTRAINED ORBIT FITTING:\")\n    \n    def orbit_objective(params, observations):\n        \"\"\"Objective for orbit fitting.\"\"\"\n        a, e, i, omega = params\n        times, ra_obs, dec_obs = observations\n        \n        # Kepler orbit (simplified)\n        M = 2 * jnp.pi * times / (a ** 1.5)\n        E = M  # Small eccentricity approximation\n        \n        # Projected positions\n        x = a * (jnp.cos(E) - e)\n        y = a * jnp.sqrt(1 - e**2) * jnp.sin(E)\n        \n        # Rotate by inclination and argument\n        ra_model = x * jnp.cos(omega) - y * jnp.sin(omega) * jnp.cos(i)\n        dec_model = x * jnp.sin(omega) + y * jnp.cos(omega) * jnp.cos(i)\n        \n        # Residuals\n        residuals = jnp.sum((ra_obs - ra_model)**2 + (dec_obs - dec_model)**2)\n        return residuals\n    \n    # Constraints: physical orbital elements\n    def orbit_constraints(params):\n        a, e, i, omega = params\n        return jnp.array([\n            a - 0.1,      # a > 0.1 AU\n            10.0 - a,     # a < 10 AU\n            e,            # e >= 0\n            1.0 - e,      # e < 1\n            i,            # i >= 0\n            jnp.pi - i,   # i <= pi\n        ])\n    \n    # Generate mock observations\n    key = random.PRNGKey(123)\n    times = jnp.linspace(0, 10, 50)\n    true_params = jnp.array([2.0, 0.3, 0.5, 1.0])\n    \n    # Mock data (using true model)\n    M_true = 2 * jnp.pi * times / (true_params[0] ** 1.5)\n    x_true = true_params[0] * jnp.cos(M_true)\n    y_true = true_params[0] * jnp.sin(M_true) * jnp.sqrt(1 - true_params[1]**2)\n    \n    ra_obs = x_true + 0.01 * random.normal(key, x_true.shape)\n    dec_obs = y_true + 0.01 * random.normal(key, y_true.shape)\n    observations = (times, ra_obs, dec_obs)\n    \n    # Solve with constraints\n    initial_params = jnp.array([1.5, 0.2, 0.4, 0.8])\n    \n    solver = jaxopt.ProjectedGradient(\n        fun=lambda p: orbit_objective(p, observations),\n        projection=jaxopt.projection.projection_box,\n        maxiter=100\n    )\n    \n    # Box constraints\n    lower_bounds = jnp.array([0.1, 0.0, 0.0, 0.0])\n    upper_bounds = jnp.array([10.0, 0.99, jnp.pi, 2*jnp.pi])\n    \n    result = solver.run(initial_params, \n                        hyperparams_proj=(lower_bounds, upper_bounds))\n    \n    print(f\"  Optimization converged: {result.state.error < 1e-3}\")\n    print(f\"  True params: {true_params}\")\n    print(f\"  Fitted params: {result.params}\")\n    \n    # 2. Quadratic programming\n    print(\"\\n2. QUADRATIC PROGRAMMING - TELESCOPE SCHEDULING:\")\n    \n    def telescope_scheduling():\n        \"\"\"Optimize telescope observation schedule.\"\"\"\n        \n        # Problem: maximize scientific value subject to constraints\n        # Variables: time allocated to each target\n        n_targets = 10\n        \n        # Scientific value (priority) of each target\n        values = jnp.array([9, 8, 7, 6, 5, 5, 4, 3, 2, 1])\n        \n        # Observability windows (hours)\n        max_observable = jnp.array([3, 4, 2, 5, 3, 2, 4, 3, 2, 1])\n        \n        # Quadratic objective (negative for maximization)\n        # Include diversity bonus for observing multiple targets\n        Q = -jnp.eye(n_targets) * 0.1  # Diversity penalty\n        c = -values  # Linear term (negative for maximization)\n        \n        # Constraints: Ax <= b\n        # Total time constraint\n        A_time = jnp.ones((1, n_targets))\n        b_time = jnp.array([8.0])  # 8 hours total\n        \n        # Combine with observability constraints\n        A_obs = jnp.eye(n_targets)\n        b_obs = max_observable\n        \n        A = jnp.vstack([A_time, A_obs])\n        b = jnp.concatenate([b_time, b_obs])\n        \n        # Also need non-negativity: x >= 0\n        \n        # Solve QP\n        qp_solver = jaxopt.QuadraticProgramming()\n        \n        # Initial guess\n        x_init = jnp.ones(n_targets) * 0.5\n        \n        solution = qp_solver.run(x_init, params_obj=(Q, c),\n                                 params_ineq=(A, b))\n        \n        schedule = solution.params\n        \n        print(f\"  Telescope schedule optimized\")\n        print(f\"  Total time used: {jnp.sum(schedule):.1f} hours\")\n        print(f\"  Targets observed: {jnp.sum(schedule > 0.01)}\")\n        print(f\"  Scientific value: {jnp.dot(values, schedule):.1f}\")\n        \n        return schedule\n    \n    schedule = telescope_scheduling()\n    \n    # 3. Proximal gradient methods\n    print(\"\\n3. PROXIMAL METHODS - SPARSE DECONVOLUTION:\")\n    \n    def sparse_deconvolution():\n        \"\"\"Deconvolve image with sparsity constraint.\"\"\"\n        \n        # Create blurred image\n        key = random.PRNGKey(456)\n        true_image = jnp.zeros((50, 50))\n        \n        # Add point sources\n        n_sources = 10\n        positions = random.choice(key, 50*50, (n_sources,), replace=False)\n        true_image = true_image.flatten()\n        true_image = true_image.at[positions].set(\n            random.uniform(key, (n_sources,), minval=0.5, maxval=2.0)\n        )\n        true_image = true_image.reshape(50, 50)\n        \n        # Blur with PSF\n        def create_psf(size=5):\n            x = jnp.arange(size) - size // 2\n            X, Y = jnp.meshgrid(x, x)\n            psf = jnp.exp(-(X**2 + Y**2) / 2)\n            return psf / jnp.sum(psf)\n        \n        psf = create_psf()\n        \n        # Convolve (simplified - using scipy in practice)\n        blurred = jax.scipy.signal.convolve2d(true_image, psf, mode='same')\n        \n        # Add noise\n        observed = blurred + 0.01 * random.normal(key, blurred.shape)\n        \n        # Deconvolution with L1 regularization for sparsity\n        def objective(x):\n            # Data fidelity\n            convolved = jax.scipy.signal.convolve2d(x.reshape(50, 50), psf, mode='same')\n            fidelity = 0.5 * jnp.sum((convolved - observed) ** 2)\n            \n            # L1 regularization\n            sparsity = 0.01 * jnp.sum(jnp.abs(x))\n            \n            return fidelity + sparsity\n        \n        # Use proximal gradient\n        prox_solver = jaxopt.ProximalGradient(\n            fun=objective,\n            prox=jaxopt.prox.prox_lasso,\n            maxiter=100\n        )\n        \n        x_init = observed.flatten()\n        result = prox_solver.run(x_init)\n        \n        deconvolved = result.params.reshape(50, 50)\n        \n        print(f\"  Sparse deconvolution completed\")\n        print(f\"  Sparsity: {jnp.sum(jnp.abs(deconvolved) > 0.01)} non-zero pixels\")\n        \n        return deconvolved\n    \n    deconvolved_image = sparse_deconvolution()\n\njaxopt_optimization()","position":{"start":{"line":692,"column":1},"end":{"line":891,"column":1}},"key":"i9WonhPMqo"},{"type":"heading","depth":2,"position":{"start":{"line":893,"column":1},"end":{"line":893,"column":1}},"children":[{"type":"text","value":"Domain-Specific Libraries","position":{"start":{"line":893,"column":1},"end":{"line":893,"column":1}},"key":"w0Fh93XSX2"}],"identifier":"domain-specific-libraries","label":"Domain-Specific Libraries","html_id":"domain-specific-libraries","implicit":true,"key":"QSIDav8AmN"},{"type":"heading","depth":3,"position":{"start":{"line":895,"column":1},"end":{"line":895,"column":1}},"children":[{"type":"text","value":"Astronomical JAX Tools","position":{"start":{"line":895,"column":1},"end":{"line":895,"column":1}},"key":"GAHYy6CQ6p"}],"identifier":"astronomical-jax-tools","label":"Astronomical JAX Tools","html_id":"astronomical-jax-tools","implicit":true,"key":"NQwDpCfdKv"},{"type":"code","lang":"python","value":"def astronomical_jax_libraries():\n    \"\"\"Specialized JAX libraries for astronomy.\"\"\"\n    \n    print(\"\\nASTRONOMICAL JAX LIBRARIES\")\n    print(\"=\" * 50)\n    \n    # 1. s2fft - Spherical harmonic transforms\n    print(\"\\n1. S2FFT - SPHERICAL HARMONICS FOR CMB:\")\n    \n    # Note: This is conceptual - s2fft would need to be installed\n    def cmb_analysis():\n        \"\"\"Analyze CMB maps using spherical harmonics.\"\"\"\n        \n        # Generate mock CMB map\n        nside = 64  # HEALPix resolution\n        npix = 12 * nside**2\n        \n        key = random.PRNGKey(789)\n        \n        # Generate random alm coefficients (simplified)\n        lmax = 100\n        n_alm = (lmax + 1) * (lmax + 2) // 2\n        \n        # Power spectrum (simplified)\n        ell = jnp.arange(lmax + 1)\n        Cl = 100 / (ell + 10) ** 2  # Simplified CMB spectrum\n        \n        # Random realization\n        alm_real = jnp.sqrt(Cl[None, :]) * random.normal(key, (1, lmax + 1))\n        \n        print(f\"  CMB analysis with lmax={lmax}\")\n        print(f\"  Power spectrum computed\")\n        \n        # Would use s2fft for actual transforms\n        # import s2fft\n        # map_data = s2fft.inverse_transform(alm_real, L=lmax)\n        \n        return alm_real\n    \n    cmb_alm = cmb_analysis()\n    \n    # 2. jax-cosmo - Cosmological calculations\n    print(\"\\n2. JAX-COSMO - COSMOLOGICAL CALCULATIONS:\")\n    \n    def cosmological_calculations():\n        \"\"\"Differentiable cosmology calculations.\"\"\"\n        \n        # Cosmological parameters\n        cosmo_params = {\n            'omega_m': 0.3,\n            'omega_de': 0.7,\n            'h': 0.7,\n            'omega_b': 0.05,\n            'n_s': 0.96,\n            'sigma_8': 0.8\n        }\n        \n        # Redshifts\n        z = jnp.linspace(0, 2, 100)\n        \n        # Comoving distance (simplified Friedmann equation)\n        def comoving_distance(z, omega_m, omega_de):\n            def integrand(z):\n                return 1 / jnp.sqrt(omega_m * (1 + z)**3 + omega_de)\n            \n            # Integrate (simplified - use quadrature in practice)\n            dz = z[1] - z[0]\n            integral = jnp.cumsum(vmap(integrand)(z)) * dz\n            \n            c_over_H0 = 3000 / 0.7  # Mpc\n            return c_over_H0 * integral\n        \n        d_c = comoving_distance(z, cosmo_params['omega_m'], cosmo_params['omega_de'])\n        \n        # Angular diameter distance\n        d_a = d_c / (1 + z)\n        \n        # Luminosity distance\n        d_l = d_c * (1 + z)\n        \n        print(f\"  Computed distances for {len(z)} redshifts\")\n        print(f\"  Max comoving distance: {d_c[-1]:.0f} Mpc\")\n        \n        # Growth function (simplified)\n        def growth_factor(z, omega_m):\n            a = 1 / (1 + z)\n            # Approximate growth factor\n            omega_m_z = omega_m / (omega_m + (1 - omega_m) * a**3)\n            return a * omega_m_z**0.55\n        \n        D_growth = vmap(lambda zi: growth_factor(zi, cosmo_params['omega_m']))(z)\n        \n        print(f\"  Growth factor at z=0: {D_growth[0]:.3f}\")\n        \n        return d_c, d_a, d_l, D_growth\n    \n    distances = cosmological_calculations()\n    \n    # 3. Neural density estimation\n    print(\"\\n3. NEURAL DENSITY ESTIMATION FOR INFERENCE:\")\n    \n    def neural_posterior_estimation():\n        \"\"\"Use neural networks for likelihood-free inference.\"\"\"\n        \n        import flax.linen as nn\n        \n        class PosteriorNetwork(nn.Module):\n            \"\"\"Neural network to approximate posterior.\"\"\"\n            \n            @nn.compact\n            def __call__(self, data, params):\n                # Concatenate data and parameters\n                x = jnp.concatenate([data, params])\n                \n                # Deep network\n                x = nn.Dense(128)(x)\n                x = nn.relu(x)\n                x = nn.Dense(128)(x)\n                x = nn.relu(x)\n                x = nn.Dense(64)(x)\n                x = nn.relu(x)\n                \n                # Output: parameters of posterior distribution\n                mean = nn.Dense(len(params))(x)\n                log_std = nn.Dense(len(params))(x)\n                \n                return mean, log_std\n        \n        print(\"  Neural posterior estimator configured\")\n        \n        # Would train on simulated data\n        # This enables likelihood-free inference for complex simulators\n        \n        return PosteriorNetwork()\n    \n    npe_model = neural_posterior_estimation()\n\nastronomical_jax_libraries()","position":{"start":{"line":897,"column":1},"end":{"line":1036,"column":1}},"key":"c4xcenIOJ0"},{"type":"heading","depth":2,"position":{"start":{"line":1038,"column":1},"end":{"line":1038,"column":1}},"children":[{"type":"text","value":"Complete Example: Gravitational Wave Analysis","position":{"start":{"line":1038,"column":1},"end":{"line":1038,"column":1}},"key":"YL3C4cqthH"}],"identifier":"complete-example-gravitational-wave-analysis","label":"Complete Example: Gravitational Wave Analysis","html_id":"complete-example-gravitational-wave-analysis","implicit":true,"key":"D4oAC0ABpn"},{"type":"heading","depth":3,"position":{"start":{"line":1040,"column":1},"end":{"line":1040,"column":1}},"children":[{"type":"text","value":"End-to-End GW Pipeline","position":{"start":{"line":1040,"column":1},"end":{"line":1040,"column":1}},"key":"EvV1G8LXMd"}],"identifier":"end-to-end-gw-pipeline","label":"End-to-End GW Pipeline","html_id":"end-to-end-gw-pipeline","implicit":true,"key":"z0TDC5hb7V"},{"type":"code","lang":"python","value":"def gravitational_wave_pipeline():\n    \"\"\"\n    Complete gravitational wave analysis pipeline.\n    Combines multiple JAX libraries for production analysis.\n    \"\"\"\n    \n    print(\"\\nGRAVITATIONAL WAVE ANALYSIS PIPELINE\")\n    print(\"=\" * 50)\n    \n    # 1. Generate GW signal\n    def generate_gw_signal(params, times):\n        \"\"\"Generate gravitational wave signal.\"\"\"\n        m1, m2, d_l, t_c, phi_c = params\n        \n        # Chirp mass\n        M_chirp = (m1 * m2) ** (3/5) / (m1 + m2) ** (1/5)\n        \n        # Frequency evolution (simplified)\n        t_to_merger = t_c - times\n        f_gw = jnp.where(\n            t_to_merger > 0,\n            (M_chirp / t_to_merger) ** (3/8),\n            0.0\n        )\n        \n        # Amplitude\n        amplitude = (M_chirp ** (5/3) * f_gw ** (2/3)) / d_l\n        \n        # Phase\n        phase = 2 * jnp.pi * jnp.cumsum(f_gw) * 0.001 + phi_c\n        \n        # Strain\n        h_plus = amplitude * jnp.cos(phase)\n        h_cross = amplitude * jnp.sin(phase)\n        \n        return h_plus, h_cross\n    \n    # 2. Detector response\n    def detector_response(h_plus, h_cross, detector_params):\n        \"\"\"Compute detector response to GW signal.\"\"\"\n        F_plus, F_cross, psi = detector_params\n        \n        strain = F_plus * h_plus + F_cross * h_cross\n        return strain\n    \n    # 3. Likelihood function\n    def log_likelihood(params, data, times, noise_psd):\n        \"\"\"Log likelihood for GW parameters.\"\"\"\n        h_plus, h_cross = generate_gw_signal(params, times)\n        \n        # Detector response (simplified - single detector)\n        detector_params = (0.5, 0.5, 0.0)  # Antenna patterns\n        h_model = detector_response(h_plus, h_cross, detector_params)\n        \n        # Matched filter in frequency domain\n        h_data_fft = jnp.fft.rfft(data)\n        h_model_fft = jnp.fft.rfft(h_model)\n        \n        # Inner product weighted by noise\n        inner_product = jnp.sum(\n            jnp.conj(h_data_fft) * h_model_fft / noise_psd\n        ).real\n        \n        # Normalization\n        norm = jnp.sum(jnp.abs(h_model_fft) ** 2 / noise_psd)\n        \n        return inner_product - 0.5 * norm\n    \n    # 4. Generate mock data\n    key = random.PRNGKey(2024)\n    n_samples = 4096\n    times = jnp.linspace(0, 1, n_samples)\n    \n    # True parameters\n    true_params = jnp.array([\n        30.0,   # m1 (solar masses)\n        25.0,   # m2\n        100.0,  # luminosity distance (Mpc)\n        0.5,    # coalescence time\n        0.0     # phase\n    ])\n    \n    # Generate signal\n    h_plus_true, h_cross_true = generate_gw_signal(true_params, times)\n    signal = detector_response(h_plus_true, h_cross_true, (0.5, 0.5, 0.0))\n    \n    # Add noise\n    noise_psd = jnp.ones(n_samples // 2 + 1)  # White noise (simplified)\n    noise = 1e-21 * random.normal(key, (n_samples,))\n    data = signal + noise\n    \n    print(f\"  Generated GW signal with SNR ≈ {jnp.max(jnp.abs(signal)) / jnp.std(noise):.1f}\")\n    \n    # 5. Parameter estimation with Numpyro\n    def gw_model(data, times):\n        \"\"\"Numpyro model for GW parameter estimation.\"\"\"\n        \n        # Priors\n        m1 = numpyro.sample('m1', dist.Uniform(5, 50))\n        m2 = numpyro.sample('m2', dist.Uniform(5, 50))\n        d_l = numpyro.sample('d_l', dist.Uniform(10, 500))\n        t_c = numpyro.sample('t_c', dist.Uniform(0.4, 0.6))\n        phi_c = numpyro.sample('phi_c', dist.Uniform(0, 2 * jnp.pi))\n        \n        params = jnp.array([m1, m2, d_l, t_c, phi_c])\n        \n        # Likelihood (simplified)\n        h_plus, h_cross = generate_gw_signal(params, times)\n        h_model = detector_response(h_plus, h_cross, (0.5, 0.5, 0.0))\n        \n        # Gaussian likelihood\n        sigma = 1e-21\n        numpyro.sample('data', dist.Normal(h_model, sigma), obs=data)\n    \n    # Run MCMC\n    print(\"\\n  Running parameter estimation...\")\n    \n    nuts_kernel = NUTS(gw_model)\n    mcmc = MCMC(nuts_kernel, num_warmup=500, num_samples=1000)\n    \n    key, run_key = random.split(key)\n    mcmc.run(run_key, data, times)\n    \n    samples = mcmc.get_samples()\n    \n    print(f\"\\n  Parameter estimates:\")\n    for param in ['m1', 'm2', 'd_l', 't_c']:\n        mean = jnp.mean(samples[param])\n        std = jnp.std(samples[param])\n        true = true_params[['m1', 'm2', 'd_l', 't_c'].index(param)]\n        print(f\"    {param}: {mean:.2f} ± {std:.2f} (true: {true:.2f})\")\n    \n    # 6. Visualization\n    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n    \n    # Signal\n    axes[0, 0].plot(times, data, 'gray', alpha=0.5, label='Data')\n    axes[0, 0].plot(times, signal, 'b', label='True signal')\n    axes[0, 0].set_xlabel('Time (s)')\n    axes[0, 0].set_ylabel('Strain')\n    axes[0, 0].set_title('Gravitational Wave Signal')\n    axes[0, 0].legend()\n    axes[0, 0].set_xlim(0.45, 0.55)\n    \n    # Mass posterior\n    axes[0, 1].scatter(samples['m1'], samples['m2'], alpha=0.1, s=1)\n    axes[0, 1].scatter(true_params[0], true_params[1], c='r', s=100, marker='*')\n    axes[0, 1].set_xlabel('m₁ (M☉)')\n    axes[0, 1].set_ylabel('m₂ (M☉)')\n    axes[0, 1].set_title('Mass Posterior')\n    \n    # Distance posterior\n    axes[1, 0].hist(samples['d_l'], bins=30, alpha=0.7)\n    axes[1, 0].axvline(true_params[2], c='r', ls='--', label='True')\n    axes[1, 0].set_xlabel('Luminosity Distance (Mpc)')\n    axes[1, 0].set_ylabel('Samples')\n    axes[1, 0].set_title('Distance Posterior')\n    axes[1, 0].legend()\n    \n    # Coalescence time\n    axes[1, 1].hist(samples['t_c'], bins=30, alpha=0.7)\n    axes[1, 1].axvline(true_params[3], c='r', ls='--', label='True')\n    axes[1, 1].set_xlabel('Coalescence Time (s)')\n    axes[1, 1].set_ylabel('Samples')\n    axes[1, 1].set_title('Merger Time Posterior')\n    axes[1, 1].legend()\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return samples\n\n# Run complete pipeline\ngw_samples = gravitational_wave_pipeline()","position":{"start":{"line":1042,"column":1},"end":{"line":1217,"column":1}},"key":"ZbtdqSweLh"},{"type":"heading","depth":2,"position":{"start":{"line":1219,"column":1},"end":{"line":1219,"column":1}},"children":[{"type":"text","value":"Key Takeaways","position":{"start":{"line":1219,"column":1},"end":{"line":1219,"column":1}},"key":"ib59n6Sxsp"}],"identifier":"key-takeaways","label":"Key Takeaways","html_id":"key-takeaways","implicit":true,"key":"sM1vTfkc9J"},{"type":"paragraph","position":{"start":{"line":1221,"column":1},"end":{"line":1227,"column":1}},"children":[{"type":"text","value":"✅ ","position":{"start":{"line":1221,"column":1},"end":{"line":1221,"column":1}},"key":"WB7VN9r6jq"},{"type":"strong","position":{"start":{"line":1221,"column":1},"end":{"line":1221,"column":1}},"children":[{"type":"text","value":"BlackJAX","position":{"start":{"line":1221,"column":1},"end":{"line":1221,"column":1}},"key":"BCswoF1Poi"}],"key":"yFgJbSvYya"},{"type":"text","value":" - State-of-the-art MCMC samplers with HMC, NUTS, and more","position":{"start":{"line":1221,"column":1},"end":{"line":1221,"column":1}},"key":"CJbcXpcCDV"},{"type":"break","position":{"start":{"line":1221,"column":1},"end":{"line":1221,"column":1}},"key":"QXJdHA9NMu"},{"type":"text","value":"✅ ","position":{"start":{"line":1221,"column":1},"end":{"line":1221,"column":1}},"key":"xyqsKskzRT"},{"type":"strong","position":{"start":{"line":1221,"column":1},"end":{"line":1221,"column":1}},"children":[{"type":"text","value":"Numpyro","position":{"start":{"line":1221,"column":1},"end":{"line":1221,"column":1}},"key":"kKL7tFJV9s"}],"key":"H8vkZcHx60"},{"type":"text","value":" - Probabilistic programming with automatic inference","position":{"start":{"line":1221,"column":1},"end":{"line":1221,"column":1}},"key":"MVskHLipIh"},{"type":"break","position":{"start":{"line":1221,"column":1},"end":{"line":1221,"column":1}},"key":"N8peFVuB3i"},{"type":"text","value":"✅ ","position":{"start":{"line":1221,"column":1},"end":{"line":1221,"column":1}},"key":"PM7kcB2dc4"},{"type":"strong","position":{"start":{"line":1221,"column":1},"end":{"line":1221,"column":1}},"children":[{"type":"text","value":"JAXopt","position":{"start":{"line":1221,"column":1},"end":{"line":1221,"column":1}},"key":"tMLNNVhrwv"}],"key":"QHAOPrVOBX"},{"type":"text","value":" - Constrained optimization and proximal methods","position":{"start":{"line":1221,"column":1},"end":{"line":1221,"column":1}},"key":"H1igxBKmQk"},{"type":"break","position":{"start":{"line":1221,"column":1},"end":{"line":1221,"column":1}},"key":"GACpVPI8KR"},{"type":"text","value":"✅ ","position":{"start":{"line":1221,"column":1},"end":{"line":1221,"column":1}},"key":"TBXCL6HmOR"},{"type":"strong","position":{"start":{"line":1221,"column":1},"end":{"line":1221,"column":1}},"children":[{"type":"text","value":"Domain-specific","position":{"start":{"line":1221,"column":1},"end":{"line":1221,"column":1}},"key":"OcLgla0COn"}],"key":"Ngnj5CI91L"},{"type":"text","value":" - Growing ecosystem of astronomical JAX tools","position":{"start":{"line":1221,"column":1},"end":{"line":1221,"column":1}},"key":"ar1JIhOxaL"},{"type":"break","position":{"start":{"line":1221,"column":1},"end":{"line":1221,"column":1}},"key":"zkYaxEdZaf"},{"type":"text","value":"✅ ","position":{"start":{"line":1221,"column":1},"end":{"line":1221,"column":1}},"key":"iFYIyyEuoZ"},{"type":"strong","position":{"start":{"line":1221,"column":1},"end":{"line":1221,"column":1}},"children":[{"type":"text","value":"Integration","position":{"start":{"line":1221,"column":1},"end":{"line":1221,"column":1}},"key":"RZq17KmPd9"}],"key":"YscCDO0NtC"},{"type":"text","value":" - Libraries work together for complete pipelines","position":{"start":{"line":1221,"column":1},"end":{"line":1221,"column":1}},"key":"YMs8km7zqp"},{"type":"break","position":{"start":{"line":1221,"column":1},"end":{"line":1221,"column":1}},"key":"H3AAcKhTcd"},{"type":"text","value":"✅ ","position":{"start":{"line":1221,"column":1},"end":{"line":1221,"column":1}},"key":"VmUOUkCImb"},{"type":"strong","position":{"start":{"line":1221,"column":1},"end":{"line":1221,"column":1}},"children":[{"type":"text","value":"Performance","position":{"start":{"line":1221,"column":1},"end":{"line":1221,"column":1}},"key":"jwgAmp6rzH"}],"key":"lr4tcHEv3F"},{"type":"text","value":" - Orders of magnitude faster than traditional tools","position":{"start":{"line":1221,"column":1},"end":{"line":1221,"column":1}},"key":"z5vGtUHPJ0"},{"type":"break","position":{"start":{"line":1221,"column":1},"end":{"line":1221,"column":1}},"key":"CFmOK01Yfm"},{"type":"text","value":"✅ ","position":{"start":{"line":1221,"column":1},"end":{"line":1221,"column":1}},"key":"sPanUcO1zi"},{"type":"strong","position":{"start":{"line":1221,"column":1},"end":{"line":1221,"column":1}},"children":[{"type":"text","value":"Differentiable","position":{"start":{"line":1221,"column":1},"end":{"line":1221,"column":1}},"key":"oZ4GwyFRGy"}],"key":"y2xu190z63"},{"type":"text","value":" - Gradients everywhere enable new methods","position":{"start":{"line":1221,"column":1},"end":{"line":1221,"column":1}},"key":"le4yTU9XS8"}],"key":"JIXsHa2Nko"},{"type":"heading","depth":2,"position":{"start":{"line":1229,"column":1},"end":{"line":1229,"column":1}},"children":[{"type":"text","value":"Next Steps","position":{"start":{"line":1229,"column":1},"end":{"line":1229,"column":1}},"key":"DWP8ujfN3k"}],"identifier":"next-steps","label":"Next Steps","html_id":"next-steps","implicit":true,"key":"qpRVBsL8ad"},{"type":"paragraph","position":{"start":{"line":1231,"column":1},"end":{"line":1231,"column":1}},"children":[{"type":"text","value":"With these specialized libraries, you can:","position":{"start":{"line":1231,"column":1},"end":{"line":1231,"column":1}},"key":"DhBQKDO6oB"}],"key":"xqLNCuLzcR"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":1232,"column":1},"end":{"line":1237,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":1232,"column":1},"end":{"line":1232,"column":1}},"children":[{"type":"text","value":"Replace traditional MCMC codes with faster JAX versions","position":{"start":{"line":1232,"column":1},"end":{"line":1232,"column":1}},"key":"UrUr43MWEj"}],"key":"pVRK8sKW3d"},{"type":"listItem","spread":true,"position":{"start":{"line":1233,"column":1},"end":{"line":1233,"column":1}},"children":[{"type":"text","value":"Build differentiable simulators for likelihood-free inference","position":{"start":{"line":1233,"column":1},"end":{"line":1233,"column":1}},"key":"Qk6spn9vIl"}],"key":"TFW5uJn4Dp"},{"type":"listItem","spread":true,"position":{"start":{"line":1234,"column":1},"end":{"line":1234,"column":1}},"children":[{"type":"text","value":"Implement neural posterior estimation","position":{"start":{"line":1234,"column":1},"end":{"line":1234,"column":1}},"key":"UkhJXnCLh9"}],"key":"ceu7KSOfPK"},{"type":"listItem","spread":true,"position":{"start":{"line":1235,"column":1},"end":{"line":1235,"column":1}},"children":[{"type":"text","value":"Create custom domain-specific tools","position":{"start":{"line":1235,"column":1},"end":{"line":1235,"column":1}},"key":"IRdPyRc6Sf"}],"key":"Fm6x7rhzQ4"},{"type":"listItem","spread":true,"position":{"start":{"line":1236,"column":1},"end":{"line":1237,"column":1}},"children":[{"type":"text","value":"Scale to massive datasets with GPU acceleration","position":{"start":{"line":1236,"column":1},"end":{"line":1236,"column":1}},"key":"oCSeVUflgG"}],"key":"hMVJJdvz0T"}],"key":"GWX4Xonr4g"},{"type":"paragraph","position":{"start":{"line":1238,"column":1},"end":{"line":1238,"column":1}},"children":[{"type":"text","value":"The JAX ecosystem continues to grow rapidly, with new libraries appearing regularly for specialized scientific applications!","position":{"start":{"line":1238,"column":1},"end":{"line":1238,"column":1}},"key":"tzEMMb9wD5"}],"key":"XH7cRVLjkJ"}],"key":"OnrX5TKhWn"}],"key":"iDkEXtBAlQ"},"references":{"cite":{"order":[],"data":{}}}}