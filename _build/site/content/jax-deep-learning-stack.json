{"version":2,"kind":"Article","sha256":"21b97bb7868f7fce2e6dd7467fd798b13cd70545a45c89a388e99ef8facb2022","slug":"jax-deep-learning-stack","location":"/03-scientific-computing-with-python/05-modern-approaches-jax-ecosystem/03-jax_deep_learning_stack.md","dependencies":[],"frontmatter":{"title":"JAX Deep Learning Stack: Flax, Optax, and Orbax","content_includes_title":false,"authors":[{"nameParsed":{"literal":"Anna Rosen","given":"Anna","family":"Rosen"},"name":"Anna Rosen","orcid":"0000-0003-4423-0660","email":"alrosen@sdsu.edu","affiliations":["San Diego State University"],"id":"contributors-myst-generated-uid-0","corresponding":true}],"license":{"content":{"id":"CC-BY-4.0","url":"https://creativecommons.org/licenses/by/4.0/","name":"Creative Commons Attribution 4.0 International","free":true,"CC":true},"code":{"id":"MIT","url":"https://opensource.org/licenses/MIT","name":"MIT License","free":true,"osi":true}},"github":"https://github.com/astrobytes-edu/astr596-modeling-universe","subject":"Modeling the Universe","venue":{"title":"ASTR 596 - Fall 2025","url":"https://www.anna-rosen.com"},"keywords":["computational astrophysics","python","numerical methods","machine learning","monte carlo","neural networks","radiative transfer","bayesian inference","JAX"],"affiliations":[{"id":"San Diego State University","name":"San Diego State University"}],"numbering":{"title":{"offset":2}},"edit_url":"https://github.com/astrobytes-edu/astr596-modeling-universe/blob/main/03-scientific-computing-with-python/05-modern-approaches-jax-ecosystem/03-jax_deep_learning_stack.md","exports":[{"format":"md","filename":"03-jax_deep_learning_stack.md","url":"/03-jax_deep_learning-9e5cb5c6974286f3a8d7c5a446ab854b.md"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"heading","depth":2,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Learning Objectives","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"qPb8hztMUi"}],"identifier":"learning-objectives","label":"Learning Objectives","html_id":"learning-objectives","implicit":true,"key":"Ic30mleXii"},{"type":"paragraph","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"By the end of this chapter, you will:","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"btO6svJjUt"}],"key":"YfYLDq2r5C"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":5,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Build large-scale neural networks with Flax","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"RHpkGF6IoX"}],"key":"E3UcnWeLR9"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"Implement advanced optimization strategies with Optax","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"aUjpNnBhLF"}],"key":"Y3deqikOal"},{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"Manage checkpoints and experiment tracking with Orbax","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"MXUIbieoX3"}],"key":"JrhFhJCGss"},{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"Train transformer models for astronomical applications","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"RIqf8oY1Ww"}],"key":"w6k3Lmm2sZ"},{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"Implement distributed training across multiple GPUs","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"lzYOjzMOAc"}],"key":"FjKUKlyibK"},{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"Build production ML pipelines for astronomy","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"cVmlaTlCDv"}],"key":"TcyL0b0GKL"}],"key":"BwAephknDX"},{"type":"heading","depth":2,"position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"Flax: Scalable Neural Networks","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"mKrnjLCnvM"}],"identifier":"flax-scalable-neural-networks","label":"Flax: Scalable Neural Networks","html_id":"flax-scalable-neural-networks","implicit":true,"key":"BrBBOzVCXv"},{"type":"heading","depth":3,"position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"text","value":"Introduction to Flax","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"iqNakWjFyr"}],"identifier":"introduction-to-flax","label":"Introduction to Flax","html_id":"introduction-to-flax","implicit":true,"key":"fPsiDaYqOL"},{"type":"code","lang":"python","value":"import jax\nimport jax.numpy as jnp\nfrom jax import random, grad, jit, vmap\nimport flax.linen as nn\nfrom flax.training import train_state\nfrom flax.core import freeze, unfreeze\nimport optax\nimport orbax.checkpoint as ocp\nfrom typing import Any, Callable, Sequence, Optional\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef flax_fundamentals():\n    \"\"\"Learn Flax's approach to neural network design.\"\"\"\n    \n    print(\"FLAX: SCALABLE NEURAL NETWORKS\")\n    print(\"=\" * 50)\n    \n    # 1. Basic Flax module\n    print(\"\\n1. BASIC FLAX MODULE:\")\n    \n    class SpectralClassifier(nn.Module):\n        \"\"\"Classify astronomical spectra.\"\"\"\n        \n        features: Sequence[int]\n        dropout_rate: float = 0.1\n        \n        @nn.compact\n        def __call__(self, x, training: bool = False):\n            # Input: (batch, wavelengths)\n            \n            for i, feat in enumerate(self.features):\n                x = nn.Dense(feat)(x)\n                \n                # Batch normalization\n                x = nn.BatchNorm(use_running_average=not training)(x)\n                \n                # Activation\n                x = nn.relu(x)\n                \n                # Dropout\n                if training:\n                    x = nn.Dropout(rate=self.dropout_rate)(x, deterministic=False)\n            \n            # Output layer (7 stellar classes)\n            x = nn.Dense(7)(x)\n            return x\n    \n    # Initialize model\n    model = SpectralClassifier(features=[128, 64, 32])\n    \n    # Create dummy input\n    key = random.PRNGKey(0)\n    dummy_input = jnp.ones((1, 1000))  # (batch, wavelengths)\n    \n    # Initialize parameters\n    params = model.init(key, dummy_input)\n    \n    # Forward pass\n    output = model.apply(params, dummy_input, training=False)\n    \n    print(f\"  Model initialized\")\n    print(f\"  Parameter tree structure: {jax.tree_map(lambda x: x.shape, params)}\")\n    print(f\"  Output shape: {output.shape}\")\n    \n    # 2. Advanced architectures\n    print(\"\\n2. CONVOLUTIONAL NETWORK FOR IMAGES:\")\n    \n    class GalaxyMorphologyNet(nn.Module):\n        \"\"\"Classify galaxy morphology from images.\"\"\"\n        \n        @nn.compact\n        def __call__(self, x, training: bool = False):\n            # Input: (batch, height, width, channels)\n            \n            # Convolutional blocks\n            x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n            x = nn.BatchNorm(use_running_average=not training)(x)\n            x = nn.relu(x)\n            x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n            \n            x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n            x = nn.BatchNorm(use_running_average=not training)(x)\n            x = nn.relu(x)\n            x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n            \n            x = nn.Conv(features=128, kernel_size=(3, 3))(x)\n            x = nn.BatchNorm(use_running_average=not training)(x)\n            x = nn.relu(x)\n            \n            # Global average pooling\n            x = jnp.mean(x, axis=(1, 2))\n            \n            # Dense layers\n            x = nn.Dense(64)(x)\n            x = nn.relu(x)\n            \n            if training:\n                x = nn.Dropout(rate=0.3)(x, deterministic=False)\n            \n            # Output: Hubble types (E, S0, Sa, Sb, Sc, Irr)\n            x = nn.Dense(6)(x)\n            \n            return x\n    \n    # Initialize CNN\n    cnn_model = GalaxyMorphologyNet()\n    dummy_image = jnp.ones((1, 128, 128, 3))\n    cnn_params = cnn_model.init(key, dummy_image)\n    \n    print(f\"  CNN initialized for 128x128 RGB images\")\n    \n    # 3. Attention mechanisms\n    print(\"\\n3. ATTENTION FOR TIME SERIES:\")\n    \n    class LightCurveTransformer(nn.Module):\n        \"\"\"Transformer for variable star classification.\"\"\"\n        \n        embed_dim: int = 256\n        num_heads: int = 8\n        num_layers: int = 4\n        mlp_dim: int = 512\n        dropout: float = 0.1\n        \n        @nn.compact\n        def __call__(self, times, fluxes, training: bool = False):\n            # Input: (batch, sequence_length)\n            batch_size, seq_len = fluxes.shape\n            \n            # Positional encoding using observation times\n            time_embed = nn.Dense(self.embed_dim)(times[:, :, None])\n            flux_embed = nn.Dense(self.embed_dim)(fluxes[:, :, None])\n            \n            x = time_embed + flux_embed\n            \n            # Transformer blocks\n            for _ in range(self.num_layers):\n                # Multi-head attention\n                attn = nn.MultiHeadDotProductAttention(\n                    num_heads=self.num_heads,\n                    dropout_rate=self.dropout if training else 0.0\n                )\n                \n                x_norm = nn.LayerNorm()(x)\n                attn_out = attn(x_norm, x_norm)\n                x = x + attn_out\n                \n                # MLP block\n                x_norm = nn.LayerNorm()(x)\n                mlp_out = nn.Sequential([\n                    nn.Dense(self.mlp_dim),\n                    nn.relu,\n                    nn.Dropout(rate=self.dropout, deterministic=not training),\n                    nn.Dense(self.embed_dim)\n                ])(x_norm)\n                x = x + mlp_out\n            \n            # Global pooling\n            x = jnp.mean(x, axis=1)\n            \n            # Classification head\n            x = nn.Dense(10)(x)  # 10 variable star types\n            \n            return x\n    \n    # Initialize transformer\n    transformer = LightCurveTransformer()\n    dummy_times = jnp.linspace(0, 100, 200)[None, :]  # (1, 200)\n    dummy_fluxes = jnp.ones((1, 200))\n    transformer_params = transformer.init(key, dummy_times, dummy_fluxes)\n    \n    print(f\"  Transformer initialized for light curves\")\n    \n    # 4. Custom layers\n    print(\"\\n4. CUSTOM LAYERS:\")\n    \n    class SpectralConvolution(nn.Module):\n        \"\"\"1D convolution with physical constraints.\"\"\"\n        \n        features: int\n        kernel_size: int\n        use_wavelength_weighting: bool = True\n        \n        @nn.compact\n        def __call__(self, x, wavelengths=None):\n            # Standard convolution\n            conv_out = nn.Conv(\n                features=self.features,\n                kernel_size=(self.kernel_size,),\n                padding='SAME'\n            )(x)\n            \n            # Wavelength-dependent weighting\n            if self.use_wavelength_weighting and wavelengths is not None:\n                # Weight by inverse wavelength (blue more important)\n                weights = 1.0 / wavelengths\n                weights = weights / jnp.mean(weights)\n                conv_out = conv_out * weights[None, :, None]\n            \n            return conv_out\n    \n    print(\"  Custom spectral convolution layer defined\")\n\nflax_fundamentals()","position":{"start":{"line":16,"column":1},"end":{"line":221,"column":1}},"key":"AChPj1P4O1"},{"type":"heading","depth":3,"position":{"start":{"line":223,"column":1},"end":{"line":223,"column":1}},"children":[{"type":"text","value":"Training with Flax","position":{"start":{"line":223,"column":1},"end":{"line":223,"column":1}},"key":"fiTw54qu8j"}],"identifier":"training-with-flax","label":"Training with Flax","html_id":"training-with-flax","implicit":true,"key":"gUcfBAocyL"},{"type":"code","lang":"python","value":"def flax_training():\n    \"\"\"Complete training pipeline with Flax.\"\"\"\n    \n    print(\"\\nFLAX TRAINING PIPELINE\")\n    print(\"=\" * 50)\n    \n    # 1. Create training state\n    print(\"\\n1. TRAINING STATE MANAGEMENT:\")\n    \n    class PhotometricRedshiftNet(nn.Module):\n        \"\"\"Estimate redshift from photometry.\"\"\"\n        \n        @nn.compact\n        def __call__(self, x, training: bool = False):\n            x = nn.Dense(128)(x)\n            x = nn.relu(x)\n            x = nn.Dropout(0.2, deterministic=not training)(x)\n            \n            x = nn.Dense(64)(x)\n            x = nn.relu(x)\n            x = nn.Dropout(0.2, deterministic=not training)(x)\n            \n            x = nn.Dense(32)(x)\n            x = nn.relu(x)\n            \n            # Output: redshift and uncertainty\n            mean = nn.Dense(1)(x)\n            log_std = nn.Dense(1)(x)\n            \n            return mean, log_std\n    \n    # Initialize\n    model = PhotometricRedshiftNet()\n    key = random.PRNGKey(42)\n    \n    # Create optimizer\n    learning_rate_schedule = optax.warmup_cosine_decay_schedule(\n        init_value=0.0,\n        peak_value=1e-3,\n        warmup_steps=100,\n        decay_steps=1000,\n        end_value=1e-5\n    )\n    \n    optimizer = optax.chain(\n        optax.clip_by_global_norm(1.0),\n        optax.adam(learning_rate_schedule)\n    )\n    \n    # Initialize training state\n    def create_train_state(rng, model, optimizer, input_shape):\n        \"\"\"Create initial training state.\"\"\"\n        dummy_input = jnp.ones(input_shape)\n        params = model.init(rng, dummy_input)\n        \n        return train_state.TrainState.create(\n            apply_fn=model.apply,\n            params=params,\n            tx=optimizer\n        )\n    \n    state = create_train_state(key, model, optimizer, (1, 5))  # 5 photometric bands\n    \n    print(f\"  Training state created with {optimizer}\")\n    \n    # 2. Loss functions\n    print(\"\\n2. LOSS FUNCTIONS:\")\n    \n    def gaussian_nll_loss(params, batch, training=True):\n        \"\"\"Gaussian negative log likelihood.\"\"\"\n        inputs, targets = batch\n        mean, log_std = state.apply_fn(params, inputs, training=training)\n        \n        # Negative log likelihood\n        std = jnp.exp(log_std)\n        nll = 0.5 * jnp.log(2 * jnp.pi) + log_std + \\\n              0.5 * ((targets - mean) / std) ** 2\n        \n        return jnp.mean(nll)\n    \n    def robust_loss(params, batch, training=True):\n        \"\"\"Robust loss using Huber.\"\"\"\n        inputs, targets = batch\n        mean, _ = state.apply_fn(params, inputs, training=training)\n        \n        delta = 0.1  # Huber delta\n        residuals = jnp.abs(targets - mean)\n        \n        loss = jnp.where(\n            residuals < delta,\n            0.5 * residuals ** 2,\n            delta * (residuals - 0.5 * delta)\n        )\n        \n        return jnp.mean(loss)\n    \n    # 3. Training step\n    print(\"\\n3. TRAINING STEP:\")\n    \n    @jit\n    def train_step(state, batch, rng):\n        \"\"\"Single training step.\"\"\"\n        dropout_rng = rng\n        \n        def loss_fn(params):\n            return gaussian_nll_loss(params, batch, training=True)\n        \n        # Compute loss and gradients\n        loss, grads = jax.value_and_grad(loss_fn)(state.params)\n        \n        # Update parameters\n        state = state.apply_gradients(grads=grads)\n        \n        return state, loss\n    \n    @jit\n    def eval_step(state, batch):\n        \"\"\"Evaluation step.\"\"\"\n        loss = gaussian_nll_loss(state.params, batch, training=False)\n        \n        inputs, targets = batch\n        mean, log_std = state.apply_fn(state.params, inputs, training=False)\n        \n        # Compute metrics\n        mse = jnp.mean((mean - targets) ** 2)\n        mae = jnp.mean(jnp.abs(mean - targets))\n        \n        # Calibration: fraction within 1-sigma\n        std = jnp.exp(log_std)\n        within_1sigma = jnp.mean(jnp.abs(mean - targets) < std)\n        \n        return {\n            'loss': loss,\n            'mse': mse,\n            'mae': mae,\n            'calibration': within_1sigma\n        }\n    \n    # 4. Data loading\n    print(\"\\n4. DATA LOADING:\")\n    \n    def create_dataset(key, n_samples=10000):\n        \"\"\"Create synthetic photometric redshift dataset.\"\"\"\n        keys = random.split(key, 6)\n        \n        # Generate redshifts\n        z_true = random.uniform(keys[0], (n_samples, 1), minval=0, maxval=3)\n        \n        # Generate photometry (simplified SED model)\n        wavelengths = jnp.array([3500, 4500, 5500, 6500, 7500])  # ugriz\n        \n        # Rest-frame SED (simplified)\n        rest_wavelengths = wavelengths[None, :] / (1 + z_true)\n        \n        # Blackbody approximation\n        T_eff = 5000  # K\n        h, c, k = 6.626e-34, 3e8, 1.38e-23\n        \n        planck = lambda lam, T: (\n            2 * h * c**2 / lam**5 / \n            (jnp.exp(h * c / (lam * k * T)) - 1)\n        )\n        \n        # Add noise\n        photometry = jnp.log10(planck(rest_wavelengths * 1e-10, T_eff))\n        photometry += 0.1 * random.normal(keys[1], photometry.shape)\n        \n        return photometry, z_true\n    \n    # Create datasets\n    train_data = create_dataset(random.PRNGKey(0), n_samples=5000)\n    val_data = create_dataset(random.PRNGKey(1), n_samples=1000)\n    \n    print(f\"  Created training set: {train_data[0].shape}\")\n    print(f\"  Created validation set: {val_data[0].shape}\")\n    \n    # 5. Training loop\n    print(\"\\n5. TRAINING LOOP:\")\n    \n    def train_epoch(state, train_data, batch_size, rng):\n        \"\"\"Train for one epoch.\"\"\"\n        X_train, y_train = train_data\n        n_samples = len(X_train)\n        n_batches = n_samples // batch_size\n        \n        # Shuffle data\n        rng, shuffle_rng = random.split(rng)\n        perm = random.permutation(shuffle_rng, n_samples)\n        X_train = X_train[perm]\n        y_train = y_train[perm]\n        \n        epoch_loss = 0.0\n        \n        for i in range(n_batches):\n            rng, step_rng = random.split(rng)\n            \n            start = i * batch_size\n            end = start + batch_size\n            batch = (X_train[start:end], y_train[start:end])\n            \n            state, loss = train_step(state, batch, step_rng)\n            epoch_loss += loss\n        \n        return state, epoch_loss / n_batches\n    \n    # Train\n    n_epochs = 10\n    batch_size = 32\n    \n    train_losses = []\n    val_metrics = []\n    \n    for epoch in range(n_epochs):\n        key, epoch_rng = random.split(key)\n        \n        # Training\n        state, train_loss = train_epoch(state, train_data, batch_size, epoch_rng)\n        train_losses.append(train_loss)\n        \n        # Validation\n        val_batch = (val_data[0][:100], val_data[1][:100])  # Sample\n        metrics = eval_step(state, val_batch)\n        val_metrics.append(metrics)\n        \n        if epoch % 2 == 0:\n            print(f\"  Epoch {epoch}: Train Loss = {train_loss:.4f}, \" +\n                  f\"Val MAE = {metrics['mae']:.4f}, \" +\n                  f\"Calibration = {metrics['calibration']:.2%}\")\n\nflax_training()","position":{"start":{"line":225,"column":1},"end":{"line":456,"column":1}},"key":"efqQI4ZCNw"},{"type":"heading","depth":2,"position":{"start":{"line":458,"column":1},"end":{"line":458,"column":1}},"children":[{"type":"text","value":"Optax: Advanced Optimization","position":{"start":{"line":458,"column":1},"end":{"line":458,"column":1}},"key":"PK0Hb4wgqj"}],"identifier":"optax-advanced-optimization","label":"Optax: Advanced Optimization","html_id":"optax-advanced-optimization","implicit":true,"key":"RecRqaH7TD"},{"type":"heading","depth":3,"position":{"start":{"line":460,"column":1},"end":{"line":460,"column":1}},"children":[{"type":"text","value":"Optimization Algorithms","position":{"start":{"line":460,"column":1},"end":{"line":460,"column":1}},"key":"eBFbTSO8jC"}],"identifier":"optimization-algorithms","label":"Optimization Algorithms","html_id":"optimization-algorithms","implicit":true,"key":"OoaoYDJY3N"},{"type":"code","lang":"python","value":"def optax_optimizers():\n    \"\"\"Advanced optimization strategies with Optax.\"\"\"\n    \n    print(\"\\nOPTAX: ADVANCED OPTIMIZATION\")\n    print(\"=\" * 50)\n    \n    # 1. Basic optimizers\n    print(\"\\n1. OPTIMIZER COMPARISON:\")\n    \n    def create_loss_landscape():\n        \"\"\"Create a challenging loss landscape.\"\"\"\n        def loss(params):\n            x, y = params\n            # Rosenbrock function (challenging optimization)\n            return (1 - x)**2 + 100 * (y - x**2)**2\n        \n        return loss\n    \n    loss_fn = create_loss_landscape()\n    \n    # Compare optimizers\n    optimizers = {\n        'SGD': optax.sgd(learning_rate=1e-3),\n        'Adam': optax.adam(learning_rate=1e-3),\n        'RMSprop': optax.rmsprop(learning_rate=1e-3),\n        'AdamW': optax.adamw(learning_rate=1e-3, weight_decay=1e-4),\n        'LAMB': optax.lamb(learning_rate=1e-3),\n    }\n    \n    initial_params = jnp.array([-1.0, 1.0])\n    \n    for name, optimizer in optimizers.items():\n        params = initial_params.copy()\n        opt_state = optimizer.init(params)\n        \n        for step in range(100):\n            grads = grad(loss_fn)(params)\n            updates, opt_state = optimizer.update(grads, opt_state)\n            params = optax.apply_updates(params, updates)\n        \n        final_loss = loss_fn(params)\n        print(f\"  {name}: Final loss = {final_loss:.6f}, Final params = {params}\")\n    \n    # 2. Learning rate schedules\n    print(\"\\n2. LEARNING RATE SCHEDULES:\")\n    \n    # Different schedules\n    schedules = {\n        'Constant': optax.constant_schedule(1e-3),\n        'Exponential': optax.exponential_decay(\n            init_value=1e-3,\n            transition_steps=100,\n            decay_rate=0.9\n        ),\n        'Cosine': optax.cosine_decay_schedule(\n            init_value=1e-3,\n            decay_steps=1000\n        ),\n        'Warmup-Cosine': optax.warmup_cosine_decay_schedule(\n            init_value=0.0,\n            peak_value=1e-3,\n            warmup_steps=100,\n            decay_steps=1000\n        ),\n        'Piecewise': optax.piecewise_constant_schedule(\n            init_value=1e-3,\n            boundaries_and_scales={200: 0.1, 400: 0.1}\n        ),\n    }\n    \n    # Visualize schedules\n    steps = jnp.arange(1000)\n    \n    for name, schedule in schedules.items():\n        lrs = [schedule(step) for step in steps]\n        print(f\"  {name}: LR range [{min(lrs):.6f}, {max(lrs):.6f}]\")\n    \n    # 3. Gradient transformations\n    print(\"\\n3. GRADIENT TRANSFORMATIONS:\")\n    \n    # Chain multiple transformations\n    optimizer_chain = optax.chain(\n        optax.clip_by_global_norm(1.0),  # Gradient clipping\n        optax.scale_by_adam(),            # Adam scaling\n        optax.add_decayed_weights(1e-4),  # Weight decay\n        optax.scale(-1e-3)                # Learning rate\n    )\n    \n    print(\"  Chained optimizer created with:\")\n    print(\"    - Global norm clipping (1.0)\")\n    print(\"    - Adam scaling\")\n    print(\"    - Weight decay (1e-4)\")\n    print(\"    - Learning rate scaling\")\n    \n    # 4. Advanced techniques\n    print(\"\\n4. ADVANCED OPTIMIZATION TECHNIQUES:\")\n    \n    # Lookahead optimizer\n    base_optimizer = optax.adam(1e-3)\n    lookahead_optimizer = optax.lookahead(base_optimizer, slow_step_size=0.5, period=5)\n    \n    print(\"  Lookahead optimizer configured\")\n    \n    # Gradient accumulation\n    def gradient_accumulation_optimizer(base_opt, accumulation_steps=4):\n        \"\"\"Accumulate gradients over multiple steps.\"\"\"\n        \n        def init_fn(params):\n            return {\n                'base_state': base_opt.init(params),\n                'accumulated_grads': jax.tree_map(jnp.zeros_like, params),\n                'step': 0\n            }\n        \n        def update_fn(grads, state, params=None):\n            accumulated_grads = jax.tree_map(\n                lambda a, g: a + g / accumulation_steps,\n                state['accumulated_grads'], grads\n            )\n            \n            step = state['step'] + 1\n            \n            if step % accumulation_steps == 0:\n                # Apply accumulated gradients\n                updates, base_state = base_opt.update(\n                    accumulated_grads, state['base_state'], params\n                )\n                # Reset accumulation\n                accumulated_grads = jax.tree_map(jnp.zeros_like, accumulated_grads)\n            else:\n                # Just accumulate\n                updates = jax.tree_map(jnp.zeros_like, grads)\n                base_state = state['base_state']\n            \n            new_state = {\n                'base_state': base_state,\n                'accumulated_grads': accumulated_grads,\n                'step': step\n            }\n            \n            return updates, new_state\n        \n        return optax.GradientTransformation(init_fn, update_fn)\n    \n    acc_optimizer = gradient_accumulation_optimizer(optax.adam(1e-3))\n    print(\"  Gradient accumulation optimizer created\")\n    \n    # 5. Per-parameter learning rates\n    print(\"\\n5. PER-PARAMETER LEARNING RATES:\")\n    \n    def create_model_with_different_lrs():\n        \"\"\"Different learning rates for different layers.\"\"\"\n        \n        class Model(nn.Module):\n            @nn.compact\n            def __call__(self, x):\n                # Feature extractor (lower LR)\n                x = nn.Dense(128, name='feature_extractor')(x)\n                x = nn.relu(x)\n                \n                # Classifier (higher LR)\n                x = nn.Dense(10, name='classifier')(x)\n                return x\n        \n        model = Model()\n        dummy_input = jnp.ones((1, 100))\n        params = model.init(random.PRNGKey(0), dummy_input)\n        \n        # Create optimizer with different LRs\n        def partition_params(params):\n            \"\"\"Partition parameters by layer.\"\"\"\n            feature_params = {'feature_extractor': params['params']['feature_extractor']}\n            classifier_params = {'classifier': params['params']['classifier']}\n            return feature_params, classifier_params\n        \n        # Different optimizers for different parts\n        feature_opt = optax.adam(1e-4)  # Lower LR\n        classifier_opt = optax.adam(1e-2)  # Higher LR\n        \n        return model, params, (feature_opt, classifier_opt)\n    \n    model, params, opts = create_model_with_different_lrs()\n    print(\"  Model with per-layer learning rates created\")\n\noptax_optimizers()","position":{"start":{"line":462,"column":1},"end":{"line":648,"column":1}},"key":"Mu1tMn9ltl"},{"type":"heading","depth":3,"position":{"start":{"line":650,"column":1},"end":{"line":650,"column":1}},"children":[{"type":"text","value":"Advanced Training Strategies","position":{"start":{"line":650,"column":1},"end":{"line":650,"column":1}},"key":"kE67orAyhr"}],"identifier":"advanced-training-strategies","label":"Advanced Training Strategies","html_id":"advanced-training-strategies","implicit":true,"key":"xBvsnjblBi"},{"type":"code","lang":"python","value":"def advanced_training_strategies():\n    \"\"\"Advanced training techniques with Optax.\"\"\"\n    \n    print(\"\\nADVANCED TRAINING STRATEGIES\")\n    print(\"=\" * 50)\n    \n    # 1. Mixed precision training\n    print(\"\\n1. MIXED PRECISION TRAINING:\")\n    \n    class MixedPrecisionModel(nn.Module):\n        \"\"\"Model with mixed precision computation.\"\"\"\n        \n        use_mixed_precision: bool = True\n        \n        @nn.compact\n        def __call__(self, x):\n            dtype = jnp.float16 if self.use_mixed_precision else jnp.float32\n            \n            # Cast to lower precision\n            x = x.astype(dtype)\n            \n            # Compute in lower precision\n            x = nn.Dense(128, dtype=dtype)(x)\n            x = nn.relu(x)\n            x = nn.Dense(64, dtype=dtype)(x)\n            x = nn.relu(x)\n            \n            # Cast back to float32 for loss\n            x = nn.Dense(10, dtype=jnp.float32)(x.astype(jnp.float32))\n            \n            return x\n    \n    # Loss scaling for mixed precision\n    def create_loss_scaled_optimizer(optimizer, loss_scale=1024):\n        \"\"\"Add loss scaling for mixed precision.\"\"\"\n        return optax.chain(\n            optax.scale(1 / loss_scale),  # Unscale gradients\n            optimizer\n        )\n    \n    base_opt = optax.adam(1e-3)\n    scaled_opt = create_loss_scaled_optimizer(base_opt)\n    \n    print(\"  Mixed precision optimizer with loss scaling created\")\n    \n    # 2. Differential learning rates\n    print(\"\\n2. DIFFERENTIAL LEARNING RATES:\")\n    \n    def layer_wise_lr_decay(base_lr, decay_factor, num_layers):\n        \"\"\"Exponentially decay LR for earlier layers.\"\"\"\n        lrs = []\n        for i in range(num_layers):\n            lr = base_lr * (decay_factor ** (num_layers - i - 1))\n            lrs.append(lr)\n        return lrs\n    \n    layer_lrs = layer_wise_lr_decay(1e-3, 0.5, 4)\n    print(f\"  Layer-wise LRs: {layer_lrs}\")\n    \n    # 3. Gradient penalty\n    print(\"\\n3. GRADIENT PENALTIES:\")\n    \n    def add_gradient_penalty(loss_fn, penalty_weight=0.1):\n        \"\"\"Add gradient penalty for regularization.\"\"\"\n        \n        def penalized_loss(params, inputs):\n            # Original loss\n            loss = loss_fn(params, inputs)\n            \n            # Gradient penalty\n            grads = grad(loss_fn)(params, inputs)\n            grad_norm = jnp.sqrt(\n                sum(jnp.sum(g**2) for g in jax.tree_leaves(grads))\n            )\n            \n            penalty = penalty_weight * grad_norm\n            \n            return loss + penalty\n        \n        return penalized_loss\n    \n    print(\"  Gradient penalty wrapper created\")\n    \n    # 4. EMA (Exponential Moving Average)\n    print(\"\\n4. EXPONENTIAL MOVING AVERAGE:\")\n    \n    def create_ema():\n        \"\"\"Create EMA of model parameters.\"\"\"\n        \n        def init_fn(params):\n            return params  # Initialize with current params\n        \n        def update_fn(params, ema_params, decay=0.999):\n            \"\"\"Update EMA parameters.\"\"\"\n            return jax.tree_map(\n                lambda e, p: decay * e + (1 - decay) * p,\n                ema_params, params\n            )\n        \n        return init_fn, update_fn\n    \n    ema_init, ema_update = create_ema()\n    \n    print(\"  EMA functions created for model averaging\")\n    \n    # 5. Stochastic Weight Averaging\n    print(\"\\n5. STOCHASTIC WEIGHT AVERAGING (SWA):\")\n    \n    class SWAState:\n        \"\"\"State for SWA training.\"\"\"\n        \n        def __init__(self, params):\n            self.params = params\n            self.swa_params = jax.tree_map(jnp.zeros_like, params)\n            self.n_averaged = 0\n        \n        def update(self, new_params, start_averaging_step, current_step):\n            \"\"\"Update SWA parameters.\"\"\"\n            if current_step >= start_averaging_step:\n                # Update running average\n                self.n_averaged += 1\n                self.swa_params = jax.tree_map(\n                    lambda swa, p: swa + (p - swa) / self.n_averaged,\n                    self.swa_params, new_params\n                )\n            \n            self.params = new_params\n            return self\n    \n    print(\"  SWA state management created\")\n\nadvanced_training_strategies()","position":{"start":{"line":652,"column":1},"end":{"line":785,"column":1}},"key":"Fqnn2fjAgi"},{"type":"heading","depth":2,"position":{"start":{"line":787,"column":1},"end":{"line":787,"column":1}},"children":[{"type":"text","value":"Orbax: Checkpointing and Experiment Management","position":{"start":{"line":787,"column":1},"end":{"line":787,"column":1}},"key":"JNHL9Vu6zf"}],"identifier":"orbax-checkpointing-and-experiment-management","label":"Orbax: Checkpointing and Experiment Management","html_id":"orbax-checkpointing-and-experiment-management","implicit":true,"key":"hGILUbSjHd"},{"type":"heading","depth":3,"position":{"start":{"line":789,"column":1},"end":{"line":789,"column":1}},"children":[{"type":"text","value":"Checkpoint Management","position":{"start":{"line":789,"column":1},"end":{"line":789,"column":1}},"key":"vJfyK0iocu"}],"identifier":"checkpoint-management","label":"Checkpoint Management","html_id":"checkpoint-management","implicit":true,"key":"cwKKwkA282"},{"type":"code","lang":"python","value":"def orbax_checkpointing():\n    \"\"\"Checkpoint management with Orbax.\"\"\"\n    \n    print(\"\\nORBAX: CHECKPOINT MANAGEMENT\")\n    print(\"=\" * 50)\n    \n    import orbax.checkpoint as ocp\n    import tempfile\n    import os\n    \n    # 1. Basic checkpointing\n    print(\"\\n1. BASIC CHECKPOINTING:\")\n    \n    # Create temporary directory for checkpoints\n    checkpoint_dir = tempfile.mkdtemp()\n    \n    # Create a simple model and optimizer\n    class SimpleModel(nn.Module):\n        @nn.compact\n        def __call__(self, x):\n            x = nn.Dense(128)(x)\n            x = nn.relu(x)\n            x = nn.Dense(10)(x)\n            return x\n    \n    model = SimpleModel()\n    key = random.PRNGKey(0)\n    params = model.init(key, jnp.ones((1, 100)))\n    \n    optimizer = optax.adam(1e-3)\n    opt_state = optimizer.init(params)\n    \n    # Create checkpoint manager\n    options = ocp.CheckpointManagerOptions(\n        max_to_keep=3,\n        keep_period=5,\n        create=True\n    )\n    \n    checkpoint_manager = ocp.CheckpointManager(\n        checkpoint_dir,\n        options=options,\n        item_names=('params', 'opt_state', 'metadata')\n    )\n    \n    # Save checkpoint\n    step = 0\n    metadata = {'epoch': 0, 'loss': 0.5, 'accuracy': 0.85}\n    \n    checkpoint_manager.save(\n        step,\n        args=ocp.args.Composite(\n            params=ocp.args.StandardSave(params),\n            opt_state=ocp.args.StandardSave(opt_state),\n            metadata=ocp.args.JsonSave(metadata)\n        )\n    )\n    \n    print(f\"  Checkpoint saved at step {step}\")\n    \n    # 2. Restoring checkpoints\n    print(\"\\n2. RESTORING CHECKPOINTS:\")\n    \n    # Restore latest checkpoint\n    restored = checkpoint_manager.restore(\n        checkpoint_manager.latest_step(),\n        args=ocp.args.Composite(\n            params=ocp.args.StandardRestore(params),\n            opt_state=ocp.args.StandardRestore(opt_state),\n            metadata=ocp.args.JsonRestore()\n        )\n    )\n    \n    print(f\"  Restored from step {checkpoint_manager.latest_step()}\")\n    print(f\"  Metadata: {restored['metadata']}\")\n    \n    # 3. Async checkpointing\n    print(\"\\n3. ASYNCHRONOUS CHECKPOINTING:\")\n    \n    async_checkpoint_manager = ocp.CheckpointManager(\n        os.path.join(checkpoint_dir, 'async'),\n        options=ocp.CheckpointManagerOptions(\n            max_to_keep=2,\n            save_interval_steps=10,\n            enable_async_checkpointing=True\n        )\n    )\n    \n    print(\"  Async checkpoint manager created\")\n    \n    # 4. Best model tracking\n    print(\"\\n4. BEST MODEL TRACKING:\")\n    \n    class BestModelTracker:\n        \"\"\"Track and save best model based on metric.\"\"\"\n        \n        def __init__(self, checkpoint_dir, metric='loss', mode='min'):\n            self.checkpoint_dir = checkpoint_dir\n            self.metric = metric\n            self.mode = mode\n            self.best_value = float('inf') if mode == 'min' else float('-inf')\n            self.checkpointer = ocp.Checkpointer(\n                ocp.StandardCheckpointHandler()\n            )\n        \n        def update(self, params, metrics, step):\n            \"\"\"Update best model if improved.\"\"\"\n            current_value = metrics[self.metric]\n            \n            is_better = (\n                (self.mode == 'min' and current_value < self.best_value) or\n                (self.mode == 'max' and current_value > self.best_value)\n            )\n            \n            if is_better:\n                self.best_value = current_value\n                \n                # Save best model\n                best_path = os.path.join(self.checkpoint_dir, 'best_model')\n                self.checkpointer.save(\n                    best_path,\n                    args=ocp.args.Composite(\n                        params=ocp.args.StandardSave(params),\n                        metrics=ocp.args.JsonSave(metrics),\n                        step=ocp.args.JsonSave({'step': step})\n                    )\n                )\n                \n                print(f\"    New best model saved: {self.metric}={current_value:.4f}\")\n                return True\n            \n            return False\n    \n    tracker = BestModelTracker(checkpoint_dir, metric='loss', mode='min')\n    \n    # Simulate training with metric tracking\n    for step in range(5):\n        fake_loss = 1.0 / (step + 1)  # Decreasing loss\n        metrics = {'loss': fake_loss, 'accuracy': 1 - fake_loss}\n        tracker.update(params, metrics, step)\n    \n    # 5. Multi-host checkpointing\n    print(\"\\n5. DISTRIBUTED CHECKPOINTING:\")\n    \n    # For multi-host training (conceptual)\n    def create_distributed_checkpoint_manager():\n        \"\"\"Create checkpoint manager for distributed training.\"\"\"\n        \n        # Would use in multi-host setting\n        # checkpoint_manager = ocp.CheckpointManager(\n        #     directory,\n        #     options=ocp.CheckpointManagerOptions(\n        #         save_interval_steps=100,\n        #         max_to_keep=3\n        #     ),\n        #     metadata={'host_count': jax.process_count()}\n        # )\n        \n        print(\"  Distributed checkpointing configured (requires multi-host setup)\")\n    \n    create_distributed_checkpoint_manager()\n    \n    # Cleanup\n    import shutil\n    shutil.rmtree(checkpoint_dir)\n\norbax_checkpointing()","position":{"start":{"line":791,"column":1},"end":{"line":959,"column":1}},"key":"RhE5EU2U8W"},{"type":"heading","depth":2,"position":{"start":{"line":961,"column":1},"end":{"line":961,"column":1}},"children":[{"type":"text","value":"Complete Training Pipeline","position":{"start":{"line":961,"column":1},"end":{"line":961,"column":1}},"key":"eIasIg6VHD"}],"identifier":"complete-training-pipeline","label":"Complete Training Pipeline","html_id":"complete-training-pipeline","implicit":true,"key":"rV0WEFbzg3"},{"type":"heading","depth":3,"position":{"start":{"line":963,"column":1},"end":{"line":963,"column":1}},"children":[{"type":"text","value":"Production ML Pipeline","position":{"start":{"line":963,"column":1},"end":{"line":963,"column":1}},"key":"VLDEPJWxLr"}],"identifier":"production-ml-pipeline","label":"Production ML Pipeline","html_id":"production-ml-pipeline","implicit":true,"key":"TzT0NdBeF7"},{"type":"code","lang":"python","value":"def complete_ml_pipeline():\n    \"\"\"Complete ML pipeline for astronomical applications.\"\"\"\n    \n    print(\"\\nCOMPLETE ML PIPELINE\")\n    print(\"=\" * 50)\n    \n    # 1. Model definition\n    class TransientClassifier(nn.Module):\n        \"\"\"Classify astronomical transients from light curves.\"\"\"\n        \n        num_classes: int = 5  # SN Ia, SN II, SN Ibc, SLSN, Kilonova\n        hidden_dim: int = 256\n        num_heads: int = 8\n        num_layers: int = 4\n        dropout: float = 0.1\n        \n        @nn.compact\n        def __call__(self, times, fluxes, errors, filters, training: bool = False):\n            batch_size, seq_len = fluxes.shape\n            \n            # Embed observations\n            obs_features = jnp.stack([\n                fluxes,\n                errors,\n                times,\n                filters\n            ], axis=-1)  # (batch, seq, 4)\n            \n            # Initial projection\n            x = nn.Dense(self.hidden_dim)(obs_features)\n            \n            # Positional encoding from times\n            pos_encoding = self.param(\n                'pos_encoding',\n                nn.initializers.normal(stddev=0.02),\n                (1, seq_len, self.hidden_dim)\n            )\n            x = x + pos_encoding\n            \n            # Transformer layers\n            for i in range(self.num_layers):\n                # Self-attention\n                attn_out = nn.MultiHeadDotProductAttention(\n                    num_heads=self.num_heads,\n                    dropout_rate=self.dropout if training else 0.0,\n                    deterministic=not training\n                )(x, x)\n                x = nn.LayerNorm()(x + attn_out)\n                \n                # FFN\n                ffn_out = nn.Sequential([\n                    nn.Dense(self.hidden_dim * 4),\n                    nn.gelu,\n                    nn.Dropout(self.dropout, deterministic=not training),\n                    nn.Dense(self.hidden_dim)\n                ])(x)\n                x = nn.LayerNorm()(x + ffn_out)\n            \n            # Global pooling with attention\n            attention_weights = nn.Dense(1)(x)\n            attention_weights = nn.softmax(attention_weights, axis=1)\n            x = jnp.sum(x * attention_weights, axis=1)\n            \n            # Classification head\n            x = nn.Dense(self.hidden_dim // 2)(x)\n            x = nn.relu(x)\n            x = nn.Dropout(self.dropout, deterministic=not training)(x)\n            logits = nn.Dense(self.num_classes)(x)\n            \n            return logits\n    \n    # 2. Data generation\n    def generate_transient_data(key, n_samples=1000):\n        \"\"\"Generate synthetic transient light curves.\"\"\"\n        keys = random.split(key, 5)\n        \n        all_times = []\n        all_fluxes = []\n        all_errors = []\n        all_filters = []\n        all_labels = []\n        \n        for i in range(n_samples):\n            # Random transient type\n            label = random.choice(keys[0], 5)\n            \n            # Generate light curve based on type\n            n_obs = random.choice(keys[1], 1, minval=20, maxval=100)[0]\n            times = jnp.sort(random.uniform(keys[2], (n_obs,), minval=0, maxval=100))\n            \n            # Different templates for different types\n            if label == 0:  # SN Ia\n                peak_time = 20.0\n                rise_time = 15.0\n                decay_time = 30.0\n            elif label == 1:  # SN II\n                peak_time = 30.0\n                rise_time = 20.0\n                decay_time = 60.0\n            else:\n                peak_time = 25.0\n                rise_time = 10.0\n                decay_time = 40.0\n            \n            # Generate flux (simplified)\n            fluxes = jnp.where(\n                times < peak_time,\n                jnp.exp(-(times - peak_time)**2 / (2 * rise_time**2)),\n                jnp.exp(-(times - peak_time)**2 / (2 * decay_time**2))\n            )\n            \n            # Add noise\n            errors = 0.05 + 0.05 * random.uniform(keys[3], (n_obs,))\n            fluxes += errors * random.normal(keys[4], (n_obs,))\n            \n            # Random filters (ugriz)\n            filters = random.choice(keys[0], 5, shape=(n_obs,))\n            \n            # Pad to fixed length\n            max_len = 100\n            padded_times = jnp.pad(times, (0, max_len - len(times)))\n            padded_fluxes = jnp.pad(fluxes, (0, max_len - len(fluxes)))\n            padded_errors = jnp.pad(errors, (0, max_len - len(errors)))\n            padded_filters = jnp.pad(filters, (0, max_len - len(filters)))\n            \n            all_times.append(padded_times)\n            all_fluxes.append(padded_fluxes)\n            all_errors.append(padded_errors)\n            all_filters.append(padded_filters)\n            all_labels.append(label)\n        \n        return (\n            jnp.stack(all_times),\n            jnp.stack(all_fluxes),\n            jnp.stack(all_errors),\n            jnp.stack(all_filters),\n            jnp.array(all_labels)\n        )\n    \n    # Generate data\n    key = random.PRNGKey(42)\n    train_data = generate_transient_data(key, n_samples=500)\n    val_data = generate_transient_data(random.PRNGKey(43), n_samples=100)\n    \n    print(f\"  Generated {len(train_data[0])} training samples\")\n    \n    # 3. Training setup\n    model = TransientClassifier()\n    \n    # Initialize\n    dummy_batch = tuple(x[:1] for x in train_data[:-1])\n    params = model.init(key, *dummy_batch)\n    \n    # Optimizer with schedule\n    schedule = optax.warmup_cosine_decay_schedule(\n        init_value=0.0,\n        peak_value=1e-3,\n        warmup_steps=50,\n        decay_steps=500\n    )\n    \n    optimizer = optax.chain(\n        optax.clip_by_global_norm(1.0),\n        optax.adamw(learning_rate=schedule, weight_decay=1e-4)\n    )\n    \n    state = train_state.TrainState.create(\n        apply_fn=model.apply,\n        params=params,\n        tx=optimizer\n    )\n    \n    # 4. Training functions\n    @jit\n    def train_step(state, batch, dropout_key):\n        \"\"\"Training step.\"\"\"\n        times, fluxes, errors, filters, labels = batch\n        \n        def loss_fn(params):\n            logits = state.apply_fn(\n                params, times, fluxes, errors, filters,\n                training=True, rngs={'dropout': dropout_key}\n            )\n            \n            # Cross-entropy loss\n            loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels)\n            loss = jnp.mean(loss)\n            \n            # L2 regularization (handled by adamw)\n            \n            return loss, logits\n        \n        (loss, logits), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n        \n        # Update\n        state = state.apply_gradients(grads=grads)\n        \n        # Metrics\n        accuracy = jnp.mean(jnp.argmax(logits, axis=-1) == labels)\n        \n        return state, {'loss': loss, 'accuracy': accuracy}\n    \n    @jit\n    def eval_step(state, batch):\n        \"\"\"Evaluation step.\"\"\"\n        times, fluxes, errors, filters, labels = batch\n        \n        logits = state.apply_fn(\n            state.params, times, fluxes, errors, filters,\n            training=False\n        )\n        \n        loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels)\n        loss = jnp.mean(loss)\n        \n        predictions = jnp.argmax(logits, axis=-1)\n        accuracy = jnp.mean(predictions == labels)\n        \n        # Per-class accuracy\n        per_class_acc = []\n        for c in range(5):\n            mask = labels == c\n            if jnp.sum(mask) > 0:\n                class_acc = jnp.mean(predictions[mask] == c)\n                per_class_acc.append(class_acc)\n        \n        return {\n            'loss': loss,\n            'accuracy': accuracy,\n            'per_class_accuracy': per_class_acc\n        }\n    \n    # 5. Training loop\n    print(\"\\n  Training transient classifier...\")\n    \n    n_epochs = 5\n    batch_size = 32\n    \n    for epoch in range(n_epochs):\n        # Training\n        key, dropout_key = random.split(key)\n        \n        n_batches = len(train_data[0]) // batch_size\n        epoch_metrics = {'loss': 0.0, 'accuracy': 0.0}\n        \n        for i in range(n_batches):\n            start = i * batch_size\n            end = start + batch_size\n            \n            batch = tuple(x[start:end] for x in train_data)\n            \n            dropout_key, step_key = random.split(dropout_key)\n            state, metrics = train_step(state, batch, step_key)\n            \n            epoch_metrics['loss'] += metrics['loss']\n            epoch_metrics['accuracy'] += metrics['accuracy']\n        \n        epoch_metrics['loss'] /= n_batches\n        epoch_metrics['accuracy'] /= n_batches\n        \n        # Validation\n        val_batch = tuple(x[:batch_size] for x in val_data)\n        val_metrics = eval_step(state, val_batch)\n        \n        print(f\"  Epoch {epoch}: \"\n              f\"Train Loss={epoch_metrics['loss']:.4f}, \"\n              f\"Train Acc={epoch_metrics['accuracy']:.2%}, \"\n              f\"Val Loss={val_metrics['loss']:.4f}, \"\n              f\"Val Acc={val_metrics['accuracy']:.2%}\")\n    \n    print(\"\\n  Training complete!\")\n    \n    return state\n\n# Run pipeline\nfinal_state = complete_ml_pipeline()","position":{"start":{"line":965,"column":1},"end":{"line":1242,"column":1}},"key":"EPSHgskdOR"},{"type":"heading","depth":2,"position":{"start":{"line":1244,"column":1},"end":{"line":1244,"column":1}},"children":[{"type":"text","value":"Key Takeaways","position":{"start":{"line":1244,"column":1},"end":{"line":1244,"column":1}},"key":"iW59AeCvBK"}],"identifier":"key-takeaways","label":"Key Takeaways","html_id":"key-takeaways","implicit":true,"key":"UBau6z1NlM"},{"type":"paragraph","position":{"start":{"line":1246,"column":1},"end":{"line":1251,"column":1}},"children":[{"type":"text","value":"✅ ","position":{"start":{"line":1246,"column":1},"end":{"line":1246,"column":1}},"key":"XWFI6FzJ8L"},{"type":"strong","position":{"start":{"line":1246,"column":1},"end":{"line":1246,"column":1}},"children":[{"type":"text","value":"Flax","position":{"start":{"line":1246,"column":1},"end":{"line":1246,"column":1}},"key":"BJ7HA2JPhq"}],"key":"Jpyc5jyf8c"},{"type":"text","value":" - Production-ready neural networks with clean module system","position":{"start":{"line":1246,"column":1},"end":{"line":1246,"column":1}},"key":"WW3S6cFhMn"},{"type":"break","position":{"start":{"line":1246,"column":1},"end":{"line":1246,"column":1}},"key":"Zf1mRTgAvN"},{"type":"text","value":"✅ ","position":{"start":{"line":1246,"column":1},"end":{"line":1246,"column":1}},"key":"eoy39LfTCz"},{"type":"strong","position":{"start":{"line":1246,"column":1},"end":{"line":1246,"column":1}},"children":[{"type":"text","value":"Optax","position":{"start":{"line":1246,"column":1},"end":{"line":1246,"column":1}},"key":"VctVaoNpDU"}],"key":"udxpUWZE9J"},{"type":"text","value":" - Composable optimization with advanced schedules and techniques","position":{"start":{"line":1246,"column":1},"end":{"line":1246,"column":1}},"key":"aV8U4tQSG1"},{"type":"break","position":{"start":{"line":1246,"column":1},"end":{"line":1246,"column":1}},"key":"XH1gTyjQCX"},{"type":"text","value":"✅ ","position":{"start":{"line":1246,"column":1},"end":{"line":1246,"column":1}},"key":"dpFDsJSVls"},{"type":"strong","position":{"start":{"line":1246,"column":1},"end":{"line":1246,"column":1}},"children":[{"type":"text","value":"Orbax","position":{"start":{"line":1246,"column":1},"end":{"line":1246,"column":1}},"key":"sozNysY4MV"}],"key":"eddsu6H9FB"},{"type":"text","value":" - Robust checkpointing and experiment management","position":{"start":{"line":1246,"column":1},"end":{"line":1246,"column":1}},"key":"ZmSUsIhNPZ"},{"type":"break","position":{"start":{"line":1246,"column":1},"end":{"line":1246,"column":1}},"key":"TTFeRdf5Zn"},{"type":"text","value":"✅ ","position":{"start":{"line":1246,"column":1},"end":{"line":1246,"column":1}},"key":"xAebanEtdv"},{"type":"strong","position":{"start":{"line":1246,"column":1},"end":{"line":1246,"column":1}},"children":[{"type":"text","value":"Integration","position":{"start":{"line":1246,"column":1},"end":{"line":1246,"column":1}},"key":"EPCIRbCFJo"}],"key":"UoUEUqusA2"},{"type":"text","value":" - Seamless workflow from model definition to production","position":{"start":{"line":1246,"column":1},"end":{"line":1246,"column":1}},"key":"QLGZ1ZAhar"},{"type":"break","position":{"start":{"line":1246,"column":1},"end":{"line":1246,"column":1}},"key":"LrZydOfKkD"},{"type":"text","value":"✅ ","position":{"start":{"line":1246,"column":1},"end":{"line":1246,"column":1}},"key":"Pv6aLIZhoK"},{"type":"strong","position":{"start":{"line":1246,"column":1},"end":{"line":1246,"column":1}},"children":[{"type":"text","value":"Scalability","position":{"start":{"line":1246,"column":1},"end":{"line":1246,"column":1}},"key":"GRKuFAUuAD"}],"key":"XRmlxfs4eN"},{"type":"text","value":" - Ready for multi-GPU and large-scale training","position":{"start":{"line":1246,"column":1},"end":{"line":1246,"column":1}},"key":"Weij0kAZgU"},{"type":"break","position":{"start":{"line":1246,"column":1},"end":{"line":1246,"column":1}},"key":"MqLwcXxGMA"},{"type":"text","value":"✅ ","position":{"start":{"line":1246,"column":1},"end":{"line":1246,"column":1}},"key":"WGRNhLlgQB"},{"type":"strong","position":{"start":{"line":1246,"column":1},"end":{"line":1246,"column":1}},"children":[{"type":"text","value":"Best Practices","position":{"start":{"line":1246,"column":1},"end":{"line":1246,"column":1}},"key":"ynWkE4uERZ"}],"key":"RDUT9L6e6x"},{"type":"text","value":" - Type safety, mixed precision, and monitoring built-in","position":{"start":{"line":1246,"column":1},"end":{"line":1246,"column":1}},"key":"cHtHLTHeJD"}],"key":"cag0hu1h8I"},{"type":"heading","depth":2,"position":{"start":{"line":1253,"column":1},"end":{"line":1253,"column":1}},"children":[{"type":"text","value":"Next Chapter Preview","position":{"start":{"line":1253,"column":1},"end":{"line":1253,"column":1}},"key":"HeiUaQaEGx"}],"identifier":"next-chapter-preview","label":"Next Chapter Preview","html_id":"next-chapter-preview","implicit":true,"key":"MYsUak1TNU"},{"type":"paragraph","position":{"start":{"line":1254,"column":1},"end":{"line":1254,"column":1}},"children":[{"type":"text","value":"Specialized Libraries: BlackJAX for MCMC, NetKet for quantum systems, and more domain-specific JAX tools.","position":{"start":{"line":1254,"column":1},"end":{"line":1254,"column":1}},"key":"T7lIIv6xnJ"}],"key":"t6NMej4NrQ"}],"key":"gM2VRNwb6o"}],"key":"rUMLNYz6BM"},"references":{"cite":{"order":[],"data":{}}}}