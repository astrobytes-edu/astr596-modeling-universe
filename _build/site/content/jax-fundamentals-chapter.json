{"version":2,"kind":"Article","sha256":"96510f7437b9f8a013ee9c50ec24f81de80f420478f487989475a8680b4141ba","slug":"jax-fundamentals-chapter","location":"/03-scientific-computing-with-python/05-modern-approaches-jax-ecosystem/01-jax_fundamentals_chapter.md","dependencies":[],"frontmatter":{"title":"JAX Fundamentals: NumPy on Steroids","content_includes_title":false,"authors":[{"nameParsed":{"literal":"Anna Rosen","given":"Anna","family":"Rosen"},"name":"Anna Rosen","orcid":"0000-0003-4423-0660","email":"alrosen@sdsu.edu","affiliations":["San Diego State University"],"id":"contributors-myst-generated-uid-0","corresponding":true}],"license":{"content":{"id":"CC-BY-4.0","url":"https://creativecommons.org/licenses/by/4.0/","name":"Creative Commons Attribution 4.0 International","free":true,"CC":true},"code":{"id":"MIT","url":"https://opensource.org/licenses/MIT","name":"MIT License","free":true,"osi":true}},"github":"https://github.com/astrobytes-edu/astr596-modeling-universe","subject":"Modeling the Universe","venue":{"title":"ASTR 596 - Fall 2025","url":"https://www.anna-rosen.com"},"keywords":["computational astrophysics","python","numerical methods","machine learning","monte carlo","neural networks","radiative transfer","bayesian inference","JAX"],"affiliations":[{"id":"San Diego State University","name":"San Diego State University"}],"numbering":{"title":{"offset":2}},"edit_url":"https://github.com/astrobytes-edu/astr596-modeling-universe/blob/main/03-scientific-computing-with-python/05-modern-approaches-jax-ecosystem/01-jax_fundamentals_chapter.md","exports":[{"format":"md","filename":"01-jax_fundamentals_chapter.md","url":"/01-jax_fundamentals_-df00f68131d797c86ed97c9ca8421499.md"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"heading","depth":2,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Learning Objectives","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"TGRGxj0QgA"}],"identifier":"learning-objectives","label":"Learning Objectives","html_id":"learning-objectives","implicit":true,"key":"UULYvbzCB0"},{"type":"paragraph","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"By the end of this chapter, you will:","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"JsFlEf02Mw"}],"key":"XalH5omlb4"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":5,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Understand JAX’s core philosophy and why it matters for scientific computing","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"HpuDmJUXOz"}],"key":"BWB161GIbk"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"Master functional programming patterns required by JAX","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"kYK2ZuvSdh"}],"key":"Le3NH33xaa"},{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"Use automatic differentiation for physics problems","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"p4UA6PA9gO"}],"key":"oQbt87acKS"},{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"Compile functions with JIT for massive speedups","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"g9D2zDmuMx"}],"key":"XGbe9RxhSx"},{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"Vectorize computations with vmap","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"ikz2Jlt28u"}],"key":"jb5Kkporfi"},{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"Generate reproducible random numbers with JAX’s PRNG","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"vRzVuAf7hi"}],"key":"NcqzQP4Lwq"}],"key":"gIM0uOW0rS"},{"type":"heading","depth":2,"position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"Why JAX? The Revolution in Scientific Computing","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"Wqe086EOYf"}],"identifier":"why-jax-the-revolution-in-scientific-computing","label":"Why JAX? The Revolution in Scientific Computing","html_id":"why-jax-the-revolution-in-scientific-computing","implicit":true,"key":"lAceVWgo51"},{"type":"code","lang":"python","value":"import jax\nimport jax.numpy as jnp\nfrom jax import grad, jit, vmap, pmap\nfrom jax import random\nimport numpy as np\nimport time\nimport matplotlib.pyplot as plt\n\ndef why_jax_matters():\n    \"\"\"Demonstrate JAX's game-changing features for astronomy.\"\"\"\n    \n    print(\"JAX: Three Transformations That Change Everything\")\n    print(\"=\" * 50)\n    \n    # 1. NumPy API but faster\n    print(\"\\n1. FAMILIAR BUT FASTER:\")\n    \n    # NumPy computation\n    np_array = np.random.randn(1000, 1000)\n    start = time.perf_counter()\n    np_result = np.sin(np_array) ** 2 + np.cos(np_array) ** 2\n    numpy_time = time.perf_counter() - start\n    \n    # JAX computation\n    jax_array = jnp.array(np_array)\n    start = time.perf_counter()\n    jax_result = jnp.sin(jax_array) ** 2 + jnp.cos(jax_array) ** 2\n    jax_time = time.perf_counter() - start\n    \n    print(f\"  NumPy time: {numpy_time*1000:.2f} ms\")\n    print(f\"  JAX time: {jax_time*1000:.2f} ms\")\n    print(f\"  Results match: {np.allclose(np_result, jax_result)}\")\n    \n    # 2. Automatic differentiation\n    print(\"\\n2. AUTOMATIC DIFFERENTIATION:\")\n    \n    def gravitational_potential(r, mass=1.0):\n        \"\"\"Gravitational potential energy.\"\"\"\n        return -mass / jnp.linalg.norm(r)\n    \n    # Get gradient automatically!\n    grad_potential = grad(gravitational_potential)\n    \n    position = jnp.array([1.0, 0.0, 0.0])\n    force = -grad_potential(position)\n    print(f\"  Position: {position}\")\n    print(f\"  Force: {force}\")\n    print(f\"  |F| = {jnp.linalg.norm(force):.4f} (expected: 1.0)\")\n    \n    # 3. Compilation with JIT\n    print(\"\\n3. JUST-IN-TIME COMPILATION:\")\n    \n    def orbital_step(state, dt):\n        \"\"\"Single step of orbital integration.\"\"\"\n        r, v = state[:3], state[3:]\n        a = -r / jnp.linalg.norm(r)**3\n        v_new = v + a * dt\n        r_new = r + v_new * dt\n        return jnp.concatenate([r_new, v_new])\n    \n    orbital_step_jit = jit(orbital_step)\n    \n    state = jnp.array([1.0, 0.0, 0.0, 0.0, 1.0, 0.0])\n    \n    # First call includes compilation\n    start = time.perf_counter()\n    _ = orbital_step_jit(state, 0.01)\n    first_time = time.perf_counter() - start\n    \n    # Subsequent calls are fast\n    start = time.perf_counter()\n    for _ in range(1000):\n        state = orbital_step_jit(state, 0.01)\n    compiled_time = time.perf_counter() - start\n    \n    # Compare with non-compiled\n    state = jnp.array([1.0, 0.0, 0.0, 0.0, 1.0, 0.0])\n    start = time.perf_counter()\n    for _ in range(1000):\n        state = orbital_step(state, 0.01)\n    python_time = time.perf_counter() - start\n    \n    print(f\"  First call (with compilation): {first_time*1000:.2f} ms\")\n    print(f\"  1000 steps compiled: {compiled_time*1000:.2f} ms\")\n    print(f\"  1000 steps python: {python_time*1000:.2f} ms\")\n    print(f\"  Speedup: {python_time/compiled_time:.1f}x\")\n    \n    # 4. Vectorization with vmap\n    print(\"\\n4. AUTOMATIC VECTORIZATION:\")\n    \n    def distance(r1, r2):\n        \"\"\"Distance between two points.\"\"\"\n        return jnp.linalg.norm(r1 - r2)\n    \n    # Vectorize over first argument\n    distances_from_origin = vmap(distance, in_axes=(0, None))\n    \n    positions = jnp.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 1, 1]])\n    origin = jnp.array([0, 0, 0])\n    \n    dists = distances_from_origin(positions, origin)\n    print(f\"  Distances from origin: {dists}\")\n    \n    return True\n\nwhy_jax_matters()","position":{"start":{"line":14,"column":1},"end":{"line":121,"column":1}},"key":"rUIH9hZj2q"},{"type":"heading","depth":2,"position":{"start":{"line":123,"column":1},"end":{"line":123,"column":1}},"children":[{"type":"text","value":"The Functional Programming Paradigm","position":{"start":{"line":123,"column":1},"end":{"line":123,"column":1}},"key":"KjEcoQVVSp"}],"identifier":"the-functional-programming-paradigm","label":"The Functional Programming Paradigm","html_id":"the-functional-programming-paradigm","implicit":true,"key":"RJcLoW1R7v"},{"type":"heading","depth":3,"position":{"start":{"line":125,"column":1},"end":{"line":125,"column":1}},"children":[{"type":"text","value":"Pure Functions: The Heart of JAX","position":{"start":{"line":125,"column":1},"end":{"line":125,"column":1}},"key":"BWrSOxLX2j"}],"identifier":"pure-functions-the-heart-of-jax","label":"Pure Functions: The Heart of JAX","html_id":"pure-functions-the-heart-of-jax","implicit":true,"key":"bBqJRKXd1y"},{"type":"code","lang":"python","value":"def functional_programming_in_jax():\n    \"\"\"JAX requires pure functional programming - here's why and how.\"\"\"\n    \n    print(\"PURE FUNCTIONS IN JAX\")\n    print(\"=\" * 50)\n    \n    # ❌ BAD: Impure function with side effects\n    global_counter = 0\n    \n    def impure_function(x):\n        global global_counter\n        global_counter += 1  # Side effect!\n        return x * global_counter\n    \n    # This won't work properly with JAX transformations\n    # result = jit(impure_function)(5)  # Would give unexpected results\n    \n    # ✅ GOOD: Pure function\n    def pure_function(x, counter):\n        \"\"\"Pure function - output depends only on inputs.\"\"\"\n        return x * counter, counter + 1\n    \n    # This works perfectly with JAX\n    pure_jit = jit(pure_function)\n    result, new_counter = pure_jit(5.0, 1.0)\n    print(f\"Pure function result: {result}, new counter: {new_counter}\")\n    \n    # Example: Stellar evolution step\n    print(\"\\n STELLAR EVOLUTION EXAMPLE:\")\n    \n    # ❌ BAD: Using mutation\n    class Star:\n        def __init__(self, mass, luminosity):\n            self.mass = mass\n            self.luminosity = luminosity\n        \n        def evolve(self, dt):\n            self.luminosity *= 1.01  # Mutating state!\n            self.mass *= 0.999\n    \n    # ✅ GOOD: Functional approach\n    def evolve_star(state, dt):\n        \"\"\"\n        Evolve star state functionally.\n        \n        Parameters\n        ----------\n        state : dict\n            Star properties {mass, luminosity, age}\n        dt : float\n            Time step\n        \n        Returns\n        -------\n        dict\n            New star state\n        \"\"\"\n        mass_loss_rate = 1e-7 * state['luminosity']\n        \n        new_state = {\n            'mass': state['mass'] - mass_loss_rate * dt,\n            'luminosity': state['luminosity'] * (1 + 0.01 * dt),\n            'age': state['age'] + dt\n        }\n        \n        return new_state\n    \n    # JAX-friendly star evolution\n    @jit\n    def evolve_star_jax(mass, luminosity, age, dt):\n        \"\"\"Evolve star with JAX.\"\"\"\n        mass_loss_rate = 1e-7 * luminosity\n        \n        new_mass = mass - mass_loss_rate * dt\n        new_luminosity = luminosity * (1 + 0.01 * dt)\n        new_age = age + dt\n        \n        return new_mass, new_luminosity, new_age\n    \n    # Run evolution\n    mass, lum, age = 1.0, 1.0, 0.0\n    for _ in range(100):\n        mass, lum, age = evolve_star_jax(mass, lum, age, 0.01)\n    \n    print(f\"After evolution: M={mass:.4f}, L={lum:.4f}, Age={age:.2f}\")\n    \n    # Carrying state through computations\n    print(\"\\nCARRYING STATE FUNCTIONALLY:\")\n    \n    from functools import partial\n    \n    @jit\n    def integrate_orbit(carry, dt):\n        \"\"\"Single integration step.\"\"\"\n        position, velocity = carry\n        acceleration = -position / jnp.linalg.norm(position)**3\n        \n        new_velocity = velocity + acceleration * dt\n        new_position = position + new_velocity * dt\n        \n        return (new_position, new_velocity)\n    \n    # Use scan for sequential computations\n    from jax.lax import scan\n    \n    def simulate_orbit(initial_state, dt, n_steps):\n        \"\"\"Simulate orbit for n_steps.\"\"\"\n        \n        def step(carry, _):\n            new_carry = integrate_orbit(carry, dt)\n            return new_carry, new_carry  # Return carry and output\n        \n        final_state, trajectory = scan(step, initial_state, None, length=n_steps)\n        return trajectory\n    \n    initial = (jnp.array([1.0, 0.0, 0.0]), jnp.array([0.0, 1.0, 0.0]))\n    trajectory = simulate_orbit(initial, 0.01, 1000)\n    \n    positions = trajectory[0]\n    print(f\"Simulated {len(positions)} orbital positions\")\n    print(f\"Final position: {positions[-1]}\")\n\nfunctional_programming_in_jax()","position":{"start":{"line":127,"column":1},"end":{"line":251,"column":1}},"key":"IDMtNQF3DT"},{"type":"heading","depth":2,"position":{"start":{"line":253,"column":1},"end":{"line":253,"column":1}},"children":[{"type":"text","value":"Automatic Differentiation: The Killer Feature","position":{"start":{"line":253,"column":1},"end":{"line":253,"column":1}},"key":"a0bDcoOGwz"}],"identifier":"automatic-differentiation-the-killer-feature","label":"Automatic Differentiation: The Killer Feature","html_id":"automatic-differentiation-the-killer-feature","implicit":true,"key":"U0ZbK0aBkd"},{"type":"heading","depth":3,"position":{"start":{"line":255,"column":1},"end":{"line":255,"column":1}},"children":[{"type":"text","value":"Gradients for Physics","position":{"start":{"line":255,"column":1},"end":{"line":255,"column":1}},"key":"IheH0PI4y4"}],"identifier":"gradients-for-physics","label":"Gradients for Physics","html_id":"gradients-for-physics","implicit":true,"key":"Z1S0ex0oVj"},{"type":"code","lang":"python","value":"def automatic_differentiation_astronomy():\n    \"\"\"Automatic differentiation for astronomical applications.\"\"\"\n    \n    print(\"AUTOMATIC DIFFERENTIATION IN ASTRONOMY\")\n    print(\"=\" * 50)\n    \n    # 1. Simple derivatives\n    print(\"\\n1. BASIC DERIVATIVES:\")\n    \n    def planck_law(wavelength, temperature):\n        \"\"\"Planck's law for blackbody radiation.\"\"\"\n        h = 6.626e-34\n        c = 3e8\n        k = 1.38e-23\n        \n        wavelength = wavelength * 1e-9  # nm to m\n        \n        numerator = 2 * h * c**2 / wavelength**5\n        denominator = jnp.exp(h * c / (wavelength * k * temperature)) - 1\n        \n        return numerator / denominator\n    \n    # Derivative with respect to temperature\n    dplanck_dT = grad(planck_law, argnums=1)\n    \n    wavelength = 500.0  # nm\n    temperature = 5778.0  # K\n    \n    intensity = planck_law(wavelength, temperature)\n    gradient = dplanck_dT(wavelength, temperature)\n    \n    print(f\"  B(λ={wavelength}nm, T={temperature}K) = {intensity:.3e}\")\n    print(f\"  ∂B/∂T = {gradient:.3e}\")\n    \n    # 2. Gradient of gravitational N-body potential\n    print(\"\\n2. N-BODY FORCES FROM POTENTIAL:\")\n    \n    def nbody_potential(positions, masses):\n        \"\"\"\n        Total gravitational potential energy.\n        \n        Parameters\n        ----------\n        positions : array shape (n, 3)\n            Positions of n bodies\n        masses : array shape (n,)\n            Masses of bodies\n        \"\"\"\n        n = len(masses)\n        potential = 0.0\n        \n        for i in range(n):\n            for j in range(i+1, n):\n                r_ij = jnp.linalg.norm(positions[i] - positions[j])\n                potential -= masses[i] * masses[j] / r_ij\n        \n        return potential\n    \n    # Get forces from potential gradient\n    def get_forces(positions, masses):\n        \"\"\"Calculate forces as negative gradient of potential.\"\"\"\n        return -grad(nbody_potential)(positions, masses)\n    \n    # Three-body system\n    positions = jnp.array([\n        [1.0, 0.0, 0.0],\n        [-0.5, 0.866, 0.0],\n        [-0.5, -0.866, 0.0]\n    ])\n    masses = jnp.array([1.0, 1.0, 1.0])\n    \n    forces = get_forces(positions, masses)\n    print(f\"  Forces on 3 bodies:\")\n    for i, f in enumerate(forces):\n        print(f\"    Body {i}: {f}\")\n    print(f\"  Total force: {jnp.sum(forces, axis=0)} (should be ~0)\")\n    \n    # 3. Hessian for optimization\n    print(\"\\n3. HESSIAN FOR FINDING MINIMA:\")\n    \n    def chi_squared(params, x_data, y_data):\n        \"\"\"Chi-squared for linear fit.\"\"\"\n        a, b = params\n        y_model = a * x_data + b\n        return jnp.sum((y_data - y_model)**2)\n    \n    # Get gradient and Hessian\n    from jax import hessian\n    \n    grad_chi2 = grad(chi_squared)\n    hess_chi2 = hessian(chi_squared)\n    \n    # Sample data\n    x_data = jnp.linspace(0, 10, 20)\n    y_true = 2.5 * x_data + 1.0\n    y_data = y_true + 0.5 * random.normal(random.PRNGKey(0), shape=x_data.shape)\n    \n    params = jnp.array([2.0, 0.5])  # Initial guess\n    \n    gradient = grad_chi2(params, x_data, y_data)\n    hessian_matrix = hess_chi2(params, x_data, y_data)\n    \n    print(f\"  Gradient at {params}: {gradient}\")\n    print(f\"  Hessian:\\n{hessian_matrix}\")\n    \n    # Use for Newton's method\n    params_new = params - jnp.linalg.inv(hessian_matrix) @ gradient\n    print(f\"  Newton step: {params} -> {params_new}\")\n    \n    # 4. Jacobian for coordinate transformations\n    print(\"\\n4. JACOBIAN FOR COORDINATE TRANSFORMS:\")\n    \n    from jax import jacfwd, jacrev\n    \n    def spherical_to_cartesian(spherical):\n        \"\"\"Convert spherical to Cartesian coordinates.\"\"\"\n        r, theta, phi = spherical\n        x = r * jnp.sin(theta) * jnp.cos(phi)\n        y = r * jnp.sin(theta) * jnp.sin(phi)\n        z = r * jnp.cos(theta)\n        return jnp.array([x, y, z])\n    \n    # Jacobian matrix\n    jacobian = jacfwd(spherical_to_cartesian)\n    \n    spherical = jnp.array([1.0, jnp.pi/4, jnp.pi/3])\n    J = jacobian(spherical)\n    \n    print(f\"  Spherical: r={spherical[0]}, θ={spherical[1]:.3f}, φ={spherical[2]:.3f}\")\n    print(f\"  Jacobian matrix:\")\n    print(f\"{J}\")\n    print(f\"  Determinant: {jnp.linalg.det(J):.3f}\")\n\nautomatic_differentiation_astronomy()","position":{"start":{"line":257,"column":1},"end":{"line":392,"column":1}},"key":"SluvIqEN1R"},{"type":"heading","depth":2,"position":{"start":{"line":394,"column":1},"end":{"line":394,"column":1}},"children":[{"type":"text","value":"JIT Compilation: Making Python Fast","position":{"start":{"line":394,"column":1},"end":{"line":394,"column":1}},"key":"hxcl31M3bg"}],"identifier":"jit-compilation-making-python-fast","label":"JIT Compilation: Making Python Fast","html_id":"jit-compilation-making-python-fast","implicit":true,"key":"e1mfHTWTtC"},{"type":"heading","depth":3,"position":{"start":{"line":396,"column":1},"end":{"line":396,"column":1}},"children":[{"type":"text","value":"Understanding JIT","position":{"start":{"line":396,"column":1},"end":{"line":396,"column":1}},"key":"P6FX228VNa"}],"identifier":"understanding-jit","label":"Understanding JIT","html_id":"understanding-jit","implicit":true,"key":"aNvRY5heT0"},{"type":"code","lang":"python","value":"def jit_compilation_deep_dive():\n    \"\"\"Deep dive into JIT compilation for scientific computing.\"\"\"\n    \n    print(\"JIT COMPILATION IN DETAIL\")\n    print(\"=\" * 50)\n    \n    # 1. Basic JIT compilation\n    print(\"\\n1. BASIC JIT:\")\n    \n    def slow_function(x):\n        \"\"\"Computationally intensive function.\"\"\"\n        result = x\n        for _ in range(100):\n            result = jnp.sin(result) + jnp.cos(result) * jnp.exp(-result**2)\n        return result\n    \n    fast_function = jit(slow_function)\n    \n    x = jnp.linspace(-2, 2, 1000)\n    \n    # Time comparison\n    start = time.perf_counter()\n    _ = slow_function(x)\n    slow_time = time.perf_counter() - start\n    \n    # First call (includes compilation)\n    start = time.perf_counter()\n    _ = fast_function(x)\n    first_time = time.perf_counter() - start\n    \n    # Second call (already compiled)\n    start = time.perf_counter()\n    _ = fast_function(x)\n    fast_time = time.perf_counter() - start\n    \n    print(f\"  Slow: {slow_time*1000:.2f} ms\")\n    print(f\"  First JIT call: {first_time*1000:.2f} ms\")\n    print(f\"  Subsequent JIT: {fast_time*1000:.2f} ms\")\n    print(f\"  Speedup: {slow_time/fast_time:.1f}x\")\n    \n    # 2. JIT with static arguments\n    print(\"\\n2. STATIC ARGUMENTS:\")\n    \n    @partial(jit, static_argnums=(1, 2))\n    def simulate_galaxy(positions, n_steps, dt):\n        \"\"\"\n        Simulate galaxy dynamics.\n        n_steps and dt are static (known at compile time).\n        \"\"\"\n        def step(pos):\n            # Simplified dynamics\n            center_of_mass = jnp.mean(pos, axis=0)\n            forces = -(pos - center_of_mass) / 100\n            return pos + forces * dt\n        \n        for _ in range(n_steps):\n            positions = step(positions)\n        \n        return positions\n    \n    # Different compiled versions for different static args\n    pos = random.normal(random.PRNGKey(0), (100, 3))\n    \n    # These create different compiled functions\n    result1 = simulate_galaxy(pos, 10, 0.1)   # Compilation 1\n    result2 = simulate_galaxy(pos, 10, 0.1)   # Uses compilation 1\n    result3 = simulate_galaxy(pos, 20, 0.1)   # New compilation!\n    \n    print(f\"  Static arguments create specialized compiled versions\")\n    \n    # 3. JIT pitfalls to avoid\n    print(\"\\n3. JIT PITFALLS:\")\n    \n    # ❌ BAD: Python control flow depending on values\n    def bad_function(x):\n        if x > 0:  # This depends on the VALUE of x\n            return jnp.sin(x)\n        else:\n            return jnp.cos(x)\n    \n    # This won't work with JIT for dynamic x\n    # jitted_bad = jit(bad_function)\n    # jitted_bad(jnp.array(0.5))  # Would fail!\n    \n    # ✅ GOOD: Use JAX control flow\n    def good_function(x):\n        return jax.lax.cond(\n            x > 0,\n            lambda x: jnp.sin(x),\n            lambda x: jnp.cos(x),\n            x\n        )\n    \n    jitted_good = jit(good_function)\n    result = jitted_good(0.5)\n    print(f\"  Conditional result: {result:.3f}\")\n    \n    # 4. Debugging JIT compilation\n    print(\"\\n4. DEBUGGING JIT:\")\n    \n    # Use jax.debug.print inside JIT\n    @jit\n    def debug_function(x):\n        x = x * 2\n        jax.debug.print(\"x after doubling: {x}\", x=x)\n        x = jnp.sin(x)\n        jax.debug.print(\"x after sin: {x}\", x=x)\n        return x\n    \n    result = debug_function(1.0)\n    \n    # Check what XLA sees\n    from jax import make_jaxpr\n    \n    def simple_function(x, y):\n        return jnp.dot(x, y) + 1\n    \n    x = jnp.ones((3, 3))\n    y = jnp.ones((3, 3))\n    \n    print(\"\\n  JAX expression tree:\")\n    print(make_jaxpr(simple_function)(x, y))\n\njit_compilation_deep_dive()","position":{"start":{"line":398,"column":1},"end":{"line":523,"column":1}},"key":"V6Rd6JvWpU"},{"type":"heading","depth":2,"position":{"start":{"line":525,"column":1},"end":{"line":525,"column":1}},"children":[{"type":"text","value":"Vectorization with vmap","position":{"start":{"line":525,"column":1},"end":{"line":525,"column":1}},"key":"DAvGRW0SkA"}],"identifier":"vectorization-with-vmap","label":"Vectorization with vmap","html_id":"vectorization-with-vmap","implicit":true,"key":"r5mVtlAat3"},{"type":"heading","depth":3,"position":{"start":{"line":527,"column":1},"end":{"line":527,"column":1}},"children":[{"type":"text","value":"Parallel Operations Made Simple","position":{"start":{"line":527,"column":1},"end":{"line":527,"column":1}},"key":"d6nBw3UdOg"}],"identifier":"parallel-operations-made-simple","label":"Parallel Operations Made Simple","html_id":"parallel-operations-made-simple","implicit":true,"key":"aO09EOQGsB"},{"type":"code","lang":"python","value":"def vmap_for_astronomy():\n    \"\"\"Vectorization patterns for astronomical computations.\"\"\"\n    \n    print(\"VMAP: AUTOMATIC VECTORIZATION\")\n    print(\"=\" * 50)\n    \n    # 1. Basic vectorization\n    print(\"\\n1. VECTORIZING DISTANCE CALCULATIONS:\")\n    \n    def angular_distance(ra1, dec1, ra2, dec2):\n        \"\"\"Angular distance between two points (haversine).\"\"\"\n        dra = ra2 - ra1\n        ddec = dec2 - dec1\n        \n        a = jnp.sin(ddec/2)**2 + jnp.cos(dec1) * jnp.cos(dec2) * jnp.sin(dra/2)**2\n        c = 2 * jnp.arcsin(jnp.sqrt(a))\n        \n        return c\n    \n    # Vectorize over first source (compare one to many)\n    vmap_one_to_many = vmap(angular_distance, in_axes=(None, None, 0, 0))\n    \n    # Vectorize over both (pairwise)\n    vmap_pairwise = vmap(angular_distance, in_axes=(0, 0, 0, 0))\n    \n    # Test data\n    ra_catalog = random.uniform(random.PRNGKey(0), (1000,), minval=0, maxval=2*jnp.pi)\n    dec_catalog = random.uniform(random.PRNGKey(1), (1000,), minval=-jnp.pi/2, maxval=jnp.pi/2)\n    \n    # Distance from single source to catalog\n    ra_source, dec_source = jnp.pi, 0.0\n    distances = vmap_one_to_many(ra_source, dec_source, ra_catalog, dec_catalog)\n    print(f\"  Distances from source: shape {distances.shape}\")\n    print(f\"  Nearest neighbor: {jnp.min(distances):.4f} rad\")\n    \n    # 2. Nested vmap for all-pairs\n    print(\"\\n2. ALL-PAIRS DISTANCES:\")\n    \n    # Nested vmap for N×N distance matrix\n    vmap_all_pairs = vmap(\n        vmap(angular_distance, in_axes=(None, None, 0, 0)),\n        in_axes=(0, 0, None, None)\n    )\n    \n    # Small sample for all-pairs\n    ra_sample = ra_catalog[:10]\n    dec_sample = dec_catalog[:10]\n    \n    distance_matrix = vmap_all_pairs(ra_sample, dec_sample, ra_sample, dec_sample)\n    print(f\"  Distance matrix shape: {distance_matrix.shape}\")\n    print(f\"  Symmetric: {jnp.allclose(distance_matrix, distance_matrix.T)}\")\n    \n    # 3. Vectorizing complex functions\n    print(\"\\n3. VECTORIZING ORBIT INTEGRATION:\")\n    \n    @jit\n    def integrate_single_orbit(initial_conditions, n_steps):\n        \"\"\"Integrate single orbit.\"\"\"\n        r0, v0 = initial_conditions[:3], initial_conditions[3:]\n        \n        def step(carry, _):\n            r, v = carry\n            a = -r / jnp.linalg.norm(r)**3\n            v_new = v + a * 0.01\n            r_new = r + v_new * 0.01\n            return (r_new, v_new), r_new\n        \n        _, trajectory = scan(step, (r0, v0), None, length=n_steps)\n        return trajectory\n    \n    # Vectorize over different initial conditions\n    vmap_orbits = vmap(integrate_single_orbit, in_axes=(0, None))\n    \n    # Multiple initial conditions (different eccentricities)\n    initial_conditions = jnp.array([\n        [1.0, 0.0, 0.0, 0.0, 1.0, 0.0],   # Circular\n        [1.0, 0.0, 0.0, 0.0, 1.2, 0.0],   # Elliptical\n        [1.0, 0.0, 0.0, 0.0, 0.8, 0.0],   # Elliptical\n    ])\n    \n    trajectories = vmap_orbits(initial_conditions, 100)\n    print(f\"  Integrated {len(trajectories)} orbits in parallel\")\n    print(f\"  Trajectories shape: {trajectories.shape}\")\n    \n    # 4. Combining vmap with grad\n    print(\"\\n4. VECTORIZED GRADIENTS:\")\n    \n    def potential(position, mass_distribution):\n        \"\"\"Gravitational potential at position.\"\"\"\n        positions, masses = mass_distribution\n        distances = vmap(lambda p: jnp.linalg.norm(position - p))(positions)\n        return -jnp.sum(masses / distances)\n    \n    # Gradient of potential\n    grad_potential = grad(potential)\n    \n    # Vectorize gradient calculation over multiple positions\n    vmap_gradient = vmap(grad_potential, in_axes=(0, None))\n    \n    # Mass distribution (galaxy model)\n    n_masses = 100\n    mass_positions = random.normal(random.PRNGKey(2), (n_masses, 3)) * 10\n    masses = random.uniform(random.PRNGKey(3), (n_masses,), minval=0.1, maxval=1.0)\n    mass_distribution = (mass_positions, masses)\n    \n    # Calculate gradient at multiple points\n    test_positions = jnp.array([\n        [0.0, 0.0, 0.0],\n        [5.0, 0.0, 0.0],\n        [0.0, 5.0, 0.0],\n        [5.0, 5.0, 5.0]\n    ])\n    \n    gradients = vmap_gradient(test_positions, mass_distribution)\n    print(f\"  Gradients at {len(test_positions)} positions:\")\n    for i, (pos, grad) in enumerate(zip(test_positions, gradients)):\n        print(f\"    Position {pos}: Force = {-grad}\")\n\nvmap_for_astronomy()","position":{"start":{"line":529,"column":1},"end":{"line":649,"column":1}},"key":"HvXGbVN3xz"},{"type":"heading","depth":2,"position":{"start":{"line":651,"column":1},"end":{"line":651,"column":1}},"children":[{"type":"text","value":"Random Numbers in JAX","position":{"start":{"line":651,"column":1},"end":{"line":651,"column":1}},"key":"jJ5MaCJmKX"}],"identifier":"random-numbers-in-jax","label":"Random Numbers in JAX","html_id":"random-numbers-in-jax","implicit":true,"key":"XMjyIKo3Sx"},{"type":"heading","depth":3,"position":{"start":{"line":653,"column":1},"end":{"line":653,"column":1}},"children":[{"type":"text","value":"Reproducible Randomness","position":{"start":{"line":653,"column":1},"end":{"line":653,"column":1}},"key":"croJX5boDh"}],"identifier":"reproducible-randomness","label":"Reproducible Randomness","html_id":"reproducible-randomness","implicit":true,"key":"D1kxSlNfQm"},{"type":"code","lang":"python","value":"def jax_random_numbers():\n    \"\"\"JAX's approach to random numbers for reproducible science.\"\"\"\n    \n    print(\"RANDOM NUMBERS IN JAX\")\n    print(\"=\" * 50)\n    \n    # 1. JAX PRNG basics\n    print(\"\\n1. PRNG BASICS:\")\n    \n    # Create a random key\n    key = random.PRNGKey(42)\n    print(f\"  Initial key: {key}\")\n    \n    # Split key for independent streams\n    key, subkey = random.split(key)\n    print(f\"  Split keys: {key}, {subkey}\")\n    \n    # Generate random numbers\n    uniform_samples = random.uniform(subkey, shape=(5,))\n    print(f\"  Uniform samples: {uniform_samples}\")\n    \n    # 2. Why explicit keys matter\n    print(\"\\n2. REPRODUCIBILITY:\")\n    \n    def monte_carlo_integration(f, n_samples, key):\n        \"\"\"Monte Carlo integration of function f over [0,1]³.\"\"\"\n        # Split key for different random numbers\n        key1, key2, key3 = random.split(key, 3)\n        \n        x = random.uniform(key1, (n_samples,))\n        y = random.uniform(key2, (n_samples,))\n        z = random.uniform(key3, (n_samples,))\n        \n        samples = vmap(f)(x, y, z)\n        return jnp.mean(samples)\n    \n    def test_function(x, y, z):\n        return x**2 + y**2 + z**2\n    \n    # Same key → same result (reproducible!)\n    key1 = random.PRNGKey(123)\n    result1 = monte_carlo_integration(test_function, 10000, key1)\n    \n    key2 = random.PRNGKey(123)  # Same seed\n    result2 = monte_carlo_integration(test_function, 10000, key2)\n    \n    print(f\"  Result 1: {result1:.6f}\")\n    print(f\"  Result 2: {result2:.6f}\")\n    print(f\"  Identical: {result1 == result2}\")\n    \n    # 3. Random numbers in parallel computations\n    print(\"\\n3. PARALLEL RANDOM STREAMS:\")\n    \n    @jit\n    def parallel_monte_carlo(keys, n_samples_per_thread):\n        \"\"\"Run MC in parallel with independent random streams.\"\"\"\n        \n        def single_mc(key):\n            samples = random.normal(key, (n_samples_per_thread,))\n            return jnp.mean(samples**2)  # Estimate <x²>\n        \n        # vmap over different keys\n        results = vmap(single_mc)(keys)\n        return results\n    \n    # Create independent keys for parallel execution\n    main_key = random.PRNGKey(0)\n    n_threads = 4\n    keys = random.split(main_key, n_threads)\n    \n    estimates = parallel_monte_carlo(keys, 10000)\n    print(f\"  Parallel estimates of <x²>: {estimates}\")\n    print(f\"  Mean: {jnp.mean(estimates):.4f} (expected: 1.0)\")\n    \n    # 4. Sampling from distributions\n    print(\"\\n4. ASTRONOMICAL DISTRIBUTIONS:\")\n    \n    key = random.PRNGKey(42)\n    \n    # Initial Mass Function (power law)\n    def sample_imf(key, n_stars, alpha=-2.35):\n        \"\"\"Sample from Salpeter IMF.\"\"\"\n        key1, key2 = random.split(key)\n        \n        # Inverse transform sampling\n        u = random.uniform(key1, (n_stars,))\n        m_min, m_max = 0.1, 100.0\n        \n        if alpha == -1:\n            masses = m_min * (m_max/m_min)**u\n        else:\n            masses = ((m_max**(alpha+1) - m_min**(alpha+1)) * u + \n                     m_min**(alpha+1))**(1/(alpha+1))\n        \n        return masses\n    \n    masses = sample_imf(key, 1000)\n    print(f\"  Sampled {len(masses)} stellar masses\")\n    print(f\"  Mass range: {jnp.min(masses):.2f} - {jnp.max(masses):.2f} M☉\")\n    print(f\"  Mean mass: {jnp.mean(masses):.2f} M☉\")\n\njax_random_numbers()","position":{"start":{"line":655,"column":1},"end":{"line":758,"column":1}},"key":"bzb8Vc5Bms"},{"type":"heading","depth":2,"position":{"start":{"line":760,"column":1},"end":{"line":760,"column":1}},"children":[{"type":"text","value":"Putting It All Together: N-Body Simulation","position":{"start":{"line":760,"column":1},"end":{"line":760,"column":1}},"key":"sfxkzJaUsK"}],"identifier":"putting-it-all-together-n-body-simulation","label":"Putting It All Together: N-Body Simulation","html_id":"putting-it-all-together-n-body-simulation","implicit":true,"key":"FTdOnAYOvF"},{"type":"heading","depth":3,"position":{"start":{"line":762,"column":1},"end":{"line":762,"column":1}},"children":[{"type":"text","value":"Complete Example with All JAX Features","position":{"start":{"line":762,"column":1},"end":{"line":762,"column":1}},"key":"Cay4XH5QHG"}],"identifier":"complete-example-with-all-jax-features","label":"Complete Example with All JAX Features","html_id":"complete-example-with-all-jax-features","implicit":true,"key":"fD9FqoFsZw"},{"type":"code","lang":"python","value":"def nbody_simulation_complete():\n    \"\"\"Complete N-body simulation showcasing all JAX features.\"\"\"\n    \n    print(\"COMPLETE N-BODY SIMULATION WITH JAX\")\n    print(\"=\" * 50)\n    \n    # Define the physics\n    @jit\n    def compute_forces(positions, masses):\n        \"\"\"Compute gravitational forces between all bodies.\"\"\"\n        n = len(masses)\n        forces = jnp.zeros_like(positions)\n        \n        for i in range(n):\n            # Vectorize force calculation from body i to all others\n            def force_from_j(j):\n                r_ij = positions[j] - positions[i]\n                dist = jnp.linalg.norm(r_ij)\n                # Softening to avoid singularities\n                dist_soft = jnp.maximum(dist, 0.01)\n                return jax.lax.cond(\n                    i == j,\n                    lambda _: jnp.zeros(3),\n                    lambda _: masses[j] * r_ij / dist_soft**3,\n                    None\n                )\n            \n            total_force = jnp.sum(vmap(force_from_j)(jnp.arange(n)), axis=0)\n            forces = forces.at[i].set(masses[i] * total_force)\n        \n        return forces\n    \n    # Integration step\n    @jit\n    def leapfrog_step(state, dt):\n        \"\"\"Single leapfrog integration step.\"\"\"\n        positions, velocities, masses = state\n        \n        # Compute forces\n        forces = compute_forces(positions, masses)\n        accelerations = forces / masses[:, None]\n        \n        # Leapfrog update\n        velocities_half = velocities + 0.5 * dt * accelerations\n        positions_new = positions + dt * velocities_half\n        \n        forces_new = compute_forces(positions_new, masses)\n        accelerations_new = forces_new / masses[:, None]\n        \n        velocities_new = velocities_half + 0.5 * dt * accelerations_new\n        \n        return (positions_new, velocities_new, masses)\n    \n    # Energy calculations for verification\n    @jit\n    def total_energy(state):\n        \"\"\"Calculate total energy of the system.\"\"\"\n        positions, velocities, masses = state\n        \n        # Kinetic energy\n        kinetic = 0.5 * jnp.sum(masses[:, None] * velocities**2)\n        \n        # Potential energy\n        potential = 0.0\n        n = len(masses)\n        for i in range(n):\n            for j in range(i+1, n):\n                r_ij = jnp.linalg.norm(positions[i] - positions[j])\n                r_soft = jnp.maximum(r_ij, 0.01)\n                potential -= masses[i] * masses[j] / r_soft\n        \n        return kinetic + potential\n    \n    # Simulate function\n    @jit\n    def simulate(initial_state, dt, n_steps):\n        \"\"\"Run full simulation.\"\"\"\n        \n        def step(carry, _):\n            state = carry\n            new_state = leapfrog_step(state, dt)\n            energy = total_energy(new_state)\n            return new_state, (new_state[0], energy)  # Return positions and energy\n        \n        final_state, (trajectory, energies) = scan(\n            step, initial_state, None, length=n_steps\n        )\n        \n        return final_state, trajectory, energies\n    \n    # Set up initial conditions (Pythagorean 3-body)\n    positions = jnp.array([\n        [1.0, 3.0, 0.0],\n        [-2.0, -1.0, 0.0],\n        [1.0, -1.0, 0.0]\n    ])\n    \n    velocities = jnp.array([\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0]\n    ])\n    \n    masses = jnp.array([3.0, 4.0, 5.0])\n    \n    initial_state = (positions, velocities, masses)\n    \n    # Run simulation\n    print(\"\\nRunning simulation...\")\n    start_time = time.perf_counter()\n    \n    final_state, trajectory, energies = simulate(\n        initial_state, dt=0.001, n_steps=10000\n    )\n    \n    elapsed = time.perf_counter() - start_time\n    print(f\"  Simulated 10,000 steps in {elapsed:.3f} seconds\")\n    \n    # Check energy conservation\n    initial_energy = total_energy(initial_state)\n    final_energy = total_energy(final_state)\n    \n    print(f\"\\n  Initial energy: {initial_energy:.6f}\")\n    print(f\"  Final energy: {final_energy:.6f}\")\n    print(f\"  Relative error: {abs(final_energy - initial_energy) / abs(initial_energy):.2e}\")\n    \n    # Plot results\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n    \n    # Trajectories\n    for i in range(3):\n        ax1.plot(trajectory[:, i, 0], trajectory[:, i, 1], \n                alpha=0.7, linewidth=1, label=f'Mass {masses[i]:.1f}')\n    ax1.set_xlabel('X')\n    ax1.set_ylabel('Y')\n    ax1.set_title('Three-Body Trajectories')\n    ax1.legend()\n    ax1.axis('equal')\n    ax1.grid(True, alpha=0.3)\n    \n    # Energy conservation\n    ax2.plot((energies - initial_energy) / abs(initial_energy))\n    ax2.set_xlabel('Time step')\n    ax2.set_ylabel('Relative energy error')\n    ax2.set_title('Energy Conservation')\n    ax2.grid(True, alpha=0.3)\n    ax2.set_yscale('log')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return trajectory\n\ntrajectory = nbody_simulation_complete()","position":{"start":{"line":764,"column":1},"end":{"line":919,"column":1}},"key":"StI59fCwoF"},{"type":"heading","depth":2,"position":{"start":{"line":921,"column":1},"end":{"line":921,"column":1}},"children":[{"type":"text","value":"Try It Yourself","position":{"start":{"line":921,"column":1},"end":{"line":921,"column":1}},"key":"DQ4ph2UJ0f"}],"identifier":"try-it-yourself","label":"Try It Yourself","html_id":"try-it-yourself","implicit":true,"key":"pvHDHUMhwv"},{"type":"heading","depth":3,"position":{"start":{"line":923,"column":1},"end":{"line":923,"column":1}},"children":[{"type":"text","value":"Exercise 1: Differentiable Cosmology","position":{"start":{"line":923,"column":1},"end":{"line":923,"column":1}},"key":"IIlngcUEVy"}],"identifier":"exercise-1-differentiable-cosmology","label":"Exercise 1: Differentiable Cosmology","html_id":"exercise-1-differentiable-cosmology","implicit":true,"key":"BcppzFR5ZH"},{"type":"code","lang":"python","value":"def differentiable_cosmology():\n    \"\"\"\n    Build a differentiable cosmological distance calculator.\n    \n    Tasks:\n    1. Implement luminosity distance as function of z, H0, Omega_m, Omega_Lambda\n    2. Use grad to get derivatives with respect to all parameters\n    3. JIT compile for speed\n    4. Fit to supernova data using gradient descent\n    \"\"\"\n    # Your code here\n    pass","position":{"start":{"line":925,"column":1},"end":{"line":938,"column":1}},"key":"C7sb0U6hig"},{"type":"heading","depth":3,"position":{"start":{"line":940,"column":1},"end":{"line":940,"column":1}},"children":[{"type":"text","value":"Exercise 2: Vectorized Light Curve Analysis","position":{"start":{"line":940,"column":1},"end":{"line":940,"column":1}},"key":"s66ACmEHPW"}],"identifier":"exercise-2-vectorized-light-curve-analysis","label":"Exercise 2: Vectorized Light Curve Analysis","html_id":"exercise-2-vectorized-light-curve-analysis","implicit":true,"key":"cln3ZmpAYy"},{"type":"code","lang":"python","value":"def analyze_light_curves_jax(times, fluxes, periods_to_test):\n    \"\"\"\n    Analyze multiple light curves using JAX.\n    \n    Requirements:\n    1. Use vmap to process multiple light curves in parallel\n    2. JIT compile the period-finding algorithm\n    3. Implement Lomb-Scargle using JAX operations\n    4. Return best periods and their uncertainties\n    \"\"\"\n    # Your code here\n    pass","position":{"start":{"line":942,"column":1},"end":{"line":955,"column":1}},"key":"QVK2AiKjf0"},{"type":"heading","depth":3,"position":{"start":{"line":957,"column":1},"end":{"line":957,"column":1}},"children":[{"type":"text","value":"Exercise 3: Differentiable Ray Tracing","position":{"start":{"line":957,"column":1},"end":{"line":957,"column":1}},"key":"hsxOa7tOF0"}],"identifier":"exercise-3-differentiable-ray-tracing","label":"Exercise 3: Differentiable Ray Tracing","html_id":"exercise-3-differentiable-ray-tracing","implicit":true,"key":"iOkNUekupx"},{"type":"code","lang":"python","value":"def ray_tracing_jax(rays, lens_params):\n    \"\"\"\n    Differentiable ray tracing through gravitational lens.\n    \n    Tasks:\n    1. Trace rays through gravitational potential\n    2. Use grad to optimize lens parameters\n    3. Implement critical curves and caustics\n    4. JIT compile for real-time visualization\n    \"\"\"\n    # Your code here\n    pass","position":{"start":{"line":959,"column":1},"end":{"line":972,"column":1}},"key":"rzfs3whP5X"},{"type":"heading","depth":2,"position":{"start":{"line":974,"column":1},"end":{"line":974,"column":1}},"children":[{"type":"text","value":"Key Takeaways","position":{"start":{"line":974,"column":1},"end":{"line":974,"column":1}},"key":"UR2iSLYv6L"}],"identifier":"key-takeaways","label":"Key Takeaways","html_id":"key-takeaways","implicit":true,"key":"iKBM0rEbP3"},{"type":"paragraph","position":{"start":{"line":976,"column":1},"end":{"line":983,"column":1}},"children":[{"type":"text","value":"✅ ","position":{"start":{"line":976,"column":1},"end":{"line":976,"column":1}},"key":"kdfnGEuRAx"},{"type":"strong","position":{"start":{"line":976,"column":1},"end":{"line":976,"column":1}},"children":[{"type":"text","value":"JAX = NumPy + autodiff + JIT + vmap","position":{"start":{"line":976,"column":1},"end":{"line":976,"column":1}},"key":"FJOLMAbolU"}],"key":"NogQjJxEU2"},{"type":"text","value":" - Composable transformations","position":{"start":{"line":976,"column":1},"end":{"line":976,"column":1}},"key":"u9SRMpGgrq"},{"type":"break","position":{"start":{"line":976,"column":1},"end":{"line":976,"column":1}},"key":"K2NGB5eydl"},{"type":"text","value":"✅ ","position":{"start":{"line":976,"column":1},"end":{"line":976,"column":1}},"key":"w2z8yvIbAx"},{"type":"strong","position":{"start":{"line":976,"column":1},"end":{"line":976,"column":1}},"children":[{"type":"text","value":"Functional programming required","position":{"start":{"line":976,"column":1},"end":{"line":976,"column":1}},"key":"CfMD82kWS0"}],"key":"VksofAe0Gi"},{"type":"text","value":" - No mutations, pure functions only","position":{"start":{"line":976,"column":1},"end":{"line":976,"column":1}},"key":"OY7DIWa7Ps"},{"type":"break","position":{"start":{"line":976,"column":1},"end":{"line":976,"column":1}},"key":"iwrG4V99Zd"},{"type":"text","value":"✅ ","position":{"start":{"line":976,"column":1},"end":{"line":976,"column":1}},"key":"jkYN3RxL6i"},{"type":"strong","position":{"start":{"line":976,"column":1},"end":{"line":976,"column":1}},"children":[{"type":"text","value":"Automatic differentiation","position":{"start":{"line":976,"column":1},"end":{"line":976,"column":1}},"key":"FnvRIkRHyl"}],"key":"C3Ce8jsdNR"},{"type":"text","value":" - Get gradients of any function for free","position":{"start":{"line":976,"column":1},"end":{"line":976,"column":1}},"key":"XA8jwPCUaO"},{"type":"break","position":{"start":{"line":976,"column":1},"end":{"line":976,"column":1}},"key":"QDPrKIKhUI"},{"type":"text","value":"✅ ","position":{"start":{"line":976,"column":1},"end":{"line":976,"column":1}},"key":"PD59LsgNtI"},{"type":"strong","position":{"start":{"line":976,"column":1},"end":{"line":976,"column":1}},"children":[{"type":"text","value":"JIT compilation","position":{"start":{"line":976,"column":1},"end":{"line":976,"column":1}},"key":"PSSYQw93rc"}],"key":"OKBV9yKZRi"},{"type":"text","value":" - Near C++ speeds from Python code","position":{"start":{"line":976,"column":1},"end":{"line":976,"column":1}},"key":"xMYjQgEj4f"},{"type":"break","position":{"start":{"line":976,"column":1},"end":{"line":976,"column":1}},"key":"m6jYG0t2Ew"},{"type":"text","value":"✅ ","position":{"start":{"line":976,"column":1},"end":{"line":976,"column":1}},"key":"eKjmvCarem"},{"type":"strong","position":{"start":{"line":976,"column":1},"end":{"line":976,"column":1}},"children":[{"type":"text","value":"vmap for parallelization","position":{"start":{"line":976,"column":1},"end":{"line":976,"column":1}},"key":"ADEuRCalrs"}],"key":"DrVwX45nzI"},{"type":"text","value":" - Vectorize any function automatically","position":{"start":{"line":976,"column":1},"end":{"line":976,"column":1}},"key":"LaRaDxvAbW"},{"type":"break","position":{"start":{"line":976,"column":1},"end":{"line":976,"column":1}},"key":"mqf1SglTM0"},{"type":"text","value":"✅ ","position":{"start":{"line":976,"column":1},"end":{"line":976,"column":1}},"key":"aZdM3wUhus"},{"type":"strong","position":{"start":{"line":976,"column":1},"end":{"line":976,"column":1}},"children":[{"type":"text","value":"Explicit random keys","position":{"start":{"line":976,"column":1},"end":{"line":976,"column":1}},"key":"xgtFqZjAhI"}],"key":"agvNXVcwe5"},{"type":"text","value":" - Reproducible randomness in parallel","position":{"start":{"line":976,"column":1},"end":{"line":976,"column":1}},"key":"zW3uU6Frr5"},{"type":"break","position":{"start":{"line":976,"column":1},"end":{"line":976,"column":1}},"key":"yw9f12GOg3"},{"type":"text","value":"✅ ","position":{"start":{"line":976,"column":1},"end":{"line":976,"column":1}},"key":"dljoK1ViJF"},{"type":"strong","position":{"start":{"line":976,"column":1},"end":{"line":976,"column":1}},"children":[{"type":"text","value":"Composable transformations","position":{"start":{"line":976,"column":1},"end":{"line":976,"column":1}},"key":"ffz9omyp0G"}],"key":"BxXVDDpR5P"},{"type":"text","value":" - Combine JIT + grad + vmap freely","position":{"start":{"line":976,"column":1},"end":{"line":976,"column":1}},"key":"OpMAcIqpkI"},{"type":"break","position":{"start":{"line":976,"column":1},"end":{"line":976,"column":1}},"key":"F44jnmhvle"},{"type":"text","value":"✅ ","position":{"start":{"line":976,"column":1},"end":{"line":976,"column":1}},"key":"TYqh5uWlM5"},{"type":"strong","position":{"start":{"line":976,"column":1},"end":{"line":976,"column":1}},"children":[{"type":"text","value":"GPU/TPU ready","position":{"start":{"line":976,"column":1},"end":{"line":976,"column":1}},"key":"gS4zNvqC1C"}],"key":"CV6HrqpRxi"},{"type":"text","value":" - Same code runs on accelerators","position":{"start":{"line":976,"column":1},"end":{"line":976,"column":1}},"key":"h8zm2xxjyd"}],"key":"AGJv8FRuPC"},{"type":"heading","depth":2,"position":{"start":{"line":985,"column":1},"end":{"line":985,"column":1}},"children":[{"type":"text","value":"Next Chapter Preview","position":{"start":{"line":985,"column":1},"end":{"line":985,"column":1}},"key":"w3mrYs100F"}],"identifier":"next-chapter-preview","label":"Next Chapter Preview","html_id":"next-chapter-preview","implicit":true,"key":"ThJAGZ5LMQ"},{"type":"paragraph","position":{"start":{"line":986,"column":1},"end":{"line":986,"column":1}},"children":[{"type":"text","value":"JAX Advanced Patterns: Control flow, custom derivatives, and performance optimization for large-scale astronomical simulations.","position":{"start":{"line":986,"column":1},"end":{"line":986,"column":1}},"key":"J3Fd7XuarT"}],"key":"D9a9G27N4w"}],"key":"mcm7UoSR64"}],"key":"YQG6m2V406"},"references":{"cite":{"order":[],"data":{}}}}