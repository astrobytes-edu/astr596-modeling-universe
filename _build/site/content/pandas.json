{"version":2,"kind":"Article","sha256":"024aee4f2f2a7d569e5ab191b4c90a4becf190a66d62e37b09ec2541c721936a","slug":"pandas","location":"/03-scientific-computing-with-python/03-advanced-scientific-computing/12-pandas.md","dependencies":[],"frontmatter":{"title":"Pandas: Data Science for Astronomy","content_includes_title":false,"authors":[{"nameParsed":{"literal":"Anna Rosen","given":"Anna","family":"Rosen"},"name":"Anna Rosen","orcid":"0000-0003-4423-0660","email":"alrosen@sdsu.edu","affiliations":["San Diego State University"],"id":"contributors-myst-generated-uid-0","corresponding":true}],"license":{"content":{"id":"CC-BY-4.0","url":"https://creativecommons.org/licenses/by/4.0/","name":"Creative Commons Attribution 4.0 International","free":true,"CC":true},"code":{"id":"MIT","url":"https://opensource.org/licenses/MIT","name":"MIT License","free":true,"osi":true}},"github":"https://github.com/astrobytes-edu/astr596-modeling-universe","subject":"Modeling the Universe","venue":{"title":"ASTR 596 - Fall 2025","url":"https://www.anna-rosen.com"},"keywords":["computational astrophysics","python","numerical methods","machine learning","monte carlo","neural networks","radiative transfer","bayesian inference","JAX"],"affiliations":[{"id":"San Diego State University","name":"San Diego State University"}],"numbering":{"title":{"offset":2}},"edit_url":"https://github.com/astrobytes-edu/astr596-modeling-universe/blob/main/03-scientific-computing-with-python/03-advanced-scientific-computing/12-pandas.md","exports":[{"format":"md","filename":"12-pandas.md","url":"/12-pandas-461f1e56fc1d0f20b996de3b82e6460d.md"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"heading","depth":2,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Learning Objectives","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"PWUsbG5fEq"}],"identifier":"learning-objectives","label":"Learning Objectives","html_id":"learning-objectives","implicit":true,"key":"cQQePBKmcf"},{"type":"paragraph","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"By the end of this chapter, you will:","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"YdFCrRWIIF"}],"key":"sG9AsWh71y"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":5,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Master DataFrame operations for astronomical catalogs","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"yaneWnvWlk"}],"key":"YAIEclOsT8"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"Perform efficient time series analysis on light curves","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"Mys3fY2FmD"}],"key":"PWRRdTe0Ov"},{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"Execute complex GroupBy operations for population studies","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"H6ToKX8qhW"}],"key":"mp0p5Fr4Xm"},{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"Merge and join datasets from multiple surveys","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"t5gKjVWGHd"}],"key":"vEG0PbP5Hw"},{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"Prepare data for machine learning pipelines","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"r8clxLmqZJ"}],"key":"e8TNSwLTam"},{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"Handle missing data and outliers systematically","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"GA6f2dUlcc"}],"key":"yXLUOzZtYp"},{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"Scale to large astronomical surveys with optimization techniques","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"v7qUTwPn05"}],"key":"rz2BGWxYVM"}],"key":"jjGH6gegcZ"},{"type":"heading","depth":2,"position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"Introduction: Why Pandas for Astronomy?","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"Tk0GEQ7EwY"}],"identifier":"introduction-why-pandas-for-astronomy","label":"Introduction: Why Pandas for Astronomy?","html_id":"introduction-why-pandas-for-astronomy","implicit":true,"key":"fTLRzYytrg"},{"type":"code","lang":"python","value":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\nimport seaborn as sns\nsns.set_style('whitegrid')\n\ndef why_pandas_for_astronomy():\n    \"\"\"Demonstrate Pandas' value for astronomical data science.\"\"\"\n    \n    print(\"Pandas bridges astronomy and data science:\")\n    print(\"\\n1. CATALOG MANAGEMENT\")\n    print(\"   - Millions of sources from surveys (SDSS, Gaia, LSST)\")\n    print(\"   - Heterogeneous data types (positions, magnitudes, spectra)\")\n    print(\"   - Missing values and quality flags\")\n    \n    print(\"\\n2. TIME SERIES ANALYSIS\")\n    print(\"   - Irregular sampling from ground-based telescopes\")\n    print(\"   - Multi-band light curves\")\n    print(\"   - Automated classification of variables\")\n    \n    print(\"\\n3. DATA SCIENCE WORKFLOW\")\n    print(\"   - Exploratory data analysis (EDA)\")\n    print(\"   - Feature engineering for ML\")\n    print(\"   - Statistical summaries by groups\")\n    \n    print(\"\\n4. MACHINE LEARNING PREPARATION\")\n    print(\"   - Data cleaning and normalization\")\n    print(\"   - Feature extraction\")\n    print(\"   - Train/test splitting while preserving groups\")\n    \n    # Quick example: Load and explore a galaxy catalog\n    np.random.seed(42)\n    n_galaxies = 10000\n    \n    catalog = pd.DataFrame({\n        'galaxy_id': np.arange(n_galaxies),\n        'ra': np.random.uniform(0, 360, n_galaxies),\n        'dec': np.random.uniform(-90, 90, n_galaxies),\n        'redshift': np.random.gamma(2, 0.5, n_galaxies),\n        'magnitude_r': np.random.normal(20, 2, n_galaxies),\n        'magnitude_g': np.random.normal(21, 2, n_galaxies),\n        'stellar_mass': 10**np.random.normal(10, 0.5, n_galaxies),\n        'morphology': np.random.choice(['E', 'S0', 'Sa', 'Sb', 'Sc', 'Irr'], n_galaxies),\n        'survey': np.random.choice(['SDSS', 'DECALS', 'HSC'], n_galaxies, p=[0.5, 0.3, 0.2])\n    })\n    \n    print(f\"\\n5. EXAMPLE CATALOG OPERATIONS:\")\n    print(f\"   Shape: {catalog.shape}\")\n    print(f\"   Memory usage: {catalog.memory_usage().sum() / 1e6:.1f} MB\")\n    print(f\"   Surveys: {catalog['survey'].value_counts().to_dict()}\")\n    print(f\"   Redshift range: {catalog['redshift'].min():.2f} - {catalog['redshift'].max():.2f}\")\n    \n    return catalog\n\ncatalog = why_pandas_for_astronomy()","position":{"start":{"line":15,"column":1},"end":{"line":72,"column":1}},"key":"W3ULrG4eDl"},{"type":"heading","depth":2,"position":{"start":{"line":74,"column":1},"end":{"line":74,"column":1}},"children":[{"type":"text","value":"DataFrames for Astronomical Catalogs","position":{"start":{"line":74,"column":1},"end":{"line":74,"column":1}},"key":"N9s0myW58d"}],"identifier":"dataframes-for-astronomical-catalogs","label":"DataFrames for Astronomical Catalogs","html_id":"dataframes-for-astronomical-catalogs","implicit":true,"key":"mCXnRXh3XZ"},{"type":"heading","depth":3,"position":{"start":{"line":76,"column":1},"end":{"line":76,"column":1}},"children":[{"type":"text","value":"Creating and Loading Catalogs","position":{"start":{"line":76,"column":1},"end":{"line":76,"column":1}},"key":"vfb1wXXWDJ"}],"identifier":"creating-and-loading-catalogs","label":"Creating and Loading Catalogs","html_id":"creating-and-loading-catalogs","implicit":true,"key":"l7twll5kXI"},{"type":"code","lang":"python","value":"def catalog_operations():\n    \"\"\"Essential DataFrame operations for astronomical catalogs.\"\"\"\n    \n    # Create a realistic stellar catalog\n    n_stars = 5000\n    \n    # Generate synthetic Gaia-like data\n    stars = pd.DataFrame({\n        'source_id': np.arange(1000000, 1000000 + n_stars),\n        'ra': np.random.uniform(120, 140, n_stars),  # Limited sky region\n        'dec': np.random.uniform(-30, -10, n_stars),\n        'parallax': np.random.gamma(2, 0.5, n_stars),  # mas\n        'parallax_error': np.random.gamma(1, 0.05, n_stars),\n        'pmra': np.random.normal(0, 5, n_stars),  # mas/yr\n        'pmdec': np.random.normal(0, 5, n_stars),  # mas/yr\n        'phot_g_mean_mag': np.random.normal(15, 2, n_stars),\n        'phot_bp_mean_mag': np.random.normal(15.5, 2, n_stars),\n        'phot_rp_mean_mag': np.random.normal(14.5, 2, n_stars),\n        'radial_velocity': np.random.normal(0, 30, n_stars),\n        'teff_val': np.random.normal(5500, 1000, n_stars),\n        'logg_val': np.random.normal(4.5, 0.5, n_stars),\n        'fe_h': np.random.normal(0, 0.3, n_stars),\n    })\n    \n    # Add some missing values (realistic)\n    rv_missing = np.random.random(n_stars) > 0.3  # 70% have RV\n    stars.loc[rv_missing, 'radial_velocity'] = np.nan\n    \n    print(\"1. BASIC CATALOG INFO:\")\n    print(stars.info())\n    \n    print(\"\\n2. STATISTICAL SUMMARY:\")\n    print(stars[['parallax', 'phot_g_mean_mag', 'teff_val']].describe())\n    \n    # Add derived columns\n    print(\"\\n3. ADDING DERIVED QUANTITIES:\")\n    \n    # Distance from parallax\n    stars['distance_pc'] = 1000 / stars['parallax']\n    \n    # Absolute magnitude\n    stars['abs_mag_g'] = stars['phot_g_mean_mag'] - 5 * np.log10(stars['distance_pc']) + 5\n    \n    # Color indices\n    stars['bp_rp'] = stars['phot_bp_mean_mag'] - stars['phot_rp_mean_mag']\n    stars['g_rp'] = stars['phot_g_mean_mag'] - stars['phot_rp_mean_mag']\n    \n    # Quality flags\n    stars['high_quality'] = (\n        (stars['parallax_error'] / stars['parallax'] < 0.1) &  # Good parallax\n        (stars['parallax'] > 0) &  # Positive parallax\n        (stars['phot_g_mean_mag'] < 18)  # Bright enough\n    )\n    \n    print(f\"High quality stars: {stars['high_quality'].sum()} / {len(stars)}\")\n    \n    # Efficient selection\n    print(\"\\n4. EFFICIENT DATA SELECTION:\")\n    \n    # Method 1: Boolean indexing\n    nearby = stars[stars['distance_pc'] < 100]\n    print(f\"Stars within 100 pc: {len(nearby)}\")\n    \n    # Method 2: Query method (more readable)\n    bright_nearby = stars.query('distance_pc < 100 & phot_g_mean_mag < 10')\n    print(f\"Bright nearby stars: {len(bright_nearby)}\")\n    \n    # Method 3: loc for complex conditions\n    solar_type = stars.loc[\n        (stars['teff_val'].between(5300, 6000)) &\n        (stars['logg_val'].between(4.0, 4.6)) &\n        (stars['fe_h'].between(-0.1, 0.1))\n    ]\n    print(f\"Solar-type stars: {len(solar_type)}\")\n    \n    return stars\n\nstars_df = catalog_operations()","position":{"start":{"line":78,"column":1},"end":{"line":157,"column":1}},"key":"hFRW3KiDzS"},{"type":"heading","depth":3,"position":{"start":{"line":159,"column":1},"end":{"line":159,"column":1}},"children":[{"type":"text","value":"Advanced Indexing and MultiIndex","position":{"start":{"line":159,"column":1},"end":{"line":159,"column":1}},"key":"oNnIrTRwgM"}],"identifier":"advanced-indexing-and-multiindex","label":"Advanced Indexing and MultiIndex","html_id":"advanced-indexing-and-multiindex","implicit":true,"key":"l20qxJw94h"},{"type":"code","lang":"python","value":"def advanced_indexing():\n    \"\"\"Advanced indexing techniques for hierarchical astronomical data.\"\"\"\n    \n    # Create multi-band photometry catalog\n    n_objects = 1000\n    n_epochs = 5\n    \n    # Generate hierarchical data\n    data = []\n    for obj_id in range(n_objects):\n        for epoch in range(n_epochs):\n            for band in ['u', 'g', 'r', 'i', 'z']:\n                mjd = 58000 + epoch * 30 + np.random.uniform(-1, 1)\n                mag = np.random.normal(20 + ord(band) * 0.01, 0.1)\n                err = np.random.uniform(0.01, 0.05)\n                \n                data.append({\n                    'object_id': obj_id,\n                    'mjd': mjd,\n                    'band': band,\n                    'magnitude': mag,\n                    'error': err\n                })\n    \n    photometry = pd.DataFrame(data)\n    \n    # Create MultiIndex\n    photometry_indexed = photometry.set_index(['object_id', 'mjd', 'band'])\n    \n    print(\"1. MULTIINDEX STRUCTURE:\")\n    print(photometry_indexed.head(10))\n    \n    # Access specific levels\n    print(\"\\n2. ACCESSING DATA:\")\n    \n    # Single object, all epochs\n    obj_0 = photometry_indexed.loc[0]\n    print(f\"Object 0 measurements: {len(obj_0)}\")\n    \n    # Cross-section: all g-band measurements\n    g_band = photometry_indexed.xs('g', level='band')\n    print(f\"G-band measurements: {len(g_band)}\")\n    \n    # Pivot for analysis\n    print(\"\\n3. PIVOTING FOR ANALYSIS:\")\n    \n    # Wide format for color analysis\n    colors = photometry.pivot_table(\n        values='magnitude',\n        index=['object_id', 'mjd'],\n        columns='band',\n        aggfunc='mean'\n    )\n    \n    # Calculate colors\n    colors['g-r'] = colors['g'] - colors['r']\n    colors['r-i'] = colors['r'] - colors['i']\n    \n    print(colors.head())\n    \n    # Hierarchical grouping\n    print(\"\\n4. HIERARCHICAL GROUPING:\")\n    \n    # Mean magnitude per object per band\n    mean_mags = photometry.groupby(['object_id', 'band'])['magnitude'].agg(['mean', 'std'])\n    print(mean_mags.head(10))\n    \n    return photometry, colors\n\nphotometry_df, colors_df = advanced_indexing()","position":{"start":{"line":161,"column":1},"end":{"line":232,"column":1}},"key":"OsQt6141xR"},{"type":"heading","depth":2,"position":{"start":{"line":234,"column":1},"end":{"line":234,"column":1}},"children":[{"type":"text","value":"Time Series Analysis for Astronomy","position":{"start":{"line":234,"column":1},"end":{"line":234,"column":1}},"key":"ISS5zKPPpr"}],"identifier":"time-series-analysis-for-astronomy","label":"Time Series Analysis for Astronomy","html_id":"time-series-analysis-for-astronomy","implicit":true,"key":"KZEU0gQwtW"},{"type":"heading","depth":3,"position":{"start":{"line":236,"column":1},"end":{"line":236,"column":1}},"children":[{"type":"text","value":"Light Curve Analysis","position":{"start":{"line":236,"column":1},"end":{"line":236,"column":1}},"key":"WXTCtlZOea"}],"identifier":"light-curve-analysis","label":"Light Curve Analysis","html_id":"light-curve-analysis","implicit":true,"key":"npCDkIdv8n"},{"type":"code","lang":"python","value":"def time_series_astronomy():\n    \"\"\"Time series analysis for variable stars and transients.\"\"\"\n    \n    # Generate realistic variable star light curve\n    np.random.seed(42)\n    \n    # RR Lyrae-like variable\n    true_period = 0.5427  # days\n    amplitude = 0.8\n    \n    # Irregular sampling (ground-based reality)\n    n_nights = 150\n    observations = []\n    \n    for night in range(n_nights):\n        # Weather: 30% chance of no observation\n        if np.random.random() < 0.3:\n            continue\n            \n        # Multiple observations per night\n        n_obs = np.random.poisson(3)\n        for _ in range(n_obs):\n            mjd = 58000 + night + np.random.uniform(0, 0.3)\n            phase = (mjd % true_period) / true_period\n            \n            # RR Lyrae-like light curve shape\n            mag = 14.5 - amplitude * (0.5 * np.sin(2*np.pi*phase) + \n                                      0.3 * np.sin(4*np.pi*phase) +\n                                      0.1 * np.sin(6*np.pi*phase))\n            \n            # Add noise\n            mag += np.random.normal(0, 0.02)\n            error = np.random.uniform(0.015, 0.025)\n            \n            observations.append({\n                'mjd': mjd,\n                'magnitude': mag,\n                'error': error,\n                'band': 'V'\n            })\n    \n    # Create DataFrame\n    lightcurve = pd.DataFrame(observations)\n    lightcurve['datetime'] = pd.to_datetime(lightcurve['mjd'] - 40587, unit='D', origin='unix')\n    lightcurve = lightcurve.sort_values('mjd')\n    \n    print(\"1. LIGHT CURVE DATA:\")\n    print(lightcurve.info())\n    print(f\"Time span: {lightcurve['mjd'].max() - lightcurve['mjd'].min():.1f} days\")\n    print(f\"Number of observations: {len(lightcurve)}\")\n    \n    # Time series features for ML\n    print(\"\\n2. FEATURE EXTRACTION FOR ML:\")\n    \n    features = {}\n    \n    # Basic statistics\n    features['mean_mag'] = lightcurve['magnitude'].mean()\n    features['std_mag'] = lightcurve['magnitude'].std()\n    features['amplitude'] = lightcurve['magnitude'].max() - lightcurve['magnitude'].min()\n    \n    # Percentiles\n    features['mag_5'] = lightcurve['magnitude'].quantile(0.05)\n    features['mag_95'] = lightcurve['magnitude'].quantile(0.95)\n    \n    # Time series specific\n    features['n_observations'] = len(lightcurve)\n    features['timespan'] = lightcurve['mjd'].max() - lightcurve['mjd'].min()\n    features['mean_sampling'] = features['timespan'] / features['n_observations']\n    \n    # Changes between consecutive observations\n    lightcurve['mag_diff'] = lightcurve['magnitude'].diff()\n    lightcurve['time_diff'] = lightcurve['mjd'].diff()\n    lightcurve['rate_of_change'] = lightcurve['mag_diff'] / lightcurve['time_diff']\n    \n    features['mean_rate_change'] = lightcurve['rate_of_change'].abs().mean()\n    features['max_rate_change'] = lightcurve['rate_of_change'].abs().max()\n    \n    # Skewness and kurtosis (shape indicators)\n    features['skewness'] = lightcurve['magnitude'].skew()\n    features['kurtosis'] = lightcurve['magnitude'].kurtosis()\n    \n    # Beyond features (for period finding)\n    from scipy.stats import skew, kurtosis\n    features['beyond1std'] = ((lightcurve['magnitude'] - features['mean_mag']).abs() > \n                              features['std_mag']).mean()\n    \n    print(\"Extracted features for ML:\")\n    for key, value in features.items():\n        print(f\"  {key}: {value:.3f}\")\n    \n    # Resampling for regular time series\n    print(\"\\n3. RESAMPLING FOR ANALYSIS:\")\n    \n    lightcurve_indexed = lightcurve.set_index('datetime')\n    \n    # Daily binning\n    daily_lc = lightcurve_indexed.resample('1D').agg({\n        'magnitude': ['mean', 'std', 'count'],\n        'error': 'mean'\n    })\n    daily_lc.columns = ['_'.join(col).strip() for col in daily_lc.columns]\n    daily_lc = daily_lc[daily_lc['magnitude_count'] > 0]\n    \n    print(f\"Daily binned: {len(daily_lc)} days with data\")\n    \n    # Rolling statistics (for trend detection)\n    print(\"\\n4. ROLLING WINDOW ANALYSIS:\")\n    \n    window_size = 10  # days\n    lightcurve_indexed['rolling_mean'] = lightcurve_indexed['magnitude'].rolling(\n        window=f'{window_size}D', min_periods=5\n    ).mean()\n    \n    lightcurve_indexed['rolling_std'] = lightcurve_indexed['magnitude'].rolling(\n        window=f'{window_size}D', min_periods=5\n    ).std()\n    \n    # Detect outliers\n    lightcurve_indexed['outlier'] = (\n        np.abs(lightcurve_indexed['magnitude'] - lightcurve_indexed['rolling_mean']) > \n        3 * lightcurve_indexed['rolling_std']\n    )\n    \n    print(f\"Outliers detected: {lightcurve_indexed['outlier'].sum()}\")\n    \n    # Period analysis preparation\n    print(\"\\n5. PERIOD ANALYSIS PREPARATION:\")\n    \n    # Create phase-folded DataFrame for different trial periods\n    def phase_fold(lc_df, period):\n        \"\"\"Phase fold light curve at given period.\"\"\"\n        df = lc_df.copy()\n        df['phase'] = (df['mjd'] % period) / period\n        return df\n    \n    # Try multiple periods\n    trial_periods = np.linspace(0.5, 0.6, 100)\n    chi2_values = []\n    \n    for period in trial_periods:\n        folded = phase_fold(lightcurve, period)\n        \n        # Bin in phase\n        phase_bins = np.linspace(0, 1, 20)\n        binned = folded.groupby(pd.cut(folded['phase'], phase_bins))['magnitude'].agg(['mean', 'std'])\n        \n        # Simple chi-squared\n        chi2 = binned['std'].mean() if not binned['std'].isna().all() else np.inf\n        chi2_values.append(chi2)\n    \n    best_period_idx = np.argmin(chi2_values)\n    best_period = trial_periods[best_period_idx]\n    \n    print(f\"Best period found: {best_period:.4f} days (true: {true_period:.4f})\")\n    \n    return lightcurve, features\n\nlightcurve_df, ml_features = time_series_astronomy()","position":{"start":{"line":238,"column":1},"end":{"line":398,"column":1}},"key":"uOq4aFesLq"},{"type":"heading","depth":2,"position":{"start":{"line":400,"column":1},"end":{"line":400,"column":1}},"children":[{"type":"text","value":"GroupBy Operations for Population Studies","position":{"start":{"line":400,"column":1},"end":{"line":400,"column":1}},"key":"LuREr5vMlW"}],"identifier":"groupby-operations-for-population-studies","label":"GroupBy Operations for Population Studies","html_id":"groupby-operations-for-population-studies","implicit":true,"key":"tmgzmLbeuJ"},{"type":"heading","depth":3,"position":{"start":{"line":402,"column":1},"end":{"line":402,"column":1}},"children":[{"type":"text","value":"Analyzing Stellar Populations","position":{"start":{"line":402,"column":1},"end":{"line":402,"column":1}},"key":"jbVpf9lnU0"}],"identifier":"analyzing-stellar-populations","label":"Analyzing Stellar Populations","html_id":"analyzing-stellar-populations","implicit":true,"key":"QNQlKYjMNn"},{"type":"code","lang":"python","value":"def population_analysis():\n    \"\"\"GroupBy operations for stellar population studies.\"\"\"\n    \n    # Create a large stellar survey dataset\n    n_stars = 50000\n    \n    # Multiple stellar populations\n    populations = []\n    \n    # Thin disk\n    n_thin = int(0.7 * n_stars)\n    thin_disk = pd.DataFrame({\n        'population': 'thin_disk',\n        'age_gyr': np.random.gamma(3, 1.5, n_thin),\n        'metallicity': np.random.normal(0, 0.2, n_thin),\n        'velocity_dispersion': np.random.gamma(20, 2, n_thin),\n        'scale_height_pc': np.random.normal(300, 50, n_thin),\n    })\n    \n    # Thick disk\n    n_thick = int(0.2 * n_stars)\n    thick_disk = pd.DataFrame({\n        'population': 'thick_disk',\n        'age_gyr': np.random.gamma(10, 2, n_thick),\n        'metallicity': np.random.normal(-0.5, 0.3, n_thick),\n        'velocity_dispersion': np.random.gamma(40, 5, n_thick),\n        'scale_height_pc': np.random.normal(900, 100, n_thick),\n    })\n    \n    # Halo\n    n_halo = n_stars - n_thin - n_thick\n    halo = pd.DataFrame({\n        'population': 'halo',\n        'age_gyr': np.random.gamma(12, 1, n_halo),\n        'metallicity': np.random.normal(-1.5, 0.5, n_halo),\n        'velocity_dispersion': np.random.gamma(100, 20, n_halo),\n        'scale_height_pc': np.random.exponential(3000, n_halo),\n    })\n    \n    # Combine populations\n    survey = pd.concat([thin_disk, thick_disk, halo], ignore_index=True)\n    \n    # Add observational properties\n    survey['apparent_mag'] = 10 + 5 * np.log10(survey['scale_height_pc']) + np.random.normal(0, 0.5, n_stars)\n    survey['color_index'] = 0.5 + 0.1 * survey['metallicity'] + np.random.normal(0, 0.1, n_stars)\n    \n    print(\"1. POPULATION STATISTICS:\")\n    population_stats = survey.groupby('population').agg({\n        'age_gyr': ['mean', 'std', 'median'],\n        'metallicity': ['mean', 'std'],\n        'velocity_dispersion': ['mean', 'std'],\n        'scale_height_pc': ['mean', 'median']\n    })\n    print(population_stats)\n    \n    print(\"\\n2. CUSTOM AGGREGATIONS:\")\n    \n    def weighted_mean(values, weights):\n        \"\"\"Calculate weighted mean.\"\"\"\n        return np.average(values, weights=weights)\n    \n    # Custom aggregation functions\n    agg_funcs = {\n        'age_gyr': ['mean', 'std', lambda x: np.percentile(x, 90)],\n        'metallicity': ['mean', 'median', lambda x: x.quantile(0.25)],\n        'velocity_dispersion': ['mean', 'max']\n    }\n    \n    custom_stats = survey.groupby('population').agg(agg_funcs)\n    custom_stats.columns = ['_'.join(col).strip() if col[1] else col[0] \n                            for col in custom_stats.columns]\n    print(custom_stats)\n    \n    print(\"\\n3. TRANSFORM OPERATIONS:\")\n    \n    # Normalize within groups\n    survey['age_normalized'] = survey.groupby('population')['age_gyr'].transform(\n        lambda x: (x - x.mean()) / x.std()\n    )\n    \n    # Rank within groups\n    survey['metallicity_rank'] = survey.groupby('population')['metallicity'].rank(\n        method='dense', ascending=False\n    )\n    \n    # Cumulative statistics\n    survey_sorted = survey.sort_values(['population', 'age_gyr'])\n    survey_sorted['cumulative_fraction'] = survey_sorted.groupby('population').cumcount() / \\\n                                           survey_sorted.groupby('population')['population'].transform('count')\n    \n    print(\"Sample of transformed data:\")\n    print(survey[['population', 'age_gyr', 'age_normalized', 'metallicity', 'metallicity_rank']].head(10))\n    \n    print(\"\\n4. GROUPED FILTERING:\")\n    \n    # Keep only populations with enough statistics\n    min_pop_size = 100\n    large_pops = survey.groupby('population').filter(lambda x: len(x) >= min_pop_size)\n    print(f\"Stars in large populations: {len(large_pops)} / {len(survey)}\")\n    \n    # Select extreme objects in each population\n    def select_extremes(group, n=100):\n        \"\"\"Select n most extreme objects by metallicity.\"\"\"\n        return group.nlargest(n, 'metallicity', keep='all')\n    \n    metal_rich = survey.groupby('population').apply(select_extremes, n=50)\n    print(f\"Metal-rich selection: {len(metal_rich)} stars\")\n    \n    print(\"\\n5. BINNING AND CATEGORICAL ANALYSIS:\")\n    \n    # Bin ages for analysis\n    survey['age_bin'] = pd.cut(survey['age_gyr'], \n                               bins=[0, 2, 5, 10, 15],\n                               labels=['young', 'intermediate', 'old', 'ancient'])\n    \n    # Cross-tabulation\n    cross_tab = pd.crosstab(survey['population'], survey['age_bin'], \n                            normalize='index') * 100\n    print(\"Age distribution by population (%):\")\n    print(cross_tab.round(1))\n    \n    return survey\n\nstellar_survey = population_analysis()","position":{"start":{"line":404,"column":1},"end":{"line":529,"column":1}},"key":"E3ii12XegR"},{"type":"heading","depth":2,"position":{"start":{"line":531,"column":1},"end":{"line":531,"column":1}},"children":[{"type":"text","value":"Data Preparation for Machine Learning","position":{"start":{"line":531,"column":1},"end":{"line":531,"column":1}},"key":"HqHian39tO"}],"identifier":"data-preparation-for-machine-learning","label":"Data Preparation for Machine Learning","html_id":"data-preparation-for-machine-learning","implicit":true,"key":"wit59Gm8uN"},{"type":"heading","depth":3,"position":{"start":{"line":533,"column":1},"end":{"line":533,"column":1}},"children":[{"type":"text","value":"Feature Engineering Pipeline","position":{"start":{"line":533,"column":1},"end":{"line":533,"column":1}},"key":"dE9KPzcgVn"}],"identifier":"feature-engineering-pipeline","label":"Feature Engineering Pipeline","html_id":"feature-engineering-pipeline","implicit":true,"key":"XMRnuwzJr0"},{"type":"code","lang":"python","value":"def ml_data_preparation():\n    \"\"\"Prepare astronomical data for machine learning.\"\"\"\n    \n    # Create a galaxy classification dataset\n    n_galaxies = 10000\n    \n    # Generate features\n    galaxies = pd.DataFrame({\n        # Photometry\n        'mag_u': np.random.normal(22, 1.5, n_galaxies),\n        'mag_g': np.random.normal(21, 1.5, n_galaxies),\n        'mag_r': np.random.normal(20, 1.5, n_galaxies),\n        'mag_i': np.random.normal(19.5, 1.5, n_galaxies),\n        'mag_z': np.random.normal(19, 1.5, n_galaxies),\n        \n        # Morphology\n        'petrosian_radius': np.random.lognormal(1, 0.5, n_galaxies),\n        'concentration': np.random.uniform(1.5, 5, n_galaxies),\n        'asymmetry': np.random.beta(2, 5, n_galaxies),\n        'smoothness': np.random.beta(2, 8, n_galaxies),\n        \n        # Spectroscopy\n        'redshift': np.random.gamma(2, 0.3, n_galaxies),\n        'velocity_dispersion': np.random.lognormal(5, 0.5, n_galaxies),\n        'h_alpha_flux': np.random.lognormal(-15, 1, n_galaxies),\n        'd4000_break': np.random.normal(1.5, 0.3, n_galaxies),\n        \n        # Environment\n        'n_neighbors_1mpc': np.random.poisson(5, n_galaxies),\n        'distance_to_nearest': np.random.exponential(0.5, n_galaxies),\n    })\n    \n    # Add classification labels (simplified)\n    def classify_galaxy(row):\n        if row['d4000_break'] > 1.6 and row['h_alpha_flux'] < -15:\n            return 'elliptical'\n        elif row['concentration'] > 3.5:\n            return 'spiral_early'\n        elif row['asymmetry'] > 0.3:\n            return 'irregular'\n        else:\n            return 'spiral_late'\n    \n    galaxies['morphology_class'] = galaxies.apply(classify_galaxy, axis=1)\n    \n    print(\"1. INITIAL DATA EXPLORATION:\")\n    print(galaxies.info())\n    print(f\"\\nClass distribution:\\n{galaxies['morphology_class'].value_counts()}\")\n    \n    print(\"\\n2. FEATURE ENGINEERING:\")\n    \n    # Color indices\n    galaxies['u_g'] = galaxies['mag_u'] - galaxies['mag_g']\n    galaxies['g_r'] = galaxies['mag_g'] - galaxies['mag_r']\n    galaxies['r_i'] = galaxies['mag_r'] - galaxies['mag_i']\n    galaxies['i_z'] = galaxies['mag_i'] - galaxies['mag_z']\n    \n    # Composite features\n    galaxies['CAS_score'] = (galaxies['concentration'] + \n                            10 * galaxies['asymmetry'] + \n                            5 * galaxies['smoothness'])\n    \n    # Logarithmic transforms for skewed features\n    galaxies['log_vdisp'] = np.log10(galaxies['velocity_dispersion'])\n    galaxies['log_halpha'] = np.log10(galaxies['h_alpha_flux'] - galaxies['h_alpha_flux'].min() + 1)\n    \n    # Interaction features\n    galaxies['density_indicator'] = galaxies['n_neighbors_1mpc'] / (galaxies['distance_to_nearest'] + 0.1)\n    \n    print(\"Added features:\", [col for col in galaxies.columns if col not in \n                              ['mag_u', 'mag_g', 'mag_r', 'mag_i', 'mag_z', \n                               'morphology_class']])\n    \n    print(\"\\n3. HANDLING MISSING DATA:\")\n    \n    # Introduce missing data (realistic scenario)\n    missing_fraction = 0.1\n    for col in ['velocity_dispersion', 'h_alpha_flux', 'd4000_break']:\n        missing_idx = np.random.choice(galaxies.index, \n                                      size=int(len(galaxies) * missing_fraction),\n                                      replace=False)\n        galaxies.loc[missing_idx, col] = np.nan\n    \n    print(f\"Missing data summary:\\n{galaxies.isnull().sum()}\")\n    \n    # Imputation strategies\n    from sklearn.impute import SimpleImputer, KNNImputer\n    \n    # Simple imputation\n    simple_imputer = SimpleImputer(strategy='median')\n    \n    # KNN imputation (better for correlated features)\n    knn_imputer = KNNImputer(n_neighbors=5)\n    \n    # Apply to numerical features\n    numerical_features = galaxies.select_dtypes(include=[np.number]).columns.drop('morphology_class', errors='ignore')\n    \n    galaxies_simple = galaxies.copy()\n    galaxies_simple[numerical_features] = simple_imputer.fit_transform(galaxies[numerical_features])\n    \n    print(\"\\n4. OUTLIER DETECTION:\")\n    \n    from sklearn.ensemble import IsolationForest\n    \n    # Isolation Forest for anomaly detection\n    iso_forest = IsolationForest(contamination=0.05, random_state=42)\n    outlier_labels = iso_forest.fit_predict(galaxies_simple[numerical_features])\n    \n    galaxies_simple['is_outlier'] = outlier_labels == -1\n    print(f\"Outliers detected: {galaxies_simple['is_outlier'].sum()}\")\n    \n    print(\"\\n5. FEATURE SCALING:\")\n    \n    from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer\n    \n    # Different scaling methods\n    scalers = {\n        'standard': StandardScaler(),\n        'robust': RobustScaler(),  # Better with outliers\n        'quantile': QuantileTransformer(output_distribution='normal')  # Gaussianize\n    }\n    \n    # Apply scaling\n    scaled_features = {}\n    for name, scaler in scalers.items():\n        scaled_features[name] = pd.DataFrame(\n            scaler.fit_transform(galaxies_simple[numerical_features]),\n            columns=numerical_features,\n            index=galaxies_simple.index\n        )\n    \n    print(\"Scaling comparison (first 5 features):\")\n    for name in scalers.keys():\n        print(f\"\\n{name.capitalize()} scaling:\")\n        print(scaled_features[name].iloc[:, :5].describe().loc[['mean', 'std']])\n    \n    print(\"\\n6. TRAIN-TEST SPLIT WITH STRATIFICATION:\")\n    \n    from sklearn.model_selection import train_test_split\n    \n    # Stratified split to maintain class balance\n    X = scaled_features['robust']\n    y = galaxies_simple['morphology_class']\n    \n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, stratify=y, random_state=42\n    )\n    \n    print(f\"Training set: {len(X_train)} samples\")\n    print(f\"Test set: {len(X_test)} samples\")\n    print(f\"\\nClass distribution in train:\\n{y_train.value_counts(normalize=True)}\")\n    print(f\"\\nClass distribution in test:\\n{y_test.value_counts(normalize=True)}\")\n    \n    return galaxies_simple, X_train, X_test, y_train, y_test\n\ngalaxies_ml, X_train, X_test, y_train, y_test = ml_data_preparation()","position":{"start":{"line":535,"column":1},"end":{"line":692,"column":1}},"key":"plw7mEwjPV"},{"type":"heading","depth":2,"position":{"start":{"line":694,"column":1},"end":{"line":694,"column":1}},"children":[{"type":"text","value":"Merging and Joining Astronomical Catalogs","position":{"start":{"line":694,"column":1},"end":{"line":694,"column":1}},"key":"k3aFHGS9Cz"}],"identifier":"merging-and-joining-astronomical-catalogs","label":"Merging and Joining Astronomical Catalogs","html_id":"merging-and-joining-astronomical-catalogs","implicit":true,"key":"R1JkjToDYt"},{"type":"heading","depth":3,"position":{"start":{"line":696,"column":1},"end":{"line":696,"column":1}},"children":[{"type":"text","value":"Cross-Matching Surveys","position":{"start":{"line":696,"column":1},"end":{"line":696,"column":1}},"key":"FrRi4k1g34"}],"identifier":"cross-matching-surveys","label":"Cross-Matching Surveys","html_id":"cross-matching-surveys","implicit":true,"key":"J0dAtulfuo"},{"type":"code","lang":"python","value":"def catalog_merging():\n    \"\"\"Merge and join operations for multi-survey astronomy.\"\"\"\n    \n    # Simulate different surveys\n    n_objects = 5000\n    \n    # Optical survey (like SDSS)\n    optical = pd.DataFrame({\n        'objid': np.arange(1000, 1000 + n_objects),\n        'ra': np.random.uniform(150, 160, n_objects),\n        'dec': np.random.uniform(-5, 5, n_objects),\n        'mag_g': np.random.normal(20, 2, n_objects),\n        'mag_r': np.random.normal(19.5, 2, n_objects),\n        'mag_i': np.random.normal(19, 2, n_objects),\n        'photoz': np.random.gamma(2, 0.3, n_objects),\n        'survey': 'SDSS'\n    })\n    \n    # X-ray survey (like Chandra)\n    n_xray = 500\n    xray_idx = np.random.choice(n_objects, n_xray, replace=False)\n    xray = pd.DataFrame({\n        'source_id': np.arange(2000, 2000 + n_xray),\n        'ra': optical.iloc[xray_idx]['ra'].values + np.random.normal(0, 0.0003, n_xray),  # Small offset\n        'dec': optical.iloc[xray_idx]['dec'].values + np.random.normal(0, 0.0003, n_xray),\n        'flux_soft': np.random.lognormal(-13, 1, n_xray),\n        'flux_hard': np.random.lognormal(-13.5, 1, n_xray),\n        'hardness_ratio': np.random.uniform(-1, 1, n_xray),\n        'survey': 'Chandra'\n    })\n    \n    # Radio survey (like FIRST)\n    n_radio = 300\n    radio_idx = np.random.choice(n_objects, n_radio, replace=False)\n    radio = pd.DataFrame({\n        'name': [f'FIRST_J{i:06d}' for i in range(n_radio)],\n        'ra': optical.iloc[radio_idx]['ra'].values + np.random.normal(0, 0.0005, n_radio),\n        'dec': optical.iloc[radio_idx]['dec'].values + np.random.normal(0, 0.0005, n_radio),\n        'flux_1400mhz': np.random.lognormal(0, 2, n_radio),\n        'spectral_index': np.random.normal(-0.7, 0.3, n_radio),\n        'survey': 'FIRST'\n    })\n    \n    print(\"1. SURVEY SUMMARIES:\")\n    print(f\"Optical: {len(optical)} objects\")\n    print(f\"X-ray: {len(xray)} objects\")\n    print(f\"Radio: {len(radio)} objects\")\n    \n    print(\"\\n2. POSITIONAL CROSS-MATCHING:\")\n    \n    # Function for angular separation\n    def angular_separation(ra1, dec1, ra2, dec2):\n        \"\"\"Calculate angular separation in arcseconds.\"\"\"\n        # Simplified for small angles\n        cos_dec = np.cos(np.radians(dec1))\n        dra = (ra2 - ra1) * cos_dec\n        ddec = dec2 - dec1\n        return 3600 * np.sqrt(dra**2 + ddec**2)\n    \n    # Cross-match optical with X-ray\n    from sklearn.neighbors import BallTree\n    \n    # Convert to radians for BallTree\n    optical_coords = np.radians(optical[['ra', 'dec']].values)\n    xray_coords = np.radians(xray[['ra', 'dec']].values)\n    \n    # Build tree and query\n    tree = BallTree(optical_coords, metric='haversine')\n    max_sep_rad = np.radians(3/3600)  # 3 arcsec\n    \n    indices, distances = tree.query_radius(xray_coords, r=max_sep_rad, return_distance=True)\n    \n    # Create matched catalog\n    matches = []\n    for i, (idx_list, dist_list) in enumerate(zip(indices, distances)):\n        if len(idx_list) > 0:\n            # Take closest match\n            best_idx = idx_list[np.argmin(dist_list)]\n            matches.append({\n                'optical_idx': best_idx,\n                'xray_idx': i,\n                'separation_arcsec': np.degrees(dist_list[np.argmin(dist_list)]) * 3600\n            })\n    \n    match_df = pd.DataFrame(matches)\n    print(f\"Optical-Xray matches: {len(match_df)} / {len(xray)}\")\n    \n    # Merge matched catalogs\n    optical_xray = optical.iloc[match_df['optical_idx'].values].reset_index(drop=True)\n    xray_matched = xray.iloc[match_df['xray_idx'].values].reset_index(drop=True)\n    \n    combined = pd.concat([optical_xray, xray_matched.drop(['ra', 'dec', 'survey'], axis=1)], axis=1)\n    combined['separation'] = match_df['separation_arcsec'].values\n    \n    print(f\"Combined catalog shape: {combined.shape}\")\n    \n    print(\"\\n3. DIFFERENT JOIN TYPES:\")\n    \n    # Create simplified catalogs for demonstration\n    cat1 = optical[['objid', 'ra', 'dec', 'mag_g']].head(100)\n    cat2 = pd.DataFrame({\n        'objid': optical['objid'].iloc[50:150].values,  # Partial overlap\n        'specz': np.random.gamma(2, 0.3, 100),\n        'quality': np.random.choice(['good', 'bad'], 100)\n    })\n    \n    # Inner join - only matched objects\n    inner = pd.merge(cat1, cat2, on='objid', how='inner')\n    print(f\"Inner join: {len(inner)} objects (both catalogs)\")\n    \n    # Left join - all from cat1\n    left = pd.merge(cat1, cat2, on='objid', how='left')\n    print(f\"Left join: {len(left)} objects (all from catalog 1)\")\n    print(f\"  With specz: {left['specz'].notna().sum()}\")\n    \n    # Outer join - all objects\n    outer = pd.merge(cat1, cat2, on='objid', how='outer')\n    print(f\"Outer join: {len(outer)} objects (union)\")\n    \n    print(\"\\n4. CONCATENATING SURVEYS:\")\n    \n    # Standardize column names\n    optical_std = optical[['ra', 'dec', 'survey']].copy()\n    optical_std['flux'] = 10**(-0.4 * optical['mag_r'])\n    \n    xray_std = xray[['ra', 'dec', 'survey']].copy()\n    xray_std['flux'] = xray['flux_soft']\n    \n    radio_std = radio[['ra', 'dec', 'survey']].copy()\n    radio_std['flux'] = radio['flux_1400mhz']\n    \n    # Concatenate all surveys\n    all_surveys = pd.concat([optical_std, xray_std, radio_std], \n                           ignore_index=True, sort=False)\n    \n    print(f\"Combined all surveys: {len(all_surveys)} total detections\")\n    print(all_surveys.groupby('survey')['flux'].describe())\n    \n    return combined, all_surveys\n\nmatched_catalog, all_surveys = catalog_merging()","position":{"start":{"line":698,"column":1},"end":{"line":840,"column":1}},"key":"AeQi09kGEE"},{"type":"heading","depth":2,"position":{"start":{"line":842,"column":1},"end":{"line":842,"column":1}},"children":[{"type":"text","value":"Performance Optimization for Large Catalogs","position":{"start":{"line":842,"column":1},"end":{"line":842,"column":1}},"key":"iFWHrnjEpA"}],"identifier":"performance-optimization-for-large-catalogs","label":"Performance Optimization for Large Catalogs","html_id":"performance-optimization-for-large-catalogs","implicit":true,"key":"xfmxpvQ67y"},{"type":"heading","depth":3,"position":{"start":{"line":844,"column":1},"end":{"line":844,"column":1}},"children":[{"type":"text","value":"Scaling to Big Data","position":{"start":{"line":844,"column":1},"end":{"line":844,"column":1}},"key":"J0uSYsBvg9"}],"identifier":"scaling-to-big-data","label":"Scaling to Big Data","html_id":"scaling-to-big-data","implicit":true,"key":"kX2A24lMpW"},{"type":"code","lang":"python","value":"def performance_optimization():\n    \"\"\"Optimize Pandas for large astronomical catalogs.\"\"\"\n    \n    print(\"1. MEMORY OPTIMIZATION:\")\n    \n    # Create large catalog\n    n = 1_000_000\n    \n    # Bad: default dtypes\n    catalog_bad = pd.DataFrame({\n        'id': np.arange(n),  # int64 (8 bytes)\n        'ra': np.random.uniform(0, 360, n),  # float64 (8 bytes)\n        'dec': np.random.uniform(-90, 90, n),  # float64\n        'mag': np.random.uniform(10, 25, n),  # float64\n        'flag': np.random.choice([0, 1], n),  # int64\n        'survey': np.random.choice(['SDSS', 'GAIA', 'WISE'], n)  # object\n    })\n    \n    memory_bad = catalog_bad.memory_usage(deep=True).sum() / 1e6\n    print(f\"Default dtypes: {memory_bad:.1f} MB\")\n    \n    # Good: optimized dtypes\n    catalog_good = pd.DataFrame({\n        'id': pd.array(np.arange(n), dtype='uint32'),  # 4 bytes\n        'ra': pd.array(np.random.uniform(0, 360, n), dtype='float32'),  # 4 bytes\n        'dec': pd.array(np.random.uniform(-90, 90, n), dtype='float32'),\n        'mag': pd.array(np.random.uniform(10, 25, n), dtype='float32'),\n        'flag': pd.array(np.random.choice([0, 1], n), dtype='bool'),  # 1 byte\n        'survey': pd.Categorical(np.random.choice(['SDSS', 'GAIA', 'WISE'], n))  # Categorical\n    })\n    \n    memory_good = catalog_good.memory_usage(deep=True).sum() / 1e6\n    print(f\"Optimized dtypes: {memory_good:.1f} MB\")\n    print(f\"Memory saved: {(1 - memory_good/memory_bad)*100:.1f}%\")\n    \n    print(\"\\n2. CHUNKING FOR LARGE FILES:\")\n    \n    def process_large_catalog(filename, chunksize=10000):\n        \"\"\"Process large catalog in chunks.\"\"\"\n        \n        results = []\n        \n        # Simulate reading chunks\n        for chunk_id in range(3):  # Normally: pd.read_csv(filename, chunksize=chunksize)\n            # Simulate chunk\n            chunk = pd.DataFrame({\n                'ra': np.random.uniform(0, 360, chunksize),\n                'dec': np.random.uniform(-90, 90, chunksize),\n                'mag': np.random.uniform(15, 22, chunksize)\n            })\n            \n            # Process chunk\n            chunk_stats = {\n                'chunk_id': chunk_id,\n                'mean_mag': chunk['mag'].mean(),\n                'bright_count': (chunk['mag'] < 18).sum(),\n                'area_coverage': (chunk['ra'].max() - chunk['ra'].min()) * \n                                (chunk['dec'].max() - chunk['dec'].min())\n            }\n            \n            results.append(chunk_stats)\n        \n        return pd.DataFrame(results)\n    \n    chunk_results = process_large_catalog('large_catalog.csv')\n    print(\"Chunk processing results:\")\n    print(chunk_results)\n    \n    print(\"\\n3. QUERY OPTIMIZATION:\")\n    \n    # Create indexed DataFrame\n    catalog_indexed = catalog_good.set_index('id')\n    \n    # Slow: Python loop\n    import time\n    \n    start = time.time()\n    bright_loop = []\n    for idx, row in catalog_good.iterrows():\n        if row['mag'] < 15 and row['dec'] > 0:\n            bright_loop.append(row['id'])\n        if len(bright_loop) >= 100:\n            break\n    time_loop = time.time() - start\n    \n    # Fast: Vectorized query\n    start = time.time()\n    bright_vector = catalog_good.query('mag < 15 & dec > 0')['id'].head(100).tolist()\n    time_vector = time.time() - start\n    \n    print(f\"Loop time: {time_loop:.4f}s\")\n    print(f\"Vectorized time: {time_vector:.4f}s\")\n    print(f\"Speedup: {time_loop/time_vector:.1f}x\")\n    \n    print(\"\\n4. PARALLEL PROCESSING:\")\n    \n    # For CPU-bound operations\n    import multiprocessing as mp\n    from functools import partial\n    \n    def calculate_color(group):\n        \"\"\"Calculate color for group of objects.\"\"\"\n        return group['mag'].mean() - group['mag'].median()\n    \n    # Split data for parallel processing\n    n_cores = mp.cpu_count()\n    chunks = np.array_split(catalog_good, n_cores)\n    \n    # Parallel apply (conceptual - actual implementation would use Dask)\n    print(f\"Would process on {n_cores} cores\")\n    \n    print(\"\\n5. USING DASK FOR SCALING:\")\n    \n    # Conceptual Dask usage\n    print(\"\"\"\n    import dask.dataframe as dd\n    \n    # Read large catalog\n    ddf = dd.read_csv('huge_catalog.csv', blocksize='100MB')\n    \n    # Operations are lazy\n    result = ddf[ddf.mag < 20].groupby('survey').mag.mean()\n    \n    # Compute when needed\n    result.compute()\n    \"\"\")\n    \n    return catalog_good\n\noptimized_catalog = performance_optimization()","position":{"start":{"line":846,"column":1},"end":{"line":977,"column":1}},"key":"uIT7f7lr6X"},{"type":"heading","depth":2,"position":{"start":{"line":979,"column":1},"end":{"line":979,"column":1}},"children":[{"type":"text","value":"Try It Yourself","position":{"start":{"line":979,"column":1},"end":{"line":979,"column":1}},"key":"trgKottM3F"}],"identifier":"try-it-yourself","label":"Try It Yourself","html_id":"try-it-yourself","implicit":true,"key":"oF5QFWwHgx"},{"type":"heading","depth":3,"position":{"start":{"line":981,"column":1},"end":{"line":981,"column":1}},"children":[{"type":"text","value":"Exercise 1: Complete Variable Star Classification Pipeline","position":{"start":{"line":981,"column":1},"end":{"line":981,"column":1}},"key":"JTIqzcxJ9S"}],"identifier":"exercise-1-complete-variable-star-classification-pipeline","label":"Exercise 1: Complete Variable Star Classification Pipeline","html_id":"exercise-1-complete-variable-star-classification-pipeline","implicit":true,"key":"RZb7bzZ70n"},{"type":"code","lang":"python","value":"def variable_star_pipeline(lightcurves_df):\n    \"\"\"\n    Build a complete pipeline for variable star classification.\n    \n    Tasks:\n    1. Extract time series features (mean, std, skewness, etc.)\n    2. Find periods using Lomb-Scargle\n    3. Extract phase-folded features\n    4. Handle missing data and outliers\n    5. Prepare features for ML classification\n    6. Split into train/test maintaining class balance\n    \n    Returns feature matrix ready for sklearn classifiers.\n    \"\"\"\n    # Your code here\n    pass","position":{"start":{"line":983,"column":1},"end":{"line":1000,"column":1}},"key":"GNVnlZURMZ"},{"type":"heading","depth":3,"position":{"start":{"line":1002,"column":1},"end":{"line":1002,"column":1}},"children":[{"type":"text","value":"Exercise 2: Multi-Survey Catalog Integration","position":{"start":{"line":1002,"column":1},"end":{"line":1002,"column":1}},"key":"y5Y2LrfehM"}],"identifier":"exercise-2-multi-survey-catalog-integration","label":"Exercise 2: Multi-Survey Catalog Integration","html_id":"exercise-2-multi-survey-catalog-integration","implicit":true,"key":"qZPKP8AttM"},{"type":"code","lang":"python","value":"def integrate_surveys(optical_df, xray_df, radio_df, ir_df):\n    \"\"\"\n    Integrate multiple astronomical surveys.\n    \n    Requirements:\n    1. Cross-match by position (handle different coordinate precisions)\n    2. Handle different column names and units\n    3. Create unified photometry columns\n    4. Calculate multi-wavelength colors\n    5. Flag objects detected in multiple surveys\n    6. Handle missing data appropriately\n    \n    Return integrated catalog with source classification.\n    \"\"\"\n    # Your code here\n    pass","position":{"start":{"line":1004,"column":1},"end":{"line":1021,"column":1}},"key":"w06Gm2i91i"},{"type":"heading","depth":3,"position":{"start":{"line":1023,"column":1},"end":{"line":1023,"column":1}},"children":[{"type":"text","value":"Exercise 3: Galaxy Cluster Analysis","position":{"start":{"line":1023,"column":1},"end":{"line":1023,"column":1}},"key":"AxhxcbOU8F"}],"identifier":"exercise-3-galaxy-cluster-analysis","label":"Exercise 3: Galaxy Cluster Analysis","html_id":"exercise-3-galaxy-cluster-analysis","implicit":true,"key":"Lh8J0kwRid"},{"type":"code","lang":"python","value":"def analyze_galaxy_cluster(galaxies_df):\n    \"\"\"\n    Comprehensive galaxy cluster analysis.\n    \n    Tasks:\n    1. Identify cluster members using redshift\n    2. Calculate velocity dispersion\n    3. Estimate cluster mass (virial theorem)\n    4. Find red sequence in color-magnitude diagram\n    5. Identify BCG (brightest cluster galaxy)\n    6. Calculate radial profiles\n    7. Prepare data for ML substructure detection\n    \"\"\"\n    # Your code here\n    pass","position":{"start":{"line":1025,"column":1},"end":{"line":1041,"column":1}},"key":"F7thEADLPt"},{"type":"heading","depth":2,"position":{"start":{"line":1043,"column":1},"end":{"line":1043,"column":1}},"children":[{"type":"text","value":"Key Takeaways","position":{"start":{"line":1043,"column":1},"end":{"line":1043,"column":1}},"key":"bkMupYnxJe"}],"identifier":"key-takeaways","label":"Key Takeaways","html_id":"key-takeaways","implicit":true,"key":"Rhu40EAor0"},{"type":"paragraph","position":{"start":{"line":1045,"column":1},"end":{"line":1052,"column":1}},"children":[{"type":"text","value":" ","position":{"start":{"line":1045,"column":1},"end":{"line":1045,"column":1}},"key":"dF62TyPOsF"},{"type":"strong","position":{"start":{"line":1045,"column":1},"end":{"line":1045,"column":1}},"children":[{"type":"text","value":"Pandas bridges astronomy and data science","position":{"start":{"line":1045,"column":1},"end":{"line":1045,"column":1}},"key":"fuRRVv9596"}],"key":"tFPFIO2yiY"},{"type":"text","value":" - From catalogs to ML pipelines","position":{"start":{"line":1045,"column":1},"end":{"line":1045,"column":1}},"key":"IMYCbtcjlT"},{"type":"break","position":{"start":{"line":1045,"column":1},"end":{"line":1045,"column":1}},"key":"Ok1XQCLbHp"},{"type":"text","value":" ","position":{"start":{"line":1045,"column":1},"end":{"line":1045,"column":1}},"key":"GlUzzF2OtE"},{"type":"strong","position":{"start":{"line":1045,"column":1},"end":{"line":1045,"column":1}},"children":[{"type":"text","value":"DataFrames handle heterogeneous data","position":{"start":{"line":1045,"column":1},"end":{"line":1045,"column":1}},"key":"R1hOd3Cx0y"}],"key":"WDEaJYB2OA"},{"type":"text","value":" - Mixed types, missing values, metadata","position":{"start":{"line":1045,"column":1},"end":{"line":1045,"column":1}},"key":"vM5tdoy0Ve"},{"type":"break","position":{"start":{"line":1045,"column":1},"end":{"line":1045,"column":1}},"key":"Aj4ALvMaSs"},{"type":"text","value":" ","position":{"start":{"line":1045,"column":1},"end":{"line":1045,"column":1}},"key":"CDxnG4sp5b"},{"type":"strong","position":{"start":{"line":1045,"column":1},"end":{"line":1045,"column":1}},"children":[{"type":"text","value":"GroupBy enables population studies","position":{"start":{"line":1045,"column":1},"end":{"line":1045,"column":1}},"key":"hQQ5xXxgEl"}],"key":"PAwTKkYyLD"},{"type":"text","value":" - Statistics by galaxy type, stellar population","position":{"start":{"line":1045,"column":1},"end":{"line":1045,"column":1}},"key":"RqiVopwfC3"},{"type":"break","position":{"start":{"line":1045,"column":1},"end":{"line":1045,"column":1}},"key":"yIIIb8LeAV"},{"type":"text","value":" ","position":{"start":{"line":1045,"column":1},"end":{"line":1045,"column":1}},"key":"OSZnjLmq2D"},{"type":"strong","position":{"start":{"line":1045,"column":1},"end":{"line":1045,"column":1}},"children":[{"type":"text","value":"Time series tools for light curves","position":{"start":{"line":1045,"column":1},"end":{"line":1045,"column":1}},"key":"fDvIRLYJNS"}],"key":"yKCTJnMVj3"},{"type":"text","value":" - Resampling, rolling windows, feature extraction","position":{"start":{"line":1045,"column":1},"end":{"line":1045,"column":1}},"key":"B3bQaQonky"},{"type":"break","position":{"start":{"line":1045,"column":1},"end":{"line":1045,"column":1}},"key":"RgdXHB1wjG"},{"type":"text","value":" ","position":{"start":{"line":1045,"column":1},"end":{"line":1045,"column":1}},"key":"ktEvCXbYMk"},{"type":"strong","position":{"start":{"line":1045,"column":1},"end":{"line":1045,"column":1}},"children":[{"type":"text","value":"Efficient merging for multi-survey science","position":{"start":{"line":1045,"column":1},"end":{"line":1045,"column":1}},"key":"Y9Ao9l0mz2"}],"key":"zzbQWWNmnw"},{"type":"text","value":" - Cross-matching, joining, concatenating","position":{"start":{"line":1045,"column":1},"end":{"line":1045,"column":1}},"key":"YT6Qtwy3KQ"},{"type":"break","position":{"start":{"line":1045,"column":1},"end":{"line":1045,"column":1}},"key":"yDbUYiuyR0"},{"type":"text","value":" ","position":{"start":{"line":1045,"column":1},"end":{"line":1045,"column":1}},"key":"VQLJw0a9P4"},{"type":"strong","position":{"start":{"line":1045,"column":1},"end":{"line":1045,"column":1}},"children":[{"type":"text","value":"Feature engineering for ML","position":{"start":{"line":1045,"column":1},"end":{"line":1045,"column":1}},"key":"OVrRve0sVU"}],"key":"fnZB6Qsoyg"},{"type":"text","value":" - From raw observations to ML-ready features","position":{"start":{"line":1045,"column":1},"end":{"line":1045,"column":1}},"key":"jNQgPJy3vq"},{"type":"break","position":{"start":{"line":1045,"column":1},"end":{"line":1045,"column":1}},"key":"SAx7yac5cG"},{"type":"text","value":" ","position":{"start":{"line":1045,"column":1},"end":{"line":1045,"column":1}},"key":"BRRlR1Wn2m"},{"type":"strong","position":{"start":{"line":1045,"column":1},"end":{"line":1045,"column":1}},"children":[{"type":"text","value":"Memory optimization crucial","position":{"start":{"line":1045,"column":1},"end":{"line":1045,"column":1}},"key":"fKgn1fLEdk"}],"key":"W6CD72clMm"},{"type":"text","value":" - Use appropriate dtypes, chunking, Dask for scaling","position":{"start":{"line":1045,"column":1},"end":{"line":1045,"column":1}},"key":"Z2BgtBHvNv"},{"type":"break","position":{"start":{"line":1045,"column":1},"end":{"line":1045,"column":1}},"key":"jPzlDEOoXC"},{"type":"text","value":" ","position":{"start":{"line":1045,"column":1},"end":{"line":1045,"column":1}},"key":"WauMj46oAe"},{"type":"strong","position":{"start":{"line":1045,"column":1},"end":{"line":1045,"column":1}},"children":[{"type":"text","value":"Vectorization is key","position":{"start":{"line":1045,"column":1},"end":{"line":1045,"column":1}},"key":"bsVQCRcLpa"}],"key":"M1Sr6sQV7L"},{"type":"text","value":" - Avoid loops, use query(), apply(), transform()","position":{"start":{"line":1045,"column":1},"end":{"line":1045,"column":1}},"key":"O1iOtNhbCa"}],"key":"hZIvX4EzPK"},{"type":"heading","depth":2,"position":{"start":{"line":1054,"column":1},"end":{"line":1054,"column":1}},"children":[{"type":"text","value":"Connecting to Your Course","position":{"start":{"line":1054,"column":1},"end":{"line":1054,"column":1}},"key":"RSPGt8ZebS"}],"identifier":"connecting-to-your-course","label":"Connecting to Your Course","html_id":"connecting-to-your-course","implicit":true,"key":"T074luzw0l"},{"type":"paragraph","position":{"start":{"line":1056,"column":1},"end":{"line":1056,"column":1}},"children":[{"type":"text","value":"This Pandas knowledge directly enables:","position":{"start":{"line":1056,"column":1},"end":{"line":1056,"column":1}},"key":"uA5toqqXTJ"}],"key":"KR8MKoBqYS"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":1057,"column":1},"end":{"line":1061,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":1057,"column":1},"end":{"line":1057,"column":1}},"children":[{"type":"strong","position":{"start":{"line":1057,"column":1},"end":{"line":1057,"column":1}},"children":[{"type":"text","value":"Project 3","position":{"start":{"line":1057,"column":1},"end":{"line":1057,"column":1}},"key":"tS7zIeug32"}],"key":"SK34LP1i3i"},{"type":"text","value":": Statistical analysis of simulation outputs","position":{"start":{"line":1057,"column":1},"end":{"line":1057,"column":1}},"key":"plA2YcOm2V"}],"key":"y78IJh7lp2"},{"type":"listItem","spread":true,"position":{"start":{"line":1058,"column":1},"end":{"line":1058,"column":1}},"children":[{"type":"strong","position":{"start":{"line":1058,"column":1},"end":{"line":1058,"column":1}},"children":[{"type":"text","value":"Project 4","position":{"start":{"line":1058,"column":1},"end":{"line":1058,"column":1}},"key":"TurRs8F1lV"}],"key":"F1B9QrjIlF"},{"type":"text","value":": Managing Monte Carlo results","position":{"start":{"line":1058,"column":1},"end":{"line":1058,"column":1}},"key":"Z3ySbPo16F"}],"key":"E3M9LJPtv9"},{"type":"listItem","spread":true,"position":{"start":{"line":1059,"column":1},"end":{"line":1059,"column":1}},"children":[{"type":"strong","position":{"start":{"line":1059,"column":1},"end":{"line":1059,"column":1}},"children":[{"type":"text","value":"Project 5","position":{"start":{"line":1059,"column":1},"end":{"line":1059,"column":1}},"key":"ANHcyahRhf"}],"key":"o5xRGsppfb"},{"type":"text","value":": Preparing data for neural networks","position":{"start":{"line":1059,"column":1},"end":{"line":1059,"column":1}},"key":"uieZpFgBKc"}],"key":"JMB8ClQlr6"},{"type":"listItem","spread":true,"position":{"start":{"line":1060,"column":1},"end":{"line":1061,"column":1}},"children":[{"type":"strong","position":{"start":{"line":1060,"column":1},"end":{"line":1060,"column":1}},"children":[{"type":"text","value":"Final Project","position":{"start":{"line":1060,"column":1},"end":{"line":1060,"column":1}},"key":"fVmffCAYge"}],"key":"QWJDgJ0OvP"},{"type":"text","value":": Professional data science workflows","position":{"start":{"line":1060,"column":1},"end":{"line":1060,"column":1}},"key":"k26RaDabE9"}],"key":"JfdAFKovwB"}],"key":"mSBm8bCOyu"},{"type":"paragraph","position":{"start":{"line":1062,"column":1},"end":{"line":1062,"column":1}},"children":[{"type":"text","value":"Combined with NumPy, SciPy, and Matplotlib, you now have the complete toolkit for modern astronomical data science, preparing you for both research and industry applications.","position":{"start":{"line":1062,"column":1},"end":{"line":1062,"column":1}},"key":"lfjn1PRcLC"}],"key":"nqmPOBljdf"}],"key":"F5WkI2F6kI"}],"key":"tx9qudXFJE"},"references":{"cite":{"order":[],"data":{}}}}