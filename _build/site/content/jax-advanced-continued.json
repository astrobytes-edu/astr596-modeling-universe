{"version":2,"kind":"Article","sha256":"09bfe025769bc9b8657172f0b945df20d3521d809e53d163b284dd13a2ca076a","slug":"jax-advanced-continued","location":"/03-scientific-computing-with-python/05-modern-approaches-jax-ecosystem/05-jax_advanced_continued.md","dependencies":[],"frontmatter":{"title":"JAX Advanced Patterns: Control Flow and Optimization (Continued)","content_includes_title":false,"authors":[{"nameParsed":{"literal":"Anna Rosen","given":"Anna","family":"Rosen"},"name":"Anna Rosen","orcid":"0000-0003-4423-0660","email":"alrosen@sdsu.edu","affiliations":["San Diego State University"],"id":"contributors-myst-generated-uid-0","corresponding":true}],"license":{"content":{"id":"CC-BY-4.0","url":"https://creativecommons.org/licenses/by/4.0/","name":"Creative Commons Attribution 4.0 International","free":true,"CC":true},"code":{"id":"MIT","url":"https://opensource.org/licenses/MIT","name":"MIT License","free":true,"osi":true}},"github":"https://github.com/astrobytes-edu/astr596-modeling-universe","subject":"Modeling the Universe","venue":{"title":"ASTR 596 - Fall 2025","url":"https://www.anna-rosen.com"},"keywords":["computational astrophysics","python","numerical methods","machine learning","monte carlo","neural networks","radiative transfer","bayesian inference","JAX"],"affiliations":[{"id":"San Diego State University","name":"San Diego State University"}],"numbering":{"title":{"offset":2}},"edit_url":"https://github.com/astrobytes-edu/astr596-modeling-universe/blob/main/03-scientific-computing-with-python/05-modern-approaches-jax-ecosystem/05-jax_advanced_continued.md","exports":[{"format":"md","filename":"05-jax_advanced_continued.md","url":"/05-jax_advanced_cont-19a947dbe51123d8440e07b68bb65a95.md"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"heading","depth":2,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Memory and Performance Optimization (Continued)","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"xdhblBBOHU"}],"identifier":"memory-and-performance-optimization-continued","label":"Memory and Performance Optimization (Continued)","html_id":"memory-and-performance-optimization-continued","implicit":true,"key":"NYAWNLTU6H"},{"type":"heading","depth":3,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Checkpointing and Memory Management","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"Iuxvq4UqFR"}],"identifier":"checkpointing-and-memory-management","label":"Checkpointing and Memory Management","html_id":"checkpointing-and-memory-management","implicit":true,"key":"SEsil6Rdci"},{"type":"code","lang":"python","value":"def memory_optimization():\n    \"\"\"Optimize memory usage in JAX computations.\"\"\"\n    \n    print(\"\\nMEMORY OPTIMIZATION IN JAX\")\n    print(\"=\" * 50)\n    \n    # 1. Gradient checkpointing\n    print(\"\\n1. GRADIENT CHECKPOINTING:\")\n    \n    from jax.experimental import checkpoint\n    \n    def deep_network(x, n_layers=50):\n        \"\"\"Deep network that would use lots of memory.\"\"\"\n        \n        @checkpoint  # Don't store intermediate activations\n        def layer(x):\n            # Expensive computation\n            x = jnp.tanh(x @ jnp.ones((100, 100)))\n            x = jnp.sin(x) + jnp.cos(x)\n            return x\n        \n        for _ in range(n_layers):\n            x = layer(x)\n        return jnp.sum(x)\n    \n    # Without checkpointing: stores all intermediate values\n    def deep_network_no_checkpoint(x, n_layers=50):\n        for _ in range(n_layers):\n            x = jnp.tanh(x @ jnp.ones((100, 100)))\n            x = jnp.sin(x) + jnp.cos(x)\n        return jnp.sum(x)\n    \n    x = random.normal(random.PRNGKey(0), (100,))\n    \n    # Compare memory usage (conceptual - actual measurement requires profiling)\n    print(\"  With checkpointing: recomputes forward pass during backprop\")\n    print(\"  Without checkpointing: stores all intermediate activations\")\n    \n    # Gradient computation\n    grad_checkpoint = grad(deep_network)\n    grad_no_checkpoint = grad(deep_network_no_checkpoint)\n    \n    g1 = grad_checkpoint(x)\n    g2 = grad_no_checkpoint(x)\n    \n    print(f\"  Gradients match: {jnp.allclose(g1, g2)}\")\n    \n    # 2. Donation for in-place updates\n    print(\"\\n2. BUFFER DONATION:\")\n    \n    @jit\n    def evolve_without_donation(state, dt):\n        \"\"\"Standard evolution - creates new arrays.\"\"\"\n        positions, velocities = state\n        new_velocities = velocities - positions * dt\n        new_positions = positions + new_velocities * dt\n        return (new_positions, new_velocities)\n    \n    @jit\n    def evolve_with_donation(state, dt):\n        \"\"\"Evolution with buffer donation - reuses memory.\"\"\"\n        positions, velocities = state\n        # JAX can reuse input buffers when safe\n        velocities = velocities - positions * dt\n        positions = positions + velocities * dt\n        return (positions, velocities)\n    \n    # Using donate_argnums for explicit donation\n    @partial(jit, donate_argnums=(0,))\n    def evolve_explicit_donation(state, dt):\n        \"\"\"Explicitly donate input buffer.\"\"\"\n        positions, velocities = state\n        velocities = velocities - positions * dt\n        positions = positions + velocities * dt\n        return (positions, velocities)\n    \n    state = (random.normal(random.PRNGKey(0), (1000, 3)),\n             random.normal(random.PRNGKey(1), (1000, 3)))\n    \n    # All produce same result, but memory usage differs\n    result1 = evolve_without_donation(state, 0.01)\n    result2 = evolve_with_donation(state, 0.01)\n    # Note: after donation, original 'state' should not be used!\n    result3 = evolve_explicit_donation(state, 0.01)\n    \n    print(f\"  Results match: {jnp.allclose(result1[0], result2[0])}\")\n    \n    # 3. Chunking large computations\n    print(\"\\n3. CHUNKING WITH SCAN:\")\n    \n    @jit\n    def chunked_matrix_multiply(A, B, chunk_size=100):\n        \"\"\"\n        Compute A @ B in chunks to control memory.\n        Useful when A or B is very large.\n        \"\"\"\n        n, k = A.shape\n        k2, m = B.shape\n        assert k == k2\n        \n        # Process in chunks along the k dimension\n        def process_chunk(carry, chunk_idx):\n            result = carry\n            start = chunk_idx * chunk_size\n            end = jnp.minimum(start + chunk_size, k)\n            \n            A_chunk = A[:, start:end]\n            B_chunk = B[start:end, :]\n            \n            result = result + A_chunk @ B_chunk\n            return result, None\n        \n        n_chunks = (k + chunk_size - 1) // chunk_size\n        initial = jnp.zeros((n, m))\n        \n        result, _ = scan(process_chunk, initial, jnp.arange(n_chunks))\n        return result\n    \n    # Test chunked multiplication\n    A = random.normal(random.PRNGKey(2), (100, 1000))\n    B = random.normal(random.PRNGKey(3), (1000, 50))\n    \n    result_direct = A @ B\n    result_chunked = chunked_matrix_multiply(A, B)\n    \n    print(f\"  Chunked multiplication error: {jnp.linalg.norm(result_direct - result_chunked):.2e}\")\n    \n    # 4. Selective computation with stop_gradient\n    print(\"\\n4. STOP_GRADIENT FOR PARTIAL DERIVATIVES:\")\n    \n    def loss_with_regularization(params, data, lambda_reg=0.1):\n        \"\"\"Loss function with optional gradient stopping.\"\"\"\n        weights, biases = params\n        x, y = data\n        \n        # Forward pass\n        predictions = x @ weights + biases\n        \n        # Main loss\n        main_loss = jnp.mean((predictions - y) ** 2)\n        \n        # Regularization (can stop gradient if needed)\n        reg_loss = lambda_reg * (jnp.sum(weights**2) + jnp.sum(biases**2))\n        \n        # Option to not backprop through regularization\n        # reg_loss = jax.lax.stop_gradient(reg_loss)\n        \n        return main_loss + reg_loss\n    \n    # Example with and without gradient stopping\n    weights = random.normal(random.PRNGKey(4), (10, 5))\n    biases = random.normal(random.PRNGKey(5), (5,))\n    params = (weights, biases)\n    \n    x = random.normal(random.PRNGKey(6), (100, 10))\n    y = random.normal(random.PRNGKey(7), (100, 5))\n    data = (x, y)\n    \n    grad_fn = grad(loss_with_regularization)\n    grads = grad_fn(params, data)\n    \n    print(f\"  Weight gradient norm: {jnp.linalg.norm(grads[0]):.3f}\")\n    print(f\"  Bias gradient norm: {jnp.linalg.norm(grads[1]):.3f}\")\n\nmemory_optimization()","position":{"start":{"line":7,"column":1},"end":{"line":173,"column":1}},"key":"pXMuTnht8g"},{"type":"heading","depth":2,"position":{"start":{"line":175,"column":1},"end":{"line":175,"column":1}},"children":[{"type":"text","value":"Multi-GPU and Distributed Computing","position":{"start":{"line":175,"column":1},"end":{"line":175,"column":1}},"key":"MblG0CusLm"}],"identifier":"multi-gpu-and-distributed-computing","label":"Multi-GPU and Distributed Computing","html_id":"multi-gpu-and-distributed-computing","implicit":true,"key":"X0OM7wdJbT"},{"type":"heading","depth":3,"position":{"start":{"line":177,"column":1},"end":{"line":177,"column":1}},"children":[{"type":"text","value":"Data and Model Parallelism with pmap","position":{"start":{"line":177,"column":1},"end":{"line":177,"column":1}},"key":"QfNfGGdG5F"}],"identifier":"data-and-model-parallelism-with-pmap","label":"Data and Model Parallelism with pmap","html_id":"data-and-model-parallelism-with-pmap","implicit":true,"key":"UUa6YJFQnK"},{"type":"code","lang":"python","value":"def distributed_computing():\n    \"\"\"Distributed and parallel computing patterns in JAX.\"\"\"\n    \n    print(\"\\nDISTRIBUTED COMPUTING WITH JAX\")\n    print(\"=\" * 50)\n    \n    # Note: These examples are conceptual - actual multi-GPU requires hardware\n    \n    # 1. Data parallelism with pmap\n    print(\"\\n1. DATA PARALLELISM WITH PMAP:\")\n    \n    # Get device count (will be 1 on single GPU/CPU)\n    n_devices = jax.device_count()\n    print(f\"  Available devices: {n_devices}\")\n    \n    @jit\n    def single_device_nbody_step(positions, velocities, masses, dt):\n        \"\"\"N-body step on single device.\"\"\"\n        n = len(masses)\n        forces = jnp.zeros_like(positions)\n        \n        for i in range(n):\n            for j in range(n):\n                if i != j:\n                    r_ij = positions[j] - positions[i]\n                    r_norm = jnp.linalg.norm(r_ij)\n                    forces = forces.at[i].add(\n                        masses[j] * r_ij / (r_norm**3 + 0.01)\n                    )\n        \n        accelerations = forces\n        velocities_new = velocities + accelerations * dt\n        positions_new = positions + velocities_new * dt\n        \n        return positions_new, velocities_new\n    \n    # Parallel version (conceptual - needs multiple devices)\n    def parallel_nbody_step(positions, velocities, masses, dt):\n        \"\"\"\n        N-body with particles distributed across devices.\n        Each device computes forces for its particles.\n        \"\"\"\n        # This would use pmap in practice\n        # positions shape: [n_devices, particles_per_device, 3]\n        \n        # All-gather pattern to share positions\n        all_positions = jax.lax.all_gather(positions, axis_name='devices')\n        \n        # Each device computes forces for its particles\n        # ... (computation here)\n        \n        # Return updated local positions and velocities\n        pass\n    \n    # 2. Sharding specifications\n    print(\"\\n2. SHARDING FOR LARGE ARRAYS:\")\n    \n    from jax.sharding import PartitionSpec as P\n    from jax.experimental import mesh_utils\n    from jax.sharding import Mesh, NamedSharding\n    \n    # Create a mesh (conceptual - requires actual devices)\n    # devices = mesh_utils.create_device_mesh((2, 2))\n    # mesh = Mesh(devices, axis_names=('x', 'y'))\n    \n    def create_sharding_example():\n        \"\"\"Example of array sharding across devices.\"\"\"\n        \n        # Define how to shard a large array\n        # P('x', 'y') means shard first dim along 'x', second along 'y'\n        # P('x', None) means shard first dim, replicate second\n        # P(None, 'y') means replicate first, shard second\n        \n        spec_2d = P('x', 'y')  # Shard both dimensions\n        spec_rows = P('x', None)  # Shard rows only\n        spec_cols = P(None, 'y')  # Shard columns only\n        spec_replicated = P(None, None)  # Replicate everywhere\n        \n        return spec_2d, spec_rows, spec_cols, spec_replicated\n    \n    specs = create_sharding_example()\n    print(\"  Sharding patterns created (requires multi-device setup)\")\n    \n    # 3. Collective operations\n    print(\"\\n3. COLLECTIVE OPERATIONS:\")\n    \n    def collective_operations_example():\n        \"\"\"\n        Examples of collective communication patterns.\n        These are used inside pmap'd functions.\n        \"\"\"\n        \n        # Inside a pmap'd function:\n        # local_sum = jnp.sum(local_data)\n        \n        # All-reduce: sum across all devices\n        # global_sum = jax.lax.psum(local_sum, axis_name='devices')\n        \n        # All-gather: collect from all devices\n        # all_data = jax.lax.all_gather(local_data, axis_name='devices')\n        \n        # Scatter: distribute data\n        # scattered = jax.lax.pscatter(data, axis_name='devices')\n        \n        print(\"  Collective ops: psum, all_gather, pscatter\")\n        print(\"  Used for communication in parallel computations\")\n    \n    collective_operations_example()\n    \n    # 4. Pipeline parallelism pattern\n    print(\"\\n4. PIPELINE PARALLELISM PATTERN:\")\n    \n    @jit\n    def pipeline_stage_1(x):\n        \"\"\"First stage of pipeline.\"\"\"\n        return jnp.tanh(x @ jnp.ones((100, 100)))\n    \n    @jit\n    def pipeline_stage_2(x):\n        \"\"\"Second stage of pipeline.\"\"\"\n        return jnp.sin(x) + jnp.cos(x)\n    \n    @jit\n    def pipeline_stage_3(x):\n        \"\"\"Third stage of pipeline.\"\"\"\n        return x @ jnp.ones((100, 50))\n    \n    def pipelined_computation(batches):\n        \"\"\"\n        Process batches through pipeline.\n        In practice, stages would run on different devices.\n        \"\"\"\n        results = []\n        \n        # Simplified pipeline (no overlap)\n        for batch in batches:\n            x = pipeline_stage_1(batch)\n            x = pipeline_stage_2(x)\n            x = pipeline_stage_3(x)\n            results.append(x)\n        \n        return jnp.stack(results)\n    \n    # Test pipeline\n    batches = [random.normal(random.PRNGKey(i), (100,)) for i in range(4)]\n    results = pipelined_computation(batches)\n    print(f\"  Pipeline processed {len(batches)} batches\")\n    print(f\"  Output shape: {results.shape}\")\n\ndistributed_computing()","position":{"start":{"line":179,"column":1},"end":{"line":330,"column":1}},"key":"zovJ9Zb9eb"},{"type":"heading","depth":2,"position":{"start":{"line":332,"column":1},"end":{"line":332,"column":1}},"children":[{"type":"text","value":"Profiling and Debugging","position":{"start":{"line":332,"column":1},"end":{"line":332,"column":1}},"key":"hqmjilHBrS"}],"identifier":"profiling-and-debugging","label":"Profiling and Debugging","html_id":"profiling-and-debugging","implicit":true,"key":"qmWmhKXFoq"},{"type":"heading","depth":3,"position":{"start":{"line":334,"column":1},"end":{"line":334,"column":1}},"children":[{"type":"text","value":"Performance Analysis Tools","position":{"start":{"line":334,"column":1},"end":{"line":334,"column":1}},"key":"BMXZpBRysA"}],"identifier":"performance-analysis-tools","label":"Performance Analysis Tools","html_id":"performance-analysis-tools","implicit":true,"key":"LkbcYMSWnq"},{"type":"code","lang":"python","value":"def profiling_and_debugging():\n    \"\"\"Tools and techniques for profiling JAX code.\"\"\"\n    \n    print(\"\\nPROFILING AND DEBUGGING JAX\")\n    print(\"=\" * 50)\n    \n    # 1. Basic timing\n    print(\"\\n1. BASIC PERFORMANCE TIMING:\")\n    \n    def time_function(f, *args, n_runs=100):\n        \"\"\"Time a JIT-compiled function.\"\"\"\n        # Compile\n        f_jit = jit(f)\n        _ = f_jit(*args)  # Trigger compilation\n        \n        # Time\n        start = time.perf_counter()\n        for _ in range(n_runs):\n            _ = f_jit(*args)\n        elapsed = time.perf_counter() - start\n        \n        return elapsed / n_runs\n    \n    def test_function(x):\n        for _ in range(10):\n            x = jnp.sin(x) @ jnp.cos(x.T)\n        return x\n    \n    x = random.normal(random.PRNGKey(0), (100, 100))\n    avg_time = time_function(test_function, x)\n    print(f\"  Average time: {avg_time*1000:.3f} ms\")\n    \n    # 2. Block until result is ready\n    print(\"\\n2. BLOCKING FOR ACCURATE TIMING:\")\n    \n    @jit\n    def async_computation(x):\n        \"\"\"JAX computations are asynchronous.\"\"\"\n        return jnp.sum(x @ x.T)\n    \n    x = random.normal(random.PRNGKey(1), (1000, 1000))\n    \n    # Wrong way (doesn't wait for completion)\n    start = time.perf_counter()\n    result = async_computation(x)  # Returns immediately!\n    wrong_time = time.perf_counter() - start\n    \n    # Right way (blocks until ready)\n    start = time.perf_counter()\n    result = async_computation(x)\n    result.block_until_ready()  # Wait for computation\n    correct_time = time.perf_counter() - start\n    \n    print(f\"  Without blocking: {wrong_time*1000:.3f} ms (incorrect!)\")\n    print(f\"  With blocking: {correct_time*1000:.3f} ms (correct)\")\n    \n    # 3. Compilation inspection\n    print(\"\\n3. INSPECTING COMPILATION:\")\n    \n    from jax import make_jaxpr\n    from jax._src.lib import xla_client\n    \n    def inspect_function(x, y):\n        \"\"\"Function to inspect.\"\"\"\n        z = x @ y\n        return jnp.sum(z * z)\n    \n    x = jnp.ones((3, 3))\n    y = jnp.ones((3, 3))\n    \n    # JAX expression\n    jaxpr = make_jaxpr(inspect_function)(x, y)\n    print(\"\\n  JAX expression (simplified IR):\")\n    print(\"  \" + str(jaxpr).split('\\n')[0][:60] + \"...\")\n    \n    # Lowered to XLA\n    lowered = jit(inspect_function).lower(x, y)\n    print(\"\\n  XLA HLO module available for inspection\")\n    \n    # 4. Debug prints and assertions\n    print(\"\\n4. DEBUGGING TOOLS:\")\n    \n    @jit\n    def debug_example(x):\n        \"\"\"Using debug utilities inside JIT.\"\"\"\n        \n        # Debug print (works inside JIT)\n        jax.debug.print(\"Input shape: {}\", x.shape)\n        \n        # Intermediate values\n        y = jnp.sin(x)\n        jax.debug.print(\"After sin - mean: {:.3f}, std: {:.3f}\", \n                       jnp.mean(y), jnp.std(y))\n        \n        # Assertions (converted to runtime checks)\n        # jax.debug.assert_(jnp.all(jnp.isfinite(y)), \"NaN detected!\")\n        \n        z = y @ y.T\n        jax.debug.print(\"Final shape: {}\", z.shape)\n        \n        return z\n    \n    x = random.normal(random.PRNGKey(2), (5, 5))\n    result = debug_example(x)\n    \n    # 5. Finding bottlenecks\n    print(\"\\n5. IDENTIFYING BOTTLENECKS:\")\n    \n    def find_bottlenecks():\n        \"\"\"Strategies for finding performance issues.\"\"\"\n        \n        # Common bottlenecks:\n        bottlenecks = {\n            \"Python loops\": \"Use scan/fori_loop instead\",\n            \"Small operations\": \"Batch into larger operations\",\n            \"Recompilation\": \"Check for changing shapes/types\",\n            \"Host-device transfer\": \"Minimize data movement\",\n            \"Unintended float64\": \"Use float32 by default\",\n            \"Missing JIT\": \"JIT-compile hot functions\",\n            \"Bad memory access\": \"Optimize data layout\"\n        }\n        \n        for issue, solution in bottlenecks.items():\n            print(f\"    {issue}: {solution}\")\n    \n    find_bottlenecks()\n\nprofiling_and_debugging()","position":{"start":{"line":336,"column":1},"end":{"line":465,"column":1}},"key":"GUyvhOqiy2"},{"type":"heading","depth":2,"position":{"start":{"line":467,"column":1},"end":{"line":467,"column":1}},"children":[{"type":"text","value":"Real-World Example: Galaxy Simulation","position":{"start":{"line":467,"column":1},"end":{"line":467,"column":1}},"key":"Asheky6xzx"}],"identifier":"real-world-example-galaxy-simulation","label":"Real-World Example: Galaxy Simulation","html_id":"real-world-example-galaxy-simulation","implicit":true,"key":"gfumLk6rkZ"},{"type":"heading","depth":3,"position":{"start":{"line":469,"column":1},"end":{"line":469,"column":1}},"children":[{"type":"text","value":"Production-Ready N-Body Code","position":{"start":{"line":469,"column":1},"end":{"line":469,"column":1}},"key":"ipZWax19Gx"}],"identifier":"production-ready-n-body-code","label":"Production-Ready N-Body Code","html_id":"production-ready-n-body-code","implicit":true,"key":"TAF1vXoZ3P"},{"type":"code","lang":"python","value":"def galaxy_simulation_production():\n    \"\"\"\n    Production-ready galaxy simulation using all JAX features.\n    Demonstrates best practices for scientific computing.\n    \"\"\"\n    \n    print(\"\\nPRODUCTION GALAXY SIMULATION\")\n    print(\"=\" * 50)\n    \n    # Configuration\n    @jit\n    def create_galaxy(n_stars, key, galaxy_type='spiral'):\n        \"\"\"Initialize a galaxy with realistic structure.\"\"\"\n        \n        key1, key2, key3, key4 = random.split(key, 4)\n        \n        if galaxy_type == 'spiral':\n            # Spiral galaxy with disk and bulge\n            \n            # Disk component (exponential profile)\n            radii = random.exponential(key1, (int(0.8 * n_stars),)) * 10.0\n            angles = random.uniform(key2, (int(0.8 * n_stars),), \n                                   minval=0, maxval=2*jnp.pi)\n            \n            # Add spiral structure\n            spiral_phase = 2.0 * jnp.log(radii + 1)\n            angles = angles + spiral_phase\n            \n            disk_x = radii * jnp.cos(angles)\n            disk_y = radii * jnp.sin(angles)\n            disk_z = random.normal(key3, (int(0.8 * n_stars),)) * 0.5\n            \n            # Bulge component (Hernquist profile)\n            n_bulge = n_stars - int(0.8 * n_stars)\n            r_bulge = random.uniform(key4, (n_bulge,)) ** (1/3) * 3.0\n            theta = jnp.arccos(1 - 2 * random.uniform(key1, (n_bulge,)))\n            phi = random.uniform(key2, (n_bulge,), minval=0, maxval=2*jnp.pi)\n            \n            bulge_x = r_bulge * jnp.sin(theta) * jnp.cos(phi)\n            bulge_y = r_bulge * jnp.sin(theta) * jnp.sin(phi)\n            bulge_z = r_bulge * jnp.cos(theta)\n            \n            # Combine\n            positions = jnp.concatenate([\n                jnp.stack([disk_x, disk_y, disk_z], axis=1),\n                jnp.stack([bulge_x, bulge_y, bulge_z], axis=1)\n            ])\n            \n            # Circular velocities (simplified)\n            all_radii = jnp.sqrt(positions[:, 0]**2 + positions[:, 1]**2)\n            v_circ = jnp.sqrt(all_radii / (all_radii + 1.0))  # Rotation curve\n            \n            velocities = jnp.zeros_like(positions)\n            velocities = velocities.at[:, 0].set(-positions[:, 1] / all_radii * v_circ)\n            velocities = velocities.at[:, 1].set(positions[:, 0] / all_radii * v_circ)\n            \n            # Masses (IMF-like distribution)\n            masses = random.pareto(key3, 2.35, (n_stars,)) * 0.1 + 0.1\n            masses = jnp.clip(masses, 0.1, 10.0)\n            \n        return positions, velocities, masses\n    \n    # Optimized force calculation\n    @jit\n    def compute_forces_fast(positions, masses, softening=0.1):\n        \"\"\"\n        Fast force calculation using vectorization.\n        O(N²) but highly optimized.\n        \"\"\"\n        n = len(masses)\n        \n        # Compute all pairwise vectors at once\n        r_ij = positions[:, None, :] - positions[None, :, :]  # (n, n, 3)\n        \n        # Distances\n        r2 = jnp.sum(r_ij**2, axis=2) + softening**2  # (n, n)\n        r3 = r2 ** 1.5\n        \n        # Mask diagonal (self-interaction)\n        mask = 1.0 - jnp.eye(n)\n        \n        # Forces\n        F_ij = r_ij / r3[:, :, None] * mask[:, :, None]  # (n, n, 3)\n        forces = jnp.sum(masses[None, :, None] * F_ij, axis=1)  # (n, 3)\n        \n        return forces * masses[:, None]\n    \n    # Adaptive time-stepping\n    @jit\n    def adaptive_timestep(positions, velocities, masses, base_dt=0.01):\n        \"\"\"Compute adaptive timestep based on local dynamics.\"\"\"\n        \n        forces = compute_forces_fast(positions, masses)\n        accelerations = forces / masses[:, None]\n        \n        # Criteria for timestep\n        v_mag = jnp.linalg.norm(velocities, axis=1)\n        a_mag = jnp.linalg.norm(accelerations, axis=1)\n        \n        # Courant condition\n        dt_courant = jnp.min(0.1 / (v_mag + 1e-10))\n        \n        # Acceleration condition\n        dt_accel = jnp.min(jnp.sqrt(0.1 / (a_mag + 1e-10)))\n        \n        # Take minimum\n        dt = jnp.minimum(dt_courant, dt_accel)\n        dt = jnp.minimum(dt, base_dt)\n        \n        return dt\n    \n    # Main evolution with all features\n    @partial(jit, static_argnums=(3,))\n    def evolve_galaxy(positions, velocities, masses, n_steps, checkpoint_every=100):\n        \"\"\"\n        Full galaxy evolution with checkpointing.\n        \"\"\"\n        \n        def step(carry, i):\n            pos, vel = carry\n            \n            # Adaptive timestep\n            dt = adaptive_timestep(pos, vel, masses)\n            \n            # Leapfrog integration\n            forces = compute_forces_fast(pos, masses)\n            acc = forces / masses[:, None]\n            \n            vel_half = vel + 0.5 * dt * acc\n            pos_new = pos + dt * vel_half\n            \n            forces_new = compute_forces_fast(pos_new, masses)\n            acc_new = forces_new / masses[:, None]\n            \n            vel_new = vel_half + 0.5 * dt * acc_new\n            \n            # Energy for monitoring\n            ke = 0.5 * jnp.sum(masses[:, None] * vel_new**2)\n            \n            # Checkpoint decision (would save to disk in practice)\n            should_checkpoint = (i % checkpoint_every) == 0\n            \n            return (pos_new, vel_new), (pos_new, ke, dt)\n        \n        final_state, (trajectory, energies, timesteps) = scan(\n            step, (positions, velocities), jnp.arange(n_steps)\n        )\n        \n        return final_state, trajectory[::checkpoint_every], energies[::checkpoint_every]\n    \n    # Initialize and run\n    key = random.PRNGKey(42)\n    n_stars = 1000\n    \n    print(f\"\\nInitializing spiral galaxy with {n_stars} stars...\")\n    positions, velocities, masses = create_galaxy(n_stars, key)\n    \n    print(\"Running simulation...\")\n    start_time = time.perf_counter()\n    \n    final_state, checkpoints, energies = evolve_galaxy(\n        positions, velocities, masses, n_steps=1000, checkpoint_every=100\n    )\n    \n    elapsed = time.perf_counter() - start_time\n    print(f\"  Completed in {elapsed:.2f} seconds\")\n    print(f\"  Steps per second: {1000/elapsed:.0f}\")\n    \n    # Analysis\n    initial_energy = energies[0]\n    final_energy = energies[-1]\n    print(f\"\\nEnergy conservation:\")\n    print(f\"  Initial: {initial_energy:.2f}\")\n    print(f\"  Final: {final_energy:.2f}\")\n    print(f\"  Drift: {abs(final_energy - initial_energy) / abs(initial_energy) * 100:.2f}%\")\n    \n    # Visualize\n    fig = plt.figure(figsize=(15, 5))\n    \n    # Initial configuration\n    ax1 = fig.add_subplot(131)\n    ax1.scatter(positions[:, 0], positions[:, 1], s=1, alpha=0.5, c=masses, cmap='YlOrRd')\n    ax1.set_xlim(-30, 30)\n    ax1.set_ylim(-30, 30)\n    ax1.set_xlabel('X [kpc]')\n    ax1.set_ylabel('Y [kpc]')\n    ax1.set_title('Initial Galaxy')\n    ax1.set_aspect('equal')\n    \n    # Final configuration\n    ax2 = fig.add_subplot(132)\n    final_pos = final_state[0]\n    ax2.scatter(final_pos[:, 0], final_pos[:, 1], s=1, alpha=0.5, c=masses, cmap='YlOrRd')\n    ax2.set_xlim(-30, 30)\n    ax2.set_ylim(-30, 30)\n    ax2.set_xlabel('X [kpc]')\n    ax2.set_ylabel('Y [kpc]')\n    ax2.set_title('Final Galaxy')\n    ax2.set_aspect('equal')\n    \n    # Energy evolution\n    ax3 = fig.add_subplot(133)\n    ax3.plot(energies)\n    ax3.set_xlabel('Checkpoint')\n    ax3.set_ylabel('Total Energy')\n    ax3.set_title('Energy Conservation')\n    ax3.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return checkpoints\n\n# Run the production simulation\ncheckpoints = galaxy_simulation_production()","position":{"start":{"line":471,"column":1},"end":{"line":687,"column":1}},"key":"QXM40aE2VC"},{"type":"heading","depth":2,"position":{"start":{"line":689,"column":1},"end":{"line":689,"column":1}},"children":[{"type":"text","value":"Best Practices Summary","position":{"start":{"line":689,"column":1},"end":{"line":689,"column":1}},"key":"VCzIOCk9b3"}],"identifier":"best-practices-summary","label":"Best Practices Summary","html_id":"best-practices-summary","implicit":true,"key":"OHOKd98LfJ"},{"type":"heading","depth":3,"position":{"start":{"line":691,"column":1},"end":{"line":691,"column":1}},"children":[{"type":"text","value":"JAX Do’s and Don’ts","position":{"start":{"line":691,"column":1},"end":{"line":691,"column":1}},"key":"CO5PvIPryG"}],"identifier":"jax-dos-and-donts","label":"JAX Do’s and Don’ts","html_id":"jax-dos-and-donts","implicit":true,"key":"iGq8HqNVqQ"},{"type":"code","lang":"python","value":"def best_practices():\n    \"\"\"Summary of JAX best practices for astronomical computing.\"\"\"\n    \n    print(\"\\nJAX BEST PRACTICES FOR ASTRONOMY\")\n    print(\"=\" * 50)\n    \n    practices = {\n        \"✅ DO\": [\n            \"Use JIT on hot loops and expensive functions\",\n            \"Write pure functional code without side effects\",\n            \"Use scan for sequential computations\",\n            \"Vectorize with vmap instead of loops\",\n            \"Profile and measure performance\",\n            \"Use float32 by default for speed\",\n            \"Leverage automatic differentiation\",\n            \"Think in terms of array operations\",\n            \"Use static_argnums for compile-time constants\",\n            \"Test numerical stability and accuracy\"\n        ],\n        \n        \"❌ DON'T\": [\n            \"Use Python loops inside JIT functions\",\n            \"Mutate arrays (use .at[].set() instead)\",\n            \"Use varying shapes (causes recompilation)\",\n            \"Forget to block_until_ready() when timing\",\n            \"Mix NumPy and JAX arrays carelessly\",\n            \"Use float64 unless necessary\",\n            \"Ignore memory consumption\",\n            \"Use global variables in JIT functions\",\n            \"Forget to split random keys\",\n            \"Skip validation of gradients\"\n        ],\n        \n        \"🚀 ADVANCED\": [\n            \"Custom VJP rules for numerical stability\",\n            \"Checkpointing for memory efficiency\",\n            \"Sharding for multi-GPU parallelism\",\n            \"Pipeline parallelism for deep models\",\n            \"Mixed precision for performance\",\n            \"Custom linear algebra primitives\",\n            \"Optimize data layout for cache\",\n            \"Use donation to reuse buffers\",\n            \"Profile XLA compilation\",\n            \"Implement custom CUDA kernels\"\n        ]\n    }\n    \n    for category, items in practices.items():\n        print(f\"\\n{category}:\")\n        for item in items:\n            print(f\"  • {item}\")\n\nbest_practices()","position":{"start":{"line":693,"column":1},"end":{"line":747,"column":1}},"key":"VA2WChEHdE"},{"type":"heading","depth":2,"position":{"start":{"line":749,"column":1},"end":{"line":749,"column":1}},"children":[{"type":"text","value":"Exercises","position":{"start":{"line":749,"column":1},"end":{"line":749,"column":1}},"key":"UhgGrnAsiN"}],"identifier":"exercises","label":"Exercises","html_id":"exercises","implicit":true,"key":"CoAMSOTfam"},{"type":"heading","depth":3,"position":{"start":{"line":751,"column":1},"end":{"line":751,"column":1}},"children":[{"type":"text","value":"Exercise 1: Adaptive Mesh Refinement","position":{"start":{"line":751,"column":1},"end":{"line":751,"column":1}},"key":"bX8w2ikW3h"}],"identifier":"exercise-1-adaptive-mesh-refinement","label":"Exercise 1: Adaptive Mesh Refinement","html_id":"exercise-1-adaptive-mesh-refinement","implicit":true,"key":"OT7yd1akHK"},{"type":"code","lang":"python","value":"def adaptive_mesh_refinement():\n    \"\"\"\n    Implement AMR for solving Poisson equation in JAX.\n    \n    Requirements:\n    - Use while_loop for adaptive refinement\n    - Custom VJP for interpolation operators\n    - JIT compile the multigrid solver\n    - Verify convergence with manufactured solution\n    \"\"\"\n    # Your code here\n    pass","position":{"start":{"line":752,"column":1},"end":{"line":765,"column":1}},"key":"RaVI5pTrds"},{"type":"heading","depth":3,"position":{"start":{"line":767,"column":1},"end":{"line":767,"column":1}},"children":[{"type":"text","value":"Exercise 2: Differentiable Radiative Transfer","position":{"start":{"line":767,"column":1},"end":{"line":767,"column":1}},"key":"ijWWAUlxfn"}],"identifier":"exercise-2-differentiable-radiative-transfer","label":"Exercise 2: Differentiable Radiative Transfer","html_id":"exercise-2-differentiable-radiative-transfer","implicit":true,"key":"y5MiIKAhsK"},{"type":"code","lang":"python","value":"def differentiable_rt():\n    \"\"\"\n    Build differentiable radiative transfer solver.\n    \n    Tasks:\n    - Implement ray marching with scan\n    - Use custom derivatives for optical depth\n    - Optimize source function parameters via gradient descent\n    - Compare with Monte Carlo solution\n    \"\"\"\n    # Your code here\n    pass","position":{"start":{"line":768,"column":1},"end":{"line":781,"column":1}},"key":"vK5hyGykxk"},{"type":"heading","depth":3,"position":{"start":{"line":783,"column":1},"end":{"line":783,"column":1}},"children":[{"type":"text","value":"Exercise 3: Parallel MCMC Sampler","position":{"start":{"line":783,"column":1},"end":{"line":783,"column":1}},"key":"gfYuHbN1xY"}],"identifier":"exercise-3-parallel-mcmc-sampler","label":"Exercise 3: Parallel MCMC Sampler","html_id":"exercise-3-parallel-mcmc-sampler","implicit":true,"key":"zXt7TAZADi"},{"type":"code","lang":"python","value":"def parallel_mcmc():\n    \"\"\"\n    Implement parallel tempered MCMC in JAX.\n    \n    Requirements:\n    - Use pmap for parallel chains\n    - Implement replica exchange with collective ops\n    - JIT compile the likelihood and proposals\n    - Test on cosmological parameter estimation\n    \"\"\"\n    # Your code here\n    pass","position":{"start":{"line":784,"column":1},"end":{"line":797,"column":1}},"key":"LvYNzuPd6f"},{"type":"heading","depth":2,"position":{"start":{"line":799,"column":1},"end":{"line":799,"column":1}},"children":[{"type":"text","value":"Key Takeaways","position":{"start":{"line":799,"column":1},"end":{"line":799,"column":1}},"key":"pnLsgUKC9m"}],"identifier":"key-takeaways","label":"Key Takeaways","html_id":"key-takeaways","implicit":true,"key":"X7Fcwb7AUI"},{"type":"paragraph","position":{"start":{"line":801,"column":1},"end":{"line":806,"column":1}},"children":[{"type":"text","value":"✅ ","position":{"start":{"line":801,"column":1},"end":{"line":801,"column":1}},"key":"ieZ8nqcKKF"},{"type":"strong","position":{"start":{"line":801,"column":1},"end":{"line":801,"column":1}},"children":[{"type":"text","value":"Control flow","position":{"start":{"line":801,"column":1},"end":{"line":801,"column":1}},"key":"RgRUffIpOr"}],"key":"Fng0J0E01x"},{"type":"text","value":" - Use lax.cond, scan, while_loop for conditionals and loops","position":{"start":{"line":801,"column":1},"end":{"line":801,"column":1}},"key":"ulnXu9V44c"},{"type":"break","position":{"start":{"line":801,"column":1},"end":{"line":801,"column":1}},"key":"qXfKASE3MM"},{"type":"text","value":"✅ ","position":{"start":{"line":801,"column":1},"end":{"line":801,"column":1}},"key":"wvznzcEUcC"},{"type":"strong","position":{"start":{"line":801,"column":1},"end":{"line":801,"column":1}},"children":[{"type":"text","value":"Custom derivatives","position":{"start":{"line":801,"column":1},"end":{"line":801,"column":1}},"key":"BxsZQqYUF1"}],"key":"NP4ZglwVTQ"},{"type":"text","value":" - Define VJP/JVP rules for numerical stability","position":{"start":{"line":801,"column":1},"end":{"line":801,"column":1}},"key":"wBmUHFzrTT"},{"type":"break","position":{"start":{"line":801,"column":1},"end":{"line":801,"column":1}},"key":"uk4e8JSuB8"},{"type":"text","value":"✅ ","position":{"start":{"line":801,"column":1},"end":{"line":801,"column":1}},"key":"vUHazPBUm8"},{"type":"strong","position":{"start":{"line":801,"column":1},"end":{"line":801,"column":1}},"children":[{"type":"text","value":"Memory optimization","position":{"start":{"line":801,"column":1},"end":{"line":801,"column":1}},"key":"dIPDTcnCti"}],"key":"kzEXIAieYt"},{"type":"text","value":" - Checkpointing, donation, and chunking strategies","position":{"start":{"line":801,"column":1},"end":{"line":801,"column":1}},"key":"Rkc4b6htHJ"},{"type":"break","position":{"start":{"line":801,"column":1},"end":{"line":801,"column":1}},"key":"Yt55QSM9d7"},{"type":"text","value":"✅ ","position":{"start":{"line":801,"column":1},"end":{"line":801,"column":1}},"key":"UabxECJO9J"},{"type":"strong","position":{"start":{"line":801,"column":1},"end":{"line":801,"column":1}},"children":[{"type":"text","value":"Parallelism","position":{"start":{"line":801,"column":1},"end":{"line":801,"column":1}},"key":"b19BJJQGNe"}],"key":"Pau1X7qANz"},{"type":"text","value":" - pmap for multi-device, sharding for large arrays","position":{"start":{"line":801,"column":1},"end":{"line":801,"column":1}},"key":"aX51Idve0t"},{"type":"break","position":{"start":{"line":801,"column":1},"end":{"line":801,"column":1}},"key":"WKd5R9wqrw"},{"type":"text","value":"✅ ","position":{"start":{"line":801,"column":1},"end":{"line":801,"column":1}},"key":"fhU3NUkXy2"},{"type":"strong","position":{"start":{"line":801,"column":1},"end":{"line":801,"column":1}},"children":[{"type":"text","value":"Profiling","position":{"start":{"line":801,"column":1},"end":{"line":801,"column":1}},"key":"juaBN4llhq"}],"key":"UVT9AT39wV"},{"type":"text","value":" - Always measure, use block_until_ready, inspect compilation","position":{"start":{"line":801,"column":1},"end":{"line":801,"column":1}},"key":"mEsnIHcOHM"},{"type":"break","position":{"start":{"line":801,"column":1},"end":{"line":801,"column":1}},"key":"HButBr2EgU"},{"type":"text","value":"✅ ","position":{"start":{"line":801,"column":1},"end":{"line":801,"column":1}},"key":"F2pElVRnFI"},{"type":"strong","position":{"start":{"line":801,"column":1},"end":{"line":801,"column":1}},"children":[{"type":"text","value":"Production code","position":{"start":{"line":801,"column":1},"end":{"line":801,"column":1}},"key":"dxIE8lhWsO"}],"key":"esFvJmWyLZ"},{"type":"text","value":" - Combine all features for real scientific applications","position":{"start":{"line":801,"column":1},"end":{"line":801,"column":1}},"key":"BNmhJd48Xw"}],"key":"o6NV44j8Ex"},{"type":"heading","depth":2,"position":{"start":{"line":808,"column":1},"end":{"line":808,"column":1}},"children":[{"type":"text","value":"Next Steps","position":{"start":{"line":808,"column":1},"end":{"line":808,"column":1}},"key":"DolJNfwwHw"}],"identifier":"next-steps","label":"Next Steps","html_id":"next-steps","implicit":true,"key":"Tizb7KvPIR"},{"type":"paragraph","position":{"start":{"line":810,"column":1},"end":{"line":810,"column":1}},"children":[{"type":"text","value":"With these advanced patterns, you’re ready to:","position":{"start":{"line":810,"column":1},"end":{"line":810,"column":1}},"key":"rZ7309hJk7"}],"key":"LAZci3Eqk6"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":811,"column":1},"end":{"line":816,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":811,"column":1},"end":{"line":811,"column":1}},"children":[{"type":"text","value":"Port existing astronomical codes to JAX","position":{"start":{"line":811,"column":1},"end":{"line":811,"column":1}},"key":"OoHU9alvg9"}],"key":"MSQWYdoy2p"},{"type":"listItem","spread":true,"position":{"start":{"line":812,"column":1},"end":{"line":812,"column":1}},"children":[{"type":"text","value":"Build differentiable physical models","position":{"start":{"line":812,"column":1},"end":{"line":812,"column":1}},"key":"tKbLcF8W91"}],"key":"aPgiwiVIpa"},{"type":"listItem","spread":true,"position":{"start":{"line":813,"column":1},"end":{"line":813,"column":1}},"children":[{"type":"text","value":"Scale to multi-GPU clusters","position":{"start":{"line":813,"column":1},"end":{"line":813,"column":1}},"key":"f7B2Cs58bV"}],"key":"CzsgoMCOru"},{"type":"listItem","spread":true,"position":{"start":{"line":814,"column":1},"end":{"line":814,"column":1}},"children":[{"type":"text","value":"Contribute to JAX ecosystem libraries","position":{"start":{"line":814,"column":1},"end":{"line":814,"column":1}},"key":"qUkkMvfFYn"}],"key":"uCvU6Vn2Yw"},{"type":"listItem","spread":true,"position":{"start":{"line":815,"column":1},"end":{"line":816,"column":1}},"children":[{"type":"text","value":"Develop novel computational methods","position":{"start":{"line":815,"column":1},"end":{"line":815,"column":1}},"key":"CG7OHZ5ujH"}],"key":"b9OoZN0tJn"}],"key":"ySIHC17rLR"},{"type":"paragraph","position":{"start":{"line":817,"column":1},"end":{"line":817,"column":1}},"children":[{"type":"text","value":"Remember: JAX rewards thinking in terms of transformations and functional programming. The initial learning curve pays off with unprecedented performance and capabilities for scientific computing!","position":{"start":{"line":817,"column":1},"end":{"line":817,"column":1}},"key":"N27227bRbG"}],"key":"CaIrEjv09N"}],"key":"fcaLxUaKRr"}],"key":"pa0KzBYGIe"},"references":{"cite":{"order":[],"data":{}}}}