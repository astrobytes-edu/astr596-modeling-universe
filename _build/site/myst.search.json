{"version":"1","records":[{"hierarchy":{"lvl1":"Syllabus - ASTR 596: Modeling the Universe"},"type":"lvl1","url":"/syllabus","position":0},{"hierarchy":{"lvl1":"Syllabus - ASTR 596: Modeling the Universe"},"content":"Fall 2025 - San Diego State University (SDSU)Fridays 11:00 AM - 1:40 PM | PA 215","type":"content","url":"/syllabus","position":1},{"hierarchy":{"lvl1":"Syllabus - ASTR 596: Modeling the Universe","lvl2":"Instructor Information"},"type":"lvl2","url":"/syllabus#instructor-information","position":2},{"hierarchy":{"lvl1":"Syllabus - ASTR 596: Modeling the Universe","lvl2":"Instructor Information"},"content":"Dr. Anna Rosen\n\nOffice: Physics 239\n\nOffice Hours: Thursdays 11:00 AM - 12:00 PM (also available by appointment or virtually)\n\nEmail: \n\nalrosen@sdsu.edu","type":"content","url":"/syllabus#instructor-information","position":3},{"hierarchy":{"lvl1":"Syllabus - ASTR 596: Modeling the Universe","lvl2":"Course Information"},"type":"lvl2","url":"/syllabus#course-information","position":4},{"hierarchy":{"lvl1":"Syllabus - ASTR 596: Modeling the Universe","lvl2":"Course Information"},"content":"Prerequisites: Physics 196; MATH 254 or 342A; or equivalent with instructor permission, or graduate standing\n\nMeeting Time: Fridays 11:00 AM - 1:40 PM\n\nFormat: ~30-45min lecture review + 2h hands-on project work\n\nLocation: PA 215\n\nCourse Website: <\n\nwww.anna-rosen.com>\n\nPlatforms: Canvas, Slack, GitHub Classroom\n\nExpectations: Students must come prepared having completed assigned JupyterBook readings","type":"content","url":"/syllabus#course-information","position":5},{"hierarchy":{"lvl1":"Syllabus - ASTR 596: Modeling the Universe","lvl3":"Class Meeting Structure","lvl2":"Course Information"},"type":"lvl3","url":"/syllabus#class-meeting-structure","position":6},{"hierarchy":{"lvl1":"Syllabus - ASTR 596: Modeling the Universe","lvl3":"Class Meeting Structure","lvl2":"Course Information"},"content":"Pre-Class Preparation (Required):\n\nComplete assigned JupyterBook chapter readings\n\nReview project requirements if new project assigned\n\nPrepare questions on material and implementation challenges\n\nFriday Class Sessions:\n\n11:00-11:45 AM: Interactive review of week’s concepts, Q&A on readings, clarification of project requirements\n\n11:45 AM-1:40 PM: Hands-on project work with pair programming, implementation support, and peer collaboration","type":"content","url":"/syllabus#class-meeting-structure","position":7},{"hierarchy":{"lvl1":"Syllabus - ASTR 596: Modeling the Universe","lvl2":"Course Description"},"type":"lvl2","url":"/syllabus#course-description","position":8},{"hierarchy":{"lvl1":"Syllabus - ASTR 596: Modeling the Universe","lvl2":"Course Description"},"content":"This course provides a hands-on introduction to the practice and theory of scientific computing, with an emphasis on numerical methods and machine learning, applied to astrophysical problems. Beginning with Python programming fundamentals and object-oriented design, the course progresses through sophisticated numerical methods including N-body dynamics, Monte Carlo radiative transfer, Bayesian inference, Gaussian processes, and culminates with neural networks. Students will implement all algorithms from first principles (“glass box” approach) before transitioning to modern frameworks (JAX ecosystem). The course emphasizes professional software development practices, responsible AI integration, and preparation for computational research careers.\n\nImportant Note: I’m not testing your astrophysics knowledge. All necessary equations and scientific background will be provided. Your task is to understand the scientific concepts, implement them correctly, and connect the computation to the physics.\n\nFor an expanded description of the course philosophy and approach, see: \n\nWhy This Course is Different","type":"content","url":"/syllabus#course-description","position":9},{"hierarchy":{"lvl1":"Syllabus - ASTR 596: Modeling the Universe","lvl2":"Course Learning Outcomes"},"type":"lvl2","url":"/syllabus#course-learning-outcomes","position":10},{"hierarchy":{"lvl1":"Syllabus - ASTR 596: Modeling the Universe","lvl2":"Course Learning Outcomes"},"content":"Upon successful completion of this course, students will be able to:\n\nImplement numerical schemes for solving scientific problems using Python, employing advanced programming paradigms including object-oriented programming (OOP).\n\nDevelop professional software practices including modular algorithm design, meaningful documentation, version control, testing, and effective code structuring.\n\nMaster key numerical techniques including numerical integration, root finding, model fitting, and solving ordinary and partial differential equations.\n\nApply Monte Carlo methods to complex astrophysical problems including radiative transfer and Bayesian inference.\n\nBuild neural networks from fundamentals implementing backpropagation and gradient descent without relying on libraries.\n\nUtilize modern computational frameworks translating implementations to the JAX ecosystem.\n\nIntegrate AI tools strategically through a scaffolded three-phase approach while maintaining deep understanding.\n\nSimulate advanced astrophysical phenomena including N-body dynamics, stellar physics, and radiative processes.\n\nCommunicate computational methods and scientific results effectively through written reports and code documentation.\n\nThink computationally about physics developing intuition for numerical stability and convergence.","type":"content","url":"/syllabus#course-learning-outcomes","position":11},{"hierarchy":{"lvl1":"Syllabus - ASTR 596: Modeling the Universe","lvl2":"Materials"},"type":"lvl2","url":"/syllabus#materials","position":12},{"hierarchy":{"lvl1":"Syllabus - ASTR 596: Modeling the Universe","lvl2":"Materials"},"content":"","type":"content","url":"/syllabus#materials","position":13},{"hierarchy":{"lvl1":"Syllabus - ASTR 596: Modeling the Universe","lvl3":"Textbooks (Free Online Resources)","lvl2":"Materials"},"type":"lvl3","url":"/syllabus#textbooks-free-online-resources","position":14},{"hierarchy":{"lvl1":"Syllabus - ASTR 596: Modeling the Universe","lvl3":"Textbooks (Free Online Resources)","lvl2":"Materials"},"content":"Required: Rosen (2025), ASTR 596 Course Website (powered by Jupyterbook/MystMD) \n\nwww​.astrobytes​-edu​.github​.io​/astr596​-modeling​-universe\n\nLinge & Langtangen (2020), \n\n*Programming for Computations - Python (2nd Edition), Springer Open\n\nDeisenroth, Faisal, & Ong (2020), \n\nMathematics for Machine Learning, Cambridge University Press\n\nTing (2025), \n\nStatistical Machine Learning for Astronomy, \n\nArXiV.org","type":"content","url":"/syllabus#textbooks-free-online-resources","position":15},{"hierarchy":{"lvl1":"Syllabus - ASTR 596: Modeling the Universe","lvl3":"Software Requirements","lvl2":"Materials"},"type":"lvl3","url":"/syllabus#software-requirements","position":16},{"hierarchy":{"lvl1":"Syllabus - ASTR 596: Modeling the Universe","lvl3":"Software Requirements","lvl2":"Materials"},"content":"Python 3.10+ with scientific stack\n\nGit and GitHub account\n\nJupyter Lab/Notebooks (Project 1 only)\n\nIDE (VS Code recommended) with ALL AI assistants disabled\n\nTerminal/command line access\n\nFor detailed setup instructions, see: \n\nSoftware Installation Guide","type":"content","url":"/syllabus#software-requirements","position":17},{"hierarchy":{"lvl1":"Syllabus - ASTR 596: Modeling the Universe","lvl3":"Computational Resources","lvl2":"Materials"},"type":"lvl3","url":"/syllabus#computational-resources","position":18},{"hierarchy":{"lvl1":"Syllabus - ASTR 596: Modeling the Universe","lvl3":"Computational Resources","lvl2":"Materials"},"content":"SDSU Instructional Cluster (Verne): Access provided in class\n\nGitHub Classroom: All projects distributed and submitted here","type":"content","url":"/syllabus#computational-resources","position":19},{"hierarchy":{"lvl1":"Syllabus - ASTR 596: Modeling the Universe","lvl2":"Grading Information"},"type":"lvl2","url":"/syllabus#grading-information","position":20},{"hierarchy":{"lvl1":"Syllabus - ASTR 596: Modeling the Universe","lvl2":"Grading Information"},"content":"","type":"content","url":"/syllabus#grading-information","position":21},{"hierarchy":{"lvl1":"Syllabus - ASTR 596: Modeling the Universe","lvl3":"Assessment Components","lvl2":"Grading Information"},"type":"lvl3","url":"/syllabus#assessment-components","position":22},{"hierarchy":{"lvl1":"Syllabus - ASTR 596: Modeling the Universe","lvl3":"Assessment Components","lvl2":"Grading Information"},"content":"Component\n\nWeight\n\nDescription\n\nProjects 1-6\n\n50%\n\n8.33% each, due biweekly on Mondays via GitHub Classroom\n\nGrowth Memos\n\n10%\n\n6 reflections at 1.67% each, submitted to Canvas as PDF\n\nTechnical Growth Synthesis\n\n5%\n\nCumulative reflection due Dec 11 via Canvas\n\nFinal Project\n\n25%\n\nJAX implementation with research component\n\nParticipation & Engagement\n\n10%\n\nActive contribution and collaboration\n\nFor detailed project requirements and rubrics, see: \n\nProject Submission Guide\n\nParticipation & Engagement (10%): Evaluated based on demonstrated preparation for class (engagement with readings, thoughtful questions), productive collaboration during pair programming, and consistent contribution to the learning environment. Chronic tardiness or lack of preparation will negatively impact this grade. This is assessed through instructor observation throughout the semester.","type":"content","url":"/syllabus#assessment-components","position":23},{"hierarchy":{"lvl1":"Syllabus - ASTR 596: Modeling the Universe","lvl3":"Grading Scale","lvl2":"Grading Information"},"type":"lvl3","url":"/syllabus#grading-scale","position":24},{"hierarchy":{"lvl1":"Syllabus - ASTR 596: Modeling the Universe","lvl3":"Grading Scale","lvl2":"Grading Information"},"content":"A: 93-100% | A-: 90-92%\n\nB+: 87-89% | B: 83-86% | B-: 80-82%\n\nC+: 77-79% | C: 73-76% | C-: 70-72%\n\nD+: 67-69% | D: 63-66% | D-: 60-62%\n\nF: Below 60%","type":"content","url":"/syllabus#grading-scale","position":25},{"hierarchy":{"lvl1":"Syllabus - ASTR 596: Modeling the Universe","lvl2":"Course Policies"},"type":"lvl2","url":"/syllabus#course-policies","position":26},{"hierarchy":{"lvl1":"Syllabus - ASTR 596: Modeling the Universe","lvl2":"Course Policies"},"content":"","type":"content","url":"/syllabus#course-policies","position":27},{"hierarchy":{"lvl1":"Syllabus - ASTR 596: Modeling the Universe","lvl3":"Attendance Policy","lvl2":"Course Policies"},"type":"lvl3","url":"/syllabus#attendance-policy","position":28},{"hierarchy":{"lvl1":"Syllabus - ASTR 596: Modeling the Universe","lvl3":"Attendance Policy","lvl2":"Course Policies"},"content":"Attendance at Friday sessions is essential. While not explicitly tracked, participation requires presence. Two absences permitted without penalty; additional absences may impact participation grade and project success.","type":"content","url":"/syllabus#attendance-policy","position":29},{"hierarchy":{"lvl1":"Syllabus - ASTR 596: Modeling the Universe","lvl3":"Late Work Policy","lvl2":"Course Policies"},"type":"lvl3","url":"/syllabus#late-work-policy","position":30},{"hierarchy":{"lvl1":"Syllabus - ASTR 596: Modeling the Universe","lvl3":"Late Work Policy","lvl2":"Course Policies"},"content":"One no-questions-asked 2-day extension per semester\n\nMust be requested 24+ hours before deadline\n\n10% penalty per day after grace period\n\nEarly submissions encouraged","type":"content","url":"/syllabus#late-work-policy","position":31},{"hierarchy":{"lvl1":"Syllabus - ASTR 596: Modeling the Universe","lvl3":"Collaboration Policy","lvl2":"Course Policies"},"type":"lvl3","url":"/syllabus#collaboration-policy","position":32},{"hierarchy":{"lvl1":"Syllabus - ASTR 596: Modeling the Universe","lvl3":"Collaboration Policy","lvl2":"Course Policies"},"content":"Pair programming encouraged during lab sessions. All submitted code must be individually written and understood. You must be able to explain every line of code you submit.","type":"content","url":"/syllabus#collaboration-policy","position":33},{"hierarchy":{"lvl1":"Syllabus - ASTR 596: Modeling the Universe","lvl3":"AI Usage Policy","lvl2":"Course Policies"},"type":"lvl3","url":"/syllabus#ai-usage-policy","position":34},{"hierarchy":{"lvl1":"Syllabus - ASTR 596: Modeling the Universe","lvl3":"AI Usage Policy","lvl2":"Course Policies"},"content":"This course uses a three-phase scaffolded approach to AI integration:\n\nPhase 1 (Weeks 1-4): Foundation building with minimal AI\n\nPhase 2 (Weeks 5-8): Strategic integration with verification\n\nPhase 3 (Weeks 9-16): Professional practice with critical evaluation\n\nFor complete AI usage guidelines and examples, see: \n\nAI Usage Policy & Guide","type":"content","url":"/syllabus#ai-usage-policy","position":35},{"hierarchy":{"lvl1":"Syllabus - ASTR 596: Modeling the Universe","lvl2":"Academic Integrity"},"type":"lvl2","url":"/syllabus#academic-integrity","position":36},{"hierarchy":{"lvl1":"Syllabus - ASTR 596: Modeling the Universe","lvl2":"Academic Integrity"},"content":"All work must be your own. Violations will be reported to the Center for Student Rights and Responsibilities. You may be asked to explain any aspect of your submitted work.\n\nSee: \n\nSDSU Academic Integrity Policy","type":"content","url":"/syllabus#academic-integrity","position":37},{"hierarchy":{"lvl1":"Syllabus - ASTR 596: Modeling the Universe","lvl2":"Accommodations"},"type":"lvl2","url":"/syllabus#accommodations","position":38},{"hierarchy":{"lvl1":"Syllabus - ASTR 596: Modeling the Universe","lvl2":"Accommodations"},"content":"Students with disabilities should contact Student Disability Services (\n\nsds@sdsu.edu, 619-594-6473) within the first two weeks of class.","type":"content","url":"/syllabus#accommodations","position":39},{"hierarchy":{"lvl1":"Syllabus - ASTR 596: Modeling the Universe","lvl2":"Land Acknowledgement"},"type":"lvl2","url":"/syllabus#land-acknowledgement","position":40},{"hierarchy":{"lvl1":"Syllabus - ASTR 596: Modeling the Universe","lvl2":"Land Acknowledgement"},"content":"We acknowledge that SDSU sits on the traditional territory of the Kumeyaay Nation.","type":"content","url":"/syllabus#land-acknowledgement","position":41},{"hierarchy":{"lvl1":"Syllabus - ASTR 596: Modeling the Universe","lvl2":"Your Responsibility"},"type":"lvl2","url":"/syllabus#your-responsibility","position":42},{"hierarchy":{"lvl1":"Syllabus - ASTR 596: Modeling the Universe","lvl2":"Your Responsibility"},"content":"This syllabus constitutes our course contract. You are responsible for reading and understanding all policies stated here. Additional details are provided in the linked course documents.","type":"content","url":"/syllabus#your-responsibility","position":43},{"hierarchy":{"lvl1":"Course Schedule & Important Dates"},"type":"lvl1","url":"/schedule","position":0},{"hierarchy":{"lvl1":"Course Schedule & Important Dates"},"content":"","type":"content","url":"/schedule","position":1},{"hierarchy":{"lvl1":"Course Schedule & Important Dates","lvl2":"Schedule Structure"},"type":"lvl2","url":"/schedule#schedule-structure","position":2},{"hierarchy":{"lvl1":"Course Schedule & Important Dates","lvl2":"Schedule Structure"},"content":"Mondays: New project assigned via GitHub Classroom; previous project due at 11:59 PMWednesdays: Growth Memo due via Canvas PDF submission (when applicable)Fridays: Class meeting (11:00 AM - 1:40 PM) with prepared discussion and project work","type":"content","url":"/schedule#schedule-structure","position":3},{"hierarchy":{"lvl1":"Course Schedule & Important Dates","lvl2":"Weekly Topics & Readings"},"type":"lvl2","url":"/schedule#weekly-topics-readings","position":4},{"hierarchy":{"lvl1":"Course Schedule & Important Dates","lvl2":"Weekly Topics & Readings"},"content":"Week\n\nDate\n\nTopic\n\nReading Assignment\n\n1\n\nAug 29\n\nCourse introduction, Python fundamentals, terminal/Git basics\n\nTBD - Python Foundations Ch. 1\n\n2\n\nSept 5\n\nNumPy/matplotlib basics, OOP, stellar physics\n\nTBD - Python Foundations Ch. 2\n\n3\n\nSept 12\n\nGravitational dynamics, numerical integration\n\nTBD - Numerical Methods Ch. 1\n\n4\n\nSept 19\n\nMonte Carlo methods, statistical sampling\n\nTBD - Numerical Methods Ch. 2\n\n5\n\nSept 26\n\nLinear regression from first principles\n\nTBD - Machine Learning Ch. 1\n\n6\n\nOct 3\n\nAdvanced regression, model selection\n\nTBD - Machine Learning Ch. 2\n\n7\n\nOct 10\n\nMonte Carlo radiative transfer\n\nTBD - Radiative Transfer Ch. 1\n\n8\n\nOct 17\n\nObservational effects, synthetic data\n\nTBD - Radiative Transfer Ch. 2\n\n9\n\nOct 24\n\nBayesian inference foundations\n\nTBD - Bayesian Methods Ch. 1\n\n10\n\nOct 31\n\nMarkov Chain Monte Carlo\n\nTBD - Bayesian Methods Ch. 2\n\n11\n\nNov 7\n\nGaussian processes and kernel methods\n\nTBD - Machine Learning Ch. 3\n\n12\n\nNov 14\n\nAdvanced GP applications and optimization\n\nTBD - Machine Learning Ch. 4\n\n13\n\nNov 21\n\nNeural network fundamentals, forward propagation\n\nTBD - Neural Networks Ch. 1\n\n14\n\nNov 28\n\nTHANKSGIVING - No class\n\n—\n\n15\n\nDec 5\n\nBackpropagation, JAX/Flax introduction\n\nTBD - Modern Frameworks Ch. 1\n\n16\n\nDec 12\n\nProject workshop and presentations prep\n\n—\n\nFinals\n\nDec 17/18\n\nFinal presentations (date TBD)\n\n—","type":"content","url":"/schedule#weekly-topics-readings","position":5},{"hierarchy":{"lvl1":"Course Schedule & Important Dates","lvl2":"Project Timeline"},"type":"lvl2","url":"/schedule#project-timeline","position":6},{"hierarchy":{"lvl1":"Course Schedule & Important Dates","lvl2":"Project Timeline"},"content":"Project\n\nTopic\n\nAssigned\n\nDue\n\nMemo Due\n\nProject 1\n\nPython/OOP/Stellar Physics\n\nAug 25\n\nSept 8\n\nSept 10\n\nProject 2\n\nODE Integration/N-Body/Monte Carlo\n\nSept 8\n\nSept 22\n\nSept 24\n\nProject 3\n\nRegression/ML Fundamentals\n\nSept 22\n\nOct 6\n\nOct 8\n\nProject 4\n\nMonte Carlo Radiative Transfer\n\nOct 6\n\nOct 20\n\nOct 22\n\nProject 5\n\nBayesian/MCMC\n\nOct 20\n\nNov 3\n\nNov 5\n\nProject 6\n\nGaussian Processes\n\nNov 3\n\nNov 17\n\nNov 19\n\nFinal Project\n\nNeural Networks + JAX\n\nNov 17\n\nDec 18\n\n—","type":"content","url":"/schedule#project-timeline","position":7},{"hierarchy":{"lvl1":"Course Schedule & Important Dates","lvl2":"Important Dates"},"type":"lvl2","url":"/schedule#important-dates","position":8},{"hierarchy":{"lvl1":"Course Schedule & Important Dates","lvl2":"Important Dates"},"content":"Date\n\nItem\n\nSubmission Method\n\nSept 8 (Mon)\n\nProject 1 Due\n\nGitHub Classroom\n\nSept 10 (Wed)\n\nGrowth Memo 1 Due\n\nCanvas PDF\n\nSept 22 (Mon)\n\nProject 2 Due\n\nGitHub Classroom\n\nSept 24 (Wed)\n\nGrowth Memo 2 Due\n\nCanvas PDF\n\nOct 6 (Mon)\n\nProject 3 Due\n\nGitHub Classroom\n\nOct 8 (Wed)\n\nGrowth Memo 3 Due\n\nCanvas PDF\n\nOct 20 (Mon)\n\nProject 4 Due\n\nGitHub Classroom\n\nOct 22 (Wed)\n\nGrowth Memo 4 Due\n\nCanvas PDF\n\nNov 3 (Mon)\n\nProject 5 Due\n\nGitHub Classroom\n\nNov 5 (Wed)\n\nGrowth Memo 5 Due\n\nCanvas PDF\n\nNov 17 (Mon)\n\nProject 6 Due\n\nGitHub Classroom\n\nNov 19 (Wed)\n\nGrowth Memo 6 Due\n\nCanvas PDF\n\nNov 21 (Fri)\n\nFinal Project Proposal Due\n\nCanvas PDF\n\nDec 5 (Fri)\n\nFinal Project Progress Report\n\nCanvas PDF\n\nDec 11 (Wed)\n\nTechnical Growth Synthesis Due\n\nCanvas PDF\n\nDec 17 or 18\n\nFinal Presentations\n\nIn-person (TBD)\n\nDec 18 (Thu)\n\nFinal Project Due\n\nGitHub + Canvas\n\nNote: Reading assignments will be updated as JupyterBook chapters are completed. Check the course website weekly for updates.","type":"content","url":"/schedule#important-dates","position":9},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework"},"type":"lvl1","url":"/ai-guidelines-final","position":0},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework"},"content":"Living Document Notice: Given the rapid evolution of AI in education, this policy is essentially a pedagogical experiment. We’re exploring together how to best integrate these powerful tools while maintaining deep learning. Your feedback and experiences will help refine these guidelines. Open communication is essential—if something isn’t working or you discover better approaches, please share during Hacking Hours or class discussions. We’re all learning how to navigate this new landscape together.","type":"content","url":"/ai-guidelines-final","position":1},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl2":"TL;DR Quick Reference"},"type":"lvl2","url":"/ai-guidelines-final#tl-dr-quick-reference","position":2},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl2":"TL;DR Quick Reference"},"content":"Phase\n\nProjects\n\nCoding with AI\n\nLearning with AI\n\nKey Rule\n\n1: Foundation\n\nProjects 1-3(Python/OOP, N-body, Regression)\n\n❌ No initial implementation✅ Debugging after 30min struggle✅ Conceptual understanding of code\n\n✅ Always OK for physics concepts✅ NotebookLM for study guides✅ Understanding docs & syntax✅ “How does this function work?”\n\nStruggle first, AI secondDocs are primary source\n\n2: Strategic\n\nProjects 4-6(MCRT, Bayesian/MCMC, GP)\n\n✅ After working solution✅ With documentation✅ Improving existing code\n\n✅ All conceptual learning✅ Extension ideas✅ Paper summaries✅ “Why does this approach work?”\n\nDocument & verify everythingCross-check with official docs\n\n3: Professional\n\nFinal Project(Neural Networks + JAX)\n\n✅ Complex problems✅ Optimization❌ Still no copy-paste\n\n✅ Research exploration✅ Advanced topics✅ Career prep\n\nMust explain every line\n\nUniversal Rules:\n\n📚 Conceptual learning: AI always encouraged for understanding physics, math, and how code works.\n\n📖 Documentation first: Official docs are your primary source. Use AI to clarify, not replace them,\n\n💻 Code implementation: Follow phase rules for writing actual code\n\n📝 Attribution: Every AI-assisted code needs documentation\n\n🧠 Ownership: Can’t explain it = Can’t submit it","type":"content","url":"/ai-guidelines-final#tl-dr-quick-reference","position":3},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl2":"Quick Links"},"type":"lvl2","url":"/ai-guidelines-final#quick-links","position":4},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl2":"Quick Links"},"content":"Course Syllabus\n\nWhy This Course is Different\n\nLearning Guide\n\nProject Submission Guide","type":"content","url":"/ai-guidelines-final#quick-links","position":5},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl2":"Should I Use AI for This? Decision Flowchart"},"type":"lvl2","url":"/ai-guidelines-final#should-i-use-ai-for-this-decision-flowchart","position":6},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl2":"Should I Use AI for This? Decision Flowchart"},"content":"flowchart TD\n    Start[I need help with something] --> Q1{What week is it?}\n    \n    Q1 -->|Weeks 1-4| Phase1[Phase 1: Foundation]\n    Q1 -->|Weeks 5-8| Phase2[Phase 2: Strategic]\n    Q1 -->|Weeks 9-16| Phase3[Phase 3: Professional]\n    \n    Phase1 --> P1Q1{Have I struggled for<br/>20-30 minutes?}\n    P1Q1 -->|No| Struggle[Keep trying!<br/>Check documentation first]\n    P1Q1 -->|Yes| P1Q2{Is it a debugging issue?}\n    P1Q2 -->|No| NoAI1[Don't use AI yet<br/>Ask instructor/peers]\n    P1Q2 -->|Yes| UseAI1[OK to use AI<br/>Document the interaction]\n    \n    Phase2 --> P2Q1{Have I checked<br/>documentation first?}\n    P2Q1 -->|No| CheckDocs[Check official docs<br/>then reconsider]\n    P2Q1 -->|Yes| P2Q2{Can I verify<br/>AI's answer?}\n    P2Q2 -->|No| NoAI2[Don't use AI<br/>Need more foundation]\n    P2Q2 -->|Yes| UseAI2[Use AI strategically<br/>Verify & document]\n    \n    Phase3 --> P3Q1{Do I understand<br/>the core concept?}\n    P3Q1 -->|No| Foundation[Build foundation first<br/>Review materials]\n    P3Q1 -->|Yes| P3Q2{Will AI enhance<br/>my solution?}\n    P3Q2 -->|No| NoNeed[No need for AI]\n    P3Q2 -->|Yes| UseAI3[Use AI professionally<br/>Critical evaluation required]\n    \n    UseAI1 --> Document[Document in code<br/>and growth memo]\n    UseAI2 --> Document\n    UseAI3 --> Document\n    \n    style Start fill:#e1f5fe\n    style Document fill:#c8e6c9\n    style Struggle fill:#fff9c4\n    style NoAI1 fill:#ffccbc\n    style NoAI2 fill:#ffccbc\n    style NoNeed fill:#ffccbc","type":"content","url":"/ai-guidelines-final#should-i-use-ai-for-this-decision-flowchart","position":7},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl2":"Course Philosophy: AI as Performance Amplifier"},"type":"lvl2","url":"/ai-guidelines-final#course-philosophy-ai-as-performance-amplifier","position":8},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl2":"Course Philosophy: AI as Performance Amplifier"},"content":"The Reality: AI tools are transforming scientific computing. You’ll work in an AI-integrated environment throughout your career.\n\nThe Challenge: Research reveals both risks and opportunities. Over-reliance impairs learning (\n\nBastani et al., 2024; \n\nKoedinger et al., 2024), while strategic use accelerates understanding (\n\nKasneci et al., 2023; \n\nBitzenbauer, 2023; \n\nTing & O’Briain, 2025). HOW you engage matters—passive consumption atrophies skills, active collaboration enhances capability.\n\nOur Approach: Develop core competencies through productive struggle, then use AI strategically to amplify performance.\n\nResearch Foundation:\n\nTing & O’Briain (2025): Astronomy students using structured AI with documentation requirements showed decreased dependence over time and accelerated learning\n\nKasneci et al. (2023): Scaffolded AI use enhances STEM learning when properly integrated\n\nBitzenbauer (2023): AI chatbots effective when students engage critically with outputs\n\nDahlkemper et al. (2023): Students often fail to identify AI errors without proper training\n\nThese studies confirm that scaffolded AI use with reflection enhances rather than replaces learning.","type":"content","url":"/ai-guidelines-final#course-philosophy-ai-as-performance-amplifier","position":9},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"🤔 When in Doubt","lvl2":"Course Philosophy: AI as Performance Amplifier"},"type":"lvl3","url":"/ai-guidelines-final#id-when-in-doubt","position":10},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"🤔 When in Doubt","lvl2":"Course Philosophy: AI as Performance Amplifier"},"content":"Build judgment through community:\n\nCome to Hacking Hours (Thursdays 11 AM - 12 PM)\n\nAsk during Friday class\n\nDiscuss with classmates\n\nEmail if urgent: \n\nalrosen@sdsu.edu\n\nYour questions help everyone develop better intuition!","type":"content","url":"/ai-guidelines-final#id-when-in-doubt","position":11},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl2":"How to Cite AI Usage - Template"},"type":"lvl2","url":"/ai-guidelines-final#how-to-cite-ai-usage-template","position":12},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl2":"How to Cite AI Usage - Template"},"content":"","type":"content","url":"/ai-guidelines-final#how-to-cite-ai-usage-template","position":13},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"In-Code Documentation Template","lvl2":"How to Cite AI Usage - Template"},"type":"lvl3","url":"/ai-guidelines-final#in-code-documentation-template","position":14},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"In-Code Documentation Template","lvl2":"How to Cite AI Usage - Template"},"content":"# AI-assisted: [Tool name] helped with [specific aspect]\n# Verified against: [documentation URL or source]\n# Original approach: [what you tried first]\n# Why AI suggestion works: [your understanding]\ndef your_function():\n    # Implementation here\n    pass","type":"content","url":"/ai-guidelines-final#in-code-documentation-template","position":15},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"Growth Memo AI Section Template","lvl2":"How to Cite AI Usage - Template"},"type":"lvl3","url":"/ai-guidelines-final#growth-memo-ai-section-template","position":16},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"Growth Memo AI Section Template","lvl2":"How to Cite AI Usage - Template"},"content":"Include a brief reflection on your AI usage. Here’s a suggested structure:## AI Usage Reflection\n\n**Most Significant AI Interaction This Project:**\nWhen was AI most helpful and why?\n\n**Critical Thinking Check:**\nDid AI give you any incorrect/misleading information? How did you verify its suggestions? \n(This helps us all learn what to watch for!)\n\n**Key Learning:**\nWhat did this interaction teach you about the problem, concept, or about using AI effectively? \n\n**Evolution of My AI Use:**\nHow has your approach to using AI changed since the last project? \n\n**Next Steps:**\nOne specific way you plan to improve your AI usage next project\n\nWrite this however works for you—paragraphs, bullets, diagrams. Emojis encouraged! Just help me understand your learning journey.","type":"content","url":"/ai-guidelines-final#growth-memo-ai-section-template","position":17},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl2":"Scaffolded AI Integration Framework"},"type":"lvl2","url":"/ai-guidelines-final#scaffolded-ai-integration-framework","position":18},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl2":"Scaffolded AI Integration Framework"},"content":"","type":"content","url":"/ai-guidelines-final#scaffolded-ai-integration-framework","position":19},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"Phase 1: Foundation Building (Weeks 1-6)","lvl2":"Scaffolded AI Integration Framework"},"type":"lvl3","url":"/ai-guidelines-final#phase-1-foundation-building-weeks-1-6","position":20},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"Phase 1: Foundation Building (Weeks 1-6)","lvl2":"Scaffolded AI Integration Framework"},"content":"Dates: Aug 29 - Oct 10 | Rule: Struggle First, AI Second\n\nPrimary: Documentation and manual implementation\n\n30-Minute Rule: Minimum struggle before AI\n\nAI Usage: Debugging only after genuine effort\n\nDocument: All interactions with verification\n\nFriday Labs: Try solving with your partner first—two brains often beat ChatGPT\n\nUsing AI for Conceptual Understanding (Always OK):\n\n✅ OK: Open NumPy docs → confused by broadcasting → “Can you explain NumPy broadcasting with examples?”\n\n✅ OK: “Why does matplotlib need both Figure and Axes objects?”\n\n❌ NOT OK: “Write a function to calculate stellar luminosity” (that’s implementation)\n\nThe Rule: Docs open first, identify confusion, then ask AI to clarify\n\nWhat counts as genuine effort:\n\nDocument 3+ approaches attempted\n\nInvestigate specific errors\n\nConsult relevant documentation\n\nCreate minimal reproducible example\n\nGood Example:# Spent 30 min on matplotlib subplots\n# Checked docs, AI clarified Figure vs Axes\n# Verified: matplotlib.org/stable/api/figure_api.html\nfig, axes = plt.subplots(2, 2, figsize=(10, 8))","type":"content","url":"/ai-guidelines-final#phase-1-foundation-building-weeks-1-6","position":21},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"Phase 2: Strategic Integration (Weeks 7-12)","lvl2":"Scaffolded AI Integration Framework"},"type":"lvl3","url":"/ai-guidelines-final#phase-2-strategic-integration-weeks-7-12","position":22},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"Phase 2: Strategic Integration (Weeks 7-12)","lvl2":"Scaffolded AI Integration Framework"},"content":"Dates: Oct 11 - Nov 21 | Rule: Documentation-First\n\nPrimary: Continue documentation-first\n\nAI Enhancement: For efficiency after understanding\n\nVerify: Cross-reference all suggestions\n\nEvaluate: Explain why suggestions work","type":"content","url":"/ai-guidelines-final#phase-2-strategic-integration-weeks-7-12","position":23},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"Phase 3: Professional Practice (Weeks 13-16)","lvl2":"Scaffolded AI Integration Framework"},"type":"lvl3","url":"/ai-guidelines-final#phase-3-professional-practice-weeks-13-16","position":24},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"Phase 3: Professional Practice (Weeks 13-16)","lvl2":"Scaffolded AI Integration Framework"},"content":"Dates: Nov 22 - Dec 18 | Rule: AI as Productivity Tool\n\nAssumption: Foundation enables evaluation\n\nUsage: Acceleration and complex problems\n\nStandard: AI work must exceed manual quality\n\nContinue: Citing all usage\n\nNow AI can handle tedious tasks:\n\nGenerate docstrings from your working code\n\nCreate unit tests for your functions\n\nRefactor repetitive code into clean functions\n\nGenerate boilerplate code (argparse, logging setup)\n\nConvert your plots into publication-quality figures\n\nWrite regex patterns for data parsing\n\nNote: Tool availability subject to change. If access issues occur, document and discuss in class.","type":"content","url":"/ai-guidelines-final#phase-3-professional-practice-weeks-13-16","position":25},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl2":"AI Usage Log (Optional but Recommended)"},"type":"lvl2","url":"/ai-guidelines-final#ai-usage-log-optional-but-recommended","position":26},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl2":"AI Usage Log (Optional but Recommended)"},"content":"","type":"content","url":"/ai-guidelines-final#ai-usage-log-optional-but-recommended","position":27},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"Building Your Technical Growth Portfolio","lvl2":"AI Usage Log (Optional but Recommended)"},"type":"lvl3","url":"/ai-guidelines-final#building-your-technical-growth-portfolio","position":28},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"Building Your Technical Growth Portfolio","lvl2":"AI Usage Log (Optional but Recommended)"},"content":"Keep a simple log of significant AI interactions for your final portfolio. For your learning, not weekly review.\n\nWhy? Documents journey, reveals patterns, provides interview examples, mirrors industry practice\n\nSimple format:Date | Project | What AI Helped With | Key Insight\n9/15 | N-body | Debugging array indexing | Check bounds BEFORE loop\n10/2 | MCRT | Understanding optical depth (after reading Rybicki) | It's cumulative, not local\n\nFinal Portfolio: Select 3-5 best examples showing Phase 1→3 evolution.\nGit Note: Your commits should show iterative development throughout each project, not single submissions.","type":"content","url":"/ai-guidelines-final#building-your-technical-growth-portfolio","position":29},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl2":"Specific Examples: Good vs Bad AI Usage"},"type":"lvl2","url":"/ai-guidelines-final#specific-examples-good-vs-bad-ai-usage","position":30},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl2":"Specific Examples: Good vs Bad AI Usage"},"content":"","type":"content","url":"/ai-guidelines-final#specific-examples-good-vs-bad-ai-usage","position":31},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"Debugging (All Phases - After Struggle)","lvl2":"Specific Examples: Good vs Bad AI Usage"},"type":"lvl3","url":"/ai-guidelines-final#debugging-all-phases-after-struggle","position":32},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"Debugging (All Phases - After Struggle)","lvl2":"Specific Examples: Good vs Bad AI Usage"},"content":"✅ GOOD: “I’m getting IndexError on line 45. I’ve checked array dimensions, confirmed indices are within bounds, and added print statements. The error happens when i=n. Why might this occur in a loop?”\n\n❌ BAD: “Fix this error: [paste entire code and error]”","type":"content","url":"/ai-guidelines-final#debugging-all-phases-after-struggle","position":33},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"Concept Understanding (Phase 2+)","lvl2":"Specific Examples: Good vs Bad AI Usage"},"type":"lvl3","url":"/ai-guidelines-final#concept-understanding-phase-2","position":34},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"Concept Understanding (Phase 2+)","lvl2":"Specific Examples: Good vs Bad AI Usage"},"content":"✅ GOOD: “I understand Euler integration accumulates error. Can you explain why RK4 has O(h^4) error while Euler has O(h)? I’ve read that it’s related to Taylor series but don’t see the connection.”\n\n❌ BAD: “Explain RK4 integration”","type":"content","url":"/ai-guidelines-final#concept-understanding-phase-2","position":35},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"Optimization (Phase 3)","lvl2":"Specific Examples: Good vs Bad AI Usage"},"type":"lvl3","url":"/ai-guidelines-final#optimization-phase-3","position":36},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"Optimization (Phase 3)","lvl2":"Specific Examples: Good vs Bad AI Usage"},"content":"✅ GOOD: “My N-body simulation works but takes 5 minutes for 1000 particles. I’m using nested loops for force calculation. Here’s my approach [show code]. What optimization strategies should I consider?”\n\n❌ BAD: “Make this code faster”","type":"content","url":"/ai-guidelines-final#optimization-phase-3","position":37},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl2":"Assessment Integration"},"type":"lvl2","url":"/ai-guidelines-final#assessment-integration","position":38},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl2":"Assessment Integration"},"content":"","type":"content","url":"/ai-guidelines-final#assessment-integration","position":39},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"AI Reflection for Growth Memos","lvl2":"Assessment Integration"},"type":"lvl3","url":"/ai-guidelines-final#ai-reflection-for-growth-memos","position":40},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"AI Reflection for Growth Memos","lvl2":"Assessment Integration"},"content":"Each Growth Memo (see \n\nGrowth Memo Guidelines for full requirements) includes a brief AI usage reflection using this template:## AI Usage Reflection\n\n**Most Significant AI Interaction This Project:**\nBrief description of when AI was most helpful and why (2-3 sentences)\n\n**Critical Thinking Check:**\nDid AI give you any incorrect/misleading information? How did you verify its suggestions? \n(2-3 sentences - helps us all learn what to watch for!)\n\n**Key Learning:**\nWhat did this interaction teach you about the problem, concept, or about using AI effectively? \n(2-3 sentences)\n\n**Evolution of My AI Use:**\nHow has your approach to using AI changed since the last project? \nAre you becoming more independent, more strategic, or finding new ways to use it? (3-4 sentences)\n\n**Next Steps:**\nOne specific way you plan to improve your AI usage next project (1-2 sentences)","type":"content","url":"/ai-guidelines-final#ai-reflection-for-growth-memos","position":41},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"Understanding Verification","lvl2":"Assessment Integration"},"type":"lvl3","url":"/ai-guidelines-final#understanding-verification","position":42},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"Understanding Verification","lvl2":"Assessment Integration"},"content":"Throughout the course, expect supportive check-ins:\n\n“Walk me through your approach here”\n\n“What alternatives did you consider?”\n\n“How would this change if...?”\n\nThese conversations support your learning—catching confusion early helps!","type":"content","url":"/ai-guidelines-final#understanding-verification","position":43},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl2":"The Cognitive Ownership Principle"},"type":"lvl2","url":"/ai-guidelines-final#the-cognitive-ownership-principle","position":44},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl2":"The Cognitive Ownership Principle"},"content":"Every line of code you submit must be code you can explain and modify\n\nAfter any AI assistance:\n\nClose the AI chat\n\nWrite the solution from memory\n\nExplain it to your rubber duck\n\nModify it to prove understanding","type":"content","url":"/ai-guidelines-final#the-cognitive-ownership-principle","position":45},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl2":"How AI Usage Affects Your Learning"},"type":"lvl2","url":"/ai-guidelines-final#how-ai-usage-affects-your-learning","position":46},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl2":"How AI Usage Affects Your Learning"},"content":"The Simple Rule: You must be able to explain every line of code you submit and why you implemented it that way.\n\nDuring class, I may ask you to walk through your approach or explain your implementation choices. This isn’t a formal assessment—it’s how we learn together and ensure you’re building real understanding. If you can’t explain your code, that’s a signal you need to revisit it until you truly understand it.\n\nYour Growth Memos will include the AI reflection template above, helping you track your own journey toward independence and mastery.","type":"content","url":"/ai-guidelines-final#how-ai-usage-affects-your-learning","position":47},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl2":"Assessment Standards for AI-Assisted Work"},"type":"lvl2","url":"/ai-guidelines-final#assessment-standards-for-ai-assisted-work","position":48},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl2":"Assessment Standards for AI-Assisted Work"},"content":"","type":"content","url":"/ai-guidelines-final#assessment-standards-for-ai-assisted-work","position":49},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"Quality Expectations","lvl2":"Assessment Standards for AI-Assisted Work"},"type":"lvl3","url":"/ai-guidelines-final#quality-expectations","position":50},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"Quality Expectations","lvl2":"Assessment Standards for AI-Assisted Work"},"content":"Work demonstrating deep understanding through:\n\nIterative development evidenced by Git history\n\nVerification practices shown in documentation\n\nConceptual mastery demonstrated in explanations\n\nProfessional integration with critical evaluation\n\nThese represent graduate-level computational science standards.","type":"content","url":"/ai-guidelines-final#quality-expectations","position":51},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"When Work Doesn’t Meet Standards","lvl2":"Assessment Standards for AI-Assisted Work"},"type":"lvl3","url":"/ai-guidelines-final#when-work-doesnt-meet-standards","position":52},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"When Work Doesn’t Meet Standards","lvl2":"Assessment Standards for AI-Assisted Work"},"content":"If submitted work lacks evidence of understanding:\n\nLearning Intervention: One-on-one discussion to identify gaps\n\nRevision Opportunity: Resubmit with enhanced documentation (one time)\n\nAdjusted Assessment: Work evaluated based on demonstrated independent understanding","type":"content","url":"/ai-guidelines-final#when-work-doesnt-meet-standards","position":53},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"Clear Violations","lvl2":"Assessment Standards for AI-Assisted Work"},"type":"lvl3","url":"/ai-guidelines-final#clear-violations","position":54},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"Clear Violations","lvl2":"Assessment Standards for AI-Assisted Work"},"content":"Extensive undocumented AI use or inability to explain submitted code:\n\nReferred to academic integrity process per university policy\n\nProject receives assessment based solely on demonstrated understanding","type":"content","url":"/ai-guidelines-final#clear-violations","position":55},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl2":"AI as Learning Companion vs. Code Generator"},"type":"lvl2","url":"/ai-guidelines-final#ai-as-learning-companion-vs-code-generator","position":56},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl2":"AI as Learning Companion vs. Code Generator"},"content":"","type":"content","url":"/ai-guidelines-final#ai-as-learning-companion-vs-code-generator","position":57},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"The Critical Distinction","lvl2":"AI as Learning Companion vs. Code Generator"},"type":"lvl3","url":"/ai-guidelines-final#the-critical-distinction","position":58},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"The Critical Distinction","lvl2":"AI as Learning Companion vs. Code Generator"},"content":"AI for Learning (ALWAYS ENCOURAGED):\n\nUnderstanding astrophysical concepts\n\nExploring theoretical foundations\n\nConnecting ideas across domains\n\nGoing beyond assignment requirements\n\nGenerating study materials\n\nClarifying mathematical derivations\n\nAI for Coding (PHASE-DEPENDENT):\n\nFollow the three-phase scaffold\n\nDocument all code assistance\n\nVerify against documentation\n\nMust explain every line","type":"content","url":"/ai-guidelines-final#the-critical-distinction","position":59},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"When AI Tutoring is ALWAYS Appropriate","lvl2":"AI as Learning Companion vs. Code Generator"},"type":"lvl3","url":"/ai-guidelines-final#when-ai-tutoring-is-always-appropriate","position":60},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"When AI Tutoring is ALWAYS Appropriate","lvl2":"AI as Learning Companion vs. Code Generator"},"content":"","type":"content","url":"/ai-guidelines-final#when-ai-tutoring-is-always-appropriate","position":61},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl4":"Conceptual Understanding (All Phases)","lvl3":"When AI Tutoring is ALWAYS Appropriate","lvl2":"AI as Learning Companion vs. Code Generator"},"type":"lvl4","url":"/ai-guidelines-final#conceptual-understanding-all-phases","position":62},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl4":"Conceptual Understanding (All Phases)","lvl3":"When AI Tutoring is ALWAYS Appropriate","lvl2":"AI as Learning Companion vs. Code Generator"},"content":"✅ Physics & Astrophysics Concepts\n\n“Explain why the virial theorem matters for stellar stability”\n\n“How does optical depth relate to extinction?”\n\n“What’s the physical intuition behind the Jeans mass?”\n\n“Connect radiative transfer to what we observe with telescopes”\n\n✅ Mathematical Foundations\n\n“Walk me through the derivation of the diffusion approximation”\n\n“Why does the Monte Carlo method converge as 1/√N?”\n\n“Explain the connection between Gaussian processes and Bayesian inference”\n\n✅ Going Deeper (Extension Work)\n\n“What would happen if I added magnetic fields to my N-body simulation?”\n\n“How could I extend this to include relativistic effects?”\n\n“What research questions could I explore with this code?”\n\n“What are current open problems in computational radiative transfer?”","type":"content","url":"/ai-guidelines-final#conceptual-understanding-all-phases","position":63},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"Recommended Learning Tools","lvl2":"AI as Learning Companion vs. Code Generator"},"type":"lvl3","url":"/ai-guidelines-final#recommended-learning-tools","position":64},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"Recommended Learning Tools","lvl2":"AI as Learning Companion vs. Code Generator"},"content":"","type":"content","url":"/ai-guidelines-final#recommended-learning-tools","position":65},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl4":"NotebookLM (Preferred for Course Study)","lvl3":"Recommended Learning Tools","lvl2":"AI as Learning Companion vs. Code Generator"},"type":"lvl4","url":"/ai-guidelines-final#notebooklm-preferred-for-course-study","position":66},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl4":"NotebookLM (Preferred for Course Study)","lvl3":"Recommended Learning Tools","lvl2":"AI as Learning Companion vs. Code Generator"},"content":"Why it’s ideal: Uses ONLY the sources you provide, dramatically reducing hallucination risk compared to general-purpose LLMs. While no AI is perfect, NotebookLM’s responses are grounded in your uploaded materials.\n\nPerfect uses:\n\nUpload course readings → Generate study guide\n\nUpload your notes → Create practice questions\n\nUpload project specs → Generate clarifying questions\n\nUpload papers → Create literature summary\n\nUse podcast feature → Audio review while commuting\n\nGenerate FAQs → Test your understanding\n\nHow to use effectively:\n\nUpload all course materials for that week\n\nAsk it to explain connections between readings\n\nGenerate different perspectives on difficult concepts\n\nCreate personalized study materials","type":"content","url":"/ai-guidelines-final#notebooklm-preferred-for-course-study","position":67},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl4":"ChatGPT (For Conceptual Exploration)","lvl3":"Recommended Learning Tools","lvl2":"AI as Learning Companion vs. Code Generator"},"type":"lvl4","url":"/ai-guidelines-final#chatgpt-for-conceptual-exploration","position":68},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl4":"ChatGPT (For Conceptual Exploration)","lvl3":"Recommended Learning Tools","lvl2":"AI as Learning Companion vs. Code Generator"},"content":"Note: SDSU provides enterprise access to all students via ChatGPT Plus\n\nPrivacy Protection: The SDSU enterprise account ensures “OpenAI doesn’t use San Diego State University workspace data to train its models.” Your conversations are private—this tool is provided to enhance your learning, not to monitor your work. Use it freely for conceptual understanding!\n\nBest for:\n\nInteractive concept exploration\n\nSocratic dialogue about physics\n\n“What if?” scenarios\n\nConnecting course material to research literature\n\nPractice problems (not homework!)\n\nCreating projects to maintain context across the semester","type":"content","url":"/ai-guidelines-final#chatgpt-for-conceptual-exploration","position":69},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"Strategic Learning Dialogues","lvl2":"AI as Learning Companion vs. Code Generator"},"type":"lvl3","url":"/ai-guidelines-final#strategic-learning-dialogues","position":70},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"Strategic Learning Dialogues","lvl2":"AI as Learning Companion vs. Code Generator"},"content":"","type":"content","url":"/ai-guidelines-final#strategic-learning-dialogues","position":71},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl4":"Example: Using AI to Deepen Understanding","lvl3":"Strategic Learning Dialogues","lvl2":"AI as Learning Companion vs. Code Generator"},"type":"lvl4","url":"/ai-guidelines-final#example-using-ai-to-deepen-understanding","position":72},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl4":"Example: Using AI to Deepen Understanding","lvl3":"Strategic Learning Dialogues","lvl2":"AI as Learning Companion vs. Code Generator"},"content":"You: \"I implemented the leapfrog integrator and it conserves energy \n     better than RK4. But I don't understand WHY. Can you help me \n     build intuition about symplectic integrators without just \n     telling me the answer?\"\n\nAI: \"Let's think about this together. First, what do you notice \n    about the structure of the leapfrog algorithm compared to RK4? \n    Specifically, how does it update positions vs velocities?\"\n\nYou: \"They're updated separately... offset by half a timestep?\"\n\nAI: \"Exactly! Now think about what conservation laws tell us about \n    physical systems. What quantity should remain constant in your \n    N-body simulation if no energy is added or removed?\"\n\n[Continue building understanding through dialogue]","type":"content","url":"/ai-guidelines-final#example-using-ai-to-deepen-understanding","position":73},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"The Learning Enhancement Framework","lvl2":"AI as Learning Companion vs. Code Generator"},"type":"lvl3","url":"/ai-guidelines-final#the-learning-enhancement-framework","position":74},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"The Learning Enhancement Framework","lvl2":"AI as Learning Companion vs. Code Generator"},"content":"Learning Goal\n\nAI Tool\n\nHow to Use\n\nExpected Outcome\n\nPre-class Prep\n\nNotebookLM\n\nAfter reading, upload materials to generate overview/connections\n\nBetter questions in class\n\nConcept Clarification\n\nChatGPT\n\nSocratic dialogue\n\nDeeper understanding\n\nExtension Ideas\n\nChatGPT\n\n“What if I tried...”\n\nResearch-quality projects\n\nStudy Materials\n\nNotebookLM\n\nGenerate from your materials\n\nPersonalized review\n\nConnection Making\n\nChatGPT Projects\n\n“How does this relate to...”\n\nIntegrated knowledge\n\nPaper Understanding\n\nChatGPT\n\nExplain papers in course context\n\nLiterature awareness","type":"content","url":"/ai-guidelines-final#the-learning-enhancement-framework","position":75},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl2":"Recommended AI Tools and Strategies"},"type":"lvl2","url":"/ai-guidelines-final#recommended-ai-tools-and-strategies","position":76},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl2":"Recommended AI Tools and Strategies"},"content":"","type":"content","url":"/ai-guidelines-final#recommended-ai-tools-and-strategies","position":77},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"Tools for This Course","lvl2":"Recommended AI Tools and Strategies"},"type":"lvl3","url":"/ai-guidelines-final#tools-for-this-course","position":78},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"Tools for This Course","lvl2":"Recommended AI Tools and Strategies"},"content":"NotebookLM: Upload course materials for grounded study aids (free through 2026)\n\nChatGPT: Conceptual learning and code debugging (SDSU enterprise access)\n\nCode Completion in IDE: DISABLED per syllabus","type":"content","url":"/ai-guidelines-final#tools-for-this-course","position":79},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"Strategic Usage Tips","lvl2":"Recommended AI Tools and Strategies"},"type":"lvl3","url":"/ai-guidelines-final#strategic-usage-tips","position":80},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"Strategic Usage Tips","lvl2":"Recommended AI Tools and Strategies"},"content":"Separate learning from implementation - Use AI freely for concepts, carefully for code\n\nRequest teaching, not answers: “Help me understand...” not “What’s the solution?”\n\nBuild mental models: “What’s the intuition behind...”\n\nVerify everything: Check AI physics against textbooks\n\nDocument insights: Keep a learning journal of AI-aided discoveries","type":"content","url":"/ai-guidelines-final#strategic-usage-tips","position":81},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"AI Learning Best Practices","lvl2":"Recommended AI Tools and Strategies"},"type":"lvl3","url":"/ai-guidelines-final#ai-learning-best-practices","position":82},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"AI Learning Best Practices","lvl2":"Recommended AI Tools and Strategies"},"content":"Two-Track System:\n\nLearning Track (unrestricted): Use AI freely for physics concepts, math, research\n\nImplementation Track (phase-restricted): Follow scaffolding rules for code\n\nExample Workflow:Monday: Read assignment → Use NotebookLM for study guide\nTuesday: Understand physics → ChatGPT dialogue (unrestricted)\nWednesday: Plan approach → Work out algorithm on paper\nThursday: Implementation → Follow phase rules for coding\nFriday: Extensions → Explore \"what if\" scenarios with AI","type":"content","url":"/ai-guidelines-final#ai-learning-best-practices","position":83},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"Signs You’re Using AI Correctly","lvl2":"Recommended AI Tools and Strategies"},"type":"lvl3","url":"/ai-guidelines-final#signs-youre-using-ai-correctly","position":84},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"Signs You’re Using AI Correctly","lvl2":"Recommended AI Tools and Strategies"},"content":"For Learning: You’re asking deeper questions each week\n\nFor Coding: Your AI usage decreases over time\n\nYou catch errors in AI suggestions\n\nYou can explain why AI’s approach works/doesn’t work\n\nYou modify AI suggestions to improve them\n\nYou’re learning faster, not just finishing faster\n\nYou’re excited about extensions beyond requirements","type":"content","url":"/ai-guidelines-final#signs-youre-using-ai-correctly","position":85},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl2":"Bottom Line"},"type":"lvl2","url":"/ai-guidelines-final#bottom-line","position":86},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl2":"Bottom Line"},"content":"AI amplifies capability—it doesn’t replace understanding. Master fundamentals AND strategic AI usage to thrive. The struggle is where learning happens; AI should enhance your journey, not bypass it.\n\nQuestions? Come to Hacking Hours or ask in class. Open communication helps everyone!","type":"content","url":"/ai-guidelines-final#bottom-line","position":87},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl2":"Quick Reference Card"},"type":"lvl2","url":"/ai-guidelines-final#quick-reference-card","position":88},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl2":"Quick Reference Card"},"content":"","type":"content","url":"/ai-guidelines-final#quick-reference-card","position":89},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"Phase 1 (Weeks 1-4): Foundation","lvl2":"Quick Reference Card"},"type":"lvl3","url":"/ai-guidelines-final#phase-1-weeks-1-4-foundation","position":90},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"Phase 1 (Weeks 1-4): Foundation","lvl2":"Quick Reference Card"},"content":"✅ Allowed: Debugging after 20 min struggle❌ Not Allowed: Concept explanations, initial implementations📝 Document: Every interaction with verification","type":"content","url":"/ai-guidelines-final#phase-1-weeks-1-4-foundation","position":91},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"Phase 2 (Weeks 5-8): Strategic","lvl2":"Quick Reference Card"},"type":"lvl3","url":"/ai-guidelines-final#phase-2-weeks-5-8-strategic","position":92},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"Phase 2 (Weeks 5-8): Strategic","lvl2":"Quick Reference Card"},"content":"✅ Allowed: Efficiency improvements after working solution❌ Not Allowed: First attempts, without documentation📝 Document: Why AI suggestion improves your approach","type":"content","url":"/ai-guidelines-final#phase-2-weeks-5-8-strategic","position":93},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"Phase 3 (Weeks 9-16): Professional","lvl2":"Quick Reference Card"},"type":"lvl3","url":"/ai-guidelines-final#phase-3-weeks-9-16-professional","position":94},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl3":"Phase 3 (Weeks 9-16): Professional","lvl2":"Quick Reference Card"},"content":"✅ Allowed: Complex problem-solving, optimization❌ Not Allowed: Work you can’t explain📝 Document: Critical evaluation of suggestions\n\nRemember: Can’t explain it = Didn’t learn it = Can’t submit it","type":"content","url":"/ai-guidelines-final#phase-3-weeks-9-16-professional","position":95},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl2":"References"},"type":"lvl2","url":"/ai-guidelines-final#references","position":96},{"hierarchy":{"lvl1":"ASTR 596 AI Policy & Learning Framework","lvl2":"References"},"content":"Bastani, H., Bastani, O., Sungu, A., Ge, H., Kabakcı, O., & Mariman, R. (2024). Generative AI can harm learning. arXiv preprint. \n\nhttps://​arxiv​.org​/abs​/2401​.12438\n\nBitzenbauer, P. (2023). ChatGPT in physics education: A pilot study on easy-to-implement activities. Contemporary Educational Technology, 15(3), ep430. \n\nhttps://​doi​.org​/10​.1088​/1361​-6552​/acc749\n\nDahlkemper, M. N., Lahme, S. Z., & Klein, P. (2023). How do physics students evaluate artificial intelligence responses on comprehension questions? A study on the perceived scientific accuracy and linguistic quality of ChatGPT. Physical Review Physics Education Research, 19(1), 010142. \n\nDahlkemper et al. (2023)\n\nKasneci, E., Sessler, K., Küchemann, S., Bannert, M., Dementieva, D., Fischer, F., ... & Kasneci, G. (2023). ChatGPT for good? On opportunities and challenges of large language models for education. Learning and Individual Differences, 103, 102274. \n\nhttps://​doi​.org​/10​.1016​/j​.learninstruc​.2023​.101874\n\nKoedinger, K. R., Aleven, V., & Stamper, J. (2024). The case for conversation-based assessment of data literacy. In Proceedings of the 14th Learning Analytics and Knowledge Conference (pp. 91-100). \n\nhttps://​doi​.org​/10​.1145​/3636555​.3636596\n\nTing, Y. S., & O’Briain, D. (2025). Teaching machine learning in the era of large language models: Lessons learned from a graduate astronomy course. Journal of Astronomy Education (in press).","type":"content","url":"/ai-guidelines-final#references","position":97},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide"},"type":"lvl1","url":"/learning-guide-final","position":0},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide"},"content":"","type":"content","url":"/learning-guide-final","position":1},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl2":"Failure Recovery Protocols"},"type":"lvl2","url":"/learning-guide-final#failure-recovery-protocols","position":2},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl2":"Failure Recovery Protocols"},"content":"","type":"content","url":"/learning-guide-final#failure-recovery-protocols","position":3},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl3":"When Things Go Wrong (And They Will)","lvl2":"Failure Recovery Protocols"},"type":"lvl3","url":"/learning-guide-final#when-things-go-wrong-and-they-will","position":4},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl3":"When Things Go Wrong (And They Will)","lvl2":"Failure Recovery Protocols"},"content":"Your code is a disaster? Good, now you’re learning:\n\nGit broke everything? git reflog shows all commits, even “lost” ones\n\nAccidentally deleted files? Check your IDE’s local history (VS Code: Timeline view) OR if you’ve been committing regularly, git checkout -- filename recovers the last committed version. This is why you should commit every time you get something working, even partially.\n\nAlgorithm completely wrong? Keep it in failed_attempts/ folder—documenting what doesn’t work is valuable\n\nCan’t understand your own code from last week? You forgot to comment. Fix it now, learn for next time.\n\nRecovery is a skill: Industry developers break things daily. The difference is they know how to recover quickly. Every disaster teaches you a new git command or IDE feature.\n\nGit saves you from yourself: Commit early, commit often, push regularly. Your future self will thank you when you need to recover that working version from 3 days ago.","type":"content","url":"/learning-guide-final#when-things-go-wrong-and-they-will","position":5},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl2":"Building Research Intuition"},"type":"lvl2","url":"/learning-guide-final#building-research-intuition","position":6},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl2":"Building Research Intuition"},"content":"Debugging IS hypothesis testing:\n\nHypothesis: “The error is in the boundary conditions”\n\nTest: Add print statement at boundaries\n\nResult: Boundaries are fine\n\nNew hypothesis: “Check the indexing in the main loop”\n\nTest: Print indices at each iteration\n\nResult: Off-by-one error found\n\nThis IS the scientific method. You’re already doing research, just with code instead of lab equipment.\n\nRead error messages like papers: Both require parsing dense technical text for the one crucial piece of information buried in paragraph 3.# ASTR 596: Course Learning Guide\n\nThis document contains practical strategies for succeeding in this course. It’s not about course policies (see syllabus) or philosophy (see “Why ASTR 596 is Different”)—it’s your technical reference when you’re stuck, confused, or need to level up your skills. Everything here is based on cognitive science research and industry best practices. Use this guide to build strong computational habits that will serve you throughout your career.","type":"content","url":"/learning-guide-final#building-research-intuition","position":7},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl2":"Learning Strategies"},"type":"lvl2","url":"/learning-guide-final#learning-strategies","position":8},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl2":"Learning Strategies"},"content":"","type":"content","url":"/learning-guide-final#learning-strategies","position":9},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl3":"Effective Learning Workflow","lvl2":"Learning Strategies"},"type":"lvl3","url":"/learning-guide-final#effective-learning-workflow","position":10},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl3":"Effective Learning Workflow","lvl2":"Learning Strategies"},"content":"Before Class:\n\nRead actively - type out examples yourself\n\nNote specific confusion points\n\nAttempt project for 30 min (primes your brain for learning)\n\nDuring Class:\n\nAsk your confusion points immediately\n\nDebug with partners\n\nTake implementation notes\n\nAfter Class:\n\nReview within 24 hours (critical for retention)\n\nImplement incrementally\n\nTest each piece before moving on","type":"content","url":"/learning-guide-final#effective-learning-workflow","position":11},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl2":"Debugging Strategies"},"type":"lvl2","url":"/learning-guide-final#debugging-strategies","position":12},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl2":"Debugging Strategies"},"content":"","type":"content","url":"/learning-guide-final#debugging-strategies","position":13},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl3":"The Systematic Approach","lvl2":"Debugging Strategies"},"type":"lvl3","url":"/learning-guide-final#the-systematic-approach","position":14},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl3":"The Systematic Approach","lvl2":"Debugging Strategies"},"content":"Read the error message - Really read it, don’t just panic\n\nIdentify the line - Where exactly is the problem?\n\nCheck your assumptions - What do you think should happen?\n\nSimplify the problem - Can you reproduce with minimal code?\n\nPrint debugging - Sometimes print() beats fancy debuggers\n\nUse Python debugger (pdb) - Set breakpoints and step through code (see example below)\n\nRubber duck debugging - Explain to an imaginary listener\n\nTake a break - Fresh eyes catch obvious errors","type":"content","url":"/learning-guide-final#the-systematic-approach","position":15},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl3":"Common Python Pitfalls","lvl2":"Debugging Strategies"},"type":"lvl3","url":"/learning-guide-final#common-python-pitfalls","position":16},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl3":"Common Python Pitfalls","lvl2":"Debugging Strategies"},"content":"# Mutable default arguments - WRONG\ndef bad_function(lst=[]):  \n    lst.append(1)\n    return lst\n\n# Correct approach\ndef good_function(lst=None):\n    if lst is None:\n        lst = []\n    lst.append(1)\n    return lst\n\nOther common issues:\n\nInteger division: Be aware of Python 2 vs 3 differences\n\nIndentation errors: Never mix tabs and spaces\n\nOff-by-one errors: Remember Python is 0-indexed\n\nScope confusion: Understand local vs global variables\n\nNumPy broadcasting: Check array shapes with .shape","type":"content","url":"/learning-guide-final#common-python-pitfalls","position":17},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl3":"Common Error Messages Decoded","lvl2":"Debugging Strategies"},"type":"lvl3","url":"/learning-guide-final#common-error-messages-decoded","position":18},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl3":"Common Error Messages Decoded","lvl2":"Debugging Strategies"},"content":"IndexError: list index out of range→ You’re trying to access element N in a list with <N elements. Check loop bounds and off-by-one errors.\n\nTypeError: 'NoneType' object is not subscriptable→ A function returned None when you expected a list/array. Check your return statements.\n\nValueError: too many values to unpack→ Mismatch between variables and returned values. Print the shape/length of what you’re unpacking.\n\nKeyError: 'key_name'→ Dictionary doesn’t have that key. Print dict.keys() to see what’s actually there.\n\nNameError: name 'variable' is not defined→ You’re using a variable before defining it, or it’s out of scope. Check spelling and indentation.","type":"content","url":"/learning-guide-final#common-error-messages-decoded","position":19},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl3":"Using Python Debugger (pdb)","lvl2":"Debugging Strategies"},"type":"lvl3","url":"/learning-guide-final#using-python-debugger-pdb","position":20},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl3":"Using Python Debugger (pdb)","lvl2":"Debugging Strategies"},"content":"import pdb\n\ndef problematic_function(x):\n    result = x * 2\n    pdb.set_trace()  # Execution stops here\n    return result / 0  # Obviously wrong\n\n# Commands in pdb:\n# n - next line\n# s - step into function\n# c - continue\n# l - list code\n# p variable - print variable\n# pp variable - pretty print\n# h - help","type":"content","url":"/learning-guide-final#using-python-debugger-pdb","position":21},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl2":"Resources & Documentation"},"type":"lvl2","url":"/learning-guide-final#resources-documentation","position":22},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl2":"Resources & Documentation"},"content":"","type":"content","url":"/learning-guide-final#resources-documentation","position":23},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl3":"Essential Python References","lvl2":"Resources & Documentation"},"type":"lvl3","url":"/learning-guide-final#essential-python-references","position":24},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl3":"Essential Python References","lvl2":"Resources & Documentation"},"content":"Resource\n\nBest For\n\nLink\n\nOfficial Python Docs\n\nLanguage features\n\ndocs.python.org\n\nNumPy Documentation\n\nArray operations\n\nnumpy.org/doc\n\nMatplotlib Gallery\n\nPlot examples\n\nmatplotlib​.org​/stable​/gallery\n\nSciPy Documentation\n\nScientific functions\n\ndocs.scipy.org\n\nReal Python\n\nTutorials\n\nrealpython.com\n\nPython Tutor\n\nVisualize execution\n\npythontutor.com","type":"content","url":"/learning-guide-final#essential-python-references","position":25},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl3":"Machine Learning & JAX","lvl2":"Resources & Documentation"},"type":"lvl3","url":"/learning-guide-final#machine-learning-jax","position":26},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl3":"Machine Learning & JAX","lvl2":"Resources & Documentation"},"content":"Resource\n\nPurpose\n\nJAX Documentation\n\nCore JAX features\n\nEquinox Docs\n\nNeural network library\n\nFlax Documentation\n\nAlternative NN library\n\nTing’s ML Review\n\nAstronomy-specific ML","type":"content","url":"/learning-guide-final#machine-learning-jax","position":27},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl3":"Recommended Video Resources","lvl2":"Resources & Documentation"},"type":"lvl3","url":"/learning-guide-final#recommended-video-resources","position":28},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl3":"Recommended Video Resources","lvl2":"Resources & Documentation"},"content":"3Blue1Brown - Visual mathematical intuition\n\nStatQuest - Statistics with clear explanations\n\nComputerphile - Computer science concepts","type":"content","url":"/learning-guide-final#recommended-video-resources","position":29},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl2":"Study Tips & Best Practices"},"type":"lvl2","url":"/learning-guide-final#study-tips-best-practices","position":30},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl2":"Study Tips & Best Practices"},"content":"","type":"content","url":"/learning-guide-final#study-tips-best-practices","position":31},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl3":"Why Projects Take Time (It’s Not You, It’s Neuroscience)","lvl2":"Study Tips & Best Practices"},"type":"lvl3","url":"/learning-guide-final#why-projects-take-time-its-not-you-its-neuroscience","position":32},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl3":"Why Projects Take Time (It’s Not You, It’s Neuroscience)","lvl2":"Study Tips & Best Practices"},"content":"Distributed Practice > Massed PracticeYour brain needs time between sessions to consolidate learning (Cepeda et al., 2006). Complex debugging and algorithm design rarely happen in single marathon sessions—they require “diffuse mode” processing, where your brain works on problems subconsciously between active work periods.\n\nPractical reality: Yes, learning to code is time-intensive. A “simple” implementation might take 8+ hours when you’re learning. But those hours spread across a week with sleep cycles between them yield better understanding than 8 straight hours of increasingly frustrated debugging.","type":"content","url":"/learning-guide-final#why-projects-take-time-its-not-you-its-neuroscience","position":33},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl3":"Code Organization Best Practices","lvl2":"Study Tips & Best Practices"},"type":"lvl3","url":"/learning-guide-final#code-organization-best-practices","position":34},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl3":"Code Organization Best Practices","lvl2":"Study Tips & Best Practices"},"content":"# Project structure\nproject_name/\n├── README.md           # Installation and usage instructions\n├── requirements.txt    # Package dependencies\n├── src/\n│   ├── __init__.py\n│   ├── physics.py     # Physics calculations\n│   ├── numerics.py    # Numerical methods\n│   └── plotting.py    # Visualization functions\n├── tests/\n│   └── test_physics.py\n├── data/\n│   └── input_files/\n├── outputs/\n│   └── figures/\n└── main.py            # Entry point","type":"content","url":"/learning-guide-final#code-organization-best-practices","position":35},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl3":"Writing Good Documentation","lvl2":"Study Tips & Best Practices"},"type":"lvl3","url":"/learning-guide-final#writing-good-documentation","position":36},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl3":"Writing Good Documentation","lvl2":"Study Tips & Best Practices"},"content":"Why this matters: In research and industry, undocumented code is dead code. Your future self (and collaborators) need to understand what you wrote and why. Good documentation is expected in any professional setting.def integrate_orbit(initial_conditions, time_span, method='RK4'):\n    \"\"\"\n    Integrate orbital dynamics using specified method.\n    \n    This is a docstring - it appears when someone types help(integrate_orbit).\n    Use triple quotes and follow NumPy/SciPy style (industry standard).\n    \n    Parameters\n    ----------\n    initial_conditions : array-like\n        [x, y, z, vx, vy, vz] initial position and velocity\n        Describe type and what it represents\n    time_span : tuple\n        (t_start, t_end) integration time bounds\n        Always specify units in docs (assumed: seconds)\n    method : str, optional\n        Integration method: 'Euler', 'RK4', or 'Leapfrog'\n        List all valid options explicitly\n    \n    Returns\n    -------\n    trajectory : ndarray\n        Shape (n_steps, 6) array of positions and velocities\n        Always specify output shape/structure\n    \n    Examples\n    --------\n    >>> ic = [1, 0, 0, 0, 1, 0]  # Circular orbit\n    >>> t_span = (0, 10)\n    >>> orbit = integrate_orbit(ic, t_span)\n    \n    Notes\n    -----\n    The RK4 method is 4th-order accurate but not symplectic.\n    For long-term stability, use 'Leapfrog' despite lower order.\n    \"\"\"\n    # Implementation here\n\nKey Documentation Principles:\n\nDocstrings are contracts - They promise what your function does\n\nParameters section - Type, shape, units, and valid ranges\n\nReturns section - Exactly what comes back and in what form\n\nExamples section - Copy-pasteable code showing usage\n\nNotes section - Gotchas, algorithm choices, or citations\n\nIndustry expectation: Every public function needs a docstring. In research, include citations to papers/equations you’re implementing.","type":"content","url":"/learning-guide-final#writing-good-documentation","position":37},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl3":"Testing Strategies","lvl2":"Study Tips & Best Practices"},"type":"lvl3","url":"/learning-guide-final#testing-strategies","position":38},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl3":"Testing Strategies","lvl2":"Study Tips & Best Practices"},"content":"Always validate your code with:\n\nKnown solutions - Reproduce textbook examples\n\nLimiting cases - Check behavior at extremes\n\nConservation laws - Verify energy/momentum when applicable\n\nUnit analysis - Ensure dimensional consistency\n\nVisualization - Plot everything; patterns reveal bugs\n\nExample of a test that teaches:def test_energy_conservation():\n    \"\"\"This test SHOULD fail for Euler method—that teaches us about numerical stability.\"\"\"\n    energy_initial = calculate_total_energy(state_0)\n    state_final = integrate_euler(state_0, dt=0.1, steps=1000)\n    energy_final = calculate_total_energy(state_final)\n    # This assertion will fail, teaching you Euler doesn't conserve energy\n    assert np.isclose(energy_initial, energy_final)  # FAILS - that's the lesson!","type":"content","url":"/learning-guide-final#testing-strategies","position":39},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl2":"Getting Help"},"type":"lvl2","url":"/learning-guide-final#getting-help","position":40},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl2":"Getting Help"},"content":"","type":"content","url":"/learning-guide-final#getting-help","position":41},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl3":"When to Seek Help","lvl2":"Getting Help"},"type":"lvl3","url":"/learning-guide-final#when-to-seek-help","position":42},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl3":"When to Seek Help","lvl2":"Getting Help"},"content":"Immediate help needed:\n\nStuck on same error for >1 hour\n\nDon’t understand project requirements\n\nTechnical issues (can’t install software, GitHub problems)\n\nHacking Hours (Thursdays 11 AM) ideal for:\n\nConceptual confusion after genuine attempt\n\nDiscussing different approaches to problems\n\nDebugging help after you’ve tried the systematic approach\n\n“Is my thinking on track?” questions\n\nFriday class questions valuable for everyone:\n\nYour confusion probably helps 3 other students\n\nReal-time problem solving benefits the whole class\n\nNo question is too basic if you’ve attempted it first","type":"content","url":"/learning-guide-final#when-to-seek-help","position":43},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl3":"How to Ask Good Questions","lvl2":"Getting Help"},"type":"lvl3","url":"/learning-guide-final#how-to-ask-good-questions","position":44},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl3":"How to Ask Good Questions","lvl2":"Getting Help"},"content":"Good question format:\"I'm trying to [goal]. \nI've attempted [what you tried].\nI expected [expected result] but got [actual result].\nI've checked [what you've verified].\nCould you help me understand [specific confusion]?\"\n\nInclude:\n\nMinimal reproducible example\n\nFull error message\n\nWhat you’ve already tried\n\nRelevant code snippet (not entire file)","type":"content","url":"/learning-guide-final#how-to-ask-good-questions","position":45},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl3":"Red Flags: You Need Help NOW","lvl2":"Getting Help"},"type":"lvl3","url":"/learning-guide-final#red-flags-you-need-help-now","position":46},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl3":"Red Flags: You Need Help NOW","lvl2":"Getting Help"},"content":"Technical Warning Signs:\n\nYour code “works” but you can’t explain why\n\nChanging things randomly hoping for success\n\nSolution is 10x longer than expected\n\nAvoiding entire project sections\n\nGreen Flags You’re Growing:\n\nYour questions are becoming more specific\n\nYou’re catching bugs faster\n\nYou can predict what will break before running code\n\nYou’re helping classmates debug\n\nWhat to do: Visit Hacking Hours or ask in class. Don’t wait!","type":"content","url":"/learning-guide-final#red-flags-you-need-help-now","position":47},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl2":"Time Management"},"type":"lvl2","url":"/learning-guide-final#time-management","position":48},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl2":"Time Management"},"content":"","type":"content","url":"/learning-guide-final#time-management","position":49},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl3":"Evidence-Based Learning Strategies","lvl2":"Time Management"},"type":"lvl3","url":"/learning-guide-final#evidence-based-learning-strategies","position":50},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl3":"Evidence-Based Learning Strategies","lvl2":"Time Management"},"content":"These techniques are proven by cognitive science to enhance retention and understanding:","type":"content","url":"/learning-guide-final#evidence-based-learning-strategies","position":51},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl4":"Active Recall (Most Powerful)","lvl3":"Evidence-Based Learning Strategies","lvl2":"Time Management"},"type":"lvl4","url":"/learning-guide-final#active-recall-most-powerful","position":52},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl4":"Active Recall (Most Powerful)","lvl3":"Evidence-Based Learning Strategies","lvl2":"Time Management"},"content":"What it is: Testing yourself WITHOUT looking at notes/code firstWhy it works: Retrieval strengthens memory more than re-reading (Karpicke & Blunt, 2011)How to use it:\n\nBefore checking documentation, try to write the function signature from memory\n\nClose your code and explain what each function does\n\nWeekly: Write down everything you remember about a topic, THEN check notes","type":"content","url":"/learning-guide-final#active-recall-most-powerful","position":53},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl4":"Spaced Repetition","lvl3":"Evidence-Based Learning Strategies","lvl2":"Time Management"},"type":"lvl4","url":"/learning-guide-final#spaced-repetition","position":54},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl4":"Spaced Repetition","lvl3":"Evidence-Based Learning Strategies","lvl2":"Time Management"},"content":"What it is: Review material at increasing intervals (1 day, 3 days, 1 week, 2 weeks)Why it works: Forgetting and re-learning strengthens long-term memory (Cepeda et al., 2006)How to use it:\n\nDay after class: Review notes (5 min)\n\nThree days later: Try to recreate key code (10 min)\n\nWeek later: Implement similar problem without looking\n\nTwo weeks: Explain concept to someone else","type":"content","url":"/learning-guide-final#spaced-repetition","position":55},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl4":"Interleaving (Mix It Up)","lvl3":"Evidence-Based Learning Strategies","lvl2":"Time Management"},"type":"lvl4","url":"/learning-guide-final#interleaving-mix-it-up","position":56},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl4":"Interleaving (Mix It Up)","lvl3":"Evidence-Based Learning Strategies","lvl2":"Time Management"},"content":"What it is: Switch between different topics/projects instead of focusing on oneWhy it works: Forces your brain to actively retrieve and apply the right method (Rohrer & Taylor, 2007)How to use it:\n\nWork on current project for 1-2 hours, then switch\n\nReview old projects before starting new ones\n\nMix conceptual learning with implementation","type":"content","url":"/learning-guide-final#interleaving-mix-it-up","position":57},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl4":"The Testing Effect","lvl3":"Evidence-Based Learning Strategies","lvl2":"Time Management"},"type":"lvl4","url":"/learning-guide-final#the-testing-effect","position":58},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl4":"The Testing Effect","lvl3":"Evidence-Based Learning Strategies","lvl2":"Time Management"},"content":"What it is: Taking practice tests improves learning more than studyingWhy it works: Identifies gaps and strengthens retrieval pathways (Roediger & Karpicke, 2006)How to use it:\n\nWrite code without any auto-complete (remember: AI/Copilot should be disabled in your IDE)\n\nPredict output before running code\n\nCreate minimal examples to test your understanding","type":"content","url":"/learning-guide-final#the-testing-effect","position":59},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl4":"Elaborative Interrogation","lvl3":"Evidence-Based Learning Strategies","lvl2":"Time Management"},"type":"lvl4","url":"/learning-guide-final#elaborative-interrogation","position":60},{"hierarchy":{"lvl1":"Troubleshooting & Learning Guide","lvl4":"Elaborative Interrogation","lvl3":"Evidence-Based Learning Strategies","lvl2":"Time Management"},"content":"What it is: Asking “why” and “how” questions while learningWhy it works: Connecting new information to existing knowledge (Dunlosky et al., 2013)How to use it:\n\nDon’t just learn WHAT broadcasting does, understand WHY NumPy designed it that way\n\nAsk: “Why does this algorithm fail for this case?”\n\nConnect: “How is this similar to what we did last week?”\n\nRemember: Everyone feels lost sometimes. The difference between success and failure isn’t ability—it’s persistence and willingness to seek help.","type":"content","url":"/learning-guide-final#elaborative-interrogation","position":61},{"hierarchy":{"lvl1":"Understanding Your Learning Journey in ASTR 596"},"type":"lvl1","url":"/course-overview-final","position":0},{"hierarchy":{"lvl1":"Understanding Your Learning Journey in ASTR 596"},"content":"","type":"content","url":"/course-overview-final","position":1},{"hierarchy":{"lvl1":"Understanding Your Learning Journey in ASTR 596","lvl2":"Why This Course is Designed the Way It Is"},"type":"lvl2","url":"/course-overview-final#why-this-course-is-designed-the-way-it-is","position":2},{"hierarchy":{"lvl1":"Understanding Your Learning Journey in ASTR 596","lvl2":"Why This Course is Designed the Way It Is"},"content":"This course follows a specific progression: Fundamentals → Classical Methods → Statistical Methods → Modern ML. This mirrors how the field itself evolved, but more importantly, each topic builds essential skills for the next. You’ll essentially recreate the historical development of computational astrophysics, but in a compressed, logical sequence that maximizes your learning.\n\nAt the heart of this course is the “glass box” philosophy — you’ll build every algorithm from scratch before using advanced libraries. This isn’t masochism; it’s pedagogy. When you implement backpropagation by hand, you understand why neural networks fail. When you code your own MCMC sampler, you recognize convergence problems. This deep understanding distinguishes computational scientists from software users. You’re learning to think, not just code.\n\nUnderstanding the “why” behind your curriculum helps you see the forest through the trees and appreciate how each assignment builds toward your growth as a computational scientist. Throughout this journey, you’ll also develop AI literacy — starting with minimal assistance while building foundations, then progressively integrating AI tools as a research amplifier once you understand what’s happening under the hood.","type":"content","url":"/course-overview-final#why-this-course-is-designed-the-way-it-is","position":3},{"hierarchy":{"lvl1":"Understanding Your Learning Journey in ASTR 596","lvl2":"What This Course Isn’t"},"type":"lvl2","url":"/course-overview-final#what-this-course-isnt","position":4},{"hierarchy":{"lvl1":"Understanding Your Learning Journey in ASTR 596","lvl2":"What This Course Isn’t"},"content":"This isn’t a survey of astronomical software packages where you learn to use astropy or MESA. You won’t be calling pre-built functions or following tutorials. Every algorithm you implement will solve real astrophysical problems. You’ll build your own versions of professional tools, understanding their strengths and limitations through direct experience.","type":"content","url":"/course-overview-final#what-this-course-isnt","position":5},{"hierarchy":{"lvl1":"Understanding Your Learning Journey in ASTR 596","lvl2":"The Four-Phase Journey"},"type":"lvl2","url":"/course-overview-final#the-four-phase-journey","position":6},{"hierarchy":{"lvl1":"Understanding Your Learning Journey in ASTR 596","lvl2":"The Four-Phase Journey"},"content":"","type":"content","url":"/course-overview-final#the-four-phase-journey","position":7},{"hierarchy":{"lvl1":"Understanding Your Learning Journey in ASTR 596","lvl3":"Phase 1: Foundation Building","lvl2":"The Four-Phase Journey"},"type":"lvl3","url":"/course-overview-final#phase-1-foundation-building","position":8},{"hierarchy":{"lvl1":"Understanding Your Learning Journey in ASTR 596","lvl3":"Phase 1: Foundation Building","lvl2":"The Four-Phase Journey"},"content":"You start with stellar physics because it’s conceptually accessible — everyone intuitively understands that hot things glow and massive things attract. Implementing a Star class teaches object-oriented thinking naturally. A star has properties (mass, temperature, luminosity) and methods (evolve, radiate, calculate_lifetime). This makes OOP concrete rather than abstract.\n\nYou’ll then build a StellarPopulation class that manages hundreds to thousands of stars simultaneously. Here’s where you’ll discover the power of vectorization — a fundamental concept in scientific computing. Instead of writing loops like:for star in stars:\n    star.luminosity = calculate_luminosity(star.mass)\n\nYou’ll learn to think in arrays:luminosities = stellar_constant * masses**3.5  # Main sequence relation, all stars at once!\n\nThis single line replaces thousands of function calls. Your code will run much faster using NumPy’s vectorized operations. This isn’t just about speed—vectorized thinking changes how you approach problems. Instead of “for each particle, calculate force,” you’ll think “calculate all forces simultaneously as matrix operations.” This mental shift is essential for everything that follows: Monte Carlo simulations, neural network operations, and JAX’s array programming paradigm all require this vectorized mindset.\n\nN-body dynamics becomes your introduction to numerical methods. The physics is simple (F = GM m/r^2) but you can’t solve it analytically for N>2. You’ll discover firsthand why algorithm choices matter when your solar system flies apart using Euler integration but remains stable with Verlet.","type":"content","url":"/course-overview-final#phase-1-foundation-building","position":9},{"hierarchy":{"lvl1":"Understanding Your Learning Journey in ASTR 596","lvl3":"Phase 2: Bridge to Statistical Thinking (Weeks 4-6)","lvl2":"The Four-Phase Journey"},"type":"lvl3","url":"/course-overview-final#phase-2-bridge-to-statistical-thinking-weeks-4-6","position":10},{"hierarchy":{"lvl1":"Understanding Your Learning Journey in ASTR 596","lvl3":"Phase 2: Bridge to Statistical Thinking (Weeks 4-6)","lvl2":"The Four-Phase Journey"},"content":"After mastering deterministic physics, you’re ready for the probabilistic world. Monte Carlo serves as the perfect bridge between these paradigms. Monte Carlo methods use random sampling to solve problems that would be intractable otherwise — imagine trying to calculate \\pi by randomly throwing darts at a circle inscribed in a square, or computing complex integrals by randomly sampling the function. You’re still solving physics problems, but now through statistical approximation rather than exact calculation. This prepares your mind for the probabilistic thinking required in machine learning, where uncertainty and randomness are features, not bugs.\n\nLinear regression introduces core ML concepts. Starting from scratch means deriving the normal equation (X^TX)\\beta = X^Ty, which shows you that ML isn’t magic — it’s using math and code to recognize patterns in your data. You’ll understand optimization, gradient descent, and regularization by building them yourself. You’ll discover how adding a simple penalty term (regularization) prevents your model from memorizing noise, a concept that extends all the way to modern neural networks.\n\nYour “aha!” moment: When you see the Central Limit Theorem emerge naturally from your Monte Carlo simulations — no matter what distribution you sample from, the mean converges to a Gaussian. This isn’t just theory; you’ll watch it happen in real-time through your own code.\n\nEach project now requires extensions where you ask “what if?” — what if we vary the initial mass distribution to the N-body code? What if we use different sampling strategies? This “I want you to think” approach mirrors real research where the interesting discoveries come from exploring beyond the minimum requirements.","type":"content","url":"/course-overview-final#phase-2-bridge-to-statistical-thinking-weeks-4-6","position":11},{"hierarchy":{"lvl1":"Understanding Your Learning Journey in ASTR 596","lvl3":"Phase 3: Advanced Statistical Methods","lvl2":"The Four-Phase Journey"},"type":"lvl3","url":"/course-overview-final#phase-3-advanced-statistical-methods","position":12},{"hierarchy":{"lvl1":"Understanding Your Learning Journey in ASTR 596","lvl3":"Phase 3: Advanced Statistical Methods","lvl2":"The Four-Phase Journey"},"content":"Radiative Transfer (RT) is arguably the most important topic in modern astrophysics, both observationally and theoretically, since it is how we understand everything we see in the universe. You’ll implement Monte Carlo Radiative Transfer (MCRT) from scratch, simulating individual photon packets as they scatter, absorb, and re-emit through dusty media. By tracking a large sample of random photon paths, you’ll predict what telescopes observe when looking through cosmic dust. You’ll connect this to your N-body project by modeling dust extinction along different sight lines through your simulated star clusters.\n\nBayesian Inference and MCMC represents the intellectual peak of the course. Bayesian inference flips traditional statistics: instead of asking “what’s the probability of this data given my model?” you ask “what’s the probability of my model given this data?” This is formalized in Bayes’ theorem:\n\nP(\\theta|D) = \\frac{P(D|\\theta) \\cdot P(\\theta)}{P(D)}\n\nwhere P(\\theta|D) is the posterior (what we want — probability of parameters given data), P(D|\\theta) is the likelihood (probability of observing this data if our model is true), P(\\theta) is the prior (our beliefs before seeing the data), and P(D) is the evidence ( normalization constant).\n\nThe revolutionary insight is the prior — you can mathematically encode what you believe is reasonable before seeing any data. These beliefs might be wrong! Often that’s fine since strong data will override weak priors. But when data is sparse (like it always is in astronomy), priors help constrain solutions to physically plausible regions. MCMC (Markov Chain Monte Carlo) is how you explore these probability distributions. Imagine a random walker that spends more time in high-probability regions, eventually mapping out the entire parameter landscape.","type":"content","url":"/course-overview-final#phase-3-advanced-statistical-methods","position":13},{"hierarchy":{"lvl1":"Understanding Your Learning Journey in ASTR 596","lvl3":"Phase 4: Modern Machine Learning (Weeks 11-16)","lvl2":"The Four-Phase Journey"},"type":"lvl3","url":"/course-overview-final#phase-4-modern-machine-learning-weeks-11-16","position":14},{"hierarchy":{"lvl1":"Understanding Your Learning Journey in ASTR 596","lvl3":"Phase 4: Modern Machine Learning (Weeks 11-16)","lvl2":"The Four-Phase Journey"},"content":"With strong statistical foundations, you’re ready for the frontier. Gaussian Processes (GPs) bridge classical statistics and modern ML. They’re still Bayesian but now you’re learning functions, not parameters. Think of GPs as a probability distribution over functions. Instead of fitting a specific curve to your data, you’re modeling all possible curves that could explain it, weighted by their probability. This lets you quantify uncertainty everywhere: you’ll know not just the predicted value but also how confident you should be in that prediction. GPs are non-parametric models meaning that they grow in complexity with your data rather than having a fixed number of parameters.\n\nThe culmination of the course is your final project: building a neural network from scratch using JAX. First, you’ll implement every component manually — forward propagation (passing data through layers of artificial neurons), backpropagation (computing gradients via the chain rule), and gradient descent (updating weights to minimize loss). You’ll build the same fundamental algorithms powering ChatGPT and DALL-E, just at a much smaller scale. This removes the black box mystique. Neural networks are just clever applications of calculus and linear algebra you already understand.\n\nWhy JAX? Developed by Google, it’s the framework of choice for cutting-edge numerical computing and ML research in industry and is gaining attention in academic research. JAX transforms scientific computing through automatic differentiation (autodiff) — it can automatically compute gradients of any function you write, no matter how complex. Remember struggling with derivatives in your orbital dynamics code? JAX handles that automatically. Those painful gradient calculations for linear regression? JAX makes them trivial. This isn’t just convenience; autodiff makes previously intractable problems solvable. You can optimize any differentiable system. Learning JAX makes you industry-ready while keeping you at the research frontier.\n\nYou’ll take one of your previous projects and extend it with neural networks to answer a new scientific question. Your simulations generate training data, neural networks learn the patterns, and suddenly you can predict outcomes without running expensive simulations.\n\nYour final “aha!” moment: When your neural network learns patterns you didn’t explicitly program—perhaps discovering a relationship in stellar evolution you hadn’t noticed, or finding an optimal sampling strategy for your MCRT code. You’ll realize you’ve built something that can discover things you don’t know.\n\nBy Phase 4, that initial feeling of “this is actually fun” has evolved into genuine research capability. You’re ready to implement any algorithm from a paper, combine classical and modern methods creatively, and contribute to the field.","type":"content","url":"/course-overview-final#phase-4-modern-machine-learning-weeks-11-16","position":15},{"hierarchy":{"lvl1":"Understanding Your Learning Journey in ASTR 596","lvl2":"Why This Progression Works"},"type":"lvl2","url":"/course-overview-final#why-this-progression-works","position":16},{"hierarchy":{"lvl1":"Understanding Your Learning Journey in ASTR 596","lvl2":"Why This Progression Works"},"content":"Each topic motivates the next: Numerical integration struggles motivate Monte Carlo. Monte Carlo motivates statistics. Statistics motivates ML. Your frustrations become the seeds of insight.\n\nComplexity ramps gradually: Start with F=ma, end with neural networks, but each step is manageable. You’re never asked to take multiple conceptual leaps simultaneously.\n\nReal astrophysics throughout: Every algorithm solves actual astronomy problems. You’re not learning abstract methods — you’re building tools astronomers use daily.\n\nModern skills emerge from fundamentals: By the end, you understand what JAX and modern tools do under the hood because you’ve built their components yourself.","type":"content","url":"/course-overview-final#why-this-progression-works","position":17},{"hierarchy":{"lvl1":"Understanding Your Learning Journey in ASTR 596","lvl2":"What You’ll Gain"},"type":"lvl2","url":"/course-overview-final#what-youll-gain","position":18},{"hierarchy":{"lvl1":"Understanding Your Learning Journey in ASTR 596","lvl2":"What You’ll Gain"},"content":"By the end of this course:\n\nYou’ll have built a portfolio of working astronomical software that solves real problems.\n\nYou’ll understand methods from classical mechanics to neural networks — not just how to use them, but why they work.\n\nYou’ll be fluent in modern tools used at Google, DeepMind, and leading research institutions.\n\nYou’ll be able to read research papers and implement new methods.\n\nYou’ll think computationally about physical problems while maintaining physical intuition about computational results.\n\nThis course teaches that computational astrophysics isn’t only about computers and astrophysics — it’s about thinking. How do we translate physical understanding into algorithms? How do we diagnose when those algorithms fail? How do we improve them? How do we know when to trust our results?\n\nThe same mathematical structures appear everywhere: calculus and optimization, linear algebra, probability theory. A small set of fundamental ideas powers everything from stellar evolution to deep learning. This unity reveals the deep elegance underlying computational science.","type":"content","url":"/course-overview-final#what-youll-gain","position":19},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way"},"type":"lvl1","url":"/why-different-final","position":0},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way"},"content":"","type":"content","url":"/why-different-final","position":1},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl2":"TL;DR: This Course Prepares You for Research Reality"},"type":"lvl2","url":"/why-different-final#tl-dr-this-course-prepares-you-for-research-reality","position":2},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl2":"TL;DR: This Course Prepares You for Research Reality"},"content":"Traditional courses teach you to follow recipes and get right answers. Research requires creating solutions to problems nobody has solved yet. This course bridges that gap through:\n\n“Glass box” modeling - Build it yourself to truly understand it.\n\nProductive struggle - Embrace confusion and frustration as the beginning of discovery.\n\nGrowth over perfection - Learn from failure rather than avoiding it.\n\nStrategic AI integration - Use tools to amplify, not replace, thinking.\n\nNeuroplasticity in action - Your brain literally rewires through challenge (this is proven neuroscience, not motivational fluff).","type":"content","url":"/why-different-final#tl-dr-this-course-prepares-you-for-research-reality","position":3},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl2":"The Research Environment You’re Entering"},"type":"lvl2","url":"/why-different-final#the-research-environment-youre-entering","position":4},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl2":"The Research Environment You’re Entering"},"content":"","type":"content","url":"/why-different-final#the-research-environment-youre-entering","position":5},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl3":"What Professional Technical Work Actually Looks Like","lvl2":"The Research Environment You’re Entering"},"type":"lvl3","url":"/why-different-final#what-professional-technical-work-actually-looks-like","position":6},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl3":"What Professional Technical Work Actually Looks Like","lvl2":"The Research Environment You’re Entering"},"content":"Whether pursuing academia, industry, or other technical careers, you’ll face:\n\nNo predetermined answers - You’re solving problems nobody has tackled before.\n\nIndependent problem-solving - Supervisors and research advisors expect solutions, not hand-holding requests.\n\nCreative adaptation - Standard methods need modification for new contexts.\n\nPeer collaboration - Working effectively with colleagues at your level.\n\nAI-integrated workflows - Strategic tool use is now becoming the standard.\n\nContinuous learning - Technology and knowledge evolve; professionals must too.\n\nCross-disciplinary thinking - Real problems ignore academic boundaries.","type":"content","url":"/why-different-final#what-professional-technical-work-actually-looks-like","position":7},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl3":"The Jarring Transition: Student → Scientist","lvl2":"The Research Environment You’re Entering"},"type":"lvl3","url":"/why-different-final#the-jarring-transition-student-scientist","position":8},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl3":"The Jarring Transition: Student → Scientist","lvl2":"The Research Environment You’re Entering"},"content":"As Martin Schwartz explains in his influential 2008 essay \n\n“The Importance of Stupidity in Scientific Research” (required reading for Week 1):\n\nUndergraduate coursework: Getting the right answers, feeling smart when you know them.\n\nGraduate research: “Immersion in the unknown,” where nobody knows the answers — that’s why it’s research.\n\nSchwartz’s key realization came when his Nobel Prize-winning advisor couldn’t solve a research problem:\n\n“That’s when it hit me: nobody did. That’s why it was a research problem.”\n\nBut here’s the critical problem Schwartz identified nearly two decades ago that still plagues STEM education today:\n\n“We don’t do a good enough job of teaching our students how to be productively stupid — that is, if we don’t feel stupid it means we’re not really trying.”\n\nDespite this recognition in 2008, most courses still haven’t addressed this gap. This course directly tackles the problem. We intentionally create opportunities for productive stupidity — the kind where you’re pushing beyond your comfort zone into genuine discovery. This requires being comfortable not knowing, so you can explore genuinely unknown territory where breakthroughs happen.","type":"content","url":"/why-different-final#the-jarring-transition-student-scientist","position":9},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl2":"Why Traditional Teaching Falls Short for Research Preparation"},"type":"lvl2","url":"/why-different-final#why-traditional-teaching-falls-short-for-research-preparation","position":10},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl2":"Why Traditional Teaching Falls Short for Research Preparation"},"content":"","type":"content","url":"/why-different-final#why-traditional-teaching-falls-short-for-research-preparation","position":11},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl3":"The “Recipe Following” Problem","lvl2":"Why Traditional Teaching Falls Short for Research Preparation"},"type":"lvl3","url":"/why-different-final#the-recipe-following-problem","position":12},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl3":"The “Recipe Following” Problem","lvl2":"Why Traditional Teaching Falls Short for Research Preparation"},"content":"Traditional Approach\n\nResearch Reality\n\n“Here’s the method, follow these steps.”\n\n“Here’s a phenomenon — figure out how to study it.”\n\n“Use this package exactly as shown.”\n\n“Choose tools, adapt them, integrate approaches.”\n\n“Avoid mistakes — they hurt your grade.”\n\n“Learn from mistakes — they drive discovery and skill development.”\n\n“What does the professor want?”\n\n“What does this result mean?”","type":"content","url":"/why-different-final#the-recipe-following-problem","position":13},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl3":"The Passive Learning Trap","lvl2":"Why Traditional Teaching Falls Short for Research Preparation"},"type":"lvl3","url":"/why-different-final#the-passive-learning-trap","position":14},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl3":"The Passive Learning Trap","lvl2":"Why Traditional Teaching Falls Short for Research Preparation"},"content":"Traditional courses accidentally train students to:\n\nWait for instructions rather than taking initiative.\n\nSee confusion as failure rather than opportunity.\n\nAvoid intellectual risks that lead to discoveries.\n\nFocus on grades over understanding.\n\nThis doesn’t prepare you for careers where creativity and independent thinking are essential.\n\nBut here’s what traditional courses rob you of: the addictive joy of discovery. There’s nothing quite like the rush of finally cracking a problem you’ve been wrestling with for hours. That “aha!” moment when disparate pieces suddenly click together. The pride of building something that works through your own effort and creativity.\n\nResearch scientists don’t endure the struggle despite the difficulty — they do it because solving hard problems and coming up with new ideas and strategies keeps things exciting. This course is designed to provide you with similar opportunities in a supportive, educational environment.","type":"content","url":"/why-different-final#the-passive-learning-trap","position":15},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl2":"Our Evidence-Based Design Choices"},"type":"lvl2","url":"/why-different-final#our-evidence-based-design-choices","position":16},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl2":"Our Evidence-Based Design Choices"},"content":"","type":"content","url":"/why-different-final#our-evidence-based-design-choices","position":17},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl3":"Research Validation","lvl2":"Our Evidence-Based Design Choices"},"type":"lvl3","url":"/why-different-final#research-validation","position":18},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl3":"Research Validation","lvl2":"Our Evidence-Based Design Choices"},"content":"Ting & O’Briain (2025) studied LLM integration in astronomy education and found:\n\nStudents decreased AI dependence over time with structured guidance.\n\nAI became a learning tool rather than shortcut.\n\nDocumentation requirements fostered metacognitive awareness.\n\nHigh student satisfaction with professional skill development.\n\nKey insight: Thoughtful AI integration with reflection requirements enhances learning while building essential 21st-century skills.\n\nLi (2024) found that new-era university students need scaffolded transitions to autonomy — they have strong abilities but weak self-control without structure. This directly informs our three-phase approach.","type":"content","url":"/why-different-final#research-validation","position":19},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl3":"Core Design Elements","lvl2":"Our Evidence-Based Design Choices"},"type":"lvl3","url":"/why-different-final#core-design-elements","position":20},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl3":"Core Design Elements","lvl2":"Our Evidence-Based Design Choices"},"content":"","type":"content","url":"/why-different-final#core-design-elements","position":21},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl4":"1. “I Want You to Think” Focus","lvl3":"Core Design Elements","lvl2":"Our Evidence-Based Design Choices"},"type":"lvl4","url":"/why-different-final#id-1-i-want-you-to-think-focus","position":22},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl4":"1. “I Want You to Think” Focus","lvl3":"Core Design Elements","lvl2":"Our Evidence-Based Design Choices"},"content":"Why: Research requires contributing ideas, not following directions.\n\nHow: Explicitly reward thinking over compliance.","type":"content","url":"/why-different-final#id-1-i-want-you-to-think-focus","position":23},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl4":"2. Growth Over Perfection","lvl3":"Core Design Elements","lvl2":"Our Evidence-Based Design Choices"},"type":"lvl4","url":"/why-different-final#id-2-growth-over-perfection","position":24},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl4":"2. Growth Over Perfection","lvl3":"Core Design Elements","lvl2":"Our Evidence-Based Design Choices"},"content":"Why: Research involves failed experiments and iteration.\n\nHow: Build resilience through productive failure.\n\nEvidence: Neural research shows error awareness directly predicts learning (Tirri & Kujala, 2016).","type":"content","url":"/why-different-final#id-2-growth-over-perfection","position":25},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl4":"3. Mandatory Project Extensions","lvl3":"Core Design Elements","lvl2":"Our Evidence-Based Design Choices"},"type":"lvl4","url":"/why-different-final#id-3-mandatory-project-extensions","position":26},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl4":"3. Mandatory Project Extensions","lvl3":"Core Design Elements","lvl2":"Our Evidence-Based Design Choices"},"content":"Why: Real research means going beyond minimums by testing new ideas.\n\nHow: Practice asking “what if?” and “why does this happen?”","type":"content","url":"/why-different-final#id-3-mandatory-project-extensions","position":27},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl4":"4. Strategic AI Integration","lvl3":"Core Design Elements","lvl2":"Our Evidence-Based Design Choices"},"type":"lvl4","url":"/why-different-final#id-4-strategic-ai-integration","position":28},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl4":"4. Strategic AI Integration","lvl3":"Core Design Elements","lvl2":"Our Evidence-Based Design Choices"},"content":"Why: AI is becoming standard in industry and research but requires critical thinking and domain expertise.\n\nHow: Three-phase scaffolding from minimal to professional use.","type":"content","url":"/why-different-final#id-4-strategic-ai-integration","position":29},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl4":"5. Pair Programming","lvl3":"Core Design Elements","lvl2":"Our Evidence-Based Design Choices"},"type":"lvl4","url":"/why-different-final#id-5-pair-programming","position":30},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl4":"5. Pair Programming","lvl3":"Core Design Elements","lvl2":"Our Evidence-Based Design Choices"},"content":"Why: Modern research is collaborative.\n\nHow: Develop communication and mutual learning skills.","type":"content","url":"/why-different-final#id-5-pair-programming","position":31},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl2":"What This Means for You"},"type":"lvl2","url":"/why-different-final#what-this-means-for-you","position":32},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl2":"What This Means for You"},"content":"","type":"content","url":"/why-different-final#what-this-means-for-you","position":33},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl3":"The Skills You’re Actually Developing","lvl2":"What This Means for You"},"type":"lvl3","url":"/why-different-final#the-skills-youre-actually-developing","position":34},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl3":"The Skills You’re Actually Developing","lvl2":"What This Means for You"},"content":"Computational Competencies:\n\nCode literacy and debugging.\n\nDocumentation and version control.\n\nPerformance optimization.\n\nTesting and validation.\n\nScientific Thinking:\n\nHypothesis formation.\n\nMethod selection.\n\nResult interpretation.\n\nLimitation assessment.\n\nProfessional Habits:\n\nIntellectual honesty.\n\nCollaborative learning.\n\nContinuous improvement.\n\nResilience through challenges.","type":"content","url":"/why-different-final#the-skills-youre-actually-developing","position":35},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl2":"Addressing Your Concerns"},"type":"lvl2","url":"/why-different-final#addressing-your-concerns","position":36},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl2":"Addressing Your Concerns"},"content":"","type":"content","url":"/why-different-final#addressing-your-concerns","position":37},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl3":"“This seems harder than other courses”","lvl2":"Addressing Your Concerns"},"type":"lvl3","url":"/why-different-final#this-seems-harder-than-other-courses","position":38},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl3":"“This seems harder than other courses”","lvl2":"Addressing Your Concerns"},"content":"Yes, it is harder — intentionally. You’re developing new neural pathways for independent thinking and creative problem-solving. This isn’t metaphorical — neuroscience research shows this literally requires brain rewiring through effortful practice.\n\nThe neuroscience is clear:\n\nYour brain physically rewires when learning challenging material. Woollett & Maguire (2011) demonstrated that London taxi drivers’ hippocampi — the brain region crucial for spatial navigation — grew measurably larger after mastering the city’s 25,000+ streets. This structural brain change occurred in adults averaging 40+ years old, proving neuroplasticity works at any age.\n\nStruggle directly triggers neuroplasticity. When you wrestle with difficult concepts, your brain releases BDNF (brain-derived neurotrophic factor), often called “Miracle Gro for the brain.” This protein stimulates the formation of new synaptic connections (Draganski et al., 2004).\n\nError signals drive learning. Moser et al. (2011) used EEG to show that the brain generates two distinct responses to mistakes: error detection (ERN) and error awareness (Pe). Students with growth mindsets showed enhanced Pe amplitude, which directly predicted improved performance. Your brain literally learns more from errors than successes.\n\nGrowth mindset has measurable neural correlates. Tirri & Kujala (2016) found that believing intelligence is malleable activates different brain networks during problem-solving, leading to enhanced error processing and better learning outcomes across thousands of replicated studies.\n\nTranslation: That uncomfortable feeling when grappling with new concepts? That’s your neurons forming new connections. You’re not “bad at this” — you’re actively growing smarter.","type":"content","url":"/why-different-final#this-seems-harder-than-other-courses","position":39},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl3":"“I’m making more mistakes than usual!”","lvl2":"Addressing Your Concerns"},"type":"lvl3","url":"/why-different-final#im-making-more-mistakes-than-usual","position":40},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl3":"“I’m making more mistakes than usual!”","lvl2":"Addressing Your Concerns"},"content":"Perfect! This is exactly what Schwartz advocates for. Remember his key insight:\n\n“We don’t do a good enough job of teaching our students how to be productively stupid.”\n\nThis course does that job. We create structured opportunities for you to feel confused, make mistakes, and push through to fix them — because that’s where real learning happens.\n\nHere’s the neuroscience of why mistakes are so powerful: Your brain is evolutionarily wired to remember failures more vividly than successes. When you make an error, your brain releases a cascade of neurotransmitters that essentially bookmark that moment — “Don’t do that again!” This is why you’ll forget a hundred correct answers but remember that one embarrassing mistake forever.\n\nIn programming, this is a superpower. Every bug you encounter, every error message you debug, every wrong approach you try gets seared into your memory. You likely won’t make that mistake again. This is far more effective than being shown the “right way” first — your brain barely registers smooth successes, but it never forgets a good failure.\n\nMistakes signal your brain is building new neural pathways. Avoiding struggle means avoiding expertise development. Every error teaches you something textbooks can’t.","type":"content","url":"/why-different-final#im-making-more-mistakes-than-usual","position":41},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl3":"“I feel lost sometimes.”","lvl2":"Addressing Your Concerns"},"type":"lvl3","url":"/why-different-final#i-feel-lost-sometimes","position":42},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl3":"“I feel lost sometimes.”","lvl2":"Addressing Your Concerns"},"content":"Everyone does. The difference between those who succeed and those who don’t isn’t ability — it’s persistence and willingness to seek help.\n\nCritical insight from research (Uwerhiavwe, 2022): Mathematical ability is socially constructed, not innate. If you’ve ever thought you’re “not a math person” or “not good with computers,” this is a learned limitation, not a biological fact. Research definitively shows these beliefs are shaped by past experiences and can be changed through new experiences, proper support, and resilience.","type":"content","url":"/why-different-final#i-feel-lost-sometimes","position":43},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl2":"Your Agency in This Process"},"type":"lvl2","url":"/why-different-final#your-agency-in-this-process","position":44},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl2":"Your Agency in This Process"},"content":"This is your education and your career. I provide opportunities; you decide your engagement level.","type":"content","url":"/why-different-final#your-agency-in-this-process","position":45},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl3":"What I Offer","lvl2":"Your Agency in This Process"},"type":"lvl3","url":"/why-different-final#what-i-offer","position":46},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl3":"What I Offer","lvl2":"Your Agency in This Process"},"content":"Structured opportunities for developing independence.\n\nProfessional skills for any technical career.\n\nSupport as you enhance your problem-solving capabilities and independent thinking skills.\n\nSafe environment for intellectual risk-taking. In fact, this is a requirement for assignments.","type":"content","url":"/why-different-final#what-i-offer","position":47},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl3":"What You Control","lvl2":"Your Agency in This Process"},"type":"lvl3","url":"/why-different-final#what-you-control","position":48},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl3":"What You Control","lvl2":"Your Agency in This Process"},"content":"Engagement level: How deeply you dive into explorations.\n\nLearning goals: Which skills you prioritize.\n\nCareer direction: Academia, industry, or something else.\n\nRelationship with challenges: Obstacles or opportunities?","type":"content","url":"/why-different-final#what-you-control","position":49},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl3":"Three Valid Approaches","lvl2":"Your Agency in This Process"},"type":"lvl3","url":"/why-different-final#three-valid-approaches","position":50},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl3":"Three Valid Approaches","lvl2":"Your Agency in This Process"},"content":"Minimum: Meet requirements, pass, move on.\n\nGrowth: Develop stronger technical and problem-solving skills.\n\nTransformation: Fundamentally change how you approach learning.\n\nAll are valid. I hope you choose deeper engagement, but it’s your decision.","type":"content","url":"/why-different-final#three-valid-approaches","position":51},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl2":"Week 1 Survival Guide"},"type":"lvl2","url":"/why-different-final#week-1-survival-guide","position":52},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl2":"Week 1 Survival Guide"},"content":"","type":"content","url":"/why-different-final#week-1-survival-guide","position":53},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl3":"Expect This Trajectory","lvl2":"Week 1 Survival Guide"},"type":"lvl3","url":"/why-different-final#expect-this-trajectory","position":54},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl3":"Expect This Trajectory","lvl2":"Week 1 Survival Guide"},"content":"Week 1-2: \"I'm completely lost\" → Normal, your brain is rewiring\nWeek 3-4: \"Some things make sense\" → Patterns emerging\nWeek 5-6: \"I can do this\" → Confidence building\nWeek 7+: \"This is actually fun\" → Mastery developing\n\nThat “actually fun” phase is real. Once you experience the satisfaction of solving something yourself — debugging that stubborn error, watching your simulation finally work, seeing your MCMC converge — you’ll understand why researchers voluntarily spend their lives tackling hard problems and consistently learn new topics and techniques independently. The struggle becomes worth it for those moments of triumph.","type":"content","url":"/why-different-final#expect-this-trajectory","position":55},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl3":"Immediate Actions","lvl2":"Week 1 Survival Guide"},"type":"lvl3","url":"/why-different-final#immediate-actions","position":56},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl3":"Immediate Actions","lvl2":"Week 1 Survival Guide"},"content":"Accept confusion and frustration as normal - Everyone feels lost initially.\n\nUse the 30-minute rule - Struggle builds problem-solving muscles.\n\nForm study partnerships - This is a small class - leverage each other.\n\nCome prepared with specific questions - “I tried X, expected Y, got Z.”","type":"content","url":"/why-different-final#immediate-actions","position":57},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl3":"Mindset Shifts to Practice","lvl2":"Week 1 Survival Guide"},"type":"lvl3","url":"/why-different-final#mindset-shifts-to-practice","position":58},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl3":"Mindset Shifts to Practice","lvl2":"Week 1 Survival Guide"},"content":"Remember the neuroscience: Every time you struggle and push through, you’re literally building new neural pathways. This isn’t motivational speaking — it’s biological fact.\n\nAdd “yet”: “I don’t understand this yet” (growth mindset activation).\n\nReframe errors: “Interesting, why did that break?” (error-positive learning).\n\nCelebrate failures: “I learned three ways that don’t work” (Edison had 1,000+ “failures” before inventing the lightbulb).\n\nValue questions: “What am I missing?” (metacognitive development).\n\nYour brain is plastic. Intelligence and ability are not fixed. Every struggle makes you literally, measurably smarter. The MRI scans prove it.","type":"content","url":"/why-different-final#mindset-shifts-to-practice","position":59},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl2":"The Bottom Line"},"type":"lvl2","url":"/why-different-final#the-bottom-line","position":60},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl2":"The Bottom Line"},"content":"You’re not just learning to code and design algorithms. You’re learning to think like a computational scientist and astrophysicist.\n\nComputational thinking requires consistent daily practice, not last-minute cramming. Ultimately, what you get from this course is proportional to what you invest. I’ve designed every element to maximize your growth — the scaffolding, the struggle, the support. But I can’t do the learning for you. The students who embrace the challenge, lean into the discomfort, and engage deeply will undergo genuine transformation. Those who do the minimum will get minimum returns.\n\nAnd please, USE THE RESOURCES available to you. “Hacking hours” aren’t just for crisis mode — come to explore ideas, dive deeper into topics that excite you, or just work alongside others. Be selfish with your learning: grab every opportunity for support, ask “dumb” questions, pursue tangents that interest you. The best students aren’t the ones who never need help; they’re the ones smart enough to seek connection and growth.\n\nRemember: The struggle is the point. That’s where the learning happens. But struggling alone when help is available? That’s just inefficient.","type":"content","url":"/why-different-final#the-bottom-line","position":61},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl3":"References & Additional Resources","lvl2":"The Bottom Line"},"type":"lvl3","url":"/why-different-final#references-additional-resources","position":62},{"hierarchy":{"lvl1":"Why ASTR 596 is Designed This Way","lvl3":"References & Additional Resources","lvl2":"The Bottom Line"},"content":"Core Readings:\n\nSchwartz, M. A. (2008). \n\nThe importance of stupidity in scientific research. Journal of Cell Science, 121(11), 1771.\n\nNeuroscience of Learning:\n\nMoser, J. S., Schroder, H. S., Heeter, C., Moran, T. P., & Lee, Y. H. (2011). \n\nMind your errors: Evidence for a neural mechanism linking growth mind-set to adaptive posterror adjustments. Psychological Science, 22(12), 1484-1489.\n\nWoollett, K., & Maguire, E. A. (2011). \n\nAcquiring “the Knowledge” of London’s layout drives structural brain changes. Current Biology, 21(24), 2109-2114.\n\nDraganski, B., Gaser, C., Busch, V., Schuierer, G., Bogdahn, U., & May, A. (2004). Neuroplasticity: Changes in grey matter induced by training. Nature, 427(6972), 311-312.\n\nGrowth Mindset & Educational Research:\n\nTirri, K., & Kujala, T. (2016). \n\nStudents’ mindsets for learning and their neural underpinnings. Psychology, 7(09), 1231-1239.\n\nUwerhiavwe, O. (2022). \n\nThe influence of learners’ mathematical social identities on their mathematics learning. Open Journal of Social Sciences, 10(13), 458-473.\n\nLi, Y. (2024). \n\nCharacteristics of the mindset and behaviour of university students in the new era and educational countermeasures. Creative Education, 15(08), 1685-1700.\n\nAI in Education:\n\nTing, Y. S. & O’Briain, D. (2025). \n\nTeaching astronomy with large language models. arXiv preprint arXiv:2506.06921.","type":"content","url":"/why-different-final#references-additional-resources","position":63},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)"},"type":"lvl1","url":"/cli-intro-revised","position":0},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)"},"content":"","type":"content","url":"/cli-intro-revised","position":1},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl2":"Before You Begin: Safety Rules"},"type":"lvl2","url":"/cli-intro-revised#before-you-begin-safety-rules","position":2},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl2":"Before You Begin: Safety Rules"},"content":"🛡️ Terminal Safety First\n\nThe terminal is powerful but unforgiving. Follow these rules ALWAYS:\n\npwd before rm - Always know where you are before deleting\n\nls before rm - Always see what you’ll delete\n\nUse Tab completion - Reduces dangerous typos\n\nKeep backups - The terminal has NO undo button\n\nThink twice, type once - Especially with delete commands\n\nRemember: One wrong character can delete everything. There is no recycle bin.","type":"content","url":"/cli-intro-revised#before-you-begin-safety-rules","position":3},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl2":"Why Use the Terminal?"},"type":"lvl2","url":"/cli-intro-revised#why-use-the-terminal","position":4},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl2":"Why Use the Terminal?"},"content":"","type":"content","url":"/cli-intro-revised#why-use-the-terminal","position":5},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"The GUI Limitation","lvl2":"Why Use the Terminal?"},"type":"lvl3","url":"/cli-intro-revised#the-gui-limitation","position":6},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"The GUI Limitation","lvl2":"Why Use the Terminal?"},"content":"When you use Finder (macOS) or File Explorer (Windows), you’re limited to what the designers decided to show you. Want to:\n\nRename 1000 files at once? Good luck clicking each one.\n\nFind all Python files modified in the last week? No easy way.\n\nRun your code on a supercomputer? There’s no GUI there.\n\nProcess data on a remote server? You need the terminal.","type":"content","url":"/cli-intro-revised#the-gui-limitation","position":7},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"The CLI Superpower","lvl2":"Why Use the Terminal?"},"type":"lvl3","url":"/cli-intro-revised#the-cli-superpower","position":8},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"The CLI Superpower","lvl2":"Why Use the Terminal?"},"content":"The command line gives you:\n\nAutomation: Do repetitive tasks in seconds, not hours\n\nRemote access: Control computers anywhere in the world\n\nPower: Access to thousands of tools not available in GUIs\n\nSpeed: Keyboard is faster than mouse for many tasks\n\nReproducibility: Save and share exact commands you ran\n\nProfessional necessity: Every computational scientist uses it\n\nReal example: Renaming simulation outputs# GUI way: Click each file, rename manually (30 minutes for 100 files)\n\n# CLI way: One command, 2 seconds (we'll learn how later)\nfor i in *.dat; do mv \"$i\" \"simulation_${i}\"; done","type":"content","url":"/cli-intro-revised#the-cli-superpower","position":9},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl2":"Understanding the Terminal (2-minute conceptual foundation)"},"type":"lvl2","url":"/cli-intro-revised#understanding-the-terminal-2-minute-conceptual-foundation","position":10},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl2":"Understanding the Terminal (2-minute conceptual foundation)"},"content":"","type":"content","url":"/cli-intro-revised#understanding-the-terminal-2-minute-conceptual-foundation","position":11},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"What’s What?","lvl2":"Understanding the Terminal (2-minute conceptual foundation)"},"type":"lvl3","url":"/cli-intro-revised#whats-what","position":12},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"What’s What?","lvl2":"Understanding the Terminal (2-minute conceptual foundation)"},"content":"Terminal: The application you open (like \n\nTerminal.app on Mac)\n\nShell: The program that interprets your commands (bash, zsh, etc.)\n\nCommand Line: Where you type commands\n\nPrompt: Shows you’re ready for input (usually ends with $ or >)","type":"content","url":"/cli-intro-revised#whats-what","position":13},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"How Commands Work","lvl2":"Understanding the Terminal (2-minute conceptual foundation)"},"type":"lvl3","url":"/cli-intro-revised#how-commands-work","position":14},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"How Commands Work","lvl2":"Understanding the Terminal (2-minute conceptual foundation)"},"content":"command -options arguments\n   ↑        ↑        ↑\n  what    how to   what to\n  to do    do it    do it on\n\nExample: ls -la /home means “list (ls) with long format and all files (-la) in the /home directory”","type":"content","url":"/cli-intro-revised#how-commands-work","position":15},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl2":"Getting Started: Opening the Terminal"},"type":"lvl2","url":"/cli-intro-revised#getting-started-opening-the-terminal","position":16},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl2":"Getting Started: Opening the Terminal"},"content":"","type":"content","url":"/cli-intro-revised#getting-started-opening-the-terminal","position":17},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"macOS","lvl2":"Getting Started: Opening the Terminal"},"type":"lvl3","url":"/cli-intro-revised#macos","position":18},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"macOS","lvl2":"Getting Started: Opening the Terminal"},"content":"Press Cmd + Space, type “Terminal”, press Enter\n\nOr: Applications → Utilities → Terminal","type":"content","url":"/cli-intro-revised#macos","position":19},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Linux","lvl2":"Getting Started: Opening the Terminal"},"type":"lvl3","url":"/cli-intro-revised#linux","position":20},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Linux","lvl2":"Getting Started: Opening the Terminal"},"content":"Press Ctrl + Alt + T\n\nOr: Look for “Terminal” in applications","type":"content","url":"/cli-intro-revised#linux","position":21},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Windows","lvl2":"Getting Started: Opening the Terminal"},"type":"lvl3","url":"/cli-intro-revised#windows","position":22},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Windows","lvl2":"Getting Started: Opening the Terminal"},"content":"Best option: Use “Git Bash” (installed with Git)\n\nAlternative: Windows Terminal or PowerShell\n\nAvoid: Command Prompt (cmd.exe) - uses different commands\n\n⚠️ Windows Users: Important Differences\n\nIf using Git Bash on Windows:\n\nSome commands work differently (we’ll note these)\n\nNo man command (use --help instead)\n\nPath separators: Use / not \\\n\nYour home is /c/Users/YourName not C:\\Users\\YourName","type":"content","url":"/cli-intro-revised#windows","position":23},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl2":"Your First Three Commands"},"type":"lvl2","url":"/cli-intro-revised#your-first-three-commands","position":24},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl2":"Your First Three Commands"},"content":"Master these three before anything else. Seriously, practice just these for 10 minutes.","type":"content","url":"/cli-intro-revised#your-first-three-commands","position":25},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"1. Where Am I? (pwd)","lvl2":"Your First Three Commands"},"type":"lvl3","url":"/cli-intro-revised#id-1-where-am-i-pwd","position":26},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"1. Where Am I? (pwd)","lvl2":"Your First Three Commands"},"content":"pwd    # Print Working Directory\n\nTry it: Type pwd and press Enter\n\nYou should see something like:\n\nmacOS/Linux: /Users/yourname or /home/yourname\n\nWindows Git Bash: /c/Users/yourname","type":"content","url":"/cli-intro-revised#id-1-where-am-i-pwd","position":27},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"2. What’s Here? (ls)","lvl2":"Your First Three Commands"},"type":"lvl3","url":"/cli-intro-revised#id-2-whats-here-ls","position":28},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"2. What’s Here? (ls)","lvl2":"Your First Three Commands"},"content":"ls      # List files and folders\n\nTry it: Type ls and press Enter\n\nYou’ll see files and folders in your current location.","type":"content","url":"/cli-intro-revised#id-2-whats-here-ls","position":29},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"3. Move Around (cd)","lvl2":"Your First Three Commands"},"type":"lvl3","url":"/cli-intro-revised#id-3-move-around-cd","position":30},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"3. Move Around (cd)","lvl2":"Your First Three Commands"},"content":"cd Desktop    # Change Directory to Desktop\ncd ..         # Go up one level\ncd ~          # Go to your home directory\n\nTry it:\n\nType cd Desktop (or any folder you see from ls)\n\nType pwd to confirm you moved\n\nType cd .. to go back\n\n✅ Quick Practice #1\n\nCan you:\n\nFind out where you are? (pwd)\n\nSee what’s in your current folder? (ls)\n\nMove to a different folder and back? (cd)\n\nIf yes, you’ve mastered the basics! If no, practice for 5 more minutes.","type":"content","url":"/cli-intro-revised#id-3-move-around-cd","position":31},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl2":"File System Navigation"},"type":"lvl2","url":"/cli-intro-revised#file-system-navigation","position":32},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl2":"File System Navigation"},"content":"","type":"content","url":"/cli-intro-revised#file-system-navigation","position":33},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Understanding Paths","lvl2":"File System Navigation"},"type":"lvl3","url":"/cli-intro-revised#understanding-paths","position":34},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Understanding Paths","lvl2":"File System Navigation"},"content":"Your computer’s files are organized in a tree:/                    # Root (top level)\n├── home/           \n│   └── yourname/    # Your home directory (~)\n│       ├── Desktop/\n│       ├── Documents/\n│       └── astr596/\n│           ├── project1/\n│           └── project2/\n\nTwo types of paths:\n\nAbsolute: Full path from root /home/yourname/Desktop\n\nRelative: Path from where you are Desktop or ../Documents","type":"content","url":"/cli-intro-revised#understanding-paths","position":35},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Navigation Shortcuts","lvl2":"File System Navigation"},"type":"lvl3","url":"/cli-intro-revised#navigation-shortcuts","position":36},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Navigation Shortcuts","lvl2":"File System Navigation"},"content":"~     # Your home directory\n.     # Current directory  \n..    # Parent directory (one up)\n-     # Previous directory","type":"content","url":"/cli-intro-revised#navigation-shortcuts","position":37},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Enhanced ls Commands","lvl2":"File System Navigation"},"type":"lvl3","url":"/cli-intro-revised#enhanced-ls-commands","position":38},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Enhanced ls Commands","lvl2":"File System Navigation"},"content":"Now let’s add options to ls:ls -a          # Show ALL files (including hidden)\nls -l          # Long format (details)\nls -la         # Combine: all files, detailed\nls -lh         # Human-readable sizes (KB, MB)\nls *.py        # List only Python files\n\nExample output of ls -lh:total 28K\n-rw-r--r-- 1 user group 2.4K Nov 15 14:23 main.py\n-rw-r--r-- 1 user group  15K Nov 15 14:20 nbody.py\ndrwxr-xr-x 2 user group 4.0K Nov 14 10:15 data/\n\nThis shows: permissions, owner, size, date, name","type":"content","url":"/cli-intro-revised#enhanced-ls-commands","position":39},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Pro Navigation with Tab Completion","lvl2":"File System Navigation"},"type":"lvl3","url":"/cli-intro-revised#pro-navigation-with-tab-completion","position":40},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Pro Navigation with Tab Completion","lvl2":"File System Navigation"},"content":"🎯 THE MOST IMPORTANT TIP\n\nTab is your best friend! Start typing, press Tab to autocomplete:cd Desk[TAB]        # Autocompletes to Desktop/\ncd ~/astr[TAB]/pr[TAB]  # Autocompletes full path\n\nTab prevents typos and saves typing. Use it CONSTANTLY.","type":"content","url":"/cli-intro-revised#pro-navigation-with-tab-completion","position":41},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl2":"File and Directory Operations"},"type":"lvl2","url":"/cli-intro-revised#file-and-directory-operations","position":42},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl2":"File and Directory Operations"},"content":"","type":"content","url":"/cli-intro-revised#file-and-directory-operations","position":43},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Creating Directories","lvl2":"File and Directory Operations"},"type":"lvl3","url":"/cli-intro-revised#creating-directories","position":44},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Creating Directories","lvl2":"File and Directory Operations"},"content":"mkdir project3                   # Make one directory\nmkdir -p data/raw/2024          # Make nested directories\nmkdir results plots analysis    # Make multiple at once","type":"content","url":"/cli-intro-revised#creating-directories","position":45},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Creating Files","lvl2":"File and Directory Operations"},"type":"lvl3","url":"/cli-intro-revised#creating-files","position":46},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Creating Files","lvl2":"File and Directory Operations"},"content":"touch README.md              # Create empty file\ntouch script.py module.py    # Create multiple files\necho \"# Project 1\" > README.md   # Create file with content","type":"content","url":"/cli-intro-revised#creating-files","position":47},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Copying Files","lvl2":"File and Directory Operations"},"type":"lvl3","url":"/cli-intro-revised#copying-files","position":48},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Copying Files","lvl2":"File and Directory Operations"},"content":"cp file1.py file2.py             # Copy file\ncp file1.py backup/              # Copy to directory\ncp -r project1/ project1_backup/ # Copy entire directory (-r = recursive)\n\n💡 Always Backup Before Dangerous Operations\n\nBefore modifying important files:cp important.py important.py.backup\n\nNow you can recover if something goes wrong!","type":"content","url":"/cli-intro-revised#copying-files","position":49},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Moving and Renaming","lvl2":"File and Directory Operations"},"type":"lvl3","url":"/cli-intro-revised#moving-and-renaming","position":50},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Moving and Renaming","lvl2":"File and Directory Operations"},"content":"mv oldname.py newname.py    # Rename file\nmv file.py ../              # Move up one directory\nmv *.dat data/             # Move all .dat files","type":"content","url":"/cli-intro-revised#moving-and-renaming","position":51},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"⚠️ Removing Files and Directories","lvl2":"File and Directory Operations"},"type":"lvl3","url":"/cli-intro-revised#id-removing-files-and-directories","position":52},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"⚠️ Removing Files and Directories","lvl2":"File and Directory Operations"},"content":"🔥 EXTREME DANGER ZONE: rm Commands\n\nTHE rm COMMAND IS PERMANENT. NO UNDO. NO RECYCLE BIN.\n\nNEVER EVER use these:\n\nrm -rf / = DELETE ENTIRE COMPUTER\n\nrm -rf ~ = DELETE ALL YOUR FILES\n\nrm -rf * = DELETE EVERYTHING IN CURRENT FOLDER\n\nWhat -rf means:\n\n-r = recursive (delete folders and everything inside)\n\n-f = force (no confirmation, even for important files)\n\nSafe practices:\n\nUse rm -i for interactive mode (asks confirmation)\n\nUse ls first to see what you’ll delete\n\nUse pwd to confirm you’re in the right place\n\nStart with rm (no flags) for single files\n\nSafe deletion examples:rm file.py                # Remove single file\nrm -i *.tmp              # Remove with confirmation\nrm -r empty_folder/      # Remove empty directory","type":"content","url":"/cli-intro-revised#id-removing-files-and-directories","position":53},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl2":"Viewing Files"},"type":"lvl2","url":"/cli-intro-revised#viewing-files","position":54},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl2":"Viewing Files"},"content":"","type":"content","url":"/cli-intro-revised#viewing-files","position":55},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Quick Views","lvl2":"Viewing Files"},"type":"lvl3","url":"/cli-intro-revised#quick-views","position":56},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Quick Views","lvl2":"Viewing Files"},"content":"cat file.py          # Show entire file\nhead file.py         # Show first 10 lines\nhead -n 20 file.py   # Show first 20 lines  \ntail file.py         # Show last 10 lines\ntail -f output.log   # Follow file updates (great for logs)\nless bigfile.txt     # Page through file (q to quit)","type":"content","url":"/cli-intro-revised#quick-views","position":57},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Searching in Files","lvl2":"Viewing Files"},"type":"lvl3","url":"/cli-intro-revised#searching-in-files","position":58},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Searching in Files","lvl2":"Viewing Files"},"content":"grep \"import\" *.py       # Find \"import\" in Python files\ngrep -n \"error\" log.txt  # Show with line numbers\ngrep -r \"TODO\" .         # Search all files recursively\ngrep -i \"warning\" log    # Case-insensitive search","type":"content","url":"/cli-intro-revised#searching-in-files","position":59},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl2":"Platform Compatibility Reference"},"type":"lvl2","url":"/cli-intro-revised#platform-compatibility-reference","position":60},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl2":"Platform Compatibility Reference"},"content":"🖥️ Command Differences Across Platforms\n\nCommand\n\nmacOS/Linux\n\nGit Bash\n\nWindows CMD\n\nAlternative\n\nls\n\n✅ Works\n\n✅ Works\n\n❌\n\nUse dir\n\npwd\n\n✅ Works\n\n✅ Works\n\n❌\n\nUse cd (no args)\n\ncat\n\n✅ Works\n\n✅ Works\n\n❌\n\nUse type\n\nrm\n\n✅ Works\n\n✅ Works\n\n❌\n\nUse del\n\nman\n\n✅ Works\n\n❌\n\n❌\n\nUse --help\n\ngrep\n\n✅ Works\n\n✅ Works\n\n❌\n\nUse findstr\n\nps aux\n\n✅ Works\n\n⚠️ Limited\n\n❌\n\nUse tasklist\n\nTip: When in doubt, use command --help to see options","type":"content","url":"/cli-intro-revised#platform-compatibility-reference","position":61},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl2":"Working with Python"},"type":"lvl2","url":"/cli-intro-revised#working-with-python","position":62},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl2":"Working with Python"},"content":"","type":"content","url":"/cli-intro-revised#working-with-python","position":63},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Running Python Scripts","lvl2":"Working with Python"},"type":"lvl3","url":"/cli-intro-revised#running-python-scripts","position":64},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Running Python Scripts","lvl2":"Working with Python"},"content":"python script.py                 # Run script\npython -m module                 # Run module\npython -c \"print('hello')\"      # Run one line\npython                          # Interactive mode (exit() to quit)\nipython                         # Better interactive mode","type":"content","url":"/cli-intro-revised#running-python-scripts","position":65},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Managing Your Environment","lvl2":"Working with Python"},"type":"lvl3","url":"/cli-intro-revised#managing-your-environment","position":66},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Managing Your Environment","lvl2":"Working with Python"},"content":"conda activate astr596    # Enter course environment\nconda deactivate         # Exit environment\nwhich python            # Check which Python is active\nconda list              # See installed packages","type":"content","url":"/cli-intro-revised#managing-your-environment","position":67},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl2":"When Things Go Wrong"},"type":"lvl2","url":"/cli-intro-revised#when-things-go-wrong","position":68},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl2":"When Things Go Wrong"},"content":"🔧 Understanding Error Messages\n\n“command not found”\n\nTypo in command name\n\nCommand not installed\n\nNot in PATH\n\nFix: Check spelling, install missing tool\n\n“permission denied”\n\nNeed admin/sudo rights\n\nFile is protected\n\nWrong ownership\n\nFix: Use sudo (carefully!) or check file permissions\n\n“no such file or directory”\n\nWrong path or filename\n\nIn wrong directory\n\nFile doesn’t exist\n\nFix: Check with pwd and ls, verify path\n\n“syntax error near unexpected token”\n\nSpecial character not escaped\n\nQuote mismatch\n\nWrong shell syntax\n\nFix: Check quotes and special characters\n\nProcess running forever\n\nPress Ctrl+C to stop\n\nIf frozen, try Ctrl+Z then kill %1\n## Input/Output Redirection\n\n### Saving Output\n\n```bash\npython script.py > output.txt    # Save output to file\npython script.py >> output.txt   # Append to file\npython script.py 2> errors.txt   # Save errors only\npython script.py &> all.txt      # Save everything","type":"content","url":"/cli-intro-revised#when-things-go-wrong","position":69},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Pipes (Combining Commands)","lvl2":"When Things Go Wrong"},"type":"lvl3","url":"/cli-intro-revised#pipes-combining-commands","position":70},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Pipes (Combining Commands)","lvl2":"When Things Go Wrong"},"content":"ls -la | grep \".py\"              # List files, filter Python\ncat data.txt | sort | uniq       # Sort and remove duplicates\nhistory | grep \"git\"             # Find git commands","type":"content","url":"/cli-intro-revised#pipes-combining-commands","position":71},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl2":"Useful Shortcuts and Tips"},"type":"lvl2","url":"/cli-intro-revised#useful-shortcuts-and-tips","position":72},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl2":"Useful Shortcuts and Tips"},"content":"","type":"content","url":"/cli-intro-revised#useful-shortcuts-and-tips","position":73},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Keyboard Shortcuts","lvl2":"Useful Shortcuts and Tips"},"type":"lvl3","url":"/cli-intro-revised#keyboard-shortcuts","position":74},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Keyboard Shortcuts","lvl2":"Useful Shortcuts and Tips"},"content":"Tab         # AUTOCOMPLETE (use constantly!)\n↑/↓         # Previous/next command\nCtrl+C      # Stop current command\nCtrl+L      # Clear screen\nCtrl+A      # Go to line beginning\nCtrl+E      # Go to line end\nCtrl+R      # Search command history\nCtrl+D      # Exit/logout","type":"content","url":"/cli-intro-revised#keyboard-shortcuts","position":75},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Wildcards (Pattern Matching)","lvl2":"Useful Shortcuts and Tips"},"type":"lvl3","url":"/cli-intro-revised#wildcards-pattern-matching","position":76},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Wildcards (Pattern Matching)","lvl2":"Useful Shortcuts and Tips"},"content":"*           # Any characters\n?           # Single character\n[abc]       # Any of a, b, c\n[0-9]       # Any digit\n\n# Examples:\nls *.py                 # All Python files\nls data_?.txt          # data_1.txt, data_2.txt, etc.\nls img_[0-9][0-9].png  # img_00.png through img_99.png","type":"content","url":"/cli-intro-revised#wildcards-pattern-matching","position":77},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl2":"Quick Reference Card"},"type":"lvl2","url":"/cli-intro-revised#quick-reference-card","position":78},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl2":"Quick Reference Card"},"content":"","type":"content","url":"/cli-intro-revised#quick-reference-card","position":79},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Essential Daily Commands","lvl2":"Quick Reference Card"},"type":"lvl3","url":"/cli-intro-revised#essential-daily-commands","position":80},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Essential Daily Commands","lvl2":"Quick Reference Card"},"content":"# Navigation\npwd                 # Where am I?\nls -la              # What's here? (all files, detailed)\ncd folder/          # Enter folder\ncd ..               # Go up one level\ncd ~                # Go home\n\n# Files & Directories\nmkdir project       # Create directory\ntouch file.py       # Create empty file\ncp source dest      # Copy\nmv old new          # Move/rename\nrm file             # Delete (CAREFUL!)\n\n# Viewing Files\ncat file            # Show entire file\nhead -20 file       # Show first 20 lines\ntail -f log         # Follow log file\ngrep \"text\" file    # Search in file\n\n# Python & Course\npython script.py            # Run Python\nconda activate astr596      # Enter environment\ngit status                  # Check git\ngit add . && git commit -m \"msg\" && git push  # Submit work\n\n# Getting Help\ncommand --help      # See command options (not 'man' on Windows)","type":"content","url":"/cli-intro-revised#essential-daily-commands","position":81},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl2":"Practice Exercises"},"type":"lvl2","url":"/cli-intro-revised#practice-exercises","position":82},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl2":"Practice Exercises"},"content":"","type":"content","url":"/cli-intro-revised#practice-exercises","position":83},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Exercise 1: Navigation Basics (10 min)","lvl2":"Practice Exercises"},"type":"lvl3","url":"/cli-intro-revised#exercise-1-navigation-basics-10-min","position":84},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Exercise 1: Navigation Basics (10 min)","lvl2":"Practice Exercises"},"content":"# 1. Find where you are\npwd\n\n# 2. Go to your home directory\ncd ~\n\n# 3. Create a practice folder\nmkdir cli_practice\ncd cli_practice\n\n# 4. Create some files\ntouch data1.txt data2.txt script.py\n\n# 5. List what you created\nls -la\n\n# 6. Go back home\ncd ..","type":"content","url":"/cli-intro-revised#exercise-1-navigation-basics-10-min","position":85},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Exercise 2: File Management (15 min)","lvl2":"Practice Exercises"},"type":"lvl3","url":"/cli-intro-revised#exercise-2-file-management-15-min","position":86},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Exercise 2: File Management (15 min)","lvl2":"Practice Exercises"},"content":"Create a project structure:mkdir -p project/{src,data,docs}\n\nCreate files in each folder\n\nCopy a file between folders\n\nRename a file\n\nSafely delete a test file","type":"content","url":"/cli-intro-revised#exercise-2-file-management-15-min","position":87},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Exercise 3: Real Task - Organize Files (20 min)","lvl2":"Practice Exercises"},"type":"lvl3","url":"/cli-intro-revised#exercise-3-real-task-organize-files-20-min","position":88},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Exercise 3: Real Task - Organize Files (20 min)","lvl2":"Practice Exercises"},"content":"# Create messy folder\nmkdir messy && cd messy\ntouch file1.py file2.py data1.txt data2.txt image1.png image2.png\n\n# Now organize them\nmkdir {code,data,images}\nmv *.py code/\nmv *.txt data/\nmv *.png images/\n\n# Verify organization\nls -la */","type":"content","url":"/cli-intro-revised#exercise-3-real-task-organize-files-20-min","position":89},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl2":"Practical Examples for ASTR 596"},"type":"lvl2","url":"/cli-intro-revised#practical-examples-for-astr-596","position":90},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl2":"Practical Examples for ASTR 596"},"content":"","type":"content","url":"/cli-intro-revised#practical-examples-for-astr-596","position":91},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Setting Up a New Project","lvl2":"Practical Examples for ASTR 596"},"type":"lvl3","url":"/cli-intro-revised#setting-up-a-new-project","position":92},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Setting Up a New Project","lvl2":"Practical Examples for ASTR 596"},"content":"# Create full project structure\nmkdir -p project2/{src,data,outputs,docs}\ncd project2\ntouch README.md requirements.txt\ntouch src/{main.py,stellar.py,utils.py}\necho \"# Project 2: N-Body Simulation\" > README.md","type":"content","url":"/cli-intro-revised#setting-up-a-new-project","position":93},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Running and Logging Simulations","lvl2":"Practical Examples for ASTR 596"},"type":"lvl3","url":"/cli-intro-revised#running-and-logging-simulations","position":94},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Running and Logging Simulations","lvl2":"Practical Examples for ASTR 596"},"content":"# Run with timing\ntime python nbody_sim.py\n\n# Run with output capture\npython nbody_sim.py > output.log 2>&1\n\n# Run multiple parameters\nfor n in 100 500 1000; do\n    python nbody.py --particles=$n > results_n$n.txt\ndone","type":"content","url":"/cli-intro-revised#running-and-logging-simulations","position":95},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Data Processing Pipeline","lvl2":"Practical Examples for ASTR 596"},"type":"lvl3","url":"/cli-intro-revised#data-processing-pipeline","position":96},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl3":"Data Processing Pipeline","lvl2":"Practical Examples for ASTR 596"},"content":"# Process all data files\nfor file in data/*.txt; do\n    python process.py \"$file\" > \"processed/$(basename $file)\"\ndone\n\n# Check results\ngrep \"converged\" processed/*.txt | wc -l","type":"content","url":"/cli-intro-revised#data-processing-pipeline","position":97},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl2":"Emergency Recovery"},"type":"lvl2","url":"/cli-intro-revised#emergency-recovery","position":98},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl2":"Emergency Recovery"},"content":"🆘 “I’m Lost, Help!”\n\nIf you’re confused or lost:\n\nWhere am I? → pwd\n\nGo home → cd ~\n\nSee what’s here → ls -la\n\nStop running process → Ctrl+C\n\nClear messy screen → Ctrl+L or clear\n\nExit and start over → exit and reopen terminal\n\nRemember: Closing and reopening terminal resets everything!","type":"content","url":"/cli-intro-revised#emergency-recovery","position":99},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl2":"Next Steps"},"type":"lvl2","url":"/cli-intro-revised#next-steps","position":100},{"hierarchy":{"lvl1":"Chapter 1: Introduction to the Command Line Interface (CLI)","lvl2":"Next Steps"},"content":"Congratulations! You now know the essential CLI commands for this course.\n\nYour learning path:\n\n✅ Master the first three commands (pwd, ls, cd)\n\n✅ Practice file operations carefully\n\n✅ Use Tab completion religiously\n\n→ Continue to course projects\n\nOptional Advanced Topics: Curious about remote computing (SSH), long-running jobs (screen/tmux), or shell scripting? Those are useful for research but not needed for ASTR 596.\n\nRemember:\n\nTab is your friend: Saves typing and prevents errors\n\npwd before rm: Always know where you are\n\nBe paranoid with rm: No undo in terminal!\n\nPractice daily: 10 minutes/day for two weeks = mastery\n\nThe command line may seem “old school” and scary at first, but it’s just typing commands instead of clicking. Within two weeks of daily use, it’ll feel natural. You’ve got this!","type":"content","url":"/cli-intro-revised#next-steps","position":101},{"hierarchy":{"lvl1":"Chapter 2: Software Setup Guide"},"type":"lvl1","url":"/software-setup-revised","position":0},{"hierarchy":{"lvl1":"Chapter 2: Software Setup Guide"},"content":"💻 Before You Start\n\nSystem Requirements:\n\n✓ 3 GB free disk space (Miniforge ~400 MB + packages ~1-2 GB)\n\n✓ Stable internet connection (will download ~800 MB total)\n\n✓ Administrator privileges on your computer\n\n✓ 60-90 minutes of uninterrupted time\n\nIf on campus: Use eduroam WiFi, not guest network (firewall issues)\n\nWant more details? See the \n\nofficial conda documentation","type":"content","url":"/software-setup-revised","position":1},{"hierarchy":{"lvl1":"Chapter 2: Software Setup Guide","lvl2":"Setup Roadmap"},"type":"lvl2","url":"/software-setup-revised#setup-roadmap","position":2},{"hierarchy":{"lvl1":"Chapter 2: Software Setup Guide","lvl2":"Setup Roadmap"},"content":"Total time: ~60 minutes\n\nYour Setup Journey\n\n📦 Step 1: Install Python with Miniforge (15 min)🌍 Step 2: Create your course environment (10 min)📝 Step 3: Install VS Code editor (10 min)✅ Step 4: Verify everything works (5 min)🚀 Step 5: Quick practice session (20 min)","type":"content","url":"/software-setup-revised#setup-roadmap","position":3},{"hierarchy":{"lvl1":"Chapter 2: Software Setup Guide","lvl2":"Understanding the Setup (2-minute read)"},"type":"lvl2","url":"/software-setup-revised#understanding-the-setup-2-minute-read","position":4},{"hierarchy":{"lvl1":"Chapter 2: Software Setup Guide","lvl2":"Understanding the Setup (2-minute read)"},"content":"Your computer probably already has Python, but it’s used by your operating system. Touching it could break things. We need:\n\nOur own Python: Separate from system Python (that’s what Miniforge provides)\n\nIsolated workspace: A bubble for course packages (that’s the conda environment)\n\nCode editor: A professional tool for writing code (that’s VS Code)\n\nThink of it like setting up a chemistry lab—you need the right equipment in a clean, isolated space where experiments won’t affect anything else.\n\n📚 Want to Learn More?\n\nFor deeper understanding of conda and environments, see:\n\nOfficial conda user guide\n\nWhy use conda environments?\n\n💡 Keep This Open\n\nTerminal Basics (you’ll need these commands throughout):\n\nWhere am I? → pwd\n\nWhat’s here? → ls\n\nMove to folder → cd foldername\n\nGo back → cd ..\n\nCopy-paste works! (Ctrl+Shift+V on Linux/Windows, Cmd+V on Mac)","type":"content","url":"/software-setup-revised#understanding-the-setup-2-minute-read","position":5},{"hierarchy":{"lvl1":"Chapter 2: Software Setup Guide","lvl2":"Step 1: Install Python via Miniforge"},"type":"lvl2","url":"/software-setup-revised#step-1-install-python-via-miniforge","position":6},{"hierarchy":{"lvl1":"Chapter 2: Software Setup Guide","lvl2":"Step 1: Install Python via Miniforge"},"content":"Miniforge gives us:\n\nPython + conda package manager (same as Anaconda/Miniconda)\n\nFree, open-source, no licensing issues\n\nWorks identically on all operating systems\n\nDefault conda-forge channel (community-maintained, most comprehensive)\n\nMinimal installation (~400 MB vs Anaconda’s ~3 GB)\n\n🤔 Why Miniforge instead of Anaconda?\n\nMiniforge (~400 MB): Just Python + conda + pip. You install only what you need.\nAnaconda (~3 GB): Pre-installs 250+ packages (Spyder, Qt, R packages, etc.) you won’t use.\nMiniconda (~400 MB): Minimal like Miniforge but defaults to Anaconda’s channel (fewer packages).\n\nAnaconda also has commercial licensing restrictions for organizations >200 people.\n\nFor academic work, Miniforge is the best choice: minimal, free, and access to the most packages. \n\nLearn more about conda variants\n\n\n\nTab two\n\n1. Download Miniforge:\n\nVisit \n\nhttps://​conda​-forge​.org​/download/\n\nFind and click the installer for your system:\n\nmacOS Apple Silicon: For M1/M2/M3 Macs\n\nmacOS x86_64: For Intel Macs\n\nLinux x86_64: For standard Linux systems\n\nLinux aarch64: For ARM-based Linux\n\nThe installer (.sh file) will download to your Downloads folder\n\n🍎 Not sure which Mac you have?\n\nRun uname -m in Terminal:\n\narm64 = Apple Silicon (use “macOS Apple Silicon” installer)\n\nx86_64 = Intel (use “macOS x86_64” installer)\n\n2. Navigate to Downloads and run installer:# Go to Downloads folder\ncd ~/Downloads\n\n# Check the file downloaded (should be ~80-100 MB)\nls -lh Miniforge3*.sh\n\n# Make it executable (required on some systems)\nchmod +x Miniforge3-*.sh\n\n# Run the installer\nbash Miniforge3-*.sh\n\n3. Follow prompts:\n\nPress ENTER to review license\n\nType yes to accept\n\nPress ENTER for default location (recommended)\n\nType yes for conda init\n\n4. Activate changes:source ~/.bashrc  # Linux\nsource ~/.zshrc   # macOS\n\n5. Verify installation:conda --version\n\n✅ Success: Shows version number like conda 24.7.1:::{tab-item} Windows\n:sync: tab2\n\n:::{admonition} ⚠️ Windows Users: Important Notes\n:class: warning\n\n1. The instructor uses macOS/Linux and hasn't personally tested these Windows instructions\n2. Most commands work in \"Git Bash\" (install Git first if needed)\n3. Use \"Miniforge Prompt\" for pure conda operations\n4. If you encounter issues:\n   - Google the exact error message (in quotes)\n   - Ask ChatGPT/Claude with the full error text\n   - Post on Slack with screenshots\n   - Find a Windows-using classmate!\n\n1. Download installer:\n\nVisit \n\nhttps://​conda​-forge​.org​/download/\n\nClick the “Windows x86_64” installer\n\nThe .exe file (~80 MB) will download to your Downloads folder\n\n2. Verify download:\n\nOpen File Explorer → Downloads folder\n\nLook for Miniforge3-Windows-x86_64.exe\n\nIf the file is <70 MB, the download failed - try again\n\n3. Run the installer:\n\nDouble-click the .exe file\n\nIf you get security warnings, click “Run anyway”\n\nRight-click → “Run as Administrator” if you encounter permission issues\n\n4. Installation wizard:\n\nClick Next\n\nAccept license\n\nSelect “Just Me” (recommended)\n\nUse default location\n\n✓ Check “Add Miniforge3 to PATH”\n\nInstall\n\n5. Open “Miniforge Prompt” from Start Menu\n\n6. Verify installation:conda --version\n\n✅ Success: Shows version number like conda 24.7.1\n\n⚠️ Didn’t Work?\n\nIf conda --version gives “command not found”:\n\nRestart your terminal (most common fix)\n\nCheck installation path matches what you selected\n\nOn Windows, use “Miniforge Prompt” not regular Command Prompt","type":"content","url":"/software-setup-revised#step-1-install-python-via-miniforge","position":7},{"hierarchy":{"lvl1":"Chapter 2: Software Setup Guide","lvl2":"Step 2: Create Your Course Environment"},"type":"lvl2","url":"/software-setup-revised#step-2-create-your-course-environment","position":8},{"hierarchy":{"lvl1":"Chapter 2: Software Setup Guide","lvl2":"Step 2: Create Your Course Environment"},"content":"⚠️ Keep Your Terminal Open!\n\nUse the same terminal window throughout setup. If you accidentally close it:\n\nOpen a new terminal\n\nRun conda activate astr596 (after creating it)\n\nContinue where you left off\n\nAn environment is an isolated workspace. Think of it as a clean room where we can install packages without affecting anything else on your computer.# Create environment with Python 3.11\nconda create -n astr596 python=3.11\n\nWhen prompted “Proceed ([y]/n)?”, type y and press Enter.# Activate your environment\nconda activate astr596\n\n✅ Success Check: Your prompt now shows (astr596) at the beginning\n\n⚠️ Common Mistake #1\n\nForgetting to activate the environment!\n\nEvery time you open a new terminal, you MUST run:conda activate astr596\n\nNo (astr596) in prompt = wrong environment = packages “not found”!","type":"content","url":"/software-setup-revised#step-2-create-your-course-environment","position":9},{"hierarchy":{"lvl1":"Chapter 2: Software Setup Guide","lvl3":"Install Essential Packages","lvl2":"Step 2: Create Your Course Environment"},"type":"lvl3","url":"/software-setup-revised#install-essential-packages","position":10},{"hierarchy":{"lvl1":"Chapter 2: Software Setup Guide","lvl3":"Install Essential Packages","lvl2":"Step 2: Create Your Course Environment"},"content":"⏱️ Installation Time\n\nPackage installation takes 5-15 minutes depending on internet speed.\nIf it’s taking longer than 20 minutes:\n\nPress Ctrl+C to cancel\n\nCheck your internet connection\n\nTry again with fewer packages at once\n\nYou have two options for package installation:","type":"content","url":"/software-setup-revised#install-essential-packages","position":11},{"hierarchy":{"lvl1":"Chapter 2: Software Setup Guide","lvl4":"Option 1: Install Everything Now (Recommended)","lvl3":"Install Essential Packages","lvl2":"Step 2: Create Your Course Environment"},"type":"lvl4","url":"/software-setup-revised#option-1-install-everything-now-recommended","position":12},{"hierarchy":{"lvl1":"Chapter 2: Software Setup Guide","lvl4":"Option 1: Install Everything Now (Recommended)","lvl3":"Install Essential Packages","lvl2":"Step 2: Create Your Course Environment"},"content":"# Install all course packages at once\nconda install numpy scipy matplotlib pandas jupyter ipython astropy h5py scikit-learn\n\n# Install JAX and jax-related packages separately (can be finicky)\nconda install jax jaxlib -c conda-forge\nconda install flax optax diffrax lineax optimistix -c conda-forge\n\n# If JAX fails with conda, use pip as fallback:\n# pip install --upgrade jax jaxlib\n\nThis ensures you won’t forget to install something later when you need it!","type":"content","url":"/software-setup-revised#option-1-install-everything-now-recommended","position":13},{"hierarchy":{"lvl1":"Chapter 2: Software Setup Guide","lvl4":"Option 2: Progressive Installation","lvl3":"Install Essential Packages","lvl2":"Step 2: Create Your Course Environment"},"type":"lvl4","url":"/software-setup-revised#option-2-progressive-installation","position":14},{"hierarchy":{"lvl1":"Chapter 2: Software Setup Guide","lvl4":"Option 2: Progressive Installation","lvl3":"Install Essential Packages","lvl2":"Step 2: Create Your Course Environment"},"content":"Start with core packages and add others as needed:# Essential packages (install these now)\nconda install numpy matplotlib jupyter ipython\n\nAdd more when you need them:# Scientific computing (Project 2+)\nconda install scipy pandas astropy\n\n# Machine learning (Project 3+)\nconda install scikit-learn h5py\nconda install jax jaxlib -c conda-forge","type":"content","url":"/software-setup-revised#option-2-progressive-installation","position":15},{"hierarchy":{"lvl1":"Chapter 2: Software Setup Guide","lvl3":"Managing Packages","lvl2":"Step 2: Create Your Course Environment"},"type":"lvl3","url":"/software-setup-revised#managing-packages","position":16},{"hierarchy":{"lvl1":"Chapter 2: Software Setup Guide","lvl3":"Managing Packages","lvl2":"Step 2: Create Your Course Environment"},"content":"📦 Package Management Commands\n\nUpdate packages (do this periodically):conda update numpy\nconda update --all  # Update everything\n\nUninstall packages (useful for troubleshooting):conda uninstall package_name\n# Then reinstall fresh:\nconda install package_name\n\nCheck what’s installed:conda list  # All packages\nconda list numpy  # Specific package version\n\n✅ Quick Test:python -c \"import numpy; print('NumPy works!')\"","type":"content","url":"/software-setup-revised#managing-packages","position":17},{"hierarchy":{"lvl1":"Chapter 2: Software Setup Guide","lvl2":"Step 3: Install and Configure VS Code"},"type":"lvl2","url":"/software-setup-revised#step-3-install-and-configure-vs-code","position":18},{"hierarchy":{"lvl1":"Chapter 2: Software Setup Guide","lvl2":"Step 3: Install and Configure VS Code"},"content":"","type":"content","url":"/software-setup-revised#step-3-install-and-configure-vs-code","position":19},{"hierarchy":{"lvl1":"Chapter 2: Software Setup Guide","lvl3":"Installation","lvl2":"Step 3: Install and Configure VS Code"},"type":"lvl3","url":"/software-setup-revised#installation","position":20},{"hierarchy":{"lvl1":"Chapter 2: Software Setup Guide","lvl3":"Installation","lvl2":"Step 3: Install and Configure VS Code"},"content":"Download from: \n\nhttps://​code​.visualstudio​.com/\n\nRun installer for your operating system\n\nLaunch VS Code","type":"content","url":"/software-setup-revised#installation","position":21},{"hierarchy":{"lvl1":"Chapter 2: Software Setup Guide","lvl3":"Minimal Setup (That’s All You Need!)","lvl2":"Step 3: Install and Configure VS Code"},"type":"lvl3","url":"/software-setup-revised#minimal-setup-thats-all-you-need","position":22},{"hierarchy":{"lvl1":"Chapter 2: Software Setup Guide","lvl3":"Minimal Setup (That’s All You Need!)","lvl2":"Step 3: Install and Configure VS Code"},"content":"Install Python Extension:\n\nClick Extensions icon in sidebar (or press Ctrl+Shift+X)\n\nSearch “Python”\n\nInstall “Python” by Microsoft (first result)\n\nSelect Your Environment:\n\nPress Ctrl+Shift+P (Windows/Linux) or Cmd+Shift+P (Mac)\n\nType “Python: Select Interpreter”\n\nChoose astr596 from the list\n\nNote: VS Code remembers this choice per folder - set once per project!\n\nThat’s it! Add other extensions only as you need them.","type":"content","url":"/software-setup-revised#minimal-setup-thats-all-you-need","position":23},{"hierarchy":{"lvl1":"Chapter 2: Software Setup Guide","lvl3":"Disable AI Assistants (Course Requirement)","lvl2":"Step 3: Install and Configure VS Code"},"type":"lvl3","url":"/software-setup-revised#disable-ai-assistants-course-requirement","position":24},{"hierarchy":{"lvl1":"Chapter 2: Software Setup Guide","lvl3":"Disable AI Assistants (Course Requirement)","lvl2":"Step 3: Install and Configure VS Code"},"content":"🚫 Critical: Disable ALL AI Coding Tools\n\nWhy this matters: AI assistants like GitHub Copilot will autocomplete your code, often suggesting entire functions or completing lines automatically. While this seems helpful, it’s catastrophic for learning because:\n\nYou’ll type def and Copilot writes the entire function (often incorrectly)\n\nYou won’t develop problem-solving skills or debugging abilities\n\nYou’ll become dependent on suggestions rather than understanding\n\nThe AI often suggests plausible-looking but subtly wrong code\n\nThis course’s philosophy: Learn to think computationally first, then use AI as a tool later (after Week 8).\n\nTo disable:\n\nPress Ctrl+, (Windows/Linux) or Cmd+, (Mac) for Settings\n\nSearch “copilot” → Uncheck ALL “Enable” options\n\nSearch “intellicode” → Uncheck “Enable”\n\nSearch “ai” → Disable any AI suggestions or completions\n\nSearch “suggest” → Consider disabling autocompletion entirely for maximum learning\n\nYou’re here to train your brain, not to train an AI to write code for you.","type":"content","url":"/software-setup-revised#disable-ai-assistants-course-requirement","position":25},{"hierarchy":{"lvl1":"Chapter 2: Software Setup Guide","lvl2":"Step 4: Test Your Setup"},"type":"lvl2","url":"/software-setup-revised#step-4-test-your-setup","position":26},{"hierarchy":{"lvl1":"Chapter 2: Software Setup Guide","lvl2":"Step 4: Test Your Setup"},"content":"Create a simple test file to verify everything works:\n\n1. Create test_setup.py:\"\"\"Quick setup test for ASTR 596\"\"\"\nprint(\"Testing imports...\")\n\nimport numpy as np\nprint(\"✓ NumPy works!\")\n\nimport matplotlib.pyplot as plt\nprint(\"✓ Matplotlib works!\")\n\n# Only test JAX if you installed it\ntry:\n    import jax\n    print(\"✓ JAX works!\")\nexcept ImportError:\n    print(\"○ JAX not installed (that's okay for now)\")\n\nprint(\"\\n🎉 Everything is installed correctly!\")\nprint(\"Your environment is ready for ASTR 596!\")\n\n2. Run the test:python test_setup.py\n\n✅ Success: You see checkmarks and the success message!","type":"content","url":"/software-setup-revised#step-4-test-your-setup","position":27},{"hierarchy":{"lvl1":"Chapter 2: Software Setup Guide","lvl2":"Step 5: Quick Practice"},"type":"lvl2","url":"/software-setup-revised#step-5-quick-practice","position":28},{"hierarchy":{"lvl1":"Chapter 2: Software Setup Guide","lvl2":"Step 5: Quick Practice"},"content":"Let’s make sure you’re comfortable with the basics:# 1. Check your environment is active\nconda activate astr596\n\n# 2. Check what's installed\nconda list\n\n# 3. Create a simple plot\npython -c \"import matplotlib.pyplot as plt; plt.plot([1,2,3]); plt.savefig('test.png'); print('Created test.png')\"\n\n# 4. Open VS Code in current folder\ncode .","type":"content","url":"/software-setup-revised#step-5-quick-practice","position":29},{"hierarchy":{"lvl1":"Chapter 2: Software Setup Guide","lvl2":"✅ Setup Complete Checklist"},"type":"lvl2","url":"/software-setup-revised#id-setup-complete-checklist","position":30},{"hierarchy":{"lvl1":"Chapter 2: Software Setup Guide","lvl2":"✅ Setup Complete Checklist"},"content":"You’re ready for the course when:\n\nOpening a new terminal and typing conda activate astr596 works\n\nThe prompt shows (astr596) after activation\n\nRunning python -c \"import numpy\" produces no errors\n\nVS Code opens when you type code .\n\nThe test script runs successfully\n\nAll checked? You’re ready!","type":"content","url":"/software-setup-revised#id-setup-complete-checklist","position":31},{"hierarchy":{"lvl1":"Chapter 2: Software Setup Guide","lvl2":"🎉 Congratulations!"},"type":"lvl2","url":"/software-setup-revised#id-congratulations","position":32},{"hierarchy":{"lvl1":"Chapter 2: Software Setup Guide","lvl2":"🎉 Congratulations!"},"content":"You’ve just accomplished what many graduate students struggle with for weeks. Your professional development environment is now:\n\n✅ Isolated from system Python (no conflicts!)\n\n✅ Reproducible (same setup works on any machine)\n\n✅ Professional-grade (same tools as research scientists)\n\nTake a screenshot of your successful test script output—you’ve earned this victory!","type":"content","url":"/software-setup-revised#id-congratulations","position":33},{"hierarchy":{"lvl1":"Chapter 2: Software Setup Guide","lvl2":"Troubleshooting"},"type":"lvl2","url":"/software-setup-revised#troubleshooting","position":34},{"hierarchy":{"lvl1":"Chapter 2: Software Setup Guide","lvl2":"Troubleshooting"},"content":"🔧 Common Issues and Solutions\n\n“Command not found: conda”\n\nRestart your terminal (most common fix!)\n\nOn Windows, use “Miniforge Prompt” not regular Command Prompt\n\nCheck if installation completed successfully\n\n“ModuleNotFoundError: No module named numpy”\n\nCheck prompt shows (astr596) - if not, run conda activate astr596\n\nVerify package is installed: conda list numpy\n\nReinstall if needed: conda install numpy\n\nVS Code can’t find Python\n\nOpen Command Palette (Ctrl/Cmd+Shift+P)\n\nRun “Python: Select Interpreter”\n\nChoose the astr596 environment\n\nRestart VS Code if needed\n\n“Permission denied” errors\n\nDon’t use sudo with conda (ever!)\n\nOn Windows, try “Run as Administrator” for installer only\n\nCheck you own the Miniforge directory\n\nPackage installation fails\n\nUpdate conda first: conda update conda\n\nClear cache: conda clean --all\n\nTry installing packages one at a time\n\nLast resort for specific package: pip install packagename\n\nBehind a corporate/university firewall?# Configure proxy (replace with your proxy details)\nconda config --set proxy_servers.http http://proxy.yourorg.com:8080\nconda config --set proxy_servers.https https://proxy.yourorg.com:8080\n\nNeed to start over completely?# Remove environment and start fresh\nconda deactivate\nconda env remove -n astr596\n# Then go back to Step 2\n\nLow on disk space?# Remove unused packages and cache\nconda clean --all\n# Check environment size\ndu -sh ~/miniforge3/envs/astr596","type":"content","url":"/software-setup-revised#troubleshooting","position":35},{"hierarchy":{"lvl1":"Chapter 2: Software Setup Guide","lvl2":"What’s Next?"},"type":"lvl2","url":"/software-setup-revised#whats-next","position":36},{"hierarchy":{"lvl1":"Chapter 2: Software Setup Guide","lvl2":"What’s Next?"},"content":"🛑 Feeling Overwhelmed?\n\nIt’s okay to take a break after Step 2 (environment creation) and continue tomorrow.\nYour progress is saved! When you return:\n\nOpen a new terminal\n\nRun conda activate astr596\n\nContinue from where you left off\n\n✅ Environment is ready\n\n→ Continue to \n\nGit and GitHub Guide\n\n→ Start working on Project 1\n\n💡 Pro Tips\n\nSave Time: Add this to your terminal config file (.bashrc/.zshrc):alias astr=\"conda activate astr596\"\n\nNow just type astr to activate!\n\nStay Organized: Create a folder structure:mkdir -p ~/astr596/{projects,notes,data}\n\nPractice Daily: Open terminal every day, even for 5 minutes. Muscle memory develops fast!","type":"content","url":"/software-setup-revised#whats-next","position":37},{"hierarchy":{"lvl1":"Chapter 2: Software Setup Guide","lvl2":"Quick Reference Card"},"type":"lvl2","url":"/software-setup-revised#quick-reference-card","position":38},{"hierarchy":{"lvl1":"Chapter 2: Software Setup Guide","lvl2":"Quick Reference Card"},"content":"Essential Commands\n\nCommand\n\nWhat it does\n\nconda activate astr596\n\nEnter course environment\n\nconda deactivate\n\nExit environment\n\nconda list\n\nShow installed packages\n\nconda install package_name\n\nInstall new package\n\npython script.py\n\nRun Python script\n\nwhich python\n\nCheck which Python is active\n\nconda env list\n\nShow all environments\n\ncode .\n\nOpen VS Code in current folder\n\nRemember: Everyone struggles with setup—it’s genuinely complex. But once it works, it just works. If you’re stuck after 15 minutes on any step, ask for help on Slack with your OS, the command you ran, and the full error message.","type":"content","url":"/software-setup-revised#quick-reference-card","position":39},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub"},"type":"lvl1","url":"/git-intro-revised","position":0},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub"},"content":"","type":"content","url":"/git-intro-revised","position":1},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl2":"Why Version Control Will Save Your Research"},"type":"lvl2","url":"/git-intro-revised#why-version-control-will-save-your-research","position":2},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl2":"Why Version Control Will Save Your Research"},"content":"Imagine this: It’s 2 AM, three days before your thesis defense. You accidentally delete 400 lines of working code while trying to “clean up.” Or worse—your laptop dies, taking six months of work with it.\n\nThis happens to someone every semester.\n\nGit prevents these disasters. It’s your safety net, collaboration tool, and scientific record all in one. By the end of this guide, you’ll never lose work again.\n\n🎯 What You’ll Learn\n\nAfter this guide, you’ll be able to:\n\n✓ Never lose code again (even if your laptop explodes)\n\n✓ Submit assignments via GitHub Classroom\n\n✓ Collaborate without fear of overwriting others’ work\n\n✓ Track exactly what changed and when\n\n✓ Recover from mistakes quickly","type":"content","url":"/git-intro-revised#why-version-control-will-save-your-research","position":3},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl2":"The Version Control Nightmare (Without Git)"},"type":"lvl2","url":"/git-intro-revised#the-version-control-nightmare-without-git","position":4},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl2":"The Version Control Nightmare (Without Git)"},"content":"We’ve all been here:stellar_evolution.py\nstellar_evolution_v2.py\nstellar_evolution_v2_FIXED.py\nstellar_evolution_v2_FIXED_actually_works.py\nstellar_evolution_OLD_DO_NOT_DELETE.py\nstellar_evolution_FINAL.py\nstellar_evolution_FINAL_REAL.py\nstellar_evolution_FINAL_REAL_USE_THIS_ONE.py\n\nWith Git, you have ONE file with complete history. Every change is tracked, documented, and reversible.","type":"content","url":"/git-intro-revised#the-version-control-nightmare-without-git","position":5},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl2":"Understanding Git: The Mental Model"},"type":"lvl2","url":"/git-intro-revised#understanding-git-the-mental-model","position":6},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl2":"Understanding Git: The Mental Model"},"content":"Think of Git as a time machine for your code with three areas:graph LR\n    A[Working Directory<br/>Your files] -->|git add| B[Staging Area<br/>Ready to save]\n    B -->|git commit| C[Repository<br/>Permanent history]\n    C -->|git push| D[GitHub<br/>Cloud backup]\n    D -->|git pull| A\n\nWorking Directory: Your actual files\n\nStaging Area: Changes ready to be saved\n\nLocal Repository: Your project’s history (on your computer)\n\nRemote Repository: Cloud backup (GitHub)","type":"content","url":"/git-intro-revised#understanding-git-the-mental-model","position":7},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl2":"Initial Setup (One Time Only)"},"type":"lvl2","url":"/git-intro-revised#initial-setup-one-time-only","position":8},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl2":"Initial Setup (One Time Only)"},"content":"","type":"content","url":"/git-intro-revised#initial-setup-one-time-only","position":9},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl3":"Step 1: Install Git","lvl2":"Initial Setup (One Time Only)"},"type":"lvl3","url":"/git-intro-revised#step-1-install-git","position":10},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl3":"Step 1: Install Git","lvl2":"Initial Setup (One Time Only)"},"content":"Git comes pre-installed. Verify with:git --version\n\nIf not installed, it will prompt you to install Xcode Command Line Tools.\n\n# Ubuntu/Debian\nsudo apt-get update\nsudo apt-get install git\n\n# Fedora\nsudo dnf install git\n\nDownload from \n\nhttps://​git​-scm​.com​/download​/win\n\nImportant: During installation, select:\n\n“Git from the command line and also from 3rd-party software”\n\n“Use Visual Studio Code as Git’s default editor”\n\n“Override default branch name” → type “main”\n\n“Git Credential Manager” (helps with authentication)\n\nAfter installation, use Git Bash for all Git commands (not Command Prompt).","type":"content","url":"/git-intro-revised#step-1-install-git","position":11},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl3":"Step 2: Configure Your Identity","lvl2":"Initial Setup (One Time Only)"},"type":"lvl3","url":"/git-intro-revised#step-2-configure-your-identity","position":12},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl3":"Step 2: Configure Your Identity","lvl2":"Initial Setup (One Time Only)"},"content":"# Tell Git who you are (for commit messages)\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@sdsu.edu\"\n\n# Set default branch name to 'main' (GitHub's default)\ngit config --global init.defaultBranch main\n\n# Set default editor (optional but helpful)\ngit config --global core.editor \"code --wait\"  # For VS Code\n\n# Verify configuration\ngit config --list","type":"content","url":"/git-intro-revised#step-2-configure-your-identity","position":13},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl3":"Step 3: Set Up GitHub Account","lvl2":"Initial Setup (One Time Only)"},"type":"lvl3","url":"/git-intro-revised#step-3-set-up-github-account","position":14},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl3":"Step 3: Set Up GitHub Account","lvl2":"Initial Setup (One Time Only)"},"content":"Go to \n\nhttps://github.com\n\nSign up with your SDSU email (critical for Student Pack!)\n\nVerify your email address\n\nApply for Student Developer Pack: \n\nhttps://​education​.github​.com​/pack\n\nFree GitHub Pro account\n\nGitHub Copilot Pro (free for students - normally $10/month)\n\nGitHub Codespaces hours\n\nAccess to 100+ developer tools\n\n📚 Student Developer Pack Benefits\n\nThe pack includes professional tools worth thousands of dollars:\n\nGitHub Copilot Pro: AI pair programmer (disabled for this course initially)\n\nCloud credits: Azure ($100), DigitalOcean, and more\n\nDomain names: Free .tech, .me domains\n\nLearning platforms: DataCamp, Educative, and others\n\nVerification typically takes 1-72 hours. You’ll get an email when approved.","type":"content","url":"/git-intro-revised#step-3-set-up-github-account","position":15},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl3":"Step 4: Set Up Authentication","lvl2":"Initial Setup (One Time Only)"},"type":"lvl3","url":"/git-intro-revised#step-4-set-up-authentication","position":16},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl3":"Step 4: Set Up Authentication","lvl2":"Initial Setup (One Time Only)"},"content":"🔐 Required Since August 2021\n\nGitHub no longer accepts passwords for Git operations. You need either:\n\nPersonal Access Token (easier) - We’ll use this\n\nSSH Keys (more secure but complex)\n\nWe’ll use Personal Access Tokens (classic) for simplicity. GitHub also offers “fine-grained” tokens with more security, but classic tokens are simpler for course work.\n\nCreate a Personal Access Token:\n\nGo to GitHub → Click your profile picture → Settings\n\nIn the left sidebar, click Developer settings\n\nUnder Personal access tokens, click Tokens (classic)\n\nClick Generate new token → Generate new token (classic)\n\nName it “ASTR 596 Course”\n\nSet expiration to after the semester ends\n\nCheck these scopes:\n\n✓ repo (all)\n\n✓ workflow (if using GitHub Actions)\n\nClick Generate token\n\nCOPY THE TOKEN NOW! You won’t see it again!\n\nSave your token securely:# Configure Git to remember credentials (so you don't paste token every time)\ngit config --global credential.helper cache  # Linux/Mac: remembers for 15 min\ngit config --global credential.helper manager  # Windows: saves permanently\n\n# First time you push, Git asks for:\n# Username: your-github-username\n# Password: paste-your-token-here (NOT your GitHub password!)\n\n💡 Token vs Password\n\nWhen Git asks for “password”, paste your TOKEN, not your actual GitHub password!\nThe token acts as your password for Git operations.","type":"content","url":"/git-intro-revised#step-4-set-up-authentication","position":17},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl2":"The Essential Five Commands"},"type":"lvl2","url":"/git-intro-revised#the-essential-five-commands","position":18},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl2":"The Essential Five Commands"},"content":"Master these five commands and you can use Git for this entire course:\n\nThe Essential Five\n\nCommand\n\nPurpose\n\ngit status\n\nWhat’s changed? (USE THIS CONSTANTLY)\n\ngit add .\n\nStage all changes for commit\n\ngit commit -m \"message\"\n\nSave changes with description\n\ngit push\n\nUpload to GitHub\n\ngit pull\n\nDownload latest changes","type":"content","url":"/git-intro-revised#the-essential-five-commands","position":19},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl2":"GitHub Classroom: Assignment Workflow"},"type":"lvl2","url":"/git-intro-revised#github-classroom-assignment-workflow","position":20},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl2":"GitHub Classroom: Assignment Workflow"},"content":"📚 How GitHub Classroom Works\n\nGitHub Classroom automates assignment distribution and collection:\n\nProfessor creates assignment with starter code\n\nYou get a personalized repository (forked from template)\n\nYou work and push changes\n\nYour last push before deadline = your submission\n\nProfessor sees all submissions automatically\n\nNo uploading files, no Canvas submissions, no “did you get my email?”\n\nNote: As of 2024, GitHub Classroom creates student repos as forks, allowing professors to update starter code even after you’ve accepted the assignment.","type":"content","url":"/git-intro-revised#github-classroom-assignment-workflow","position":21},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl3":"Accepting Your First Assignment","lvl2":"GitHub Classroom: Assignment Workflow"},"type":"lvl3","url":"/git-intro-revised#accepting-your-first-assignment","position":22},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl3":"Accepting Your First Assignment","lvl2":"GitHub Classroom: Assignment Workflow"},"content":"Click assignment link (provided on Canvas/Slack)\n\nLink looks like: https://classroom.github.com/a/xyz123\n\nAccept the assignment\n\nFirst time: Authorize GitHub Classroom\n\nSelect your name from the roster\n\nClick “Accept this assignment”\n\nWait for repository creation (~30 seconds)\n\nYou’ll see “Your assignment has been created”\n\nClick the link to your repository\n\nYour repository is created!\n\nURL format: github.com/sdsu-astr596/project1-yourusername\n\nThis is YOUR personal copy","type":"content","url":"/git-intro-revised#accepting-your-first-assignment","position":23},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl3":"Working on Assignments","lvl2":"GitHub Classroom: Assignment Workflow"},"type":"lvl3","url":"/git-intro-revised#working-on-assignments","position":24},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl3":"Working on Assignments","lvl2":"GitHub Classroom: Assignment Workflow"},"content":"# 1. Clone your assignment repository\ngit clone https://github.com/sdsu-astr596/project1-yourusername.git\ncd project1-yourusername\n\n# 2. Work on your code\n# Edit files, test, debug...\n\n# 3. Check what's changed\ngit status\n\n# 4. Stage your changes\ngit add .\n\n# 5. Commit with descriptive message\ngit commit -m \"Implement stellar mass calculation\"\n\n# 6. Push to GitHub (this is your submission!)\ngit push\n\n⚠️ Critical: Submission = Push\n\nYour submission is whatever is pushed to GitHub by the deadline!\n\nNo “submit” button needed\n\nPush early, push often\n\nYou can push unlimited times before deadline\n\nCheck \n\nGitHub.com to verify your code is there","type":"content","url":"/git-intro-revised#working-on-assignments","position":25},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl3":"Verifying Your Submission","lvl2":"GitHub Classroom: Assignment Workflow"},"type":"lvl3","url":"/git-intro-revised#verifying-your-submission","position":26},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl3":"Verifying Your Submission","lvl2":"GitHub Classroom: Assignment Workflow"},"content":"Always verify your submission:\n\nGo to your repository on \n\nGitHub.com\n\nCheck that your latest changes are visible\n\nLook for the green checkmark on your commit\n\nCheck the timestamp (must be before deadline!)\n\n💡 Pro Tip: Push Early and Often\n\nDon’t wait until 11:59 PM to push! Push every time you make progress:\n\nImplemented a function? Push.\n\nFixed a bug? Push.\n\nAdded comments? Push.\n\nThis way you always have a submission, even if something goes wrong at the last minute.","type":"content","url":"/git-intro-revised#verifying-your-submission","position":27},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl2":"Basic Git Workflow"},"type":"lvl2","url":"/git-intro-revised#basic-git-workflow","position":28},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl2":"Basic Git Workflow"},"content":"Let’s practice the complete workflow:","type":"content","url":"/git-intro-revised#basic-git-workflow","position":29},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl3":"Creating a New Repository","lvl2":"Basic Git Workflow"},"type":"lvl3","url":"/git-intro-revised#creating-a-new-repository","position":30},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl3":"Creating a New Repository","lvl2":"Basic Git Workflow"},"content":"# Create project folder\nmkdir my_analysis\ncd my_analysis\n\n# Initialize Git repository\ngit init\n\n# Create a Python file\necho \"print('Hello ASTR 596!')\" > hello.py\n\n# Create a .gitignore file\ncat > .gitignore << EOF\n# Python\n__pycache__/\n*.py[cod]\n*$py.class\n*.so\n.Python\nenv/\nvenv/\n.ipynb_checkpoints/\n.pytest_cache/\n\n# Data files (usually too large)\n*.fits\n*.hdf5\n*.npy\n*.npz\ndata/\n\n# OS files\n.DS_Store\nThumbs.db\n.Trash-*\n\n# IDE\n.vscode/\n.idea/\n*.swp\n*.swo\n\n# Personal\nscratch/\nnotes_to_self.txt\nEOF\n\n# Check status\ngit status\n\n# Add everything\ngit add .\n\n# First commit\ngit commit -m \"Initial commit with hello.py and .gitignore\"","type":"content","url":"/git-intro-revised#creating-a-new-repository","position":31},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl3":"Daily Workflow Cycle","lvl2":"Basic Git Workflow"},"type":"lvl3","url":"/git-intro-revised#daily-workflow-cycle","position":32},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl3":"Daily Workflow Cycle","lvl2":"Basic Git Workflow"},"content":"# Start of work session\ngit pull                     # Get latest changes (if working with others)\n\n# ... do your work ...\n\ngit status                    # What changed?\ngit diff                      # See specific changes\ngit add .                     # Stage everything\ngit commit -m \"Clear message\" # Save snapshot\ngit push                      # Backup to cloud","type":"content","url":"/git-intro-revised#daily-workflow-cycle","position":33},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl2":"Writing Good Commit Messages"},"type":"lvl2","url":"/git-intro-revised#writing-good-commit-messages","position":34},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl2":"Writing Good Commit Messages"},"content":"📝 Commit Message Best Practices\n\nGood messages explain what and why:\n\n“Fix energy conservation bug in leapfrog integrator”\n\n“Add mass-luminosity relationship for main sequence stars”\n\n“Optimize N-body force calculation with spatial hashing”\n\nBad messages are vague or useless:\n\n“Fixed stuff”\n\n“asdfasdf”\n\n“done”\n\n“why won’t this work”\n\nFuture you will thank current you for clear messages!","type":"content","url":"/git-intro-revised#writing-good-commit-messages","position":35},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl3":"Commit Message Format","lvl2":"Writing Good Commit Messages"},"type":"lvl3","url":"/git-intro-revised#commit-message-format","position":36},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl3":"Commit Message Format","lvl2":"Writing Good Commit Messages"},"content":"For longer commits:git commit\n# Opens editor for multi-line message:\n\n# Fix incorrect gravitational constant in N-body simulation\n#\n# The constant was off by a factor of 4π due to unit conversion\n# error. This affected all orbital period calculations.\n# \n# Fixes #12","type":"content","url":"/git-intro-revised#commit-message-format","position":37},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl2":"Common Scenarios and Solutions"},"type":"lvl2","url":"/git-intro-revised#common-scenarios-and-solutions","position":38},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl2":"Common Scenarios and Solutions"},"content":"","type":"content","url":"/git-intro-revised#common-scenarios-and-solutions","position":39},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl3":"“I Forgot to Pull First”","lvl2":"Common Scenarios and Solutions"},"type":"lvl3","url":"/git-intro-revised#i-forgot-to-pull-first","position":40},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl3":"“I Forgot to Pull First”","lvl2":"Common Scenarios and Solutions"},"content":"git push\n# Error: failed to push some refs...\n\n# Solution:\ngit pull\n# If there are conflicts, resolve them (see below)\ngit push","type":"content","url":"/git-intro-revised#i-forgot-to-pull-first","position":41},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl3":"“I Made a Mistake in My Last Commit”","lvl2":"Common Scenarios and Solutions"},"type":"lvl3","url":"/git-intro-revised#i-made-a-mistake-in-my-last-commit","position":42},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl3":"“I Made a Mistake in My Last Commit”","lvl2":"Common Scenarios and Solutions"},"content":"# Fix the file\n# Then amend the commit\ngit add .\ngit commit --amend -m \"New message\"\ngit push --force  # Only if you already pushed!","type":"content","url":"/git-intro-revised#i-made-a-mistake-in-my-last-commit","position":43},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl3":"“I Need to Undo Changes”","lvl2":"Common Scenarios and Solutions"},"type":"lvl3","url":"/git-intro-revised#i-need-to-undo-changes","position":44},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl3":"“I Need to Undo Changes”","lvl2":"Common Scenarios and Solutions"},"content":"# Discard changes to specific file (not staged)\ngit checkout -- file.py\n\n# Undo last commit but keep changes\ngit reset --soft HEAD~1\n\n# Nuclear option: discard ALL local changes\ngit reset --hard HEAD","type":"content","url":"/git-intro-revised#i-need-to-undo-changes","position":45},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl3":"Merge Conflicts","lvl2":"Common Scenarios and Solutions"},"type":"lvl3","url":"/git-intro-revised#merge-conflicts","position":46},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl3":"Merge Conflicts","lvl2":"Common Scenarios and Solutions"},"content":"When Git can’t automatically merge:\n\nGit marks conflicts in files:<<<<<<< HEAD\nyour changes\n=======\ntheir changes\n>>>>>>> branch-name\n\nEdit file to resolve:# Keep the version you want (or combine them)\ncombined final version\n\nComplete the merge:git add .\ngit commit -m \"Resolve merge conflict\"\ngit push","type":"content","url":"/git-intro-revised#merge-conflicts","position":47},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl2":"Quick Reference Card"},"type":"lvl2","url":"/git-intro-revised#quick-reference-card","position":48},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl2":"Quick Reference Card"},"content":"Git Commands Reference\n\nCommand\n\nWhat it does\n\nBasic Workflow\n\n\n\ngit init\n\nCreate new repository\n\ngit clone <url>\n\nCopy repository from GitHub\n\ngit status\n\nShow what’s changed\n\ngit add <file>\n\nStage specific file\n\ngit add .\n\nStage all changes\n\ngit commit -m \"msg\"\n\nSave snapshot with message\n\ngit push\n\nUpload to GitHub\n\ngit pull\n\nDownload from GitHub\n\nViewing History\n\n\n\ngit log\n\nShow commit history\n\ngit log --oneline\n\nCompact history view\n\ngit log --graph\n\nVisual branch history\n\ngit diff\n\nShow unstaged changes\n\ngit diff --staged\n\nShow staged changes\n\nUndoing Things\n\n\n\ngit checkout -- <file>\n\nDiscard changes to file\n\ngit reset HEAD <file>\n\nUnstage file\n\ngit reset --soft HEAD~1\n\nUndo last commit, keep changes\n\ngit reset --hard HEAD\n\nDiscard ALL local changes\n\ngit revert <commit>\n\nUndo specific commit\n\nBranches (Advanced)\n\n\n\ngit branch\n\nList branches\n\ngit branch <name>\n\nCreate branch\n\ngit checkout <branch>\n\nSwitch branches\n\ngit merge <branch>\n\nMerge branch into current","type":"content","url":"/git-intro-revised#quick-reference-card","position":49},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl2":"Practice Exercises (optional but recommended)"},"type":"lvl2","url":"/git-intro-revised#practice-exercises-optional-but-recommended","position":50},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl2":"Practice Exercises (optional but recommended)"},"content":"","type":"content","url":"/git-intro-revised#practice-exercises-optional-but-recommended","position":51},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl3":"Exercise 1: Your First Repository (15 min)","lvl2":"Practice Exercises (optional but recommended)"},"type":"lvl3","url":"/git-intro-revised#exercise-1-your-first-repository-15-min","position":52},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl3":"Exercise 1: Your First Repository (15 min)","lvl2":"Practice Exercises (optional but recommended)"},"content":"Create a new repository called git-practice\n\nAdd a Python file with a simple function\n\nCreate a proper .gitignore\n\nMake your first commit\n\nCreate a README.md file\n\nMake a second commit\n\nView your history with git log","type":"content","url":"/git-intro-revised#exercise-1-your-first-repository-15-min","position":53},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl3":"Exercise 2: GitHub Workflow (20 min)","lvl2":"Practice Exercises (optional but recommended)"},"type":"lvl3","url":"/git-intro-revised#exercise-2-github-workflow-20-min","position":54},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl3":"Exercise 2: GitHub Workflow (20 min)","lvl2":"Practice Exercises (optional but recommended)"},"content":"Create a repository on \n\nGitHub.com (New → Repository)\n\nClone it locally\n\nAdd your practice code\n\nPush to GitHub\n\nVerify on \n\nGitHub.com\n\nMake changes on \n\nGitHub.com (edit README)\n\nPull changes locally","type":"content","url":"/git-intro-revised#exercise-2-github-workflow-20-min","position":55},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl3":"Exercise 3: Recovery Practice (15 min)","lvl2":"Practice Exercises (optional but recommended)"},"type":"lvl3","url":"/git-intro-revised#exercise-3-recovery-practice-15-min","position":56},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl3":"Exercise 3: Recovery Practice (15 min)","lvl2":"Practice Exercises (optional but recommended)"},"content":"Make some changes to a file\n\nUse git diff to see changes\n\nDiscard changes with git checkout\n\nMake new changes and commit them\n\nUndo the commit with git reset --soft HEAD~1\n\nRecommit with a better message","type":"content","url":"/git-intro-revised#exercise-3-recovery-practice-15-min","position":57},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl2":"✅ Git Proficiency Checklist"},"type":"lvl2","url":"/git-intro-revised#id-git-proficiency-checklist","position":58},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl2":"✅ Git Proficiency Checklist"},"content":"You’re ready for this course when you can:\n\nClone a repository from GitHub Classroom\n\nMake changes and commit them with clear messages\n\nPush changes to GitHub\n\nCheck status and diff before committing\n\nRecover from basic mistakes\n\nVerify your submission on \n\nGitHub.com\n\nAll checked? You’re ready to use Git for all assignments!","type":"content","url":"/git-intro-revised#id-git-proficiency-checklist","position":59},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl2":"Troubleshooting"},"type":"lvl2","url":"/git-intro-revised#troubleshooting","position":60},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl2":"Troubleshooting"},"content":"🔧 Common Issues and Fixes\n\n“Permission denied (publickey)”\n\nYou’re using SSH but haven’t set up keys\n\nUse HTTPS URLs instead: https://github.com/...\n\nOr set up SSH keys (see GitHub docs)\n\n“Authentication failed”\n\nPersonal Access Token issue\n\nCreate new token following steps above\n\nMake sure to paste TOKEN, not password\n\n“Cannot push to repository”\n\nCheck you’re in the right directory: pwd\n\nVerify remote URL: git remote -v\n\nMake sure you have commits: git log\n\n“Large files” error (>100MB)\n\nGit won’t accept huge files\n\nAdd to .gitignore: *.fits, *.hdf5, etc.\n\nUse git rm --cached largefile to remove\n\nAccidentally committed sensitive data\n\nDO NOT push yet!\n\nRemove with: git reset --hard HEAD~1\n\nIf already pushed, contact instructor immediately","type":"content","url":"/git-intro-revised#troubleshooting","position":61},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl2":"Beyond Basics: Useful Features"},"type":"lvl2","url":"/git-intro-revised#beyond-basics-useful-features","position":62},{"hierarchy":{"lvl1":"Chapter 3: Introduction to Git and GitHub","lvl2":"Beyond Basics: Useful Features"},"content":"🚀 Level Up Your Git Skills\n\nStashing (temporary storage):git stash          # Save changes temporarily\ngit stash pop      # Restore changes\n\nViewing specific commits:git show abc123    # Show specific commit\ngit diff HEAD~2    # Compare with 2 commits ago\n\nFinding bugs with bisect:git bisect start\ngit bisect bad      # Current version is broken\ngit bisect good v1.0  # v1.0 was working\n# Git helps you find when bug was introduced\n\nAliases for efficiency:git config --global alias.st status\ngit config --global alias.cm commit\ngit config --global alias.br branch\n# Now use: git st, git cm, etc.\n## Resources\n\n- **Official Git Book** (free): https://git-scm.com/book\n- **GitHub's Tutorial**: https://try.github.io\n- **Visual Git Guide**: https://marklodato.github.io/visual-git-guide/\n- **Oh Shit, Git!?!**: https://ohshitgit.com (recovery from mistakes)\n- **GitHub Classroom Guide**: https://classroom.github.com/help\n\n## Next Steps\n\n1. ✅ Git is configured\n2. ✅ GitHub account ready\n3. ✅ Can clone, commit, and push\n4. → Continue to [Command Line Interface Guide](03-cli-intro-guide)\n5. → Accept your first assignment on GitHub Classroom!\n\n---\n\n**Remember:** Git seems complex but you only need ~5 commands for this course. Focus on the essential workflow: `status`, `add`, `commit`, `push`, `pull`. Everything else is optional!","type":"content","url":"/git-intro-revised#beyond-basics-useful-features","position":63},{"hierarchy":{"lvl1":"Getting Started"},"type":"lvl1","url":"/index-1","position":0},{"hierarchy":{"lvl1":"Getting Started"},"content":"Your Foundation for Computational Astrophysics\nBefore we model the universe, we need to set up our computational laboratory. This module establishes the professional development environment and essential skills you’ll use throughout ASTR 596 and your research career.\n\nModule Philosophy: Tools Shape Thinking\n\nThe difference between a computational scientist and someone who codes isn’t knowledge of Python syntax — it’s mastery of the ecosystem. Version control, environment management, and command-line fluency aren’t just “nice to have” skills; they fundamentally change how you approach problems, collaborate, and think about reproducible science.","type":"content","url":"/index-1","position":1},{"hierarchy":{"lvl1":"Getting Started","lvl2":"Module Overview"},"type":"lvl2","url":"/index-1#module-overview","position":2},{"hierarchy":{"lvl1":"Getting Started","lvl2":"Module Overview"},"content":"💻 Command Line Interface\n\nMaster the terminal with essential commands for file management, navigation, and automation\n\n🔧 Software Setup\n\nInstall Python with Miniforge, configure VS Code, and create your isolated course environment\n\n📝 Git & GitHub\n\nVersion control fundamentals, GitHub Classroom workflow, and collaboration best practices","type":"content","url":"/index-1#module-overview","position":3},{"hierarchy":{"lvl1":"Getting Started","lvl2":"Why This Order Matters"},"type":"lvl2","url":"/index-1#why-this-order-matters","position":4},{"hierarchy":{"lvl1":"Getting Started","lvl2":"Why This Order Matters"},"content":"graph LR\n    A[CLI Skills] --> B[Software Setup]\n    B --> C[Version Control]\n    C --> D[Ready for Projects]\n    \n    A1[Navigate & Execute] --> A\n    B1[Run Install Commands] --> B\n    C1[Use Git in Terminal] --> C\n    D1[Submit Assignments] --> D\n    \n    style A fill:#e3f2fd\n    style D fill:#f3e5f5\n\nComplete these guides in order. Each builds on the previous:\n\nCLI first: Learn to navigate before installing anything\n\nSetup second: Use your CLI skills to install tools\n\nGit last: Apply everything to manage your code","type":"content","url":"/index-1#why-this-order-matters","position":5},{"hierarchy":{"lvl1":"Getting Started","lvl2":"Learning Path"},"type":"lvl2","url":"/index-1#learning-path","position":6},{"hierarchy":{"lvl1":"Getting Started","lvl2":"Learning Path"},"content":"🎯 Module Goals\n\nAfter completing this module in order, you will:\n\n✅ Navigate the terminal with confidence using pwd, ls, cd, and Tab completion.✅ Have a fully configured Python environment isolated from system Python.✅ Understand conda environments and why they prevent dependency conflicts.✅ Use Git for every assignment submission via GitHub Classroom.✅ Debug common issues independently using error messages.✅ Appreciate why every computational scientist relies on these tools daily.","type":"content","url":"/index-1#learning-path","position":7},{"hierarchy":{"lvl1":"Getting Started","lvl2":"Time Investment"},"type":"lvl2","url":"/index-1#time-investment","position":8},{"hierarchy":{"lvl1":"Getting Started","lvl2":"Time Investment"},"content":"Module Total: 4-5 hours\n\nCLI Introduction: 90 minutes\n\nSoftware Setup: 60 minutes\n\nGit Introduction: 90 minutes\n\nPractice: 60+ minutes\n\nDaily Practice: 5-10 min/day\n\nTerminal navigation\n\nGit commits\n\nPython scripts\n\nAfter 2 weeks: fluency","type":"content","url":"/index-1#time-investment","position":9},{"hierarchy":{"lvl1":"Getting Started","lvl2":"Common Challenges (and Solutions)"},"type":"lvl2","url":"/index-1#common-challenges-and-solutions","position":10},{"hierarchy":{"lvl1":"Getting Started","lvl2":"Common Challenges (and Solutions)"},"content":"⚠️ Expected Hurdles\n\nChallenge 1: “The terminal is scary”Solution: Start with just 3 commands (pwd, ls, cd). Everything else builds on these.\n\nChallenge 2: “Command not found” errorsSolution: Usually means wrong environment. Check for (astr596) in your prompt.\n\nChallenge 3: Git says “failed to push”Solution: Someone else pushed first. Pull, resolve conflicts, then push.\n\nChallenge 4: “This doesn’t work on Windows”Solution: Use Git Bash for Unix-like commands. We’ve noted all platform differences.\n\nChallenge 5: “I accidentally deleted files with rm”Solution: Why we emphasize backups and teach safe deletion practices first!","type":"content","url":"/index-1#common-challenges-and-solutions","position":11},{"hierarchy":{"lvl1":"Getting Started","lvl2":"Your First Week Checklist"},"type":"lvl2","url":"/index-1#your-first-week-checklist","position":12},{"hierarchy":{"lvl1":"Getting Started","lvl2":"Your First Week Checklist"},"content":"📋 Setup Verification\n\nComplete these milestones in order:\n\nAfter CLI Guide\n\nCan navigate directories without clicking\n\nUnderstand pwd, ls, cd, and Tab completion\n\nKnow the dangers of rm -rf\n\nAfter Software Setup\n\nMiniforge installed and working\n\nVS Code configured with Python extension\n\nCan activate astr596 environment\n\nTest script runs successfully\n\nAfter Git Guide\n\nGitHub account with Student Developer Pack\n\nPersonal Access Token configured\n\nCan clone, commit, and push\n\nUnderstand GitHub Classroom workflow\n\nReady for Project 1\n\nClone assignment from GitHub Classroom\n\nMake commits with meaningful messages\n\nPush changes to submit work","type":"content","url":"/index-1#your-first-week-checklist","position":13},{"hierarchy":{"lvl1":"Getting Started","lvl2":"Quick Command Reference"},"type":"lvl2","url":"/index-1#quick-command-reference","position":14},{"hierarchy":{"lvl1":"Getting Started","lvl2":"Quick Command Reference"},"content":"Essential Commands You’ll Use Daily\n\nCommand\n\nWhat It Does\n\nTerminal Basics\n\n\n\npwd\n\nShow current directory location\n\nls -la\n\nList all files with details\n\ncd folder_name\n\nChange to different directory\n\nTab key\n\nAutocomplete (use constantly!)\n\nPython Environment\n\n\n\nconda activate astr596\n\nEnter your course environment\n\npython script.py\n\nRun your Python code\n\nwhich python\n\nVerify correct Python is active\n\nGit Workflow\n\n\n\ngit status\n\nCheck what’s changed\n\ngit add .\n\nStage all changes\n\ngit commit -m \"message\"\n\nSave your work\n\ngit push\n\nSubmit to GitHub","type":"content","url":"/index-1#quick-command-reference","position":15},{"hierarchy":{"lvl1":"Getting Started","lvl2":"Getting Help"},"type":"lvl2","url":"/index-1#getting-help","position":16},{"hierarchy":{"lvl1":"Getting Started","lvl2":"Getting Help"},"content":"🆘 Support Resources\n\nSetup Issues:\n\nRead error messages carefully (they usually tell you what’s wrong)\n\nGoogle the exact error message in quotes\n\nCheck our troubleshooting sections in each guide\n\nPost on course Slack with: OS, command tried, full error\n\nConceptual Questions:\n\nWhy do we need virtual environments?\n\nHow does Git track changes?\n\nWhat’s the difference between terminal and shell?\n→ Great office hours questions!\n\nPlatform-Specific Issues:\n\nWindows: Use Git Bash for Unix commands\n\nMac: If commands fail, check if you need to install Xcode tools\n\nLinux: Usually works out of the box\n\nRemember: Everyone struggles with setup. Once it’s working, it just works.","type":"content","url":"/index-1#getting-help","position":17},{"hierarchy":{"lvl1":"Getting Started","lvl2":"Module Completion"},"type":"lvl2","url":"/index-1#module-completion","position":18},{"hierarchy":{"lvl1":"Getting Started","lvl2":"Module Completion"},"content":"🎉 Success Indicators\n\nYou’re ready for Python Fundamentals when you can:\n\nOpen terminal and navigate to any folder using cd and ls\n\nActivate your environment with conda activate astr596\n\nCreate a Git repository, make commits, and push to GitHub\n\nRun Python scripts from the command line\n\nFix basic issues using error messages and documentation\n\nIf you can do these five things, you have the foundation for everything in this course!","type":"content","url":"/index-1#module-completion","position":19},{"hierarchy":{"lvl1":"Getting Started","lvl2":"What’s Next?"},"type":"lvl2","url":"/index-1#whats-next","position":20},{"hierarchy":{"lvl1":"Getting Started","lvl2":"What’s Next?"},"content":"After completing this module, you’ll begin the Python Fundamentals module where you’ll:\n\nWrite your first object-oriented stellar evolution model\n\nUse the terminal skills daily for running scripts\n\nSubmit all work via Git and GitHub Classroom\n\nBuild on this foundation for the rest of your career\n\nHow to Succeed in This Module\n\nDon’t Rush: Setup problems compound. Better to spend an extra hour now than fight your environment all semester.\n\nPractice Daily: Spend 10 minutes each day using the terminal. Muscle memory develops quickly with consistent practice.\n\nCommit Often: Make Git commits after every small success. You can’t over-commit, but you can under-commit.\n\nRead Error Messages: They usually tell you exactly what’s wrong. Copy the full error when asking for help.\n\nKeep Notes: Document what worked for your specific system. You’ll need these notes again for future projects.\n\nRemember: These tools aren’t just course requirements—they’re essential skills for any scientific research career. The more you invest in mastering them now, the more prepared you’ll be for real research challenges ahead.\n\nReady to begin? Start with \n\nChapter 1: Command Line Interface →","type":"content","url":"/index-1#whats-next","position":21},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows"},"type":"lvl1","url":"/python-environment-v3","position":0},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows"},"content":"","type":"content","url":"/python-environment-v3","position":1},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl2":"Learning Objectives"},"type":"lvl2","url":"/python-environment-v3#learning-objectives","position":2},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl2":"Learning Objectives"},"content":"By the end of this chapter, you will be able to:\n\nConfigure and navigate IPython as your primary interactive computing environment\n\nDiagnose why identical code produces different results on different machines\n\nExplain how Python locates and loads code when you type import\n\nIdentify the hidden dangers of Jupyter notebooks that corrupt scientific results\n\nCreate reproducible computational environments using conda\n\nDebug environment problems systematically using diagnostic tools\n\nTransform notebook explorations into reproducible Python scripts\n\nExecute Python code effectively from the terminal and IPython environment","type":"content","url":"/python-environment-v3#learning-objectives","position":3},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl2":"Prerequisites Check"},"type":"lvl2","url":"/python-environment-v3#prerequisites-check","position":4},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl2":"Prerequisites Check"},"content":"✅ Before Starting This Chapter\n\nYou have completed the Getting Started module (basic setup, CLI, git)\n\nYou can navigate directories using cd, ls, and pwd\n\nYou have Miniforge installed with the astr596 environment created\n\nYou can activate your conda environment: conda activate astr596\n\nYou understand file paths (absolute vs. relative)\n\nIf any boxes are unchecked, review the Getting Started module first.","type":"content","url":"/python-environment-v3#prerequisites-check","position":5},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl2":"Chapter Overview"},"type":"lvl2","url":"/python-environment-v3#chapter-overview","position":6},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl2":"Chapter Overview"},"content":"Picture this: You download code from a groundbreaking astronomy paper, eager to reproduce their results. You run it exactly as instructed. Instead of the published results, you get error messages, or worse — completely different numbers with no indication why. This frustrating scenario happens to nearly every computational scientist, from undergraduates to professors. The problem isn’t bad code or user error; it’s that scientific computing happens in complex environments where tiny differences can cascade into complete failures.\n\nenvironment\nAn isolated Python installation with its own packages and settings\n\nThis chapter reveals the hidden machinery that makes Python work (or not work) on your computer. You’ll discover why the same code produces different results on different machines, master IPython as your computational laboratory, understand the dangers of Jupyter notebooks, and learn to create truly reproducible computational environments. These aren’t just technical skills — they’re the foundation of trustworthy computational science.\n\nBy chapter’s end, you’ll transform from someone who hopes code works to someone who knows exactly why it works (or doesn’t). You’ll diagnose “module not found” errors in seconds, create environments that work identically on any machine, and understand the critical difference between exploration and reproducible science. Let’s begin by exploring the tool that will become your new best friend: IPython.","type":"content","url":"/python-environment-v3#chapter-overview","position":7},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl2":"1.1 IPython: Your Computational Laboratory"},"type":"lvl2","url":"/python-environment-v3#id-1-1-ipython-your-computational-laboratory","position":8},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl2":"1.1 IPython: Your Computational Laboratory"},"content":"IPython\nInteractive Python - an enhanced interpreter for scientific computing\n\nWhile you could use the basic Python interpreter by typing python, IPython (just type ipython instead) transforms your terminal into a powerful environment for scientific exploration. Think of it as the difference between a basic calculator and a graphing calculator — both do math, but one is designed for serious work. Let’s see why every professional computational scientist prefers IPython over the basic Python REPL.\n\nREPL\nRead-Eval-Print Loop - an interactive programming environment","type":"content","url":"/python-environment-v3#id-1-1-ipython-your-computational-laboratory","position":9},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"Launching Your Laboratory","lvl2":"1.1 IPython: Your Computational Laboratory"},"type":"lvl3","url":"/python-environment-v3#launching-your-laboratory","position":10},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"Launching Your Laboratory","lvl2":"1.1 IPython: Your Computational Laboratory"},"content":"First, ensure you’re in the right environment, then launch IPython:\n\n# IMPORTANT: These are terminal commands, not Python code!\n# Type these in your terminal/command prompt, not in Python\n\n# Step 1: Open your terminal (Terminal on Mac/Linux, Anaconda Prompt on Windows)\n# Step 2: Activate your conda environment\nprint(\"In terminal: conda activate astr596\")\n\n# Step 3: Launch IPython\nprint(\"In terminal: ipython\")\n\n# You'll see something like this appear:\nprint(\"\"\"\nPython 3.11.5 | packaged by conda-forge\nIPython 8.14.0 -- An enhanced Interactive Python\nIn [1]: \n\"\"\")\n\n# The 'In [1]:' prompt means you're now in IPython, ready to type Python code\nprint(\"Now you can type Python commands at the In [1]: prompt\")\n\nNotice the prompt says In [1]: instead of >>> (which is what basic Python shows). This numbering system is your first hint that IPython is different — it remembers everything. Each command you type gets a number, making it easy to reference previous work.","type":"content","url":"/python-environment-v3#launching-your-laboratory","position":11},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"The Power of Memory","lvl2":"1.1 IPython: Your Computational Laboratory"},"type":"lvl3","url":"/python-environment-v3#the-power-of-memory","position":12},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"The Power of Memory","lvl2":"1.1 IPython: Your Computational Laboratory"},"content":"IPython maintains a complete history of your session, accessible through special variables:\n\n# Type these commands one at a time in IPython\nimport math\n\n# Earth's radius in km\nradius = 6371  \n\n# Calculate Earth's volume\nvolume = (4/3) * math.pi * radius**3\nprint(f\"Earth's volume: {volume:.2e} km³\")\n\n# In IPython, you can reference previous work:\nprint(\"In IPython, Out[n] stores outputs, In[n] stores inputs\")\nprint(\"The underscore _ references the last output\")\n\n🤔 Check Your Understanding\n\nWhat’s the difference between In[5] and Out[5] in IPython?\n\nSolution\n\nIn[5] contains the actual text/code you typed in cell 5 (as a string)\n\nOut[5] contains the result/value that cell 5 produced (if any)\n\nFor example:\n\nIn[5] might be \"2 + 2\"\n\nOut[5] would be 4\n\nThis history system lets you reference and reuse previous computations without retyping.","type":"content","url":"/python-environment-v3#the-power-of-memory","position":13},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"Tab Completion: Your Exploration Tool","lvl2":"1.1 IPython: Your Computational Laboratory"},"type":"lvl3","url":"/python-environment-v3#tab-completion-your-exploration-tool","position":14},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"Tab Completion: Your Exploration Tool","lvl2":"1.1 IPython: Your Computational Laboratory"},"content":"Tab completion helps you discover what’s available without memorizing everything:\n\nimport math\n\n# In real IPython, type: math.<TAB>\n# Shows all available functions\navailable_functions = [item for item in dir(math) if not item.startswith('_')]\nprint(\"math module contains:\", available_functions[:10], \"...\")\n\n# To see functions containing 'sin' (math.*sin*?<TAB> in IPython):\nsin_functions = [item for item in dir(math) if 'sin' in item]\nprint(\"\\nFunctions with 'sin':\", sin_functions)\n\n","type":"content","url":"/python-environment-v3#tab-completion-your-exploration-tool","position":15},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"Magic Commands: IPython’s Superpowers","lvl2":"1.1 IPython: Your Computational Laboratory"},"type":"lvl3","url":"/python-environment-v3#magic-commands-ipythons-superpowers","position":16},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"Magic Commands: IPython’s Superpowers","lvl2":"1.1 IPython: Your Computational Laboratory"},"content":"magic command\nIPython commands prefixed with % providing special functionality\n\nIPython’s magic commands give you capabilities far beyond standard Python:\n\n# In IPython, you would use: %timeit sum(range(1000))\n# Here we simulate timing comparisons\nimport timeit\n\n# Time list comprehension\ntime1 = timeit.timeit('[i**2 for i in range(100)]', number=10000)\nprint(f\"List comprehension: {time1*100:.4f} µs per loop\")\n\n# Time map/lambda approach\ntime2 = timeit.timeit('list(map(lambda x: x**2, range(100)))', number=10000)\nprint(f\"Map with lambda: {time2*100:.4f} µs per loop\")\n\n# Show which is faster\nprint(f\"\\nList comprehension is {time2/time1:.1f}x faster\")\n\n🚨 Common Bug Alert: Platform-Specific Timing\n\nTiming results vary significantly between machines due to:\n\nCPU speed and architecture\n\nSystem load and background processes\n\nPython version and compilation options\n\nNever assume timing results from one machine apply to another. Always benchmark on your target system.\n\n🌟 The More You Know: When 0.1 Seconds = 28 Dead\n\nTODO: should be be in Chapter 2 instead in the floating-point/precision section? Most likely yes\n\nOn February 25, 1991, during the Gulf War, a Patriot missile defense system failed to intercept an incoming Iraqi Scud missile, resulting in 28 American soldiers killed and 98 wounded. The cause? A timing error that accumulated due to floating-point precision issues.\n\nThe system’s internal clock counted time in tenths of seconds, but 0.1 cannot be exactly represented in binary floating-point arithmetic—it’s actually 0.099999999... in binary. After running for 100 hours, this tiny error had accumulated to 0.34 seconds. In that time, a Scud missile travels over 600 meters, causing the Patriot system to look in the wrong part of the sky (\n\nGAO Report, 1992).\n\nThe truly tragic part? The software fix had already been written and was in transit to the base when the attack occurred. As noted in the official report: “The Patriot system was designed to operate for only 14 hours at a time... No one expected the system to run continuously for 100 hours.”\n\nThis disaster teaches us two critical lessons: First, tiny numerical errors compound over time—what seems like a trivial rounding error can have deadly consequences. Second, environment assumptions matter—software designed for one use case (14-hour operation) failed catastrophically when used differently (100-hour operation). When you learn about floating-point precision in Chapter 2, remember: those “boring” numerical details can literally be matters of life and death.","type":"content","url":"/python-environment-v3#magic-commands-ipythons-superpowers","position":17},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"Getting Help Instantly","lvl2":"1.1 IPython: Your Computational Laboratory"},"type":"lvl3","url":"/python-environment-v3#getting-help-instantly","position":18},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"Getting Help Instantly","lvl2":"1.1 IPython: Your Computational Laboratory"},"content":"IPython makes documentation accessible without leaving your workflow:\n\nimport math\n\n# In real IPython, use: math.sqrt?\nprint(\"In IPython, use ? for quick help:\")\nprint(\"  math.sqrt?  - shows documentation\")\nprint(\"  math.sqrt?? - shows source code (if available)\")\nprint(\"\\nExample documentation for math.sqrt:\")\nprint(\"  Return the square root of x.\")\nprint(\"  Domain: x ≥ 0, Range: result ≥ 0\")\n\n💡 Computational Thinking: Interactive Exploration\n\nThe ability to quickly test ideas and explore APIs interactively is fundamental to computational science. IPython’s environment encourages experimentation:\n\nExplore → Test → Refine\n\nThis rapid iteration cycle is how algorithms are born and bugs are discovered. This pattern appears everywhere: interactive debuggers, REPLs in other languages, and computational notebooks all follow this explore-test-refine cycle.","type":"content","url":"/python-environment-v3#getting-help-instantly","position":19},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"Managing Your Workspace","lvl2":"1.1 IPython: Your Computational Laboratory"},"type":"lvl3","url":"/python-environment-v3#managing-your-workspace","position":20},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"Managing Your Workspace","lvl2":"1.1 IPython: Your Computational Laboratory"},"content":"\n\nimport sys\n\n# Create some variables for demonstration\ndata = [1, 2, 3, 4, 5]\nresult = sum(data)\nname = \"Earth\"\n\n# Show variables (simulating %who in IPython)\ncurrent_vars = [var for var in dir() \n                if not var.startswith('_') and var not in ['sys', 'timeit', 'math']]\nprint(\"Variables in workspace (%who in IPython):\", current_vars[:3])\n\n# Detailed info (simulating %whos in IPython)\nprint(\"\\nDetailed variable info (%whos in IPython):\")\nfor var in current_vars[:3]:\n    obj = eval(var)\n    print(f\"  {var:10} {type(obj).__name__:10} {str(obj)[:30]}\")\n\n🌟 The More You Know: The Reproducibility Crisis\n\nIn 2016, a survey in Nature found that more than 70% of researchers failed to reproduce another scientist’s experiments, and more than 50% failed to reproduce their own experiments (\n\nBaker, 2016). While this survey focused on experimental sciences, computational reproducibility faces unique challenges.\n\nA study by Stodden et al. (2018) attempted to reproduce 204 computational results from Science magazine and found that only 26% could be reproduced without author assistance (\n\nStodden et al., 2018). The main barriers? Missing dependencies, absent random seeds, and undocumented computational environments—exactly what you’re learning to manage with IPython’s session history and environment controls.\n\nTools like IPython’s %history and %save commands create an audit trail that helps ensure your future self (and others) can reproduce your work. It’s not just good practice—it’s becoming a requirement at many journals!","type":"content","url":"/python-environment-v3#managing-your-workspace","position":21},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl2":"1.2 Understanding Python’s Hidden Machinery"},"type":"lvl2","url":"/python-environment-v3#id-1-2-understanding-pythons-hidden-machinery","position":22},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl2":"1.2 Understanding Python’s Hidden Machinery"},"content":"module\nA Python file containing code that can be imported and used in other programs\n\nWhen you type a simple line like import math, a complex process unfolds behind the scenes. Understanding this machinery is the difference between guessing why code fails and knowing exactly how to fix it.","type":"content","url":"/python-environment-v3#id-1-2-understanding-pythons-hidden-machinery","position":23},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"The Import System Exposed","lvl2":"1.2 Understanding Python’s Hidden Machinery"},"type":"lvl3","url":"/python-environment-v3#the-import-system-exposed","position":24},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"The Import System Exposed","lvl2":"1.2 Understanding Python’s Hidden Machinery"},"content":"import system\nPython’s mechanism for loading code from external files\n\nLet’s peek behind the curtain to understand Python’s import system:\n\nimport sys\nfrom pathlib import Path\n\n# Where is Python running from?\nprint(f\"Python executable: {sys.executable}\")\n\n# What version are we using?\nprint(f\"Python version: {sys.version.split()[0]}\")\n\n# Where will Python look for modules?\nprint(\"\\nPython searches these locations (in order):\")\nfor i, path in enumerate(sys.path[:5], 1):\n    # Shorten paths for readability\n    display_path = str(path).replace(str(Path.home()), \"~\")\n    print(f\"  {i}. {display_path}\")\nprint(\"  ... and more\")\n\nsys.path\nPython’s list of directories to search when importing modules\n\nThis search path (sys.path) determines everything. When you import something, Python checks each directory in order and uses the first match it finds.","type":"content","url":"/python-environment-v3#the-import-system-exposed","position":25},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"Debugging Import Problems","lvl2":"1.2 Understanding Python’s Hidden Machinery"},"type":"lvl3","url":"/python-environment-v3#debugging-import-problems","position":26},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"Debugging Import Problems","lvl2":"1.2 Understanding Python’s Hidden Machinery"},"content":"Here’s a diagnostic function you’ll use throughout your career, broken into manageable stages:\n\nStage 1: Check Environment\n\ndef check_python_environment():\n    \"\"\"Verify we're in the correct Python environment.\"\"\"\n    import sys\n    \n    env_path = sys.executable\n    if 'astr596' in env_path:\n        return True, f\"✓ Correct environment: {env_path}\"\n    return False, f\"✗ Wrong environment: {env_path}\"\n\nstatus, message = check_python_environment()\nprint(message)\n\nStage 2: Try Import\n\ndef try_import(module_name):\n    \"\"\"Attempt to import a module with informative error.\"\"\"\n    try:\n        module = __import__(module_name)\n        if hasattr(module, '__file__'):\n            location = module.__file__\n        else:\n            location = \"built-in module\"\n        return True, f\"✓ Found: {location}\"\n    except ImportError as e:\n        return False, f\"✗ Failed: {str(e)}\"\n\n# Test with standard library\nsuccess, msg = try_import('math')\nprint(msg)\n\nStage 3: Suggest Fix\n\ndef suggest_import_fix(module_name, error):\n    \"\"\"Suggest solutions for import failures.\"\"\"\n    if \"No module named\" in str(error):\n        return f\"Install with: conda install {module_name}\"\n    elif \"cannot import name\" in str(error):\n        return \"Check version compatibility\"\n    return \"Verify environment activation\"\n\n# Example usage\nfix = suggest_import_fix('numpy', \"No module named 'numpy'\")\nprint(fix)\n\n🤔 Check Your Understanding\n\nYou get ModuleNotFoundError: No module named 'astropy'. What are three possible causes?\n\nSolution\n\nWrong environment: You’re not in the conda environment where astropy is installed\n\nNot installed: Astropy isn’t installed in the current environment\n\nPath issues: Python’s sys.path doesn’t include the directory containing astropy\n\nTo diagnose:which python          # Are you using the right Python?\nconda list astropy    # Is it installed?\npython -c \"import sys; print(sys.path)\"  # Where is Python looking?\n\nThe most common cause is forgetting to activate your conda environment!","type":"content","url":"/python-environment-v3#debugging-import-problems","position":27},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"Multiple Pythons: A Common Disaster","lvl2":"1.2 Understanding Python’s Hidden Machinery"},"type":"lvl3","url":"/python-environment-v3#multiple-pythons-a-common-disaster","position":28},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"Multiple Pythons: A Common Disaster","lvl2":"1.2 Understanding Python’s Hidden Machinery"},"content":"Most systems have multiple Python installations, leading to confusion:\n\nfrom pathlib import Path\n\n# Common Python locations on Unix-like systems\npossible_pythons = [\n    '/usr/bin/python3',          # System Python\n    '/usr/local/bin/python3',    # Homebrew Python (Mac)\n    '~/miniforge3/bin/python',   # Conda Python\n    '~/.pyenv/shims/python',     # Pyenv Python\n]\n\nprint(\"Potential Python locations on Unix-like systems:\")\nfor path in possible_pythons:\n    expanded_path = Path(path).expanduser()\n    exists = \"✓\" if expanded_path.exists() else \"✗\"\n    print(f\"  {exists} {path}\")\n\nprint(\"\\nThis is why 'conda activate' is crucial!\")\n\n🚨 Common Bug Alert: The Wrong Python\n\nSymptom: Code works in terminal but fails in IDE, or vice versa\n\nCause: Different tools using different Python installations\n\nFix: Always verify with:which python       # Unix/Mac\nwhere python       # Windows\n\nPrevention: Always activate your conda environment first!","type":"content","url":"/python-environment-v3#multiple-pythons-a-common-disaster","position":29},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl2":"1.3 Jupyter Notebooks: Beautiful Disasters Waiting to Happen"},"type":"lvl2","url":"/python-environment-v3#id-1-3-jupyter-notebooks-beautiful-disasters-waiting-to-happen","position":30},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl2":"1.3 Jupyter Notebooks: Beautiful Disasters Waiting to Happen"},"content":"Jupyter notebook\nWeb-based platform that executes code in cells while maintaining state\n\nJupyter notebooks seem perfect for scientific computing - you can mix code, results, and explanations in one document. However, they harbor dangerous flaws that can corrupt your scientific results. You may use them for Short Project 1 to understand their appeal, but then you must abandon them for more robust approaches.","type":"content","url":"/python-environment-v3#id-1-3-jupyter-notebooks-beautiful-disasters-waiting-to-happen","position":31},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"The Seductive Power of Notebooks","lvl2":"1.3 Jupyter Notebooks: Beautiful Disasters Waiting to Happen"},"type":"lvl3","url":"/python-environment-v3#the-seductive-power-of-notebooks","position":32},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"The Seductive Power of Notebooks","lvl2":"1.3 Jupyter Notebooks: Beautiful Disasters Waiting to Happen"},"content":"To start Jupyter (after activating your environment):\n\n# In terminal:\n# conda activate astr596\n# jupyter lab\n\nprint(\"Jupyter Lab would open at: http://localhost:8888\")\nprint(\"You can create notebooks, write code in cells, and see results inline\")\n\n","type":"content","url":"/python-environment-v3#the-seductive-power-of-notebooks","position":33},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"The Hidden State Monster","lvl2":"1.3 Jupyter Notebooks: Beautiful Disasters Waiting to Happen"},"type":"lvl3","url":"/python-environment-v3#the-hidden-state-monster","position":34},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"The Hidden State Monster","lvl2":"1.3 Jupyter Notebooks: Beautiful Disasters Waiting to Happen"},"content":"The most insidious problem: notebooks maintain the hidden state between cell executions:\n\n# Simulating notebook cells with execution order problems\n\n# Cell 1 (first execution)\ngravity = 980  # cm/s², Earth's gravity\nprint(f\"Cell 1: Set gravity = {gravity}\")\n\n# Cell 2 (depends on gravity)\nimport math\ndef calculate_fall_time(height):\n    \"\"\"Calculate time using gravity from when function was created.\"\"\"\n    return math.sqrt(2 * height / gravity)\n\nprint(f\"Cell 2: Defined function with gravity = {gravity}\")\n\n# Cell 3 (changes gravity)\ngravity = 371  # cm/s², Mars gravity\nprint(f\"Cell 3: Changed gravity = {gravity}\")\n\n# Cell 4 (which gravity does this use?)\ntime = calculate_fall_time(100)\nprint(f\"Cell 4: Fall time = {time:.2f} seconds\")\nprint(f\"  But function still uses gravity = 980!\")\nprint(f\"  This hidden state causes wrong results!\")\n\n🔧 Debug This!\n\nA student’s notebook has these cells:Cell 1: data = [1, 2, 3]\nCell 2: result = sum(data) / len(data)\nCell 3: data.append(4)\nCell 4: print(f\"Average: {result}\")\n\nThey run cells in order: 1, 2, 3, 4, 2, 4. What prints the second time?\n\nSolution\n\nThe second execution of Cell 4 prints: Average: 2.5\n\nExecution trace:\n\nCell 1: data = [1, 2, 3]\n\nCell 2: result = 2.0 (sum=6, len=3)\n\nCell 3: data = [1, 2, 3, 4]\n\nCell 4: Prints \"Average: 2.0\"\n\nCell 2 again: result = 2.5 (sum=10, len=4)\n\nCell 4 again: Prints \"Average: 2.5\"\n\nThis demonstrates how re-running cells creates different states than sequential execution!","type":"content","url":"/python-environment-v3#the-hidden-state-monster","position":35},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"Memory Accumulation Disasters","lvl2":"1.3 Jupyter Notebooks: Beautiful Disasters Waiting to Happen"},"type":"lvl3","url":"/python-environment-v3#memory-accumulation-disasters","position":36},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"Memory Accumulation Disasters","lvl2":"1.3 Jupyter Notebooks: Beautiful Disasters Waiting to Happen"},"content":"\n\nimport sys\n\n# Simulating repeated cell execution\nbig_data = []\n\nprint(\"Initial memory state\")\n\n# First run of the cell\nfor i in range(100):\n    big_data.append([0] * 1000)\n    \n# Calculate approximate memory usage\nsize_mb = sys.getsizeof(big_data) / (1024 * 1024)\nprint(f\"After 1st run: ~{size_mb:.1f} MB\")\n\n# Second run (accumulates!)\nfor i in range(100):\n    big_data.append([0] * 1000)\n    \nsize_mb = sys.getsizeof(big_data) / (1024 * 1024)\nprint(f\"After 2nd run: ~{size_mb:.1f} MB\")\n\nprint(\"\\nEach run ADDS to memory - notebooks don't reset!\")\n\n🌟 The More You Know: The $6 Trillion Excel Error\n\nIn 2013, graduate student Thomas Herndon couldn’t reproduce the results from a highly influential economics paper by Carmen Reinhart and Kenneth Rogoff. This paper, \n\n“Growth in a Time of Debt,” had been cited by politicians worldwide to justify austerity policies affecting millions of people.\n\nWhen Herndon finally obtained the original Excel spreadsheet, he discovered a coding error: five countries were accidentally excluded from a calculation due to an Excel formula that didn’t include all rows. This simple mistake skewed the results, showing that high debt caused negative growth when the corrected analysis showed much weaker effects (\n\nHerndon, Ash, and Pollin, 2014). The implications were staggering — this spreadsheet error influenced global economic policy.\n\nJust like hidden state in Jupyter notebooks, the error was invisible in the final spreadsheet. The lesson? Computational transparency and reproducibility aren’t just academic exercises — they have real-world consequences. Always make your computational process visible and reproducible!","type":"content","url":"/python-environment-v3#memory-accumulation-disasters","position":37},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"The Notebook-to-Script Transition","lvl2":"1.3 Jupyter Notebooks: Beautiful Disasters Waiting to Happen"},"type":"lvl3","url":"/python-environment-v3#the-notebook-to-script-transition","position":38},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"The Notebook-to-Script Transition","lvl2":"1.3 Jupyter Notebooks: Beautiful Disasters Waiting to Happen"},"content":"script\nPlain text file (.py extension) with Python code that executes top-to-bottom\n\nAfter Project 1, we’ll abandon notebooks for scripts. Here’s why scripts are superior:\n\nScript vs Notebook Comparison\n\nAspect\n\nNotebooks\n\nScripts\n\nExecution Order\n\nAmbiguous, user-determined\n\nTop-to-bottom, always\n\nHidden State\n\nAccumulates invisibly\n\nFresh start each run\n\nVersion Control\n\nJSON mess with outputs\n\nClean text diffs\n\nTesting\n\nNearly impossible\n\nStraightforward\n\nDebugging\n\nCell-by-cell only\n\nProfessional tools\n\nReproducibility\n\nOften impossible\n\nGuaranteed\n\n💡 Computational Thinking: Reproducible by Design\n\nReproducibility isn’t just about sharing code — it’s about ensuring that code produces identical results regardless of who runs it or when. Scripts enforce this by eliminating hidden state and ambiguous execution order.\n\nThis principle extends beyond Python: declarative configurations, containerization, and infrastructure-as-code all follow the same philosophy of explicit, reproducible computation.\n\nRemember the mantra: “It should work the same way every time, for everyone.”","type":"content","url":"/python-environment-v3#the-notebook-to-script-transition","position":39},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl2":"1.4 Scripts: Write Once, Run Anywhere (Correctly)"},"type":"lvl2","url":"/python-environment-v3#id-1-4-scripts-write-once-run-anywhere-correctly","position":40},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl2":"1.4 Scripts: Write Once, Run Anywhere (Correctly)"},"content":"Python scripts are simple text files containing Python code, executed from top to bottom, the same way every time. No hidden state, no ambiguity, just predictable execution.","type":"content","url":"/python-environment-v3#id-1-4-scripts-write-once-run-anywhere-correctly","position":41},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"From IPython to Script","lvl2":"1.4 Scripts: Write Once, Run Anywhere (Correctly)"},"type":"lvl3","url":"/python-environment-v3#from-ipython-to-script","position":42},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"From IPython to Script","lvl2":"1.4 Scripts: Write Once, Run Anywhere (Correctly)"},"content":"Start by experimenting in IPython:\n\n# Quick calculation in IPython\nearth_mass = 5.97e27  # g\nmoon_mass = 7.35e25   # g\nratio = earth_mass / moon_mass\nprint(f\"Earth is {ratio:.1f}× more massive than the Moon\")\n\nNow create a proper script. Save this as mass_ratio.py:\n\n#!/usr/bin/env python\n\"\"\"Calculate mass ratios between celestial bodies.\"\"\"\n\n# Constants in CGS units [g]\nEARTH_MASS = 5.97e27\nMOON_MASS = 7.35e25\nSUN_MASS = 1.99e33\n\ndef calculate_ratio(mass1, mass2):\n    \"\"\"Calculate mass ratio between two bodies.\n    \n    Args:\n        mass1: Mass of first body [g]\n        mass2: Mass of second body [g]\n        \n    Returns:\n        float: Ratio of mass1 to mass2\n    \"\"\"\n    if mass2 == 0:\n        raise ValueError(\"Cannot divide by zero mass\")\n    return mass1 / mass2\n\ndef main():\n    \"\"\"Main execution function.\"\"\"\n    # Earth to Moon ratio\n    earth_moon = calculate_ratio(EARTH_MASS, MOON_MASS)\n    print(f\"Earth is {earth_moon:.1f}× more massive than the Moon\")\n    \n    # Sun to Earth ratio\n    sun_earth = calculate_ratio(SUN_MASS, EARTH_MASS)\n    print(f\"Sun is {sun_earth:.0f}× more massive than Earth\")\n\n# This pattern makes the script both runnable and importable\nif __name__ == \"__main__\":\n    main()\n\n","type":"content","url":"/python-environment-v3#from-ipython-to-script","position":43},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"The if __name__ == \"__main__\" Pattern","lvl2":"1.4 Scripts: Write Once, Run Anywhere (Correctly)"},"type":"lvl3","url":"/python-environment-v3#the-if-name-main-pattern","position":44},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"The if __name__ == \"__main__\" Pattern","lvl2":"1.4 Scripts: Write Once, Run Anywhere (Correctly)"},"content":"name\nPython variable that equals “main” when run directly, or the module name when imported\n\nThis crucial pattern makes your code both runnable and importable. Don’t worry if this seems confusing at first — it’s a pattern you’ll use in every script you write, and it will become second nature:\n\n# Understanding the __name__ variable\ndef useful_function(x):\n    \"\"\"A function others might want to use.\"\"\"\n    return x ** 2\n\n# Python automatically sets __name__ based on how the file is used\nprint(f\"Module's __name__ is: {__name__}\")\n\n# This is the magic pattern - memorize it even if you don't fully understand it yet\nif __name__ == \"__main__\":\n    # This code ONLY runs when you execute the script directly\n    # It does NOT run when someone imports your script\n    print(\"Running as a script!\")\n    result = useful_function(5)\n    print(f\"5 squared is {result}\")\n    \n# Think of it like this:\n# - Running directly: Python sets __name__ to \"__main__\", so the code runs\n# - Importing: Python sets __name__ to the filename, so the code doesn't run\n\n🤔 Check Your Understanding\n\nWhy would you want code that behaves differently when imported versus run directly?\n\nSolution\n\nThis pattern serves multiple purposes:\n\nTesting: Include test code that runs when developing but not when others use your functions\n\nReusability: Others can import your functions without triggering test/demo code\n\nLibrary Design: Create modules that work both as tools and standalone programs\n\nDevelopment: Test functions immediately while writing them\n\nExample:# orbital_mechanics.py\ndef orbital_period(a, M):\n    # ... calculation ...\n    return period\n\nif __name__ == \"__main__\":\n    # Test with Earth's orbit\n    period = orbital_period(1.496e13, 1.989e33)\n    print(f\"Earth's period: {period/86400:.1f} days\")","type":"content","url":"/python-environment-v3#the-if-name-main-pattern","position":45},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl2":"1.5 Creating Reproducible Environments"},"type":"lvl2","url":"/python-environment-v3#id-1-5-creating-reproducible-environments","position":46},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl2":"1.5 Creating Reproducible Environments"},"content":"conda\nPackage and environment manager for isolated Python installations\n\nvirtual environment\nAn isolated Python installation with its own packages, preventing conflicts between projects\n\nYour code’s behavior depends on its environment — Python version, installed packages, even operating system. Creating reproducible environments ensures your code works identically everywhere.","type":"content","url":"/python-environment-v3#id-1-5-creating-reproducible-environments","position":47},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"The Conda Solution","lvl2":"1.5 Creating Reproducible Environments"},"type":"lvl3","url":"/python-environment-v3#the-conda-solution","position":48},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"The Conda Solution","lvl2":"1.5 Creating Reproducible Environments"},"content":"Conda creates isolated environments — separate Python installations with their own packages:\n\nprint(\"\"\"Essential conda commands:\n\n# Create new environment with specific Python version\nconda create -n myproject python=3.11\n\n# Activate environment (ALWAYS do this first!)\nconda activate myproject\n\n# Install packages\nconda install numpy scipy matplotlib\n\n# List installed packages\nconda list\n\n# Deactivate when done\nconda deactivate\n\n# Remove environment completely\nconda env remove -n myproject\n\"\"\")\n\n","type":"content","url":"/python-environment-v3#the-conda-solution","position":49},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"Environment Files: Reproducibility in Practice","lvl2":"1.5 Creating Reproducible Environments"},"type":"lvl3","url":"/python-environment-v3#environment-files-reproducibility-in-practice","position":50},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"Environment Files: Reproducibility in Practice","lvl2":"1.5 Creating Reproducible Environments"},"content":"Create an environment.yml file that others can use to recreate your exact setup:\n\nenvironment_yml = \"\"\"name: astr596_project\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.11\n  - numpy=1.24\n  - scipy=1.11\n  - matplotlib=3.7\n  - ipython\n  - jupyter\n  - pip\n  - pip:\n    - astroquery==0.4.6\n\"\"\"\n\nprint(\"environment.yml content:\")\nprint(environment_yml)\n\nprint(\"\\nOthers recreate your environment with:\")\nprint(\"conda env create -f environment.yml\")\nprint(\"conda activate astr596_project\")\n\n🚨 Common Bug Alert: Channel Confusion\n\nProblem: Package not found or wrong version installed\n\nCause: Different conda channels have different package versions\n\nSolution: Always specify channels in environment.yml\n\nBest Practice: Use conda-forge channel for scientific packageschannels:\n  - conda-forge  # Always list this first\n  - defaults     # Fallback if needed","type":"content","url":"/python-environment-v3#environment-files-reproducibility-in-practice","position":51},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"Proper Path Management","lvl2":"1.5 Creating Reproducible Environments"},"type":"lvl3","url":"/python-environment-v3#proper-path-management","position":52},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"Proper Path Management","lvl2":"1.5 Creating Reproducible Environments"},"content":"Stop hardcoding paths that break on other systems. If you’ve only used Jupyter notebooks, you might not realize that file paths like C:\\Users\\YourName\\data.txt only work on YOUR computer:\n\nfrom pathlib import Path\nimport os\n\n# BAD: Only works on one specific computer\nbad_path = '/Users/yourname/research/data.txt'\nprint(f\"BAD (hardcoded): {bad_path}\")\nprint(\"  Problem: This exact folder structure doesn't exist on other computers!\")\n\n# GOOD: Works everywhere (relative to where your script is)\nscript_dir = Path.cwd()  # cwd = \"current working directory\" (where you are now)\ndata_file = script_dir / 'data' / 'observations.txt'  # The / operator joins paths!\nprint(f\"\\nGOOD (relative): {data_file}\")\nprint(\"  This looks for 'data' folder relative to where your script runs\")\n\n# BETTER: Handle missing files gracefully (defensive programming!)\nif data_file.exists():\n    print(f\"\\n✓ Found data at: {data_file}\")\nelse:\n    print(f\"\\n✗ Data not found at: {data_file}\")\n    print(f\"    (Expected - this is just a demo)\")\n\n# BEST: Use configuration with environment variables\n# Environment variables are settings your computer stores\n# You can set DATA_DIR to different paths on different computers\ndata_dir = Path(os.getenv('DATA_DIR', './data'))  # Use DATA_DIR or default to ./data\nprint(f\"\\nBEST (configurable): {data_dir}\")\nprint(\"  Falls back to './data' if DATA_DIR environment variable not set\")\n\n","type":"content","url":"/python-environment-v3#proper-path-management","position":53},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"Random Seed Control","lvl2":"1.5 Creating Reproducible Environments"},"type":"lvl3","url":"/python-environment-v3#random-seed-control","position":54},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"Random Seed Control","lvl2":"1.5 Creating Reproducible Environments"},"content":"Make “random” results reproducible:\n\nimport random\n\ndef reproducible_random(seed=42):\n    \"\"\"Generate reproducible random numbers.\n    \n    Args:\n        seed: Random seed for reproducibility\n        \n    Returns:\n        list: Five random numbers (always same for same seed)\n    \"\"\"\n    random.seed(seed)\n    \n    # These will be the same every time with same seed\n    values = [random.random() for _ in range(5)]\n    return values\n\n# Run multiple times - same results with same seed\nprint(\"First run: \", [f\"{x:.3f}\" for x in reproducible_random(42)])\nprint(\"Second run:\", [f\"{x:.3f}\" for x in reproducible_random(42)])\n\n# Different seed = different results\nprint(\"New seed:  \", [f\"{x:.3f}\" for x in reproducible_random(137)])\n\nprint(\"\\nAlways document your random seeds in papers!\")\n\n🌟 The More You Know: Reproducing a Nobel Prize Discovery\n\nWhen the LIGO Scientific Collaboration announced the first detection of gravitational waves on February 11, 2016, the scientific community demanded proof. This wasn’t just any claim - it confirmed a 100-year-old prediction by Einstein and would earn the 2017 Nobel Prize in Physics.\n\nThe LIGO team didn’t just publish their results (\n\nAbbott et al., 2016) - they provided complete reproducibility. They released:\n\nRaw data: 32 seconds of strain data around the event (GW150914) from both detectors\n\nAnalysis code: Complete Python scripts and Jupyter notebooks on \n\nGitHub\n\nEnvironment specifications: Exact versions of all software dependencies\n\nRandom seeds: For all Monte Carlo analyses in their statistical validation\n\nThe result? Within days, independent scientists worldwide confirmed the detection. As the team noted in their software paper, “The analyses used to support the first direct detection... have been reproduced by independent scientists using the publicly available data and software” (\n\nAbbott et al., 2021).\n\nThe LIGO team even created “blind injections” — fake signals secretly added to the data stream to test their analysis pipeline. Only after the analysis was complete would they reveal whether a signal was real or injected. This prevented confirmation bias and ensured their computational methods were robust.\n\nYour random.seed() calls and environment files might seem trivial now, but they’re the same practices that enabled one of the most important physics discoveries of the 21st century to be independently verified. When you set a random seed, you’re following in the footsteps of Nobel laureates!","type":"content","url":"/python-environment-v3#random-seed-control","position":55},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl2":"1.6 Essential Debugging Strategies"},"type":"lvl2","url":"/python-environment-v3#id-1-6-essential-debugging-strategies","position":56},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl2":"1.6 Essential Debugging Strategies"},"content":"defensive programming\nWriting code that anticipates and handles failures gracefully\n\nWhen code fails (and it will), systematic debugging saves hours of frustration. Here are strategies that work every time, demonstrating defensive programming.","type":"content","url":"/python-environment-v3#id-1-6-essential-debugging-strategies","position":57},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"The Universal First Check","lvl2":"1.6 Essential Debugging Strategies"},"type":"lvl3","url":"/python-environment-v3#the-universal-first-check","position":58},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"The Universal First Check","lvl2":"1.6 Essential Debugging Strategies"},"content":"Before anything else, verify your environment:\n\nimport sys\nimport os\nfrom pathlib import Path\n\ndef environment_check():\n    \"\"\"Universal debugging first check.\n    \n    Returns:\n        bool: True if in correct environment\n    \"\"\"\n    print(\"=== Environment Debug Check ===\")\n    print(f\"Python: {sys.executable}\")\n    print(f\"Version: {sys.version.split()[0]}\")\n    print(f\"Current dir: {os.getcwd()}\")\n    \n    # Check for conda environment\n    if 'conda' in sys.executable or 'miniconda' in sys.executable:\n        # Extract environment name\n        path_parts = sys.executable.split(os.sep)\n        if 'envs' in path_parts:\n            env_idx = path_parts.index('envs')\n            env_name = path_parts[env_idx + 1] if env_idx + 1 < len(path_parts) else \"base\"\n            \n            if 'astr596' in env_name:\n                print(f\"✓ Correct environment: {env_name}\")\n                return True\n            else:\n                print(f\"✗ Wrong environment: {env_name}\")\n                print(\"  Fix: conda activate astr596\")\n                return False\n    \n    print(\"✗ Not in a conda environment\")\n    print(\"  Fix: conda activate astr596\")\n    return False\n\n# Run the check\nenvironment_check()\n\n","type":"content","url":"/python-environment-v3#the-universal-first-check","position":59},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"Using IPython’s Debugger","lvl2":"1.6 Essential Debugging Strategies"},"type":"lvl3","url":"/python-environment-v3#using-ipythons-debugger","position":60},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"Using IPython’s Debugger","lvl2":"1.6 Essential Debugging Strategies"},"content":"When code crashes, IPython’s %debug magic enters the debugger at the crash point:\n\ndef divide_list(numbers, divisor):\n    \"\"\"Divide all numbers in a list.\n    \n    Args:\n        numbers: List of numbers to divide\n        divisor: Number to divide by\n        \n    Returns:\n        list: Results of division\n    \"\"\"\n    # This will crash if divisor is zero\n    return [n / divisor for n in numbers]\n\nprint(\"\"\"In IPython, after an error occurs, type: %debug\n\nDebugger commands:\n  p variable  - print variable value\n  l          - list code around error\n  u/d        - go up/down the call stack\n  c          - continue execution\n  n          - next line\n  s          - step into function\n  q          - quit debugger\n  \nExample session:\n  ipdb> p divisor\n  0\n  ipdb> p numbers\n  [1, 2, 3]\n  ipdb> q\n\"\"\")\n\n💡 Computational Thinking: Defensive Programming\n\nDefensive programming means assuming things will go wrong and coding accordingly. Instead of hoping files exist, check first. Instead of assuming imports work, verify them.\n\nThis mindset—expect failure, handle it gracefully — separates robust scientific code from scripts that work “sometimes.”\n\nPattern:# Fragile code\ndata = open('file.txt').read()\n\n# Defensive code\nif Path('file.txt').exists():\n    with open('file.txt') as f:\n        data = f.read()\nelse:\n    print(\"Warning: file.txt not found, using defaults\")\n    data = default_data\n\nThis pattern appears everywhere: network requests that might timeout, sensors that might malfunction, or data that might be corrupted.","type":"content","url":"/python-environment-v3#using-ipythons-debugger","position":61},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl2":"Practice Exercises"},"type":"lvl2","url":"/python-environment-v3#practice-exercises","position":62},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl2":"Practice Exercises"},"content":"","type":"content","url":"/python-environment-v3#practice-exercises","position":63},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"Exercise 1.1: IPython Mastery","lvl2":"Practice Exercises"},"type":"lvl3","url":"/python-environment-v3#exercise-1-1-ipython-mastery","position":64},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"Exercise 1.1: IPython Mastery","lvl2":"Practice Exercises"},"content":"Part A: Follow These Steps (5 min)\n\nExecute these commands exactly in IPython:\n\n# Step 1: Import and explore\nimport math\n\n# Step 2: Check available functions\nprint(\"Functions:\", [x for x in dir(math) if not x.startswith('_')][:5])\n\n# Step 3: Time a simple calculation\nimport timeit\ntime = timeit.timeit('sum(range(100))', number=10000)\nprint(f\"Time: {time*100:.4f} µs per loop\")\n\nPart B: Modify the Approach (10 min)\n\nCompare two methods for squaring numbers:\n\nimport timeit\n\n# Method 1: List comprehension\ntime1 = timeit.timeit('[i**2 for i in range(100)]', number=1000)\n\n# Method 2: Map with lambda\ntime2 = timeit.timeit('list(map(lambda x: x**2, range(100)))', number=1000)\n\nprint(f\"List comp: {time1*1000:.2f} ms\")\nprint(f\"Map/lambda: {time2*1000:.2f} ms\")\nprint(f\"Ratio: {time2/time1:.2f}x\")\n\n# Which is faster? Why?\n\nPart C: Apply Your Knowledge (15 min)\n\nDesign your own timing experiment for calculating factorials:\n\n# Create three different ways to calculate factorials for 1-20\n# Hint: Consider recursion, iteration, and math.factorial\n\nimport math\nimport timeit\n\n# Method 1: Using math.factorial\ndef method1(n):\n    return [math.factorial(i) for i in range(1, n+1)]\n\n# Method 2: Iterative approach\ndef method2(n):\n    results = []\n    for i in range(1, n+1):\n        fact = 1\n        for j in range(1, i+1):\n            fact *= j\n        results.append(fact)\n    return results\n\n# Method 3: Recursive (be careful with large n!)\ndef factorial_recursive(n):\n    if n <= 1:\n        return 1\n    return n * factorial_recursive(n-1)\n\ndef method3(n):\n    return [factorial_recursive(i) for i in range(1, n+1)]\n\n# Time each approach\nt1 = timeit.timeit('method1(20)', globals=globals(), number=1000)\nt2 = timeit.timeit('method2(20)', globals=globals(), number=1000)\nt3 = timeit.timeit('method3(20)', globals=globals(), number=1000)\n\nprint(f\"math.factorial: {t1*1000:.2f} ms\")\nprint(f\"Iterative:      {t2*1000:.2f} ms\")\nprint(f\"Recursive:      {t3*1000:.2f} ms\")\nprint(f\"\\nFastest: math.factorial (built-in C implementation)\")","type":"content","url":"/python-environment-v3#exercise-1-1-ipython-mastery","position":65},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"Exercise 1.2: Notebook State Detective","lvl2":"Practice Exercises"},"type":"lvl3","url":"/python-environment-v3#exercise-1-2-notebook-state-detective","position":66},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"Exercise 1.2: Notebook State Detective","lvl2":"Practice Exercises"},"content":"Part A: Trace Execution (5 min)\n\nGiven this execution order, trace the state:\n\nprint(\"\"\"Execution order: Cell 1, Cell 3, Cell 2, Cell 4, Cell 2, Cell 4\n\nCell 1: counter = 0\n        data = []\n\nCell 2: counter += 1\n        data.append(counter)\n\nCell 3: counter = 10\n\nCell 4: print(f\"Counter: {counter}, Data: {data}\")\n\nWork through each step on paper first.\"\"\")\n\nPart B: Find the Final State (10 min)\n\n# Simulate the execution\ncounter = 0      # Cell 1\ndata = []        # Cell 1\ncounter = 10     # Cell 3\ncounter += 1     # Cell 2\ndata.append(counter)  # Cell 2\nprint(f\"After first Cell 4: Counter: {counter}, Data: {data}\")\n\ncounter += 1     # Cell 2 again\ndata.append(counter)  # Cell 2 again\nprint(f\"After second Cell 4: Counter: {counter}, Data: {data}\")\n\nPart C: Explain the Danger (15 min)\n\nWrite a paragraph explaining why this execution pattern would be:\n\nHard to debug\n\nImpossible to reproduce\n\nDangerous for scientific computing\n\nConsider: What if this was calculating orbital trajectories?","type":"content","url":"/python-environment-v3#exercise-1-2-notebook-state-detective","position":67},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"Exercise 1.3: Environment Diagnostic Tool","lvl2":"Practice Exercises"},"type":"lvl3","url":"/python-environment-v3#exercise-1-3-environment-diagnostic-tool","position":68},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"Exercise 1.3: Environment Diagnostic Tool","lvl2":"Practice Exercises"},"content":"Part A: Basic Diagnostic (5 min)\n\nimport sys\nimport os\n\n# Basic environment info\nprint(f\"Python: {sys.version.split()[0]}\")\nprint(f\"Location: {sys.executable}\")\nprint(f\"Working dir: {os.getcwd()}\")\n\nPart B: Check Key Packages (10 min)\n\n# Check for scientific packages\npackages = ['numpy', 'scipy', 'matplotlib', 'pandas']\n\nfor pkg in packages:\n    try:\n        __import__(pkg)\n        print(f\"✓ {pkg}\")\n    except ImportError:\n        print(f\"✗ {pkg}\")\n\nPart C: Complete Diagnostic (15 min)\n\nimport sys\nimport subprocess\nfrom pathlib import Path\nimport os\n\ndef full_diagnostic():\n    \"\"\"Complete environment diagnostic.\"\"\"\n    print(\"=== Full Environment Diagnostic ===\\n\")\n    \n    # 1. Python info\n    print(f\"1. Python: {sys.version.split()[0]}\")\n    print(f\"   Location: {sys.executable}\")\n    \n    # 2. Environment name\n    if 'conda' in sys.executable:\n        parts = sys.executable.split(os.sep)\n        if 'envs' in parts:\n            idx = parts.index('envs')\n            env = parts[idx + 1] if idx + 1 < len(parts) else \"base\"\n            print(f\"\\n2. Environment: {env}\")\n    \n    # 3. Search paths (first 3)\n    print(\"\\n3. Python paths:\")\n    for i, path in enumerate(sys.path[:3], 1):\n        display = str(path).replace(str(Path.home()), \"~\")\n        print(f\"   {i}. {display}\")\n    \n    # 4. Package count\n    try:\n        result = subprocess.run(\n            [sys.executable, '-m', 'pip', 'list'],\n            capture_output=True, text=True, timeout=5\n        )\n        count = len(result.stdout.strip().split('\\n')) - 2\n        print(f\"\\n4. Total packages: {count}\")\n    except:\n        print(\"\\n4. Could not count packages\")\n    \n    return True\n\nfull_diagnostic()","type":"content","url":"/python-environment-v3#exercise-1-3-environment-diagnostic-tool","position":69},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"Exercise 1.4: Variable Star Exercise Thread","lvl2":"Practice Exercises"},"type":"lvl3","url":"/python-environment-v3#exercise-1-4-variable-star-exercise-thread","position":70},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"Exercise 1.4: Variable Star Exercise Thread","lvl2":"Practice Exercises"},"content":"Chapter 1: Variable Star Exercise Thread\n\n# Chapter 1: Variable Star Basics - Setting up our environment\n# This simple start will grow into a full VariableStar class by Chapter 6\n\n# Define basic properties of a Cepheid variable star\nstar_name = \"Delta Cephei\"\nperiod = 5.366319  # days\nmagnitude_mean = 3.95\nmagnitude_amplitude = 0.88\n\n# Store in a simple dictionary for now\nvariable_star = {\n    'name': star_name,\n    'period': period,\n    'mag_mean': magnitude_mean,\n    'mag_amp': magnitude_amplitude\n}\n\nprint(f\"Variable star {variable_star['name']}:\")\nprint(f\"  Period: {variable_star['period']:.3f} days\")\nprint(f\"  Magnitude: {variable_star['mag_mean']:.2f} ± {variable_star['mag_amp']:.2f}\")\n\n# Save to file for next chapter with error handling (defensive programming!)\nimport json\ntry:\n    with open('variable_star_ch1.json', 'w') as f:\n        json.dump(variable_star, f, indent=2)\n    print(\"\\n✓ Data saved successfully for Chapter 2!\")\n    \n    # Verify the save worked by reading it back\n    with open('variable_star_ch1.json', 'r') as f:\n        verification = json.load(f)\n    print(f\"  Verified: {verification['name']} data intact\")\n    \nexcept IOError as e:\n    print(f\"\\n✗ Warning: Could not save data: {e}\")\n    print(\"  You'll need to recreate this in Chapter 2\")\nexcept json.JSONDecodeError as e:\n    print(f\"\\n✗ Warning: Data corruption detected: {e}\")\n    print(\"  Try running this cell again\")","type":"content","url":"/python-environment-v3#exercise-1-4-variable-star-exercise-thread","position":71},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl2":"Main Takeaways"},"type":"lvl2","url":"/python-environment-v3#main-takeaways","position":72},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl2":"Main Takeaways"},"content":"This chapter has revealed the hidden complexity underlying every Python program you’ll write. You’ve learned that when code fails to run or produces different results on different machines, it’s rarely due to the code itself — it’s the environment surrounding that code. Understanding this distinction transforms you from someone who gets frustrated by errors to someone who systematically diagnoses and fixes them.\n\nIPython is more than just an enhanced Python prompt - it’s a scientific laboratory where ideas become code. The ability to quickly test hypotheses, examine results, and iterate on solutions is fundamental to computational science. The magic commands like %timeit and %debug aren’t just conveniences; they’re essential tools that separate casual coding from professional scientific computing. Master IPython now, and you’ll use it daily throughout your research career.\n\nThe Jupyter notebook trap is real and dangerous. While notebooks seem perfect for scientific work — mixing code, results, and narrative — their hidden state and execution ambiguity make them unsuitable for serious scientific computing. The out-of-order execution problem isn’t a minor inconvenience; it’s a fundamental flaw that can corrupt your scientific results. After Project 1, you’ll leave notebooks behind for the reliability of scripts, but understanding their dangers now will help you recognize similar issues in other tools.\n\nScripts enforce reproducibility through simplicity. By executing top-to-bottom every time, they eliminate the ambiguity that plagues notebooks. The if __name__ == \"__main__\" pattern might seem like unnecessary boilerplate now, but it’s the key to writing code that’s both immediately useful and reusable by others. This pattern embodies a core principle: good scientific code serves multiple purposes without compromising any of them.\n\nCreating reproducible environments isn’t just about making your code run on other machines — it’s about scientific integrity. When you can’t reproduce your own results from six months ago, you’ve lost the thread of your research. The tools you’ve learned — conda environments, environment files, proper path handling, and seed control — aren’t optional extras. They’re the foundation of trustworthy computational science. Every major discovery in computational science, from gravitational waves to exoplanet detection, has depended on reproducible environments.\n\nThe debugging strategies you’ve learned will save you countless hours. The universal first check — verifying your environment — solves most “mysterious” errors. Systematic import debugging reveals exactly why modules can’t be found. IPython’s debugger lets you examine failures at the moment they occur. These aren’t just troubleshooting techniques; they’re the difference between guessing and knowing.\n\nRemember: computational science and performing astronomical research isn’t just about writing code that works once. It’s about creating reliable, reproducible tools that advance human knowledge. The practices you’ve learned in this chapter — from IPython exploration to environment management — are the foundation of that reliability. Defensive programming isn’t paranoia; it’s professionalism.","type":"content","url":"/python-environment-v3#main-takeaways","position":73},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl2":"Definitions"},"type":"lvl2","url":"/python-environment-v3#definitions","position":74},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl2":"Definitions"},"content":"conda: A package and environment management system that creates isolated Python installations with specific package versions, ensuring reproducibility across different machines.\n\ndefensive programming: Writing code that anticipates and handles potential failures gracefully, checking inputs, validating assumptions, and providing informative error messages rather than assuming everything will work correctly.\n\nenvironment: An isolated Python installation with its own interpreter, packages, and settings, preventing conflicts between projects with different requirements.\n\nimport system: Python’s mechanism for loading code from external files (modules) into your current program, searching through directories listed in sys.path in order.\n\nIPython: Interactive Python — an enhanced version of the basic Python interpreter designed specifically for scientific computing, offering features like magic commands, tab completion, and integrated help.\n\nJupyter notebook: A web-based interactive computing platform that allows you to create notebooks combining code, results, and text in cells that can be executed individually while maintaining state between executions.\n\nmagic command: Special IPython commands prefixed with % (line magics) or %% (cell magics) that provide functionality beyond standard Python, such as timing code execution or entering the debugger.\n\nmodule: A Python file containing code (functions, classes, variables) that can be imported and used in other Python programs, enabling code reuse and organization.\n\nREPL: Read-Eval-Print Loop — an interactive programming environment that takes single expressions, evaluates them, and returns the result immediately, facilitating exploration and testing.\n\nreproducibility: The ability to obtain consistent results using the same data and code, regardless of who runs it, when, or on what machine — fundamental to scientific integrity.\n\nscript: A plain text file containing Python code that executes from top to bottom when run, providing consistent and reproducible execution without hidden state.\n\nsys.path: Python’s list of directories to search when importing modules, checked in order until a matching module is found, determining what code is available to import.\n\nname: A special Python variable that equals \"__main__\" when a script is run directly, or the module name when imported, enabling code to behave differently in each context.","type":"content","url":"/python-environment-v3#definitions","position":75},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl2":"Key Takeaways"},"type":"lvl2","url":"/python-environment-v3#key-takeaways","position":76},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl2":"Key Takeaways"},"content":"✓ IPython is your primary tool: Use it for exploration, testing, and quick calculations — not the basic Python interpreter\n\n✓ Environment problems cause most “broken code”: When code fails, check your environment first with sys.executable\n\n✓ Notebooks corrupt scientific computing: Hidden state and out-of-order execution make results irreproducible\n\n✓ Scripts enforce reproducibility: Top-to-bottom execution eliminates ambiguity and hidden state\n\n✓ The __name__ pattern enables reusability: Code can be both runnable and importable without modification\n\n✓ Conda environments isolate projects: Each project gets its own Python and packages, preventing conflicts\n\n✓ Always specify package versions: Use environment.yml files to ensure others can recreate your exact setup\n\n✓ Paths should be relative, not absolute: Use pathlib.Path for cross-platform compatibility\n\n✓ Control randomness with seeds: Set random seeds for reproducible “random” results\n\n✓ Systematic debugging saves time: Check environment → verify imports → test incrementally\n\n✓ Defensive programming prevents disasters: Assume things will fail and handle errors gracefully","type":"content","url":"/python-environment-v3#key-takeaways","position":77},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl2":"Quick Reference Tables"},"type":"lvl2","url":"/python-environment-v3#quick-reference-tables","position":78},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl2":"Quick Reference Tables"},"content":"Essential IPython Commands\n\nCommand\n\nPurpose\n\nExample\n\n%timeit\n\nTime code execution\n\n%timeit sum(range(1000))\n\n%run\n\nRun script keeping variables\n\n%run analysis.py\n\n%debug\n\nEnter debugger after error\n\n%debug\n\n%who\n\nList all variables\n\n%who\n\n%whos\n\nDetailed variable information\n\n%whos\n\n%reset\n\nClear all variables\n\n%reset -f\n\n%history\n\nShow command history\n\n%history -n 10\n\n%save\n\nSave code to file\n\n%save script.py 1-10\n\n?\n\nQuick help\n\nlen?\n\n??\n\nShow source code\n\nlen??\n\nEnvironment Debugging Checklist\n\nCheck\n\nCommand\n\nWhat to Look For\n\nPython location\n\nwhich python\n\nShould show conda environment path\n\nPython version\n\npython --version\n\nShould match project requirements\n\nActive environment\n\nconda info --envs\n\nAsterisk marks active environment\n\nInstalled packages\n\nconda list\n\nVerify required packages present\n\nImport paths\n\npython -c \"import sys; print(sys.path)\"\n\nShould include project directories\n\nPackage location\n\npython -c \"import pkg; print(pkg.__file__)\"\n\nShould be in conda environment","type":"content","url":"/python-environment-v3#quick-reference-tables","position":79},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl2":"Python Module & Method Reference"},"type":"lvl2","url":"/python-environment-v3#python-module-method-reference","position":80},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl2":"Python Module & Method Reference"},"content":"Your Growing Python Toolkit\n\nThis reference section starts with Chapter 1’s tools and will expand with each chapter. By course end, you’ll have built a comprehensive personal Python reference covering everything from basic file operations to advanced machine learning libraries. Think of this as your Python cookbook that you’re writing as you learn — bookmark it, you’ll use it constantly!\n\nThis reference section catalogs all Python modules, functions, and methods introduced in this chapter. It will grow throughout the course as you learn new tools. Think of this as your personal Python toolkit that you’re building piece by piece.","type":"content","url":"/python-environment-v3#python-module-method-reference","position":81},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"Standard Library Modules","lvl2":"Python Module & Method Reference"},"type":"lvl3","url":"/python-environment-v3#standard-library-modules","position":82},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"Standard Library Modules","lvl2":"Python Module & Method Reference"},"content":"sys module - System-specific parameters and functionsimport sys\n\nsys.executable - Path to the Python interpreter currently running\n\nsys.version - Python version information as a string\n\nsys.path - List of directories Python searches for modules\n\nsys.getsizeof(object) - Returns size of object in bytes\n\nos module - Operating system interfaceimport os\n\nos.getcwd() - Returns current working directory as string\n\nos.getenv('VAR_NAME', default) - Gets environment variable value\n\nos.sep - Path separator for your OS (‘/’ on Unix, ‘\\’ on Windows)\n\nmath module - Mathematical functionsimport math\n\nmath.pi - Mathematical constant π (3.14159...)\n\nmath.sqrt(x) - Square root of x\n\nmath.factorial(n) - Returns n! (n factorial)\n\npathlib module - Object-oriented filesystem pathsfrom pathlib import Path\n\nPath() - Creates a path object\n\nPath.cwd() - Current working directory as Path object\n\nPath.home() - User’s home directory\n\npath.exists() - Returns True if path exists\n\npath.expanduser() - Expands ~ to home directory\n\npath / 'subdir' - Joins paths using / operator\n\nrandom module - Generate random numbersimport random\n\nrandom.seed(n) - Initialize random number generator for reproducibility\n\nrandom.random() - Random float between 0.0 and 1.0\n\ntimeit module - Time small code snippetsimport timeit\n\ntimeit.timeit(code, number=N) - Times code execution N times, returns total seconds\n\nsubprocess module - Run external commandsimport subprocess\n\nsubprocess.run(cmd, capture_output=True, text=True) - Executes command and captures output\n\njson module - JSON encoder and decoderimport json\n\njson.dump(obj, file) - Write Python object as JSON to file\n\njson.load(file) - Read JSON from file into Python object","type":"content","url":"/python-environment-v3#standard-library-modules","position":83},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"Built-in Functions (No Import Needed)","lvl2":"Python Module & Method Reference"},"type":"lvl3","url":"/python-environment-v3#built-in-functions-no-import-needed","position":84},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"Built-in Functions (No Import Needed)","lvl2":"Python Module & Method Reference"},"content":"Core Functions\n\nprint(*args) - Display output to console\n\nlen(obj) - Returns length/size of object\n\nsum(iterable) - Sum all elements in an iterable\n\nrange(start, stop, step) - Generate sequence of numbers\n\nenumerate(iterable, start=0) - Returns index-value pairs\n\ndir(object) - List all attributes of an object\n\ntype(object) - Returns the type of an object\n\neval(string) - Evaluates a Python expression string (use carefully!)\n\nType Conversions\n\nint(x) - Convert to integer\n\nfloat(x) - Convert to floating-point number\n\nstr(x) - Convert to string\n\nlist(x) - Convert to list\n\nObject Inspection\n\nhasattr(obj, 'attribute') - Check if object has an attribute\n\ngetattr(obj, 'attribute', default) - Get attribute value or default\n\n__import__(name) - Import module by name (rarely used directly)\n\nFile Operationswith open('file.txt', 'r') as f:\n    content = f.read()\n\nopen(filename, mode) - Open file (‘r’ read, ‘w’ write, ‘a’ append)\n\nfile.read() - Read entire file as string\n\nfile.write(string) - Write string to file\n\nwith statement - Ensures file is properly closed after use","type":"content","url":"/python-environment-v3#built-in-functions-no-import-needed","position":85},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"IPython Magic Commands","lvl2":"Python Module & Method Reference"},"type":"lvl3","url":"/python-environment-v3#ipython-magic-commands","position":86},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"IPython Magic Commands","lvl2":"Python Module & Method Reference"},"content":"These special commands only work in IPython, not regular Python scripts. They’re prefixed with % for line magics or %% for cell magics.\n\nTiming and Profiling\n\n%timeit code - Time execution with statistical analysis\n\n%time code - Time single execution\n\nWorkspace Management\n\n%who - List all variables\n\n%whos - Detailed variable information with types and values\n\n%reset - Clear all variables from memory\n\n%reset -f - Force reset without confirmation\n\nCode Execution\n\n%run script.py - Execute Python script in IPython\n\n%save filename.py n-m - Save lines n through m to file\n\n%load filename.py - Load code from file into cell\n\nDebugging\n\n%debug - Enter debugger after an exception\n\n%pdb - Automatic debugger on exceptions\n\nHelp and History\n\n? - Quick help (e.g., len?)\n\n?? - Show source code (e.g., len??)\n\n%history - Show command history\n\n%history -n 10 - Show last 10 commands\n\nIPython Special Variables\n\nIn[n] - Input history (code you typed in cell n)\n\nOut[n] - Output history (result from cell n)\n\n_ - Previous output\n\n__ - Second-to-last output\n\n___ - Third-to-last output","type":"content","url":"/python-environment-v3#ipython-magic-commands","position":87},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"Data Structures and Patterns","lvl2":"Python Module & Method Reference"},"type":"lvl3","url":"/python-environment-v3#data-structures-and-patterns","position":88},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"Data Structures and Patterns","lvl2":"Python Module & Method Reference"},"content":"List Comprehensions# Pattern: [expression for item in iterable if condition]\nsquares = [x**2 for x in range(10)]\nfiltered = [x for x in data if x > 0]\n\nDictionary Creation# Creating dictionaries for data organization\nstar_data = {\n    'name': 'Delta Cephei',\n    'period': 5.366,\n    'magnitude': 3.95\n}\n\nThe if __name__ == \"__main__\" Pattern# Makes code both importable and runnable\nif __name__ == \"__main__\":\n    # This code only runs when script is executed directly\n    main()","type":"content","url":"/python-environment-v3#data-structures-and-patterns","position":89},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"Quick Usage Examples","lvl2":"Python Module & Method Reference"},"type":"lvl3","url":"/python-environment-v3#quick-usage-examples","position":90},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"Quick Usage Examples","lvl2":"Python Module & Method Reference"},"content":"\n\n# Finding what's in a module\nimport math\nmath_functions = [x for x in dir(math) if not x.startswith('_')]\nprint(f\"math has {len(math_functions)} functions\")\n\n# Checking your environment\nimport sys\nprint(f\"Python lives at: {sys.executable}\")\n\n# Safe file reading\nfrom pathlib import Path\nfile = Path(\"data.txt\")\nif file.exists():\n    content = file.read_text()\nelse:\n    print(\"File not found - using defaults\")\n\n# Reproducible randomness\nimport random\nrandom.seed(42)  # Always produces same \"random\" sequence\n\n","type":"content","url":"/python-environment-v3#quick-usage-examples","position":91},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"When to Use What?","lvl2":"Python Module & Method Reference"},"type":"lvl3","url":"/python-environment-v3#when-to-use-what","position":92},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl3":"When to Use What?","lvl2":"Python Module & Method Reference"},"content":"For file paths: Always use pathlib.Path, never hardcode strings\n\nFor timing code: Use %timeit in IPython, timeit.timeit() in scripts\n\nFor checking Python: Use sys.executable and sys.version\n\nFor randomness: Always set random.seed() for reproducibility\n\nFor debugging: Use %debug in IPython after errors occur\n\nFor exploration: Use dir() and ? to discover functionality\n\nThis reference will expand as you learn new modules. Keep it handy, you’ll refer to it often as you build your computational toolkit!","type":"content","url":"/python-environment-v3#when-to-use-what","position":93},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl2":"Next Chapter Preview"},"type":"lvl2","url":"/python-environment-v3#next-chapter-preview","position":94},{"hierarchy":{"lvl1":"Chapter 1: Computational Environments & Scientific Workflows","lvl2":"Next Chapter Preview"},"content":"Now that you understand your computational environment and can work effectively in IPython, Chapter 2 will transform Python into a powerful scientific calculator. You’ll discover why 0.1 + 0.2 ≠ 0.3 in Python (and every programming language), learn to handle the numerical precision issues that plague computational physics, and understand how computers actually represent numbers. These fundamentals might seem basic, but small numerical errors compound exponentially - a tiny rounding error in an orbital calculation can send your simulated spacecraft to the wrong planet. Get ready to master the subtle art of computational arithmetic where every digit matters and where understanding floating-point representation can mean the difference between a successful mission and a spectacular failure.","type":"content","url":"/python-environment-v3#next-chapter-preview","position":95},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator"},"type":"lvl1","url":"/python-calculator-v3","position":0},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator"},"content":"","type":"content","url":"/python-calculator-v3","position":1},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl2":"Learning Objectives"},"type":"lvl2","url":"/python-calculator-v3#learning-objectives","position":2},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl2":"Learning Objectives"},"content":"By the end of this chapter, you will be able to:\n\nUse Python as an interactive scientific calculator with proper operator precedence\n\nUnderstand how computers represent integers, floats, and complex numbers in memory\n\nExplain why 0.1 + 0.2 ≠ 0.3 and handle floating-point comparisons correctly\n\nRecognize and avoid catastrophic cancellation and numerical overflow/underflow\n\nChoose appropriate numeric types for different astronomical calculations\n\nFormat output elegantly using f-strings with scientific notation and alignment\n\nConvert between data types safely and understand when conversions lose information\n\nCreate defensive numerical code that catches precision problems early","type":"content","url":"/python-calculator-v3#learning-objectives","position":3},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl2":"Prerequisites Check"},"type":"lvl2","url":"/python-calculator-v3#prerequisites-check","position":4},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl2":"Prerequisites Check"},"content":"✅ Before Starting This Chapter\n\nYou can launch IPython and use basic magic commands (Chapter 1)\n\nYou understand the difference between scripts and interactive sessions (Chapter 1)\n\nYou can navigate your file system and activate your conda environment (Chapter 1)\n\nYour astr596 environment is activated and IPython is ready\n\nIf any boxes are unchecked, review Chapter 1 first.","type":"content","url":"/python-calculator-v3#prerequisites-check","position":5},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl2":"Chapter Overview"},"type":"lvl2","url":"/python-calculator-v3#chapter-overview","position":6},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl2":"Chapter Overview"},"content":"Now that you’ve mastered setting up your computational environment and understand how Python finds and loads code, it’s time to transform IPython into your personal calculator. But this chapter goes far beyond simple arithmetic - you’re about to discover the hidden complexity of numerical computation that can make the difference between discovering an exoplanet and missing it entirely due to rounding errors.\n\nYou’ll learn why spacecraft have crashed, why some astronomical calculations fail catastrophically, and how to write code that handles the extreme scales of the universe — from the quantum foam at 10⁻³⁵ meters to the observable universe at 10²⁶ meters. The floating-point precision issues we explore here aren’t academic exercises; they’re the source of real bugs that have corrupted simulations, invalidated published results, and caused spacecraft navigation errors costing hundreds of millions of dollars.\n\nBy mastering these fundamentals now, you’ll develop the numerical intuition that separates computational scientists from programmers who just happen to work with scientific data. Every orbital integrator you build, every spectrum you analyze, and every statistical test you run will rely on the concepts in this chapter. Let’s begin your journey from calculator user to numerical computing expert.","type":"content","url":"/python-calculator-v3#chapter-overview","position":7},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl2":"2.1 Python as Your Scientific Calculator"},"type":"lvl2","url":"/python-calculator-v3#id-2-1-python-as-your-scientific-calculator","position":8},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl2":"2.1 Python as Your Scientific Calculator"},"content":"operator precedence\nThe order in which Python evaluates mathematical operations\n\nFire up IPython (remember from Chapter 1 — not the basic Python interpreter, we want the enhanced features) and let’s start with the basics. Python handles arithmetic operations naturally, but there are subtleties that matter for scientific work:\n\n# Basic arithmetic - but watch the precision!\nprint(f\"2 + 2 = {2 + 2}\")\nprint(f\"10 / 3 = {10 / 3}\")  # Division always gives a float\nprint(f\"2 ** 10 = {2 ** 10}\")  # Exponentiation - the power operator\n\nNotice that 10 / 3 gives us 3.3333333333333335—not exactly 1/3! This tiny imprecision at the end might seem trivial, but it’s your first glimpse into a fundamental challenge of computational science.","type":"content","url":"/python-calculator-v3#id-2-1-python-as-your-scientific-calculator","position":9},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl3":"Operator Precedence: A Source of Real Bugs","lvl2":"2.1 Python as Your Scientific Calculator"},"type":"lvl3","url":"/python-calculator-v3#operator-precedence-a-source-of-real-bugs","position":10},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl3":"Operator Precedence: A Source of Real Bugs","lvl2":"2.1 Python as Your Scientific Calculator"},"content":"Python follows PEMDAS (Parentheses, Exponents, Multiplication/Division, Addition/Subtraction), but relying on memorized rules causes expensive errors. Let’s see this with a real astronomical calculation:\n\n# Calculate orbital velocity: v = sqrt(GM/r)\n# Using CGS units (standard in astrophysics)\nG = 6.67e-8   # Gravitational constant (cm³ g⁻¹ s⁻²)\nM = 1.989e33  # Solar mass (grams)\nr = 1.496e13  # 1 AU (cm)\n\n# WRONG - operator precedence error!\nv_wrong = G * M / r ** 0.5\nprint(f\"Wrong velocity: {v_wrong:.2e} cm/s\")\n\n# CORRECT - parentheses clarify intent\nv_right = (G * M / r) ** 0.5\nprint(f\"Correct velocity: {v_right:.2e} cm/s\")\nprint(f\"That's {v_right/1e5:.1f} km/s - Earth's orbital speed!\")\n\n# The error factor\nerror_factor = v_wrong / v_right\nprint(f\"Wrong answer is {error_factor:.0f}× too large!\")\n\n📐 Units in Computational Astrophysics\n\nWe use CGS (centimeter-gram-second) units throughout this course because they’re standard in stellar physics, galactic dynamics, and most theoretical astrophysics papers. You’ll see CGS in plasma physics calculations, stellar structure models, and cosmological simulations. SI units appear more frequently in planetary science, spacecraft dynamics, and gravitational wave astronomy.\n\nQuick reference: 1 parsec = 3.086×10¹⁸ cm, 1 M_☉ = 1.989×10³³ g, 1 L_☉ = 3.828×10³³ erg/s. Always verify which system a paper uses — unit confusion has caused spacecraft failures!\n\nThe wrong version calculated (GM/\\sqrt{r}) instead of \\sqrt{(GM/r)} — a factor of \\sqrt{GM} error, which for Earth’s orbit is about \n\n1013 times too large!\n\n🤔 Check Your Understanding\n\nBefore running this code, predict the result of: -2**2 + 3*4//2\n\nWork through it step by step:\n\nFirst, identify the operations: negation, exponentiation, multiplication, floor division, addition\n\nApply PEMDAS rules (remember: exponentiation before negation!)\n\nWrite your predicted answer\n\nSolution\n\nLet’s work through -2**2 + 3*4//2 step by step:\n\nExponentiation first: 2**2 = 4\n\nThen negation: -4 (the negative applies after exponentiation!)\n\nMultiplication: 3*4 = 12\n\nFloor division: 12//2 = 6\n\nFinally addition: -4 + 6 = 2\n\nThe result is 2. The tricky part is that -2**2 equals -4, not 4! Python interprets this as -(2**2), not (-2)**2. This subtle precedence rule has caused real bugs in astronomical calculations.","type":"content","url":"/python-calculator-v3#operator-precedence-a-source-of-real-bugs","position":11},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl3":"Complete Arithmetic Operators","lvl2":"2.1 Python as Your Scientific Calculator"},"type":"lvl3","url":"/python-calculator-v3#complete-arithmetic-operators","position":12},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl3":"Complete Arithmetic Operators","lvl2":"2.1 Python as Your Scientific Calculator"},"content":"floor division\nDivision that rounds toward negative infinity (operator //)\n\nmodulo\nRemainder after division (operator %)\n\nPython provides operators beyond basic arithmetic that prove essential for astronomical calculations:\n\n# Integer division - useful for time calculations\ndays = 17\nweeks = days // 7  # Floor division\nremaining_days = days % 7  # Modulo (remainder)\nprint(f\"{days} days = {weeks} weeks and {remaining_days} days\")\n\n# Warning: Floor division rounds toward negative infinity!\nprint(f\"17 // 3 = {17 // 3}\")    # Result: 5\nprint(f\"-17 // 3 = {-17 // 3}\")  # Result: -6, not -5!\n\n# This catches many astronomers by surprise\nprint(\"\\nBe careful with negative values:\")\nprint(f\"int(-17/3) = {int(-17/3)} (truncates toward zero)\")\nprint(f\"-17 // 3 = {-17 // 3} (floors toward -infinity)\")\n\n🚨 Common Bug Alert: Negative Floor Division\n\nFloor division with negative numbers often surprises astronomers calculating phases or time intervals before an epoch. When working with Julian dates or phases that can be negative, always test your edge cases or use int(a/b) for truncation toward zero.","type":"content","url":"/python-calculator-v3#complete-arithmetic-operators","position":13},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl2":"2.2 How Python Stores Numbers: Critical for Scientific Computing"},"type":"lvl2","url":"/python-calculator-v3#id-2-2-how-python-stores-numbers-critical-for-scientific-computing","position":14},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl2":"2.2 How Python Stores Numbers: Critical for Scientific Computing"},"content":"Here’s where your journey gets interesting! You’re about to peek behind the curtain and see how computers really think about numbers. This knowledge is your superpower — it’s what lets you calculate the trajectory to send New Horizons to Pluto, 3 billion miles away, and arrive within 72 seconds of the predicted time.","type":"content","url":"/python-calculator-v3#id-2-2-how-python-stores-numbers-critical-for-scientific-computing","position":15},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl3":"Integers: Arbitrary Precision Power","lvl2":"2.2 How Python Stores Numbers: Critical for Scientific Computing"},"type":"lvl3","url":"/python-calculator-v3#integers-arbitrary-precision-power","position":16},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl3":"Integers: Arbitrary Precision Power","lvl2":"2.2 How Python Stores Numbers: Critical for Scientific Computing"},"content":"arbitrary precision\nPython integers can grow to any size limited only by available memory\n\nUnlike many languages, Python integers have unlimited precision — a huge advantage for astronomy where we routinely deal with enormous numbers:\n\n# Number of atoms in the observable universe (approximate)\natoms_in_universe = 10 ** 80\nprint(f\"Atoms in universe: {atoms_in_universe}\")\n\n# Python handles it perfectly!\natoms_squared = atoms_in_universe ** 2\nprint(f\"Can even square it: {atoms_squared}\")\n\n# Memory usage scales with size\nimport sys\nprint(f\"\\nMemory usage comparison:\")\nprint(f\"Small integer (42) uses: {sys.getsizeof(42)} bytes\")\nprint(f\"Universe atoms uses: {sys.getsizeof(atoms_in_universe)} bytes\")\nprint(f\"Atoms squared uses: {sys.getsizeof(atoms_squared)} bytes\")\n\nThis arbitrary precision is wonderful but comes with a cost — Python integers use more memory than fixed-size integers in compiled languages. This is why specialized numerical libraries become essential for large datasets.","type":"content","url":"/python-calculator-v3#integers-arbitrary-precision-power","position":17},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl3":"Floating-Point Numbers: The Heart of Numerical Computing","lvl2":"2.2 How Python Stores Numbers: Critical for Scientific Computing"},"type":"lvl3","url":"/python-calculator-v3#floating-point-numbers-the-heart-of-numerical-computing","position":18},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl3":"Floating-Point Numbers: The Heart of Numerical Computing","lvl2":"2.2 How Python Stores Numbers: Critical for Scientific Computing"},"content":"IEEE 754\nInternational standard for floating-point arithmetic in binary\n\nmantissa\nThe significant digits of a floating-point number\n\nThis is it — the concept that separates casual programmers from computational scientists! Don’t worry if this seems complex at first; every professional scientist had to learn these same lessons.\n\nTODO: add 32- vs. 64-bit (single vs. double precision), state that Python uses 64-bit by default, but as we’ll see with jax its default is 32-bit.\n\nFloating-point numbers use IEEE 754 representation: 64 bits split into sign (1 bit), exponent (11 bits), and mantissa (52 bits). This creates fundamental limitations that every computational scientist must understand:\n\n# The famous example that confuses beginners\nresult = 0.1 + 0.2\nprint(f\"0.1 + 0.2 = {result}\")\nprint(f\"0.1 + 0.2 == 0.3? {result == 0.3}\")\nprint(f\"Actual stored value: {result:.17f}\")\n\n# Let's see what's really stored\nfrom decimal import Decimal\nprint(f\"\\nWhat Python actually stores:\")\nprint(f\"0.1 is really: {Decimal(0.1)}\")\nprint(f\"0.2 is really: {Decimal(0.2)}\")\nprint(f\"0.3 is really: {Decimal(0.3)}\")\n\nWhy does this happen? Just as 1/3 can’t be exactly represented in decimal (0.33333...), 1/10 can’t be exactly represented in binary. This has crashed spacecraft and corrupted years of simulations!\n\n🤔 Check Your Understanding\n\nWill this return True or False? Think carefully before testing:result = 0.1 * 3 == 0.3\n\nMake your prediction, then test it. What do you think causes the result?\n\nSolution\n\nThis returns False! Here’s why:print(f\"0.1 * 3 = {0.1 * 3:.17f}\")\nprint(f\"0.3 =     {0.3:.17f}\")\n\nYou’ll see that 0.1 * 3 gives approximately 0.30000000000000004, while 0.3 is approximately 0.29999999999999999. They’re different by about 5.5×10⁻¹⁷ - tiny but not zero!\n\nThis isn’t a quirk — it’s fundamental to how every computer on Earth handles decimals. This is why we always use math.isclose() for floating-point comparisons.\n\n💡 Computational Thinking: Representation Limits\n\nEvery number system has values it cannot represent exactly. In base 10, we can’t write 1/3 exactly. In base 2 (binary), we can’t write 1/10 exactly. This isn’t a flaw — it’s a fundamental property of finite representation.\n\nThis pattern appears everywhere in computing:\n\nJPEG images lose information through compression\n\nMP3s approximate sound waves\n\nFloating-point approximates real numbers\n\nNeural networks approximate functions\n\nUnderstanding representation limits helps you choose the right tool for each task. The key insight: always assume floating-point arithmetic is approximate, never exact.","type":"content","url":"/python-calculator-v3#floating-point-numbers-the-heart-of-numerical-computing","position":19},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl3":"Machine Epsilon: The Smallest Distinguishable Difference","lvl2":"2.2 How Python Stores Numbers: Critical for Scientific Computing"},"type":"lvl3","url":"/python-calculator-v3#machine-epsilon-the-smallest-distinguishable-difference","position":20},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl3":"Machine Epsilon: The Smallest Distinguishable Difference","lvl2":"2.2 How Python Stores Numbers: Critical for Scientific Computing"},"content":"machine epsilon\nSmallest positive float that, when added to 1.0, gives a result different from 1.0\n\nReady for something mind-blowing? Your computer literally cannot tell the difference between some numbers that are mathematically different! This isn’t a flaw — it’s a fundamental property of finite systems.\n\nimport sys\n\n# Machine epsilon - the fundamental precision limit\nepsilon = sys.float_info.epsilon\nprint(f\"Machine epsilon: {epsilon}\")\nprint(f\"That's about {epsilon:.2e}\")\n\n# Numbers closer than epsilon to 1.0 cannot be distinguished\ntest1 = 1.0 + epsilon/2\ntest2 = 1.0 + epsilon\n\nprint(f\"\\nCan Python tell these apart from 1.0?\")\nprint(f\"1.0 + ε/2 = {test1}, equals 1.0? {test1 == 1.0}\")\nprint(f\"1.0 + ε   = {test2}, equals 1.0? {test2 == 1.0}\")\n\n# This affects astronomical calculations\nau_cm = 1.496e13  # 1 AU in centimeters\ntiny_change = au_cm * epsilon\nprint(f\"\\nAt 1 AU distance ({au_cm:.2e} cm):\")\nprint(f\"We cannot detect changes smaller than {tiny_change:.2e} cm\")\nprint(f\"That's about {tiny_change/100:.2f} meters!\")\n\nWhen the Kepler Space Telescope searched for exoplanets by detecting brightness dips of 0.01%, understanding machine epsilon was essential to distinguish real planetary transits from numerical noise.\n\n🌟 The More You Know: The Patriot Missile Timing Disaster\n\nOn February 25, 1991, an American Patriot missile battery in Dharan, Saudi Arabia, failed to intercept an incoming Iraqi Scud missile, resulting in 28 deaths and 98 injuries. The cause was a floating-point timing error that had accumulated over 100 hours of continuous operation.\n\nThe system’s internal clock measured time in tenths of seconds using 24-bit floating-point arithmetic. But 0.1 cannot be exactly represented in binary — it’s actually stored as 0.099999999... After 100 hours of operation (360,000 increments), this tiny error had accumulated to 0.34 seconds (\n\nGAO Report IMTEC-92-26, 1992).\n\nIn 0.34 seconds, a Scud missile travels over 600 meters — enough to completely miss the intercept. The Patriot radar looked in the wrong part of the sky and never saw the incoming missile.\n\nThe software had been designed for 14-hour maximum operation cycles, but field conditions in Desert Storm required continuous operation. Ironically, a software patch fixing this exact issue was literally in transit to Dharan when the attack occurred (\n\nSkeel, R. (1992). “Roundoff Error and the Patriot Missile.” SIAM News, 25(4)).\n\nThe lesson: never use floating-point for precise time accumulation. Use integer counters and convert to float only for calculations. This tragedy shows that numerical errors aren’t just about wrong answers — they can cost lives.","type":"content","url":"/python-calculator-v3#machine-epsilon-the-smallest-distinguishable-difference","position":21},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl3":"Safe Floating-Point Comparisons","lvl2":"2.2 How Python Stores Numbers: Critical for Scientific Computing"},"type":"lvl3","url":"/python-calculator-v3#safe-floating-point-comparisons","position":22},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl3":"Safe Floating-Point Comparisons","lvl2":"2.2 How Python Stores Numbers: Critical for Scientific Computing"},"content":"Never use == with floating-point numbers! Here’s how to compare them safely:\n\nStage 1: Understanding the Problem\n\n# The problem with direct comparison\nvelocity = 299792458.0  # Speed of light in m/s\ncalculated = 299792457.999999999  # From some calculation\n\nprint(f\"Velocity: {velocity}\")\nprint(f\"Calculated: {calculated}\")\nprint(f\"Difference: {velocity - calculated:.2e}\")\nprint(f\"Are they equal? {velocity == calculated}\")\nprint(\"Even tiny differences fail equality test!\")\n\nStage 2: Basic Solution\n\ndef safe_compare_simple(a, b, tolerance=1e-9):\n    \"\"\"Compare floats with absolute tolerance.\"\"\"\n    difference = abs(a - b)\n    return difference <= tolerance\n\n# Test our function\nv1 = 299792458.0\nv2 = 299792457.999999999\n\nresult = safe_compare_simple(v1, v2, tolerance=1e-6)\nprint(f\"Within tolerance? {result}\")\n\nStage 3: Professional Solution\n\nimport math\n\ndef safe_compare(a, b, rel_tol=1e-9, abs_tol=1e-15):\n    \"\"\"Compare floats safely with relative and absolute tolerance.\"\"\"\n    # Handle special cases\n    if math.isnan(a) or math.isnan(b):\n        return False\n    if math.isinf(a) or math.isinf(b):\n        return a == b\n    \n    # Use relative and absolute tolerance\n    return abs(a - b) <= max(rel_tol * max(abs(a), abs(b)), abs_tol)\n\n# Better - use Python's built-in (available since Python 3.5)\nresult = math.isclose(v1, v2, rel_tol=1e-9)\nprint(f\"Using math.isclose: {result}\")\n\n🚨 Common Bug Alert: The Float Comparison Trap\n\nTODO: Need to find reference for this. What team? Is this made up?\n\nThis actual code from a published exoplanet detection pipeline failed intermittently:# REAL BUG that made it to production!\ndef check_transit_depth(observed, expected):\n    if observed == expected:  # BUG: Float comparison with ==\n        return \"Perfect match!\"\n    elif observed > expected:\n        return \"Deeper than expected\"\n    else:\n        return \"Shallower than expected\"\n\n# This failed even when values looked identical!\ndepth1 = 0.001 * 3  # From three measurements\ndepth2 = 0.003      # Expected value\nprint(check_transit_depth(depth1, depth2))  # Returns \"Shallower\" not \"Perfect\"!\n\nThe fix: Always use tolerance-based comparison. The team lost three months of observations before finding this bug.","type":"content","url":"/python-calculator-v3#safe-floating-point-comparisons","position":23},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl2":"2.3 Numerical Hazards in Astronomical Computing"},"type":"lvl2","url":"/python-calculator-v3#id-2-3-numerical-hazards-in-astronomical-computing","position":24},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl2":"2.3 Numerical Hazards in Astronomical Computing"},"content":"You’ve mastered how Python stores numbers — now let’s see these concepts in action with real scientific calculations! This is where things get exciting: you’re about to learn the techniques that enabled Cassini to thread the gap between Saturn’s rings, that allow LIGO to detect gravitational waves smaller than a proton’s width, and that help the Event Horizon Telescope image black holes.\n\nTODO: This is a bit over the top, consider rephrasing.","type":"content","url":"/python-calculator-v3#id-2-3-numerical-hazards-in-astronomical-computing","position":25},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl3":"Catastrophic Cancellation: When Subtraction Destroys Precision","lvl2":"2.3 Numerical Hazards in Astronomical Computing"},"type":"lvl3","url":"/python-calculator-v3#catastrophic-cancellation-when-subtraction-destroys-precision","position":26},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl3":"Catastrophic Cancellation: When Subtraction Destroys Precision","lvl2":"2.3 Numerical Hazards in Astronomical Computing"},"content":"catastrophic cancellation\nLoss of significant digits when subtracting nearly equal floating-point numbers\n\nHere’s a numerical phenomenon that sounds scary but becomes manageable once you understand it! Catastrophic cancellation happens when you subtract nearly equal numbers, eliminating most significant digits and leaving only rounding errors.\n\nStage 1: The Problem\n\nimport math\n\n# Computing 1 - cos(x) for small x (common in orbital calculations)\nx = 1e-8  # Small angle in radians\n\n# Direct computation - catastrophic cancellation!\ncos_x = math.cos(x)\nresult_bad = 1 - cos_x\nprint(f\"Direct: 1 - cos({x}) = {result_bad}\")\nprint(f\"This result is completely wrong!\")\n\nStage 2: Understanding Why\n\n# Let's see what's happening\nprint(f\"cos({x}) = {cos_x:.20f}\")\nprint(f\"1.0 =      1.00000000000000000000\")\nprint(f\"Difference loses all significant digits!\")\n\n# The true value (from Taylor series)\ntrue_value = x**2 / 2  # For small x, 1-cos(x) ≈ x²/2\nprint(f\"\\nTrue value: {true_value:.6e}\")\nprint(f\"Our result: {result_bad:.6e}\")\nprint(f\"Relative error: {abs(result_bad - true_value)/true_value:.1%}\")\n\nStage 3: The Solution\n\n# Better: use mathematical identity\n# 1 - cos(x) = 2sin²(x/2)\nresult_good = 2 * math.sin(x/2) ** 2\nprint(f\"Using identity: {result_good:.6e}\")\nprint(f\"True value:     {true_value:.6e}\")\nprint(f\"Much better! Error: {abs(result_good - true_value)/true_value:.2e}\")\n\n# This preserves precision for small angles\n\n🌟 The More You Know: Mars Climate Orbiter’s $327 Million Mistake\n\nThe Mars Climate Orbiter was destroyed on September 23, 1999, when it entered Mars’ atmosphere at 57 kilometers altitude instead of the planned 140-226 kilometers. While the famous cause was a metric/imperial unit mix-up (Lockheed Martin used pound-force seconds while NASA used newton-seconds), the navigation software also struggled with numerical precision issues.\n\nThe spacecraft’s trajectory correction maneuvers involved calculating tiny velocity changes (often less than 1 m/s) while traveling at over 20,000 m/s. This requires computing the difference between nearly equal large numbers — exactly the catastrophic cancellation problem we just solved (\n\nStephenson et al., 1999).\n\nWhen you’re 400 million kilometers from Earth and need millimeter-per-second precision in velocity, every numerical trick matters. The MCO failure led NASA to implement much stricter numerical validation in all navigation software (\n\nNASA, 2000).","type":"content","url":"/python-calculator-v3#catastrophic-cancellation-when-subtraction-destroys-precision","position":27},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl3":"Overflow and Underflow in Astronomical Scales","lvl2":"2.3 Numerical Hazards in Astronomical Computing"},"type":"lvl3","url":"/python-calculator-v3#overflow-and-underflow-in-astronomical-scales","position":28},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl3":"Overflow and Underflow in Astronomical Scales","lvl2":"2.3 Numerical Hazards in Astronomical Computing"},"content":"overflow\nWhen a calculation exceeds the maximum representable floating-point value\n\nunderflow\nWhen a calculation produces a value too small to represent, becoming zero\n\nGet ready to work with numbers that break normal intuition! Astronomy deals with scales that push Python to its limits — from subatomic particles in neutron stars to galaxy clusters spanning millions of light-years.\n\nStage 1: The Overflow Problem\n\n# Calculating total luminosity\nL_sun = 3.828e33  # Solar luminosity (erg/s)\nn_stars_galaxy = 1e11  # Stars in a galaxy\nn_galaxies = 2e12  # Galaxies in observable universe\n\n# This might overflow!\ntry:\n    # Try direct multiplication\n    L_universe = L_sun * n_stars_galaxy * n_galaxies\n    print(f\"Universe luminosity: {L_universe:.2e} erg/s\")\nexcept OverflowError:\n    print(\"Direct calculation overflowed!\")\n    print(\"Number too large for float representation\")\n\nStage 2: The Solution - Working in Log Space\n\nimport math\n\n# When direct calculation fails, work in log space\nlog_L_sun = math.log10(L_sun)\nlog_n_stars = math.log10(n_stars_galaxy)\nlog_n_galaxies = math.log10(n_galaxies)\n\n# Add logarithms instead of multiplying\nlog_L_universe = log_L_sun + log_n_stars + log_n_galaxies\n\nprint(f\"Universe luminosity: 10^{log_L_universe:.1f} erg/s\")\nprint(f\"That's 10^{log_L_universe:.1f} ergs every second!\")\n\n# For comparison, the Sun's lifetime energy output\nlog_sun_lifetime = math.log10(L_sun) + math.log10(3.15e7 * 1e10)  # 10 billion years\nprint(f\"Sun's total lifetime output: 10^{log_sun_lifetime:.1f} ergs\")\n\n💡 Computational Thinking: Working in Transformed Space\n\nWhen direct calculation fails, transform your problem into a space where it succeeds. This universal pattern appears throughout computational science:\n\nLog space: Avoid overflow/underflow in products\n\nFourier space: Turn convolution into multiplication\n\nSpherical coordinates: Simplify radially symmetric problems\n\nStandardized variables: Compare different scales\n\nThe key insight: the same problem can be easy or impossible depending on your representation. When you hit numerical limits, ask yourself whether there’s a transformation that makes the problem tractable.\n\nRemember: transforming to a different space isn’t cheating — it’s often the mathematically correct approach that reveals the true structure of your problem.\n\n🚨 Common Bug Alert: Silent Underflow\n\nUnlike overflow, underflow to zero is silent — no error is raised!tiny = 1e-200\ntinier = tiny * tiny  # Underflows to 0.0 silently!\nif tinier == 0:\n    print(\"Warning: Underflow detected!\")\n\nFor probability calculations, always work in log space to avoid underflow.","type":"content","url":"/python-calculator-v3#overflow-and-underflow-in-astronomical-scales","position":29},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl3":"Defensive Programming with Numerical Checks","lvl2":"2.3 Numerical Hazards in Astronomical Computing"},"type":"lvl3","url":"/python-calculator-v3#defensive-programming-with-numerical-checks","position":30},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl3":"Defensive Programming with Numerical Checks","lvl2":"2.3 Numerical Hazards in Astronomical Computing"},"content":"You’ve seen the challenges — now here’s your toolkit for conquering them! Defensive programming might sound cautious, but it’s actually incredibly empowering. These validation techniques protect every major astronomical data pipeline.\n\nStage 1: Basic Validation (10 lines)\n\ndef validate_positive(value, name=\"value\"):\n    \"\"\"Ensure value is positive.\"\"\"\n    if value <= 0:\n        raise ValueError(f\"{name} must be positive: {value}\")\n    return value\n\n# Example use\nmass = validate_positive(1.989e33, \"stellar mass\")\nprint(f\"Valid mass: {mass:.2e} g\")\n\nStage 2: Safe Division\n\ndef safe_divide(a, b, epsilon=1e-15):\n    \"\"\"Division with protection against near-zero denominators.\"\"\"\n    if abs(b) < epsilon:\n        raise ValueError(f\"Division by near-zero: {b:.2e}\")\n    return a / b\n\n# Test with safe division\nforce = safe_divide(G * M, r * r)\nprint(f\"Gravitational acceleration: {force:.2e} cm/s²\")\n\n# Would catch divide-by-zero before it happens\n\nStage 3: Complete Validation (15 lines)\n\nimport math\n\ndef validate_magnitude(value, name=\"value\", max_exp=100):\n    \"\"\"Ensure value is reasonable for astronomical calculations.\"\"\"\n    if value == 0:\n        return value\n    \n    if not math.isfinite(value):\n        raise ValueError(f\"{name} is not finite: {value}\")\n    \n    log_val = abs(math.log10(abs(value)))\n    if log_val > max_exp:\n        raise ValueError(f\"{name} has unreasonable magnitude: {value:.2e}\")\n    \n    return value\n\n# Test validation\ntest_value = validate_magnitude(L_sun, \"luminosity\", max_exp=50)\nprint(f\"Validated luminosity: {test_value:.2e} erg/s\")\n\nStage 4: Domain-Specific Validation\n\ndef validate_schwarzschild_input(mass_grams):\n    \"\"\"\n    Validate mass for black hole calculations.\n    Ensures physically reasonable values for computational stability.\n    \n    Args:\n        mass_grams: Mass in grams\n        \n    Returns:\n        float: Validated mass\n    \"\"\"\n    MIN_MASS = 2e31  # ~0.01 solar masses (brown dwarf lower limit)\n    MAX_MASS = 2e44  # ~10¹¹ solar masses (largest known black holes)\n    \n    if not MIN_MASS <= mass_grams <= MAX_MASS:\n        raise ValueError(\n            f\"Mass {mass_grams:.2e} g outside physical range \"\n            f\"[{MIN_MASS:.2e}, {MAX_MASS:.2e}]\"\n        )\n    \n    return mass_grams\n\n# Test with realistic astrophysical masses\nstellar_bh = validate_schwarzschild_input(10 * 1.989e33)  # 10 M☉\nprint(f\"Stellar black hole mass valid: {stellar_bh:.2e} g\")\n\ntry:\n    # This would fail - too small for any collapsed object\n    validate_schwarzschild_input(1e20)\nexcept ValueError as e:\n    print(f\"Validation caught error: {e}\")\n\n","type":"content","url":"/python-calculator-v3#defensive-programming-with-numerical-checks","position":31},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl2":"2.4 Complex Numbers for Wave Physics"},"type":"lvl2","url":"/python-calculator-v3#id-2-4-complex-numbers-for-wave-physics","position":32},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl2":"2.4 Complex Numbers for Wave Physics"},"content":"complex number\nNumber with real and imaginary parts, written as a + bj in Python\n\nWelcome to one of the most elegant corners of mathematics! Complex numbers might sound intimidating, but they’re actually your gateway to understanding everything from stellar oscillations to gravitational waves. Python handles complex numbers as naturally as integers.\n\nWhy ‘j’ not ‘i’?\nPython follows electrical engineering convention using j for the imaginary unit. This avoids conflicts with i commonly used as an index variable and prevents confusion in computational physics where i often represents current.\n\n# Complex numbers in Python use 'j' for the imaginary unit\nz = 3 + 4j\nprint(f\"Complex number: {z}\")\nprint(f\"Real part: {z.real}\")\nprint(f\"Imaginary part: {z.imag}\")\nprint(f\"Magnitude: {abs(z)}\")\n\n# Euler's formula - the most beautiful equation in mathematics\nimport cmath\neuler = cmath.exp(1j * math.pi)\nprint(f\"\\ne^(iπ) = {euler.real:.0f}\")\nprint(f\"Tiny imaginary part {euler.imag:.2e} is just rounding error\")\n\n# Phase calculation (useful for wave interference)\nphase = cmath.phase(z)\nprint(f\"\\nPhase of {z}: {phase:.3f} radians\")\nprint(f\"That's {math.degrees(phase):.1f} degrees\")\n\nComplex numbers aren’t just mathematical abstractions — they’re essential for:\n\nFourier transforms (spectral analysis)\n\nQuantum mechanics (wave functions)\n\nSignal processing (interferometry)\n\nEvery spectrum you’ve ever seen from a telescope was processed using complex numbers!\n\n💡 Computational Thinking: Complex Numbers as 2D Vectors\n\nThink of complex numbers as 2D vectors that know how to multiply:\n\nAddition: vector addition\n\nMultiplication: rotation and scaling\n\nMagnitude: vector length\n\nPhase: angle from positive real axis\n\nThis pattern — representing compound data as single objects with rich behavior — appears throughout scientific computing. Master this concept here, and you’ll recognize it everywhere!","type":"content","url":"/python-calculator-v3#id-2-4-complex-numbers-for-wave-physics","position":33},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl2":"2.5 Variables and Dynamic Typing"},"type":"lvl2","url":"/python-calculator-v3#id-2-5-variables-and-dynamic-typing","position":34},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl2":"2.5 Variables and Dynamic Typing"},"content":"dynamic typing\nPython determines and can change variable types at runtime\n\nNow that you understand how Python represents numbers, let’s see how it manages them! Variables in Python are names that refer to objects, not containers that hold values. This distinction matters:\n\n# Variables are references, not containers\nstellar_mass = 1.989e33  # Solar mass in grams\nsolar_mass = stellar_mass  # Both names refer to the SAME number\n\nprint(f\"stellar_mass: {stellar_mass:.2e} g\")\nprint(f\"solar_mass: {solar_mass:.2e} g\")\nprint(f\"Same object? {stellar_mass is solar_mass}\")\n\n# But numbers are immutable, so this is safe\nstellar_mass = stellar_mass * 2  # Creates NEW number\nprint(f\"\\nAfter doubling stellar_mass:\")\nprint(f\"stellar_mass: {stellar_mass:.2e} g\")\nprint(f\"solar_mass (unchanged): {solar_mass:.2e} g\")\n\nPython is dynamically typed — variables can refer to any type of object:\n\n# Dynamic typing in action\nobservation = 42  # Integer\nprint(f\"Type: {type(observation).__name__}, Value: {observation}\")\n\nobservation = 42.0  # Now a float\nprint(f\"Type: {type(observation).__name__}, Value: {observation}\")\n\nobservation = \"42 measurements\"  # Now a string\nprint(f\"Type: {type(observation).__name__}, Value: {observation}\")\n\n# This flexibility is powerful but requires discipline\n# In scientific code, changing types unexpectedly is usually a bug!\n\n","type":"content","url":"/python-calculator-v3#id-2-5-variables-and-dynamic-typing","position":35},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl2":"2.6 Strings and Scientific Output Formatting"},"type":"lvl2","url":"/python-calculator-v3#id-2-6-strings-and-scientific-output-formatting","position":36},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl2":"2.6 Strings and Scientific Output Formatting"},"content":"f-string\nFormatted string literal for elegant output (f\"...{variable}...\")\n\nTime to make your results shine! Clear output formatting isn’t just about aesthetics — it’s about scientific communication. The formatting skills you learn here will help you create publication-quality output.\n\n# Basic f-string formatting\nstar_name = \"Betelgeuse\"\ndistance_ly = 548\nluminosity = 1.2e5  # Solar luminosities\n\nprint(f\"{star_name} is {distance_ly} light-years away\")\nprint(f\"Luminosity: {luminosity:.2e} L☉\")\n\n","type":"content","url":"/python-calculator-v3#id-2-6-strings-and-scientific-output-formatting","position":37},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl3":"Format Specifications for Scientific Data","lvl2":"2.6 Strings and Scientific Output Formatting"},"type":"lvl3","url":"/python-calculator-v3#format-specifications-for-scientific-data","position":38},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl3":"Format Specifications for Scientific Data","lvl2":"2.6 Strings and Scientific Output Formatting"},"content":"F-strings support sophisticated formatting perfect for scientific output:\n\n# Comprehensive formatting examples\nvalue = 1234.56789\n\nprint(f\"Fixed-point (2 decimals): {value:.2f}\")\nprint(f\"Scientific notation: {value:.2e}\")\nprint(f\"Width 10, 2 decimals: {value:10.2f}\")\nprint(f\"Thousands separator: {value:,.0f}\")\n\n# Create a formatted table of stellar data\nstars = [\n    (\"Sirius\", -1.46, 8.6),\n    (\"Canopus\", -0.74, 310),\n    (\"Arcturus\", -0.05, 37),\n]\n\nprint(f\"\\n{'Star':<10} {'Magnitude':>10} {'Distance (ly)':>15}\")\nprint(\"-\" * 37)\nfor name, mag, dist in stars:\n    print(f\"{name:<10} {mag:>10.2f} {dist:>15.1f}\")\n\n🤔 Check Your Understanding\n\nWhat will this print?redshift = 0.00123456\nprint(f\"z = {redshift:.2e}\")\n\nPredict the format before running it.\n\nSolution\n\nThis prints: z = 1.23e-03\n\nThe .2e format specifier means:\n\nUse scientific notation (e)\n\nShow 2 digits after the decimal point\n\nPython automatically handles the exponent\n\nSo 0.00123456 becomes 1.23 × 10⁻³, displayed as 1.23e-03.","type":"content","url":"/python-calculator-v3#format-specifications-for-scientific-data","position":39},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl2":"2.7 Type System and Conversions"},"type":"lvl2","url":"/python-calculator-v3#id-2-7-type-system-and-conversions","position":40},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl2":"2.7 Type System and Conversions"},"content":"type conversion\nChanging data from one type to another (e.g., string to float)\n\nPython’s type system strikes a perfect balance — flexible enough for rapid exploration but strong enough to catch errors. Understanding types and conversions is crucial for handling data from any source.\n\n# Type checking\nvalue = 3.14159\nprint(f\"Type of {value}: {type(value).__name__}\")\nprint(f\"Is it a float? {isinstance(value, float)}\")\n\n# Type conversion\ntext = \"2.718\"\nnumber = float(text)\nprint(f\"\\nConverted '{text}' to {number}\")\n\n# Dangerous conversion - truncation!\nprint(f\"\\nint(3.9) = {int(3.9)}\")  # Truncates, doesn't round!\nprint(f\"round(3.9) = {round(3.9)}\")  # Use this for rounding\n\n🤔 Check Your Understanding\n\nWhat happens when you convert -3.7 to an integer? Predict the result:\n\na) -4 (rounds to nearest integer)\nb) -3 (truncates toward zero)\nc) -3 (floors toward negative infinity)\nd) Error\n\nSolution\n\nThe answer is b) -3 (truncates toward zero).\n\nPython’s int() truncates toward zero, not toward negative infinity like floor division!print(f\"int(-3.7) = {int(-3.7)}\")    # Gives -3 (toward zero)\nprint(f\"-3.7 // 1 = {-3.7 // 1}\")    # Gives -4.0 (toward -infinity)\n\nThis inconsistency has caused numerous bugs in astronomical calculations where negative values represent positions before an epoch.\n\n🚨 Common Bug Alert: Silent Type Conversion Errors\n\nTO DO: Check this! Likely not true, need to find references. WHAT TEAM?\nThis bug appeared in actual pulsar timing software:# REAL BUG: Calculating rotation phase\nperiod_ms = \"2.3\"  # Read from config file as string\nrotations = 1000 / int(float(period_ms))  # int() truncates to 2!\n\n# Should be 434.78 rotations, but gets 500\n# The team published incorrect rotation rates for 17 pulsars!\n\nAlways use float() for decimal strings, then explicitly round if needed.","type":"content","url":"/python-calculator-v3#id-2-7-type-system-and-conversions","position":41},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl2":"2.8 The Math Module: Your Scientific Calculator"},"type":"lvl2","url":"/python-calculator-v3#id-2-8-the-math-module-your-scientific-calculator","position":42},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl2":"2.8 The Math Module: Your Scientific Calculator"},"content":"Here comes the fun part — Python’s math module transforms your computer into a scientific calculator more powerful than anything that existed when we sent humans to the Moon!\n\nimport math\n\n# Fundamental constants\nprint(f\"π = {math.pi}\")\nprint(f\"e = {math.e}\")\nprint(f\"τ = {math.tau}\")  # tau = 2π, useful for circular motion\n\n# Trigonometry (always in radians!)\nangle_degrees = 30\nangle_radians = math.radians(angle_degrees)\nprint(f\"\\nsin(30°) = {math.sin(angle_radians):.4f}\")\nprint(f\"cos(30°) = {math.cos(angle_radians):.4f}\")\n\n# Logarithms for magnitude calculations\nflux_ratio = 100\nmagnitude_diff = 2.5 * math.log10(flux_ratio)\nprint(f\"\\nFlux ratio {flux_ratio} = {magnitude_diff:.1f} magnitudes\")\n\n# Special functions\nprint(f\"\\nGamma(5) = {math.gamma(5)} = 4!\")\nprint(f\"Error function: erf(1) = {math.erf(1):.4f}\")\n\nRemember: trigonometric functions use radians, not degrees! This is a constant source of bugs in astronomical code.","type":"content","url":"/python-calculator-v3#id-2-8-the-math-module-your-scientific-calculator","position":43},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl2":"2.9 From Interactive to Script"},"type":"lvl2","url":"/python-calculator-v3#id-2-9-from-interactive-to-script","position":44},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl2":"2.9 From Interactive to Script"},"content":"Congratulations! You’ve been exploring Python interactively, testing ideas and learning how numbers behave. Now let’s transform your interactive explorations into a reusable script:\n\n#!/usr/bin/env python\n\"\"\"\nschwarzschild_radius.py\nCalculate Schwarzschild radius with proper numerical handling.\n\"\"\"\n\nimport math\nimport sys\n\ndef schwarzschild_radius_simple(mass_grams):\n    \"\"\"Calculate Schwarzschild radius in cm.\"\"\"\n    G = 6.67e-8   # cm³/g/s²\n    c = 2.998e10  # cm/s\n    \n    rs = 2 * G * mass_grams / c**2\n    return rs\n\ndef schwarzschild_radius_validated(mass_grams):\n    \"\"\"Calculate with input validation.\"\"\"\n    G = 6.67e-8\n    c = 2.998e10\n    \n    if mass_grams <= 0:\n        raise ValueError(f\"Mass must be positive: {mass_grams}\")\n    \n    rs = 2 * G * mass_grams / c**2\n    return rs\n\ndef schwarzschild_radius_robust(mass_grams):\n    \"\"\"Handle extreme masses using log space.\"\"\"\n    G = 6.67e-8\n    c = 2.998e10\n    \n    if mass_grams <= 0:\n        raise ValueError(f\"Mass must be positive\")\n    \n    # Use log space for extreme masses\n    if mass_grams > 1e45:  # Galaxy cluster scale\n        log_rs = math.log10(2*G) + math.log10(mass_grams) - 2*math.log10(c)\n        return 10**log_rs\n    \n    return 2 * G * mass_grams / c**2\n\n# Test our functions\nif __name__ == \"__main__\":\n    test_masses = {\n        \"Earth\": 5.972e27,\n        \"Sun\": 1.989e33,\n        \"Sgr A*\": 8.2e39,\n    }\n    \n    for name, mass in test_masses.items():\n        rs = schwarzschild_radius_robust(mass)\n        print(f\"{name}: Rs = {rs:.2e} cm ({rs/1e5:.2f} km)\")\n\n","type":"content","url":"/python-calculator-v3#id-2-9-from-interactive-to-script","position":45},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl2":"2.10 Variable Star Exercise Thread"},"type":"lvl2","url":"/python-calculator-v3#id-2-10-variable-star-exercise-thread","position":46},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl2":"2.10 Variable Star Exercise Thread"},"content":"Let’s continue building on our variable star from Chapter 1, adding magnitude calculations:\n\n# Chapter 2: Variable Star - Adding Magnitude Calculations\nimport json\nimport math\n\n# Create sample data (in real use, load from Chapter 1)\nstar = {\n    'name': 'Delta Cephei',\n    'period': 5.366319,\n    'mag_mean': 3.95,\n    'mag_amp': 0.88\n}\n\ndef calculate_phase(time, period):\n    \"\"\"Calculate phase (0-1) for given time.\"\"\"\n    phase = (time % period) / period\n    return phase\n\ndef magnitude_at_phase(mean_mag, amplitude, phase):\n    \"\"\"\n    Calculate magnitude at given phase.\n    Using simplified sinusoidal variation.\n    Real Cepheids have asymmetric light curves!\n    \"\"\"\n    # Magnitude gets SMALLER (brighter) at maximum\n    variation = amplitude * math.cos(2 * math.pi * phase)\n    return mean_mag + variation\n\n# Test with our star\ntest_time = 2.7  # days\nphase = calculate_phase(test_time, star['period'])\ncurrent_mag = magnitude_at_phase(star['mag_mean'], \n                                 star['mag_amp'], \n                                 phase)\n\nprint(f\"{star['name']} at time {test_time:.1f} days:\")\nprint(f\"  Phase: {phase:.3f}\")\nprint(f\"  Magnitude: {current_mag:.2f}\")\nprint(f\"  Brightness: {10**(-0.4 * current_mag):.3f} (relative flux)\")\n\n# Save enhanced data for Chapter 3\nstar['phase_function'] = 'sinusoidal'\nstar['last_calculated'] = {'time': test_time, 'phase': phase, 'magnitude': current_mag}\n\ntry:\n    with open('variable_star_ch2.json', 'w') as f:\n        json.dump(star, f, indent=2)\n    print(\"\\n✓ Data saved for Chapter 3!\")\nexcept IOError as e:\n    print(f\"\\n✗ Could not save: {e}\")\n\n","type":"content","url":"/python-calculator-v3#id-2-10-variable-star-exercise-thread","position":47},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl2":"Practice Exercises"},"type":"lvl2","url":"/python-calculator-v3#practice-exercises","position":48},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl2":"Practice Exercises"},"content":"","type":"content","url":"/python-calculator-v3#practice-exercises","position":49},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl3":"Exercise 2.1: Magnitude and Flux Conversions","lvl2":"Practice Exercises"},"type":"lvl3","url":"/python-calculator-v3#exercise-2-1-magnitude-and-flux-conversions","position":50},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl3":"Exercise 2.1: Magnitude and Flux Conversions","lvl2":"Practice Exercises"},"content":"Part A: Follow These Steps (5 min)\n\nExecute this exact code to understand magnitude-flux conversion:\n\nimport math\n\n# Step 1: Define the conversion formula\ndef mag_to_flux(magnitude, zero_point=0.0):\n    \"\"\"Convert magnitude to relative flux.\"\"\"\n    flux = 10**((zero_point - magnitude) / 2.5)\n    return flux\n\n# Step 2: Test with a specific magnitude\ntest_mag = 10.0\ntest_flux = mag_to_flux(test_mag)\nprint(f\"Magnitude {test_mag} = flux {test_flux:.6f}\")\n\n# Step 3: Verify the logarithmic relationship\nmag_diff = 5.0  # 5 magnitude difference\nflux_ratio = mag_to_flux(0) / mag_to_flux(mag_diff)\nprint(f\"{mag_diff} mag difference = {flux_ratio:.1f}× flux ratio\")\n\nPart B: Modify and Extend (10 min)\n\nNow add the inverse function and test round-trip conversion:\n\ndef flux_to_mag(flux, zero_point=0.0):\n    \"\"\"Convert flux to magnitude with error handling.\"\"\"\n    if flux <= 0:\n        return float('inf')  # Infinitely faint\n    \n    magnitude = zero_point - 2.5 * math.log10(flux)\n    return magnitude\n\n# Test round-trip conversion\noriginal_mag = 15.5\nflux = mag_to_flux(original_mag)\nrecovered_mag = flux_to_mag(flux)\nerror = abs(original_mag - recovered_mag)\n\nprint(f\"Original: {original_mag}\")\nprint(f\"Recovered: {recovered_mag}\")\nprint(f\"Error: {error:.2e}\")\nprint(f\"\\nWhy isn't error exactly zero?\")\nprint(\"Floating-point arithmetic introduces tiny errors!\")\n\nPart C: Apply to Real Data (15 min)\n\nCreate a function that correctly averages multiple magnitude measurements:\n\ndef average_magnitudes_wrong(mag_list):\n    \"\"\"WRONG: Simple arithmetic mean of magnitudes.\"\"\"\n    return sum(mag_list) / len(mag_list)\n\ndef average_magnitudes_correct(mag_list):\n    \"\"\"\n    CORRECT: Convert to flux, average, convert back.\n    This is how professional astronomy software works!\n    \"\"\"\n    if not mag_list:\n        raise ValueError(\"Empty magnitude list\")\n    \n    # Check for unreasonable values\n    for mag in mag_list:\n        if mag < -30 or mag > 40:\n            raise ValueError(f\"Magnitude {mag} outside reasonable range\")\n    \n    # Convert to fluxes\n    fluxes = [mag_to_flux(m) for m in mag_list]\n    \n    # Average the fluxes\n    mean_flux = sum(fluxes) / len(fluxes)\n    \n    # Convert back to magnitude\n    return flux_to_mag(mean_flux)\n\n# Test with example data\ntest_mags = [10.0, 10.5, 11.0]\n\nwrong_result = average_magnitudes_wrong(test_mags)\ncorrect_result = average_magnitudes_correct(test_mags)\n\nprint(f\"Magnitudes: {test_mags}\")\nprint(f\"Wrong (arithmetic mean): {wrong_result:.3f}\")\nprint(f\"Correct (flux-weighted): {correct_result:.3f}\")\nprint(f\"Difference: {wrong_result - correct_result:.3f} magnitudes\")\nprint(f\"\\nThis difference compounds with more measurements!\")","type":"content","url":"/python-calculator-v3#exercise-2-1-magnitude-and-flux-conversions","position":51},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl3":"Exercise 2.2: Numerical Hazard Detection","lvl2":"Practice Exercises"},"type":"lvl3","url":"/python-calculator-v3#exercise-2-2-numerical-hazard-detection","position":52},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl3":"Exercise 2.2: Numerical Hazard Detection","lvl2":"Practice Exercises"},"content":"Part A: Identify the Problem (5 min)\n\nRun this code and identify the numerical hazard:\n\n# Calculating small differences in large numbers\ndistance1 = 1.496e13  # 1 AU in cm\ndistance2 = 1.496e13 + 100  # 1 meter further\n\ndifference = distance2 - distance1\nprint(f\"Distance 1: {distance1:.10e} cm\")\nprint(f\"Distance 2: {distance2:.10e} cm\")\nprint(f\"Difference: {difference} cm\")\nprint(f\"Expected: 100 cm\")\nprint(f\"\\nWhat hazard is this demonstrating?\")\n\nPart B: Implement Detection (10 min)\n\nCreate a function to detect potential catastrophic cancellation:\n\ndef detect_cancellation_risk(a, b, threshold=0.01):\n    \"\"\"\n    Detect if subtracting a and b risks catastrophic cancellation.\n    \n    Returns True if |a-b| < threshold * max(|a|, |b|)\n    \"\"\"\n    if a == 0 or b == 0:\n        return False\n    \n    difference = abs(a - b)\n    scale = max(abs(a), abs(b))\n    relative_diff = difference / scale\n    \n    is_risky = relative_diff < threshold\n    \n    if is_risky:\n        print(f\"WARNING: Catastrophic cancellation risk!\")\n        print(f\"Relative difference: {relative_diff:.2e}\")\n    \n    return is_risky\n\n# Test with our distance example\ndetect_cancellation_risk(distance1, distance2)\n\n# Test with safe calculation\ndetect_cancellation_risk(100, 50)\n\nPart C: Apply to Orbital Mechanics (15 min)\n\nImplement safe calculation of orbital energy changes:\n\ndef orbital_energy_change_unsafe(r1, r2, M):\n    \"\"\"\n    UNSAFE: Direct calculation of energy change.\n    E = -GM/(2r) for circular orbit\n    \"\"\"\n    G = 6.67e-8  # CGS\n    E1 = -G * M / (2 * r1)\n    E2 = -G * M / (2 * r2)\n    return E2 - E1  # Catastrophic cancellation for r1 ≈ r2!\n\ndef orbital_energy_change_safe(r1, r2, M):\n    \"\"\"\n    SAFE: Reformulated to avoid cancellation.\n    ΔE = GM/2 * (1/r1 - 1/r2) = GM/2 * (r2-r1)/(r1*r2)\n    \"\"\"\n    G = 6.67e-8\n    \n    if r1 == r2:\n        return 0.0\n    \n    # Use reformulated expression\n    delta_E = G * M / 2 * (r2 - r1) / (r1 * r2)\n    return delta_E\n\n# Test with nearly equal radii (1 AU ± 1 km)\nr1 = 1.496e13  # 1 AU in cm\nr2 = 1.496e13 + 1e5  # 1 km further\nM = 1.989e33  # Solar mass\n\nunsafe = orbital_energy_change_unsafe(r1, r2, M)\nsafe = orbital_energy_change_safe(r1, r2, M)\n\nprint(f\"Unsafe calculation: {unsafe:.6e} erg\")\nprint(f\"Safe calculation:   {safe:.6e} erg\")\nprint(f\"Relative difference: {abs(unsafe-safe)/abs(safe):.2%}\")","type":"content","url":"/python-calculator-v3#exercise-2-2-numerical-hazard-detection","position":53},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl3":"Exercise 2.3: Build a Robust Calculator (Challenge)","lvl2":"Practice Exercises"},"type":"lvl3","url":"/python-calculator-v3#exercise-2-3-build-a-robust-calculator-challenge","position":54},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl3":"Exercise 2.3: Build a Robust Calculator (Challenge)","lvl2":"Practice Exercises"},"content":"Complete Project (20-30 min)\n\nBuild a scientific calculator with proper error handling:\n\nimport math\n\nclass ScientificCalculator:\n    \"\"\"A calculator with numerical safety features.\"\"\"\n    \n    def __init__(self):\n        self.history = []\n        self.epsilon = sys.float_info.epsilon\n    \n    def safe_log(self, x, base=math.e):\n        \"\"\"Logarithm with validation.\"\"\"\n        if x <= 0:\n            raise ValueError(f\"Cannot take log of {x}\")\n        \n        if base == math.e:\n            result = math.log(x)\n        elif base == 10:\n            result = math.log10(x)\n        else:\n            result = math.log(x) / math.log(base)\n        \n        self.history.append(f\"log_{base}({x}) = {result}\")\n        return result\n    \n    def safe_power(self, base, exponent):\n        \"\"\"Power operation with overflow protection.\"\"\"\n        # Check for potential overflow\n        if abs(base) > 1 and exponent > 100:\n            # Use log space\n            log_result = exponent * math.log10(abs(base))\n            if log_result > 300:  # Would overflow\n                return f\"10^{log_result:.1f}\"\n        \n        result = base ** exponent\n        self.history.append(f\"{base}^{exponent} = {result}\")\n        return result\n    \n    def compare_floats(self, a, b, tolerance=1e-9):\n        \"\"\"Safe float comparison.\"\"\"\n        return math.isclose(a, b, rel_tol=tolerance)\n\n# Test your calculator\ncalc = ScientificCalculator()\n\n# Test logarithm\nprint(f\"log₁₀(1000) = {calc.safe_log(1000, 10)}\")\n\n# Test power with large numbers\nprint(f\"10^300 = {calc.safe_power(10, 300)}\")\n\n# Test float comparison\na = 0.1 + 0.2\nb = 0.3\nprint(f\"0.1 + 0.2 == 0.3? {calc.compare_floats(a, b)}\")\n\n# Show history\nprint(\"\\nCalculation history:\")\nfor item in calc.history:\n    print(f\"  {item}\")","type":"content","url":"/python-calculator-v3#exercise-2-3-build-a-robust-calculator-challenge","position":55},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl2":"Main Takeaways"},"type":"lvl2","url":"/python-calculator-v3#main-takeaways","position":56},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl2":"Main Takeaways"},"content":"You’ve just built the foundation for all numerical astronomy you’ll ever do. The seemingly simple act of adding two numbers opens a universe of complexity that affects every calculation from orbital mechanics to cosmological simulations. The key insight isn’t that floating-point arithmetic is broken — it’s that it’s approximate by design, and understanding these approximations separates successful computational scientists from those who publish retracted papers due to numerical errors.\n\nThe defensive programming techniques you learned here might seem overcautious at first, but they’re battle-tested practices from real astronomical software. That safe_divide function has prevented countless divide-by-zero errors in production code. The validation checks have caught bugs that would have wasted weeks of supercomputer time. Working in log space isn’t just a clever trick — for many astronomical calculations spanning the extreme scales of our universe, it’s the only way to get meaningful answers.\n\nPerhaps most importantly, you’ve learned to think about numbers the way computers do. When you see 0.1 + 0.2, you now know it’s not exactly 0.3, and more crucially, you know why. When you calculate the distance to a galaxy, you instinctively think about whether the number might overflow. When you subtract two nearly-equal values, alarm bells ring about catastrophic cancellation. This numerical awareness will serve you throughout your career.\n\nThe disasters we discussed — Patriot missile, Mars Climate Orbiter — weren’t caused by incompetent programmers but by the subtle numerical issues you now understand. The successes — New Horizons reaching Pluto, LIGO detecting gravitational waves — all required mastery of exactly these concepts. You’re now equipped with the same numerical tools that enabled these triumphs.","type":"content","url":"/python-calculator-v3#main-takeaways","position":57},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl2":"Definitions"},"type":"lvl2","url":"/python-calculator-v3#definitions","position":58},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl2":"Definitions"},"content":"arbitrary precision: Python integers can grow to any size limited only by available memory, unlike fixed-size integers in compiled languages.\n\ncatastrophic cancellation: Loss of significant digits when subtracting nearly equal floating-point numbers, leaving mostly rounding errors.\n\nCGS units: Centimeter-gram-second unit system, standard in astrophysics (versus SI/MKS units used in physics).\n\ncomplex number: Number with real and imaginary parts, written as a + bj in Python, essential for wave physics.\n\ndynamic typing: Python’s ability to determine and change variable types at runtime without explicit declarations.\n\nf-string: Formatted string literal (f\"...{variable}...\") introduced in Python 3.6 for elegant output formatting.\n\nfloor division: Division that rounds toward negative infinity, using the // operator.\n\nIEEE 754: The international standard for floating-point arithmetic, defining how real numbers are represented in binary.\n\nmachine epsilon: Smallest positive floating-point number that, when added to 1.0, produces a result different from 1.0 (~2.2e-16).\n\nmantissa: The significant digits of a floating-point number, also called the significand.\n\nmodulo: Remainder after division, using the % operator.\n\noperator precedence: The order in which Python evaluates mathematical operations (PEMDAS).\n\noverflow: When a calculation exceeds the maximum representable floating-point value (~1.8e308).\n\ntype conversion: Changing data from one type to another (e.g., string to float), which may lose information.\n\nunderflow: When a calculation produces a value smaller than the minimum representable positive float, resulting in zero.","type":"content","url":"/python-calculator-v3#definitions","position":59},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl2":"Key Takeaways"},"type":"lvl2","url":"/python-calculator-v3#key-takeaways","position":60},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl2":"Key Takeaways"},"content":"✓ Floating-point arithmetic is approximate by design — never use == to compare floats\n\n✓ Machine epsilon (~2.2e-16) sets the fundamental precision limit for calculations\n\n✓ Catastrophic cancellation occurs when subtracting nearly equal numbers — use mathematical identities\n\n✓ Work in log space to handle astronomical scales without overflow/underflow\n\n✓ Python integers have unlimited precision but use more memory than floats\n\n✓ Variables are references to objects, not containers holding values\n\n✓ Defensive programming with validation prevents numerical disasters\n\n✓ F-strings provide powerful formatting for scientific output\n\n✓ Complex numbers are essential for wave physics and spectral analysis\n\n✓ Always use radians with trigonometric functions\n\n✓ Type conversion can lose information — always validate","type":"content","url":"/python-calculator-v3#key-takeaways","position":61},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl2":"Python Module & Method Reference (Chapter 2 Additions)"},"type":"lvl2","url":"/python-calculator-v3#python-module-method-reference-chapter-2-additions","position":62},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl2":"Python Module & Method Reference (Chapter 2 Additions)"},"content":"","type":"content","url":"/python-calculator-v3#python-module-method-reference-chapter-2-additions","position":63},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl3":"Math Module Functions","lvl2":"Python Module & Method Reference (Chapter 2 Additions)"},"type":"lvl3","url":"/python-calculator-v3#math-module-functions","position":64},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl3":"Math Module Functions","lvl2":"Python Module & Method Reference (Chapter 2 Additions)"},"content":"Mathematical Constantsimport math\n\nmath.pi - π (3.14159...)\n\nmath.e - Euler’s number (2.71828...)\n\nmath.tau - τ = 2π (6.28318...)\n\nArithmetic Functions\n\nmath.sqrt(x) - Square root\n\nmath.log(x) - Natural logarithm\n\nmath.log10(x) - Base-10 logarithm\n\nmath.exp(x) - Exponential (e^x)\n\nTrigonometric Functions (use radians!)\n\nmath.sin(x), math.cos(x), math.tan(x) - Basic trig\n\nmath.radians(degrees) - Convert degrees to radians\n\nmath.degrees(radians) - Convert radians to degrees\n\nSpecial Functions\n\nmath.gamma(x) - Gamma function\n\nmath.erf(x) - Error function\n\nmath.isclose(a, b, rel_tol=1e-9) - Safe float comparison\n\nmath.isfinite(x) - Check if finite (not inf or nan)\n\nmath.isnan(x) - Check if Not-a-Number","type":"content","url":"/python-calculator-v3#math-module-functions","position":65},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl3":"Complex Number Module","lvl2":"Python Module & Method Reference (Chapter 2 Additions)"},"type":"lvl3","url":"/python-calculator-v3#complex-number-module","position":66},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl3":"Complex Number Module","lvl2":"Python Module & Method Reference (Chapter 2 Additions)"},"content":"import cmath\n\ncmath.exp(x) - Complex exponential\n\ncmath.phase(z) - Argument/phase of complex number\n\ncmath.polar(z) - Convert to polar form (r, theta)\n\ncmath.rect(r, theta) - Convert from polar to rectangular","type":"content","url":"/python-calculator-v3#complex-number-module","position":67},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl3":"System Information","lvl2":"Python Module & Method Reference (Chapter 2 Additions)"},"type":"lvl3","url":"/python-calculator-v3#system-information","position":68},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl3":"System Information","lvl2":"Python Module & Method Reference (Chapter 2 Additions)"},"content":"import sys\n\nsys.float_info.epsilon - Machine epsilon (~2.2e-16)\n\nsys.float_info.max - Maximum float (~1.8e308)\n\nsys.float_info.min - Minimum positive float (~2.2e-308)\n\nsys.getsizeof(obj) - Memory size in bytes","type":"content","url":"/python-calculator-v3#system-information","position":69},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl3":"F-String Format Specifiers","lvl2":"Python Module & Method Reference (Chapter 2 Additions)"},"type":"lvl3","url":"/python-calculator-v3#f-string-format-specifiers","position":70},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl3":"F-String Format Specifiers","lvl2":"Python Module & Method Reference (Chapter 2 Additions)"},"content":"Format\n\nMeaning\n\nExample\n\nResult\n\n:.2f\n\nFixed decimal\n\nf\"{3.14159:.2f}\"\n\n3.14\n\n:.2e\n\nScientific notation\n\nf\"{1234:.2e}\"\n\n1.23e+03\n\n:10.2f\n\nWidth and decimals\n\nf\"{3.14:10.2f}\"\n\n      3.14\n\n:,.0f\n\nThousands separator\n\nf\"{1234567:,.0f}\"\n\n1,234,567\n\n:<10\n\nLeft align\n\nf\"{'test':<10}\"\n\ntest      \n\n:>10\n\nRight align\n\nf\"{'test':>10}\"\n\n      test\n\n:^10\n\nCenter align\n\nf\"{'test':^10}\"\n\n  test  ","type":"content","url":"/python-calculator-v3#f-string-format-specifiers","position":71},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl3":"Type Checking and Conversion","lvl2":"Python Module & Method Reference (Chapter 2 Additions)"},"type":"lvl3","url":"/python-calculator-v3#type-checking-and-conversion","position":72},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl3":"Type Checking and Conversion","lvl2":"Python Module & Method Reference (Chapter 2 Additions)"},"content":"Function\n\nPurpose\n\nExample\n\ntype(x)\n\nGet object type\n\ntype(3.14) → <class 'float'>\n\nisinstance(x, type)\n\nCheck type\n\nisinstance(3.14, float) → True\n\nfloat(x)\n\nConvert to float\n\nfloat(\"3.14\") → 3.14\n\nint(x)\n\nConvert to int (truncates!)\n\nint(3.9) → 3\n\ncomplex(r, i)\n\nCreate complex\n\ncomplex(3, 4) → (3+4j)\n\nround(x, n)\n\nRound to n decimals\n\nround(3.14159, 2) → 3.14","type":"content","url":"/python-calculator-v3#type-checking-and-conversion","position":73},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl2":"Next Chapter Preview"},"type":"lvl2","url":"/python-calculator-v3#next-chapter-preview","position":74},{"hierarchy":{"lvl1":"Chapter 2: Python as Your Astronomical Calculator","lvl2":"Next Chapter Preview"},"content":"Armed with a deep understanding of Python’s numeric types and the perils of floating-point arithmetic, you’re ready for Chapter 3: Control Flow & Logic. You’ll learn to make your code dynamic with if-statements and loops, building algorithms that can adapt to data and iterate until convergence. The numerical foundations from this chapter become essential when you’re checking whether a calculation has converged, comparing values within tolerance, or detecting numerical instabilities in your simulations. Get ready to transform static calculations into intelligent algorithms that can make decisions and repeat tasks — the essence of computational thinking.","type":"content","url":"/python-calculator-v3#next-chapter-preview","position":75},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic"},"type":"lvl1","url":"/python-control-flow-v2","position":0},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic"},"content":"","type":"content","url":"/python-control-flow-v2","position":1},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl2":"Learning Objectives"},"type":"lvl2","url":"/python-control-flow-v2#learning-objectives","position":2},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl2":"Learning Objectives"},"content":"By the end of this chapter, you will be able to:\n\nDesign algorithms using structured pseudocode before writing any Python code\n\nImplement conditional statements (if/elif/else) with proper handling of edge cases\n\nChoose appropriate loop structures (for vs while) based on problem requirements\n\nMaster all comparison operators (>, <, >=, <=, ==, !=) and logical operators (and, or, not)\n\nHandle floating-point comparisons safely in conditional statements\n\nDebug logic errors systematically using IPython’s debugger and assert statements\n\nWrite efficient list comprehensions while knowing when to avoid them\n\nRecognize and apply universal algorithmic patterns across different problems\n\nBuild defensive code that validates assumptions and catches errors early","type":"content","url":"/python-control-flow-v2#learning-objectives","position":3},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl2":"Prerequisites Check"},"type":"lvl2","url":"/python-control-flow-v2#prerequisites-check","position":4},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl2":"Prerequisites Check"},"content":"✅ Before Starting This Chapter\n\nYou can launch IPython and use magic commands like %timeit (Chapter 1)\n\nYou understand floating-point precision and comparison issues (Chapter 2)\n\nYou can write and run Python scripts from the terminal (Chapter 1)\n\nYou can use f-strings for formatted output (Chapter 2)\n\nYour astr596 environment is activated and working\n\nIf any boxes are unchecked, review the indicated chapters first.","type":"content","url":"/python-control-flow-v2#prerequisites-check","position":5},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl2":"Chapter Overview"},"type":"lvl2","url":"/python-control-flow-v2#chapter-overview","position":6},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl2":"Chapter Overview"},"content":"Programming is fundamentally about teaching computers to make decisions and repeat tasks. When you write an if statement or a loop, you’re translating human logic into instructions a machine can follow. But here’s the critical insight that separates computational thinkers from mere coders: the logic must be designed before it’s implemented. This chapter transforms you from someone who writes code to someone who designs algorithms.\n\nWe’ll start with the lost art of pseudocode — not as a bureaucratic exercise, but as the difference between code that works by accident and code that works by design. You’ll learn to recognize universal patterns that appear across all of computational physics: iteration, accumulation, filtering, mapping, and reduction. These patterns will appear in every project you build, from N-body simulations to neural networks. Whether you’re folding light curves to find exoplanet periods or iterating until your stellar model converges, these patterns form the backbone of computational astronomy.\n\nThe control flow structures we explore here are where your numerical calculations from Chapter 2 become dynamic algorithms. Every convergence test, every adaptive timestep, every Monte Carlo acceptance criterion depends on mastering these concepts deeply, not just syntactically. By chapter’s end, you’ll see code not as a sequence of commands, but as a carefully orchestrated flow of decisions and iterations that solve real scientific problems. You’ll write algorithms that could process data from the James Webb Space Telescope or control the adaptive optics on the next generation of ground-based observatories.","type":"content","url":"/python-control-flow-v2#chapter-overview","position":7},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl2":"3.1 Algorithmic Thinking: The Lost Art of Pseudocode"},"type":"lvl2","url":"/python-control-flow-v2#id-3-1-algorithmic-thinking-the-lost-art-of-pseudocode","position":8},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl2":"3.1 Algorithmic Thinking: The Lost Art of Pseudocode"},"content":"pseudocode\nHuman-readable algorithm description focusing on logic over syntax\n\nMost students jump straight from problem to code, then wonder why they spend hours debugging. Professional computational scientists spend more time thinking than typing. Pseudocode is how we think precisely about algorithms without getting distracted by syntax. Think of it as your algorithm’s blueprint — you wouldn’t build a telescope without optical designs, so why write code without algorithmic designs?","type":"content","url":"/python-control-flow-v2#id-3-1-algorithmic-thinking-the-lost-art-of-pseudocode","position":9},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"Why Pseudocode Matters in Scientific Computing","lvl2":"3.1 Algorithmic Thinking: The Lost Art of Pseudocode"},"type":"lvl3","url":"/python-control-flow-v2#why-pseudocode-matters-in-scientific-computing","position":10},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"Why Pseudocode Matters in Scientific Computing","lvl2":"3.1 Algorithmic Thinking: The Lost Art of Pseudocode"},"content":"Consider this scenario: You need to implement adaptive timestepping for an orbital integrator. Without pseudocode, you’ll likely write code, run it, watch orbits spiral incorrectly, debug for hours, and maybe get it working through trial and error. With pseudocode, you’ll identify edge cases, boundary conditions, and logical flaws before writing a single line of Python.\n\n# WITHOUT PSEUDOCODE (typical student approach):\n# \"I'll figure it out as I code...\"\ndef integrate_naive(state, t_end):\n    dt = 0.01\n    while state.time < t_end:\n        new_state = step(state, dt)\n        error = estimate_error(state, new_state)\n        if error > tolerance:\n            dt = dt * 0.5  # Seems reasonable?\n        state = new_state\n    return state\n# Wait, this doesn't work... infinite loop when error is bad!\n# Also, dt never increases... hours of debugging ahead\n\nNow let’s see how pseudocode reveals problems immediately! This is exactly how professional astronomers design algorithms for everything from orbit calculations to galaxy simulations. You’re about to learn the same systematic approach used at NASA’s Jet Propulsion Laboratory for spacecraft navigation and at the Space Telescope Science Institute for Hubble’s scheduling algorithms.","type":"content","url":"/python-control-flow-v2#why-pseudocode-matters-in-scientific-computing","position":11},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"The Three Levels of Pseudocode Refinement","lvl2":"3.1 Algorithmic Thinking: The Lost Art of Pseudocode"},"type":"lvl3","url":"/python-control-flow-v2#the-three-levels-of-pseudocode-refinement","position":12},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"The Three Levels of Pseudocode Refinement","lvl2":"3.1 Algorithmic Thinking: The Lost Art of Pseudocode"},"content":"Professional algorithm development happens in stages, each revealing different issues. Don’t worry if this feels strange at first — every programmer has felt that way! But once you embrace pseudocode, you’ll save countless hours of debugging. Let’s build this skill together:\n\nLevel 1: Conceptual Overview (The Big Picture)WHILE simulation not done:       # WHILE means \"repeat as long as condition is true\"\n    Take a step\n    Check IF step was good        # IF means \"only do this when condition is true\"\n    Adjust timestep\n\nThis level helps you understand the overall flow. The WHILE construct creates a loop that continues until some condition becomes false. The IF construct makes a decision based on a condition. Already, we can ask critical questions: What defines “done”? What makes a step “good”? How much should we adjust? These questions matter!\n\n🤔 Check Your Understanding\n\nBefore continuing, identify at least two problems with the Level 1 pseudocode above. What could go wrong?\n\nSolution\n\nNo exit condition if step is never “good” — infinite loop risk!\n\nNo bounds on timestep adjustment — could grow infinitely or shrink to zero\n\n“Simulation done” is vague — need precise termination condition\n\nNo error handling — what if the integration fails completely?\n\nThese aren’t nitpicks — they’re the difference between code that runs and code that runs correctly!\n\nLevel 2: Structural Detail (The Flow)FUNCTION adaptive_integrate(initial_state, end_time):  # FUNCTION groups reusable code\n    state ← initial_state                              # ← means \"assign value to variable\"\n    dt ← estimate_initial_timestep(state)\n    \n    WHILE time < end_time:                             # Loop continues while time hasn't reached end\n        DO:                                             # DO-UNTIL creates a loop that runs at least once\n            trial_step = integrate(state, dt)\n            error = compute_error(trial_step)\n        UNTIL error < tolerance OR dt < dt_min         # OR means \"either condition can be true\"\n        \n        state = trial_step\n        dt = adjust_timestep(error, dt)\n    \n    RETURN state                                        # RETURN sends value back to caller\n\nNow we see the retry logic and minimum timestep safeguard. The DO-UNTIL construct ensures we attempt at least one integration step. The OR operator means either condition being true will exit the inner loop. FUNCTION defines a reusable block of code that can be called with arguments and RETURN a result.\n\nLevel 3: Implementation-Ready (Stage 1: Core Logic)FUNCTION adaptive_integrate(initial_state, end_time, tolerance):\n    state ← initial_state\n    dt ← estimate_initial_timestep(state)\n    \n    WHILE state.time < end_time:\n        trial_state ← rk4_step(state, dt)\n        error ← estimate_error(state, trial_state)\n        \n        IF error < tolerance:                          # Decision point\n            state ← trial_state\n            dt ← min(dt * 1.5, dt_max)                # Can grow\n        ELSE:                                          # ELSE handles \"otherwise\" case\n            dt ← max(dt * 0.5, dt_min)                # Must shrink\n\nLevel 3: Implementation-Ready (Stage 2: Add Safety)FUNCTION adaptive_integrate(initial_state, end_time, tolerance):\n    state ← initial_state\n    dt ← estimate_initial_timestep(state)\n    dt_min ← 1e-10 * (end_time - initial_state.time)\n    dt_max ← 0.1 * (end_time - initial_state.time)\n    \n    WHILE state.time < end_time:\n        step_accepted ← False                          # Boolean flag (True/False)\n        attempts ← 0\n        \n        WHILE NOT step_accepted AND attempts < MAX_ATTEMPTS:  # NOT inverts, AND requires both\n            trial_state ← rk4_step(state, dt)\n            error ← estimate_error(state, trial_state)\n            \n            IF error < tolerance:\n                step_accepted ← True\n                state ← trial_state\n            ELSE:\n                dt ← max(dt * 0.5, dt_min)\n            attempts ← attempts + 1\n\nEach refinement level reveals new issues and solutions. The NOT operator inverts a boolean value (True becomes False, False becomes True). The AND operator requires both conditions to be true. This is computational thinking in action!\n\n💡 Computational Thinking: The Sentinel Pattern\n\nPATTERN: Sentinel Values\n\nA sentinel is a special value that signals “stop processing.” This pattern appears everywhere in computing:# Reading until special marker\ndata = []\nwhile True:\n    value = get_next_value()\n    if value == -999:  # Sentinel value\n        break\n    data.append(value)\n\nReal-world applications:\n\nFITS files: END keyword marks end of header\n\nNetwork protocols: Message terminators like “\\r\\n”\n\nTelescope data: -999 for missing observations\n\nString processing: Null terminators in C strings\n\nThe sentinel pattern is how computers know when to stop! You’re using the same technique that controls internet data packets and spacecraft telemetry streams.\n\n💡 Computational Thinking: The Universal Pattern of Adaptive Algorithms\n\nAdaptive timestepping is an instance of a universal pattern:\n\nPATTERN: Adaptive Refinement\n\nAttempt action with current parameters\n\nEvaluate quality of result\n\nIf quality insufficient: refine parameters and retry\n\nIf quality acceptable: proceed and possibly coarsen\n\nInclude safeguards against infinite refinement\n\nThis pattern appears everywhere in computational astrophysics:\n\nAdaptive mesh refinement (AMR) in galaxy formation simulations\n\nStep size control in stellar evolution codes like MESA\n\nLearning rate scheduling in neural networks for photometric redshifts\n\nConvergence acceleration in self-consistent field calculations\n\nImportance sampling in Monte Carlo radiative transfer\n\nOnce you recognize this pattern, you’ll see it in every sophisticated astronomical code!","type":"content","url":"/python-control-flow-v2#the-three-levels-of-pseudocode-refinement","position":13},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl2":"3.2 Boolean Logic in Scientific Computing"},"type":"lvl2","url":"/python-control-flow-v2#id-3-2-boolean-logic-in-scientific-computing","position":14},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl2":"3.2 Boolean Logic in Scientific Computing"},"content":"Every decision in your code ultimately reduces to true or false. But in scientific computing, these decisions often involve floating-point numbers, where equality is treacherous and precision is limited. Let’s master this fundamental building block that underlies everything from data quality checks to convergence criteria!","type":"content","url":"/python-control-flow-v2#id-3-2-boolean-logic-in-scientific-computing","position":15},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"The Complete Set of Comparison Operators","lvl2":"3.2 Boolean Logic in Scientific Computing"},"type":"lvl3","url":"/python-control-flow-v2#the-complete-set-of-comparison-operators","position":16},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"The Complete Set of Comparison Operators","lvl2":"3.2 Boolean Logic in Scientific Computing"},"content":"Python provides six comparison operators that return boolean values (True or False):\n\n# All comparison operators in Python\ntemperature = 5778  # Kelvin (Sun's surface)\n\n# The six fundamental comparisons\nprint(f\"Greater than: {temperature > 6000}\")           # False\nprint(f\"Less than: {temperature < 6000}\")              # True\nprint(f\"Greater or equal: {temperature >= 5778}\")      # True\nprint(f\"Less or equal: {temperature <= 5778}\")         # True\nprint(f\"Equal to: {temperature == 5778}\")              # True\nprint(f\"Not equal to: {temperature != 6000}\")          # True\n\n# Chaining comparisons (Python's elegant feature!)\nprint(f\"\\nMain sequence star? {3000 < temperature < 50000}\")  # True\n# This is equivalent to: (3000 < temperature) AND (temperature < 50000)\n\nThe != operator (not equal) is particularly useful for filtering out sentinel values or checking if something has changed. The ability to chain comparisons like 3000 < temperature < 50000 is a Python feature that makes code more readable and matches mathematical notation.","type":"content","url":"/python-control-flow-v2#the-complete-set-of-comparison-operators","position":17},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"The Three Logical Operators: AND, OR, NOT","lvl2":"3.2 Boolean Logic in Scientific Computing"},"type":"lvl3","url":"/python-control-flow-v2#the-three-logical-operators-and-or-not","position":18},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"The Three Logical Operators: AND, OR, NOT","lvl2":"3.2 Boolean Logic in Scientific Computing"},"content":"Python’s logical operators combine or modify boolean values:\n\n# Demonstrating all three logical operators\nis_bright = True\nis_variable = False\n\n# AND: Both conditions must be true\nprint(f\"Bright AND variable: {is_bright and is_variable}\")  # False\n\n# OR: At least one condition must be true  \nprint(f\"Bright OR variable: {is_bright or is_variable}\")     # True\n\n# NOT: Inverts the boolean value\nprint(f\"NOT bright: {not is_bright}\")                       # False\nprint(f\"NOT variable: {not is_variable}\")                   # True\n\n# Complex combinations (real telescope scheduling logic!)\nobservable = True\nweather_good = True\ncalibrated = False\n\ncan_observe = observable and weather_good and (not calibrated or calibrated)\nprint(f\"\\nCan observe? {can_observe}\")\n\n# Truth table demonstration\nprint(\"\\nTruth Table for AND:\")\nfor a in [True, False]:\n    for b in [True, False]:\n        print(f\"  {a:5} AND {b:5} = {a and b}\")\n\n","type":"content","url":"/python-control-flow-v2#the-three-logical-operators-and-or-not","position":19},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"Special Comparison Operators: is, in","lvl2":"3.2 Boolean Logic in Scientific Computing"},"type":"lvl3","url":"/python-control-flow-v2#special-comparison-operators-is-in","position":20},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"Special Comparison Operators: is, in","lvl2":"3.2 Boolean Logic in Scientific Computing"},"content":"Python has two special operators that are incredibly useful in scientific programming:\n\nimport math\n\n# The 'is' operator checks identity (same object in memory)\na = [1, 2, 3]\nb = [1, 2, 3]\nc = a\n\nprint(f\"a == b: {a == b}\")  # True - same values\nprint(f\"a is b: {a is b}\")  # False - different objects\nprint(f\"a is c: {a is c}\")  # True - same object\n\n# Special case: None should always use 'is'\nresult = None\nprint(f\"Checking None: {result is None}\")  # Preferred\n# Don't use: result == None\n\n# The 'in' operator checks membership\nstellar_types = ['O', 'B', 'A', 'F', 'G', 'K', 'M']\nprint(f\"\\nIs 'G' a stellar type? {'G' in stellar_types}\")  # True\nprint(f\"Is 'X' a stellar type? {'X' in stellar_types}\")    # False\n\n# Works with strings too!\nfilename = \"observations_2024.fits\"\nprint(f\"Is FITS file? {'fits' in filename}\")  # True\n\n","type":"content","url":"/python-control-flow-v2#special-comparison-operators-is-in","position":21},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"The Walrus Operator: Assignment Expressions (Python 3.8+)","lvl2":"3.2 Boolean Logic in Scientific Computing"},"type":"lvl3","url":"/python-control-flow-v2#the-walrus-operator-assignment-expressions-python-3-8","position":22},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"The Walrus Operator: Assignment Expressions (Python 3.8+)","lvl2":"3.2 Boolean Logic in Scientific Computing"},"content":"Python 3.8 introduced the walrus operator (:=) which allows assignment within expressions:\n\n# Traditional approach - two steps\ndata = get_observations()  # Assume this function exists\nif len(data) > 100:\n    print(f\"Large dataset: {len(data)} observations\")\n    # Notice we call len(data) twice!\n\n# With walrus operator - assign and test in one line\n# if (n := len(data)) > 100:\n#     print(f\"Large dataset: {n} observations\")\n#     # Now n contains the length, no need to recalculate!\n\n# Real astronomical example (simulated)\ndef check_observation_quality(observations):\n    \"\"\"Check if we have enough high-quality observations.\"\"\"\n    # Without walrus operator:\n    good_obs = [obs for obs in observations if obs['snr'] > 5]\n    if len(good_obs) >= 10:\n        print(f\"Found {len(good_obs)} good observations\")\n        return good_obs\n    \n    # With walrus operator (Python 3.8+):\n    # if (good_count := len(good_obs)) >= 10:\n    #     print(f\"Found {good_count} good observations\")\n    #     return good_obs\n    \n    return None\n\n# Useful in while loops too\n# while (line := file.readline()):  # Read and check in one step\n#     process(line)\n\nprint(\"Note: Walrus operator requires Python 3.8+\")\nprint(\"It's useful but not essential - all code can be written without it\")\n\n📝 Note on the Walrus Operator\n\nThe walrus operator (:=) is optional syntactic sugar introduced in Python 3.8. While it can make some code more concise, it’s perfectly fine to write code without it. Many astronomers still use Python 3.7 or earlier, so don’t rely on it for shared code.\n\nUse it when:\n\nYou need to use a value in a condition and then reuse it in the body\n\nReading files line by line in a while loop\n\nAvoiding repeated expensive calculations\n\nAvoid it when:\n\nIt makes the code harder to read\n\nWorking with Python < 3.8\n\nThe traditional approach is clearer","type":"content","url":"/python-control-flow-v2#the-walrus-operator-assignment-expressions-python-3-8","position":23},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"The Floating-Point Equality Trap","lvl2":"3.2 Boolean Logic in Scientific Computing"},"type":"lvl3","url":"/python-control-flow-v2#the-floating-point-equality-trap","position":24},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"The Floating-Point Equality Trap","lvl2":"3.2 Boolean Logic in Scientific Computing"},"content":"Never use == with floating-point numbers! Even tiny rounding errors break equality:\n\n# The floating-point trap strikes!\ncalculated_temp = 5778.0000000001\nexpected_temp = 5778.0\n\nprint(f\"Calculated == Expected? {calculated_temp == expected_temp}\")  # False!\nprint(f\"Tiny difference: {calculated_temp - expected_temp:.2e}\")\n\n# The solution: tolerance-based comparison\ndef safe_equal(a, b, rel_tol=1e-9, abs_tol=1e-12):\n    \"\"\"\n    Safe floating-point comparison for scientific computing.\n    Used in actual telescope pointing systems!\n    \"\"\"\n    # Handle exact equality (includes infinities)\n    if a == b:\n        return True\n    \n    # Handle NaN (NaN != NaN by IEEE standard)\n    if math.isnan(a) or math.isnan(b):\n        return False\n    \n    # Handle infinity cases\n    if math.isinf(a) or math.isinf(b):\n        return a == b\n    \n    # Normal comparison with tolerance\n    # Note: math.isclose() does this internally, but understanding it matters!\n    return abs(a - b) <= max(rel_tol * max(abs(a), abs(b)), abs_tol)\n\n# Test our safe comparison\nprint(f\"\\n0.1 + 0.2 == 0.3? {0.1 + 0.2 == 0.3}\")  # False!\nprint(f\"Safe equal? {safe_equal(0.1 + 0.2, 0.3)}\")  # True!\nprint(f\"math.isclose? {math.isclose(0.1 + 0.2, 0.3)}\")  # True!\n\n⚠️ Common Bug Alert: The Equality Trap\n\nTODO: Check this story! Is it made up? Need references.\n\nNever use == with floating-point numbers! Even tiny rounding errors break equality.\n\nWrong (caused real satellite collision near-miss):if velocity == 0.0:  # Dangerous!\n    print(\"At rest\")\n\nRight (used in actual spacecraft code):if abs(velocity) < 1e-10:  # Safe!\n    print(\"Effectively at rest\")\n\nThe International Space Station uses similar tolerance checks for all docking maneuvers!","type":"content","url":"/python-control-flow-v2#the-floating-point-equality-trap","position":25},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"Short-Circuit Evaluation: Order Matters!","lvl2":"3.2 Boolean Logic in Scientific Computing"},"type":"lvl3","url":"/python-control-flow-v2#short-circuit-evaluation-order-matters","position":26},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"Short-Circuit Evaluation: Order Matters!","lvl2":"3.2 Boolean Logic in Scientific Computing"},"content":"short-circuit evaluation\nStopping logical evaluation once result is determined\n\nPython’s and and or operators use short-circuit evaluation — they stop evaluating as soon as the result is determined:\n\n# Short-circuit evaluation prevents errors and saves computation\nstellar_data = []  # Empty for demonstration\n\n# WRONG - will crash if data is empty!\n# if stellar_data[0] > 0 and len(stellar_data) > 0:  # IndexError!\n\n# CORRECT - checks length first (short-circuits if empty)\nif len(stellar_data) > 0 and stellar_data[0] > 0:\n    print(\"First star has positive measurement\")\nelse:\n    print(\"No data or first measurement not positive\")\n\n# Even more Pythonic (empty list evaluates to False)\nif stellar_data and stellar_data[0] > 0:\n    print(\"First star has positive measurement\")\n\n# OR also short-circuits\ndef expensive_check():\n    print(\"  Running expensive calculation...\")\n    return True\n\n# This won't call expensive_check() because True or anything is True\nprint(\"\\nShort-circuit OR demonstration:\")\nresult = True or expensive_check()  # expensive_check never runs!\nprint(f\"Result: {result}\")\n\n# But this will call it\nresult = False or expensive_check()  # expensive_check must run\nprint(f\"Result: {result}\")\n\n🌟 Why This Matters: Satellite Collision Avoidance\n\nTODO: Get references, is this true?\nThe European Space Agency uses boolean logic chains for collision warnings:def check_collision_risk(satellite1, satellite2):\n    \"\"\"Actual logic used for collision avoidance (simplified)\"\"\"\n    \n    # Order matters for efficiency!\n    # Check cheap calculations first\n    if distance > safe_threshold:  \n        return False  # No need to calculate velocities\n    \n    # Only if close, calculate expensive velocity vectors\n    if relative_velocity < 0:  \n        return False  # Moving apart\n    \n    # Only if approaching, do complex uncertainty calculation\n    if combined_uncertainty < max_allowed:\n        return True  # COLLISION RISK!\n    \n    return False\n\nChecking distance first avoids millions of expensive velocity calculations per day. A single wrong comparison could mean losing a $500 million satellite! In 2009, Iridium 33 and Cosmos 2251 collided because their warning system failed to properly evaluate these conditions.\n\nNote: This example simplifies the actual collision avoidance algorithms for pedagogical clarity. Real systems use complex orbital mechanics and probability distributions, but the core principle of ordered boolean evaluation remains crucial.","type":"content","url":"/python-control-flow-v2#short-circuit-evaluation-order-matters","position":27},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl2":"3.3 Conditional Statements: Teaching Computers to Decide"},"type":"lvl2","url":"/python-control-flow-v2#id-3-3-conditional-statements-teaching-computers-to-decide","position":28},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl2":"3.3 Conditional Statements: Teaching Computers to Decide"},"content":"guard clause\nEarly return statement that handles edge cases before main logic\n\nConditional statements are where your code makes decisions. In scientific computing, these decisions often involve numerical thresholds, convergence criteria, and boundary conditions. Let’s build your intuition for writing robust conditionals that could run on spacecraft or control telescopes!","type":"content","url":"/python-control-flow-v2#id-3-3-conditional-statements-teaching-computers-to-decide","position":29},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"The if Statement: Your First Decision Maker","lvl2":"3.3 Conditional Statements: Teaching Computers to Decide"},"type":"lvl3","url":"/python-control-flow-v2#the-if-statement-your-first-decision-maker","position":30},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"The if Statement: Your First Decision Maker","lvl2":"3.3 Conditional Statements: Teaching Computers to Decide"},"content":"The if statement is the simplest conditional — it executes code only when a condition is true:\n\n# Basic if statement\nmagnitude = 4.5\n\nif magnitude < 6.0:\n    print(f\"Star is visible to naked eye (mag {magnitude})\")\n\n# Nothing happens if condition is false\nmagnitude = 8.0\nif magnitude < 6.0:\n    print(\"This won't print\")\n    \n# Multiple statements in if block\nstellar_mass = 10.0  # Solar masses\n\nif stellar_mass > 8:\n    print(\"Massive star detected!\")\n    print(f\"Mass: {stellar_mass} M☉\")\n    print(\"Will end as supernova\")\n    remnant = \"neutron star or black hole\"\n\n","type":"content","url":"/python-control-flow-v2#the-if-statement-your-first-decision-maker","position":31},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"The if-else Statement: Binary Decisions","lvl2":"3.3 Conditional Statements: Teaching Computers to Decide"},"type":"lvl3","url":"/python-control-flow-v2#the-if-else-statement-binary-decisions","position":32},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"The if-else Statement: Binary Decisions","lvl2":"3.3 Conditional Statements: Teaching Computers to Decide"},"content":"The else clause provides an alternative when the condition is false:\n\n# Binary decision with if-else\nredshift = 0.8\n\nif redshift < 0.1:\n    classification = \"nearby galaxy\"\nelse:\n    classification = \"distant galaxy\"\n    \nprint(f\"z = {redshift}: {classification}\")\n\n# You can have multiple statements in each block\nobservation_snr = 3.5  # Signal-to-noise ratio\n\nif observation_snr >= 5.0:\n    print(\"High quality detection\")\n    process_immediately = True\n    confidence = \"high\"\nelse:\n    print(\"Low SNR - needs verification\")\n    process_immediately = False\n    confidence = \"low\"\n\n","type":"content","url":"/python-control-flow-v2#the-if-else-statement-binary-decisions","position":33},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"The elif Statement: Multiple Choices","lvl2":"3.3 Conditional Statements: Teaching Computers to Decide"},"type":"lvl3","url":"/python-control-flow-v2#the-elif-statement-multiple-choices","position":34},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"The elif Statement: Multiple Choices","lvl2":"3.3 Conditional Statements: Teaching Computers to Decide"},"content":"The elif (else if) statement allows multiple conditions to be checked in sequence:\n\ndef classify_stellar_remnant(mass_solar):\n    \"\"\"\n    Determine stellar remnant type based on initial mass.\n    Demonstrates guard clauses and elif chains.\n    Based on Chandrasekhar limit and stellar evolution theory.\n    \"\"\"\n    # Guard clauses - validate input FIRST\n    if mass_solar <= 0:\n        raise ValueError(f\"Mass must be positive: {mass_solar}\")\n    \n    if not math.isfinite(mass_solar):\n        raise ValueError(f\"Mass must be finite: {mass_solar}\")\n    \n    # Main classification logic with elif chain\n    if mass_solar < 0.08:\n        remnant = \"brown dwarf (failed star)\"\n    elif mass_solar < 8:\n        remnant = \"white dwarf\"\n    elif mass_solar < 25:\n        remnant = \"neutron star\"\n    else:  # Final catch-all\n        remnant = \"black hole\"\n    \n    # Add uncertainty near boundaries (real issue in astronomy!)\n    boundaries = [0.08, 8, 25]\n    min_distance = min(abs(mass_solar - b) for b in boundaries)\n    \n    if min_distance < 0.5:\n        remnant += \" (near boundary - uncertain)\"\n    \n    return remnant\n\n# Test our classifier\nprint(classify_stellar_remnant(1.0))   # Our Sun's fate\nprint(classify_stellar_remnant(7.8))   # Near boundary!\nprint(classify_stellar_remnant(30))    # Massive star fate\n\n","type":"content","url":"/python-control-flow-v2#the-elif-statement-multiple-choices","position":35},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"Guard Clauses: Fail Fast, Fail Clear","lvl2":"3.3 Conditional Statements: Teaching Computers to Decide"},"type":"lvl3","url":"/python-control-flow-v2#guard-clauses-fail-fast-fail-clear","position":36},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"Guard Clauses: Fail Fast, Fail Clear","lvl2":"3.3 Conditional Statements: Teaching Computers to Decide"},"content":"Guard clauses handle special cases immediately, preventing deep nesting and making code clearer. This pattern is essential for scientific code where invalid inputs can cause subtle bugs hours into a simulation!\n\ndef calculate_orbital_period(a, M, validate=True):\n    \"\"\"\n    Kepler's third law with comprehensive validation.\n    This defensive style could have saved Mars Climate Orbiter!\n    \n    Parameters:\n        a: Semi-major axis [AU]\n        M: Central mass [solar masses]\n    \"\"\"\n    # Guard clauses handle problems immediately\n    if validate:\n        if a <= 0:\n            raise ValueError(f\"Semi-major axis must be positive: {a} AU\")\n        if M <= 0:\n            raise ValueError(f\"Mass must be positive: {M} M☉\")\n        \n        # Check for orbit inside Schwarzschild radius!\n        rs_au = 2.95e-8 * M  # Schwarzschild radius in AU\n        if a < rs_au:\n            raise ValueError(f\"Orbit inside black hole event horizon: a={a} AU, Rs={rs_au} AU\")\n    \n    # Main calculation - only runs if guards pass\n    G_au_msun = 39.478  # G in AU³/M☉/year²\n    period_years = math.sqrt(a**3 / M)  # Simplified Kepler's third law\n    \n    # Sanity check result\n    if validate and period_years > 13.8e9:\n        import warnings\n        warnings.warn(f\"Period exceeds age of universe: {period_years:.2e} years\")\n    \n    return period_years\n\n# Test with real systems\nprint(f\"Earth: {calculate_orbital_period(1.0, 1.0):.2f} years\")\nprint(f\"Mercury: {calculate_orbital_period(0.387, 1.0):.2f} years\")\nprint(f\"Proxima Centauri b: {calculate_orbital_period(0.0485, 0.122):.3f} years\")\n\n","type":"content","url":"/python-control-flow-v2#guard-clauses-fail-fast-fail-clear","position":37},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"The Ternary Operator: Compact Conditionals","lvl2":"3.3 Conditional Statements: Teaching Computers to Decide"},"type":"lvl3","url":"/python-control-flow-v2#the-ternary-operator-compact-conditionals","position":38},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"The Ternary Operator: Compact Conditionals","lvl2":"3.3 Conditional Statements: Teaching Computers to Decide"},"content":"Python’s ternary operator provides a compact way to write simple if-else statements:\n\n# Ternary operator: value_if_true if condition else value_if_false\nmagnitude = 3.5\nvisibility = \"visible\" if magnitude < 6.0 else \"not visible\"\nprint(f\"Star with magnitude {magnitude} is {visibility}\")\n\n# Useful for setting defaults based on conditions\nexposure_time = 30  # seconds\nquality = \"good\" if exposure_time > 10 else \"poor\"\n\n# Can be nested but don't overdo it!\nstellar_class = \"G\"\ntemperature = 5778 if stellar_class == \"G\" else (7500 if stellar_class == \"A\" else 3500)\nprint(f\"Class {stellar_class} star: ~{temperature}K\")\n\n🌟 Why This Matters: The Mars Climate Orbiter Disaster\n\nIn 1999, NASA lost the $327.6 million Mars Climate Orbiter because one team used metric units while another used imperial. A simple guard clause could have saved it:\n\nNote: The $327.6 million figure represents total mission cost. This anecdote simplifies a complex failure to emphasize the importance of unit validation. The actual failure involved multiple factors, but the unit confusion was the primary cause identified in NASA’s investigation reports.def process_thrust_data(force, units):\n    \"\"\"This guard clause would have saved $327 million!\"\"\"\n    \n    # Validate units BEFORE processing\n    valid_units = {'N': 1.0, 'lbf': 4.448222}  # Conversion factors\n    \n    if units not in valid_units:\n        raise ValueError(f\"Unknown units: {units}. Use 'N' or 'lbf'\")\n    \n    # Convert to standard units (Newtons)\n    force_newtons = force * valid_units[units]\n    \n    # Additional sanity check\n    if force_newtons > 1000:  # Typical max thruster force\n        warnings.warn(f\"Unusually high thrust: {force_newtons} N\")\n    \n    return force_newtons\n\n# The actual error: ground software sent lbf, flight expected N\n# Result: 4.45× error accumulated over months!\n\nThe orbiter entered Mars atmosphere at 57 km instead of 226 km altitude and disintegrated. Your guard clauses aren’t just good practice — they prevent disasters!","type":"content","url":"/python-control-flow-v2#the-ternary-operator-compact-conditionals","position":39},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl2":"3.4 Loops: The Heart of Scientific Computation"},"type":"lvl2","url":"/python-control-flow-v2#id-3-4-loops-the-heart-of-scientific-computation","position":40},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl2":"3.4 Loops: The Heart of Scientific Computation"},"content":"Now that you’ve mastered making decisions with conditionals, let’s make your code repeat tasks efficiently! Loops are where your programs gain superpowers — they’re the difference between analyzing one star and analyzing millions. Every N-body simulation, every light curve analysis, every Monte Carlo calculation depends on loops. The patterns you learn here will appear in every algorithm you write for the rest of your career.","type":"content","url":"/python-control-flow-v2#id-3-4-loops-the-heart-of-scientific-computation","position":41},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"The for Loop: Iterating Over Sequences","lvl2":"3.4 Loops: The Heart of Scientific Computation"},"type":"lvl3","url":"/python-control-flow-v2#the-for-loop-iterating-over-sequences","position":42},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"The for Loop: Iterating Over Sequences","lvl2":"3.4 Loops: The Heart of Scientific Computation"},"content":"The for loop iterates over any sequence (list, tuple, string, range):\n\n# Basic for loop\nstellar_types = ['O', 'B', 'A', 'F', 'G', 'K', 'M']\n\nfor spectral_class in stellar_types:\n    print(f\"Class {spectral_class} star\")\n\n# Using range() for counting\nprint(\"\\nCounting with range:\")\nfor i in range(5):  # 0, 1, 2, 3, 4 (not 5!)\n    print(f\"Observation {i}\")\n\n# Range with start, stop, step\nprint(\"\\nEvery 2nd hour from 20:00 to 02:00:\")\nfor hour in range(20, 26, 2):  # 20, 22, 24\n    print(f\"{hour:02d}:00\")\n\n","type":"content","url":"/python-control-flow-v2#the-for-loop-iterating-over-sequences","position":43},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"The Accumulator Pattern in Astronomy","lvl2":"3.4 Loops: The Heart of Scientific Computation"},"type":"lvl3","url":"/python-control-flow-v2#the-accumulator-pattern-in-astronomy","position":44},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"The Accumulator Pattern in Astronomy","lvl2":"3.4 Loops: The Heart of Scientific Computation"},"content":"accumulator pattern\nIteratively combining values into a running aggregate\n\nThe accumulator pattern is fundamental to scientific computing:\n\n# Calculating center of mass for a star cluster\nstar_masses = [1.2, 0.8, 2.1, 0.5, 1.5]  # Solar masses\nstar_positions = [0.1, 0.3, 0.5, 0.7, 0.9]  # Parsecs from origin\n\ntotal_mass = 0\nweighted_position = 0\n\nfor mass, position in zip(star_masses, star_positions):\n    total_mass += mass\n    weighted_position += mass * position\n\ncenter_of_mass = weighted_position / total_mass\nprint(f\"Cluster center of mass: {center_of_mass:.3f} pc\")\nprint(f\"Total cluster mass: {total_mass:.1f} M☉\")\n\n# This same pattern calculates:\n# - Barycenter of binary systems (how we detect exoplanets!)\n# - Average stellar metallicity in galaxies\n# - Integrated luminosity functions\n# - Weighted mean magnitudes\n\n","type":"content","url":"/python-control-flow-v2#the-accumulator-pattern-in-astronomy","position":45},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"Common for Loop Patterns","lvl2":"3.4 Loops: The Heart of Scientific Computation"},"type":"lvl3","url":"/python-control-flow-v2#common-for-loop-patterns","position":46},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"Common for Loop Patterns","lvl2":"3.4 Loops: The Heart of Scientific Computation"},"content":"Python provides several useful functions for loop patterns:\n\n# enumerate() gives you index and value\nmagnitudes = [10.2, 10.1, 9.5, 10.3, 8.2, 10.2]\n\nprint(\"Finding bright events with enumerate:\")\nfor i, mag in enumerate(magnitudes):\n    if mag < 9.0:\n        print(f\"  Alert! Index {i}: magnitude {mag}\")\n\n# zip() for parallel iteration\ntimes = [0, 1, 2, 3, 4]  # seconds\npositions = [0, 4.9, 19.6, 44.1, 78.4]  # meters\n\nprint(\"\\nParallel iteration with zip:\")\nfor t, x in zip(times, positions):\n    if t > 0:  # Avoid division by zero\n        velocity = x / t\n        print(f\"  t={t}s: v={velocity:.1f} m/s\")\n\n","type":"content","url":"/python-control-flow-v2#common-for-loop-patterns","position":47},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"The while Loop: Conditional Iteration","lvl2":"3.4 Loops: The Heart of Scientific Computation"},"type":"lvl3","url":"/python-control-flow-v2#the-while-loop-conditional-iteration","position":48},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"The while Loop: Conditional Iteration","lvl2":"3.4 Loops: The Heart of Scientific Computation"},"content":"The while loop continues as long as a condition remains true:\n\n# Basic while loop\ncount = 0\nwhile count < 3:\n    print(f\"Iteration {count}\")\n    count += 1  # Don't forget to update!\n\n# While loop for convergence\nprint(\"\\nConvergence example:\")\nvalue = 100.0\ntarget = 1.0\niteration = 0\n\nwhile abs(value - target) > 0.01 and iteration < 100:  # Safety limit!\n    value = value * 0.9 + target * 0.1  # Gradual approach\n    iteration += 1\n    if iteration <= 3 or iteration % 10 == 0:  # Print selectively\n        print(f\"  Iter {iteration}: value = {value:.3f}\")\n\nprint(f\"Converged to {value:.3f} after {iteration} iterations\")\n\n","type":"content","url":"/python-control-flow-v2#the-while-loop-conditional-iteration","position":49},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"Loop Control: break, continue, and else","lvl2":"3.4 Loops: The Heart of Scientific Computation"},"type":"lvl3","url":"/python-control-flow-v2#loop-control-break-continue-and-else","position":50},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"Loop Control: break, continue, and else","lvl2":"3.4 Loops: The Heart of Scientific Computation"},"content":"Python provides additional loop control statements:\n\n# break: Exit loop early\nprint(\"Using break to find first detection:\")\nobservations = [0.1, 0.3, 0.2, 5.8, 0.4, 6.2]\n\nfor obs in observations:\n    if obs > 1.0:\n        print(f\"First significant detection: {obs}\")\n        break  # Stop searching\n        \n# continue: Skip to next iteration\nprint(\"\\nUsing continue to skip bad data:\")\nmeasurements = [1.2, -999, 2.3, -999, 3.4]\n\nfor value in measurements:\n    if value == -999:  # Sentinel value\n        continue  # Skip this iteration\n    print(f\"Processing: {value}\")\n\n# else clause: Runs if loop completes without break\nprint(\"\\nLoop else clause:\")\nsearch_list = [1, 2, 3, 4, 5]\ntarget = 7\n\nfor item in search_list:\n    if item == target:\n        print(\"Found!\")\n        break\nelse:  # This runs because we didn't break\n    print(f\"Target {target} not found in list\")\n\n","type":"content","url":"/python-control-flow-v2#loop-control-break-continue-and-else","position":51},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"The pass Statement: Placeholder","lvl2":"3.4 Loops: The Heart of Scientific Computation"},"type":"lvl3","url":"/python-control-flow-v2#the-pass-statement-placeholder","position":52},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"The pass Statement: Placeholder","lvl2":"3.4 Loops: The Heart of Scientific Computation"},"content":"The pass statement does nothing — useful as a placeholder:\n\n# pass as placeholder\nfor i in range(3):\n    if i == 1:\n        pass  # TODO: Add processing here later\n    else:\n        print(f\"Processing {i}\")\n\n# Often used in exception handling\ntry:\n    risky_operation = 1 / 1  # No error this time\nexcept ZeroDivisionError:\n    pass  # Silently ignore this specific error\n\nprint(\"Continued after pass\")\n\n","type":"content","url":"/python-control-flow-v2#the-pass-statement-placeholder","position":53},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"Nested Loops: Processing 2D Data","lvl2":"3.4 Loops: The Heart of Scientific Computation"},"type":"lvl3","url":"/python-control-flow-v2#nested-loops-processing-2d-data","position":54},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"Nested Loops: Processing 2D Data","lvl2":"3.4 Loops: The Heart of Scientific Computation"},"content":"Loops can be nested to process multi-dimensional data:\n\n# Nested loops for 2D grid (like CCD image)\nprint(\"Processing 3x3 pixel grid:\")\nfor row in range(3):\n    for col in range(3):\n        pixel_value = row * 3 + col  # Simulated pixel value\n        print(f\"({row},{col})={pixel_value}\", end=\"  \")\n    print()  # New line after each row\n\n# More realistic: Finding peaks in 2D data\ndata_2d = [\n    [1, 2, 1],\n    [2, 9, 2],  # Peak at (1,1)\n    [1, 2, 1]\n]\n\nprint(\"\\nFinding peaks in 2D array:\")\nfor i in range(len(data_2d)):\n    for j in range(len(data_2d[i])):\n        if data_2d[i][j] > 5:\n            print(f\"Peak at ({i},{j}): value={data_2d[i][j]}\")\n\n⚠️ Common Bug Alert: Off-by-One Errors\n\nThe most common bug in all of programming! Python’s zero-indexing catches everyone:\n\nClassic Mistake (lost data from Hubble!):observations = [1, 2, 3, 4, 5]\n# Trying to process all elements\nfor i in range(1, len(observations)):  # OOPS! Skips first element\n    process(observations[i])\n\n# Or worse - going past the end\nfor i in range(len(observations) + 1):  # IndexError on last iteration!\n    process(observations[i])\n\nRemember:\n\nrange(n) gives 0, 1, ..., n-1 (NOT including n!)\n\nList of length n has indices 0 to n-1\n\nThe last element is at index len(list) - 1\n\nThis bug has crashed spacecraft software, corrupted astronomical databases, and frustrated millions of programmers. Double-check your ranges!\n\n⚠️ Common Bug Alert: Infinite While Loops\n\nDon’t worry — everyone writes an infinite loop occasionally! Even senior programmers at NASA have done it. Here are real cases:\n\nCase 1: Floating-point precision prevents exact equalityx = 0.0\nwhile x != 1.0:  # INFINITE LOOP!\n    x += 0.1  # After 10 additions, x ≈ 0.9999999999\n\n# Fix: Use tolerance\nwhile abs(x - 1.0) > 1e-10:\n    x += 0.1\n\nCase 2: Forgetting to update loop variablei = 0\nwhile i < 10:\n    process(data)\n    # Forgot: i += 1  # INFINITE LOOP!\n\nAlways add a maximum iteration safeguard!","type":"content","url":"/python-control-flow-v2#nested-loops-processing-2d-data","position":55},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl2":"3.5 List Comprehensions: Elegant and Efficient"},"type":"lvl2","url":"/python-control-flow-v2#id-3-5-list-comprehensions-elegant-and-efficient","position":56},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl2":"3.5 List Comprehensions: Elegant and Efficient"},"content":"Now that you’ve mastered loops, let’s evolve them into something even more powerful! List comprehensions are Python’s gift to scientific programmers. They transform verbose loops into concise, readable, and faster expressions.","type":"content","url":"/python-control-flow-v2#id-3-5-list-comprehensions-elegant-and-efficient","position":57},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"From Loop to Comprehension","lvl2":"3.5 List Comprehensions: Elegant and Efficient"},"type":"lvl3","url":"/python-control-flow-v2#from-loop-to-comprehension","position":58},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"From Loop to Comprehension","lvl2":"3.5 List Comprehensions: Elegant and Efficient"},"content":"\n\n# Traditional loop approach\nsquares_loop = []\nfor x in range(10):\n    if x % 2 == 0:  # Even numbers only\n        squares_loop.append(x**2)\nprint(f\"Loop result: {squares_loop}\")\n\n# List comprehension - same result, clearer intent!\nsquares_comp = [x**2 for x in range(10) if x % 2 == 0]\nprint(f\"Comprehension: {squares_comp}\")\n\n# The anatomy of a list comprehension:\n# [expression for item in sequence if condition]\n#      ↓           ↓         ↓           ↓\n#  Transform   Variable  Source    Filter (optional)\n\n","type":"content","url":"/python-control-flow-v2#from-loop-to-comprehension","position":59},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"Real Astronomical Applications","lvl2":"3.5 List Comprehensions: Elegant and Efficient"},"type":"lvl3","url":"/python-control-flow-v2#real-astronomical-applications","position":60},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"Real Astronomical Applications","lvl2":"3.5 List Comprehensions: Elegant and Efficient"},"content":"\n\n# Filter and transform magnitude data from a survey\nmagnitudes = [12.3, 15.1, 13.7, 16.2, 14.5, 17.1, 11.8, 18.5, 13.2]\n\n# Get fluxes for observable stars (mag < 16, typical small telescope limit)\nobservable_fluxes = [10**(-0.4 * mag) \n                     for mag in magnitudes \n                     if mag < 16.0]\n\nprint(f\"Observable star count: {len(observable_fluxes)}/{len(magnitudes)}\")\nprint(f\"Brightest flux: {max(observable_fluxes):.2e}\")\nprint(f\"Faintest flux: {min(observable_fluxes):.2e}\")\n\n# Dictionary comprehension (bonus!)\nstar_dict = {f\"star_{i}\": mag \n             for i, mag in enumerate(magnitudes) \n             if mag < 15}\nprint(f\"\\nBright stars dictionary: {star_dict}\")\n\n","type":"content","url":"/python-control-flow-v2#real-astronomical-applications","position":61},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"When NOT to Use Comprehensions","lvl2":"3.5 List Comprehensions: Elegant and Efficient"},"type":"lvl3","url":"/python-control-flow-v2#when-not-to-use-comprehensions","position":62},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"When NOT to Use Comprehensions","lvl2":"3.5 List Comprehensions: Elegant and Efficient"},"content":"\n\n# BAD: Too complex - unreadable!\n# result = [process(x) if condition(x) else alternative(y) \n#           for x, y in zip(list1, list2) \n#           if validate(x) and check(y)]\n\n# GOOD: Clear loop for complex logic\ndef classify_galaxies(redshifts, luminosities):\n    \"\"\"When logic is complex, loops are clearer!\"\"\"\n    classifications = []\n    for z, L in zip(redshifts, luminosities):\n        if z < 0.5 and L > 1e10:\n            classifications.append(\"nearby bright\")\n        elif z < 0.5:\n            classifications.append(\"nearby faint\")\n        elif L > 1e10:\n            classifications.append(\"distant bright\")\n        else:\n            classifications.append(\"distant faint\")\n    return classifications\n\n# Clear and maintainable!\nz_values = [0.1, 0.8, 0.3, 1.2]\nL_values = [5e9, 2e10, 8e10, 3e9]\ngalaxy_types = classify_galaxies(z_values, L_values)\nprint(f\"Galaxy classifications: {galaxy_types}\")\n\n","type":"content","url":"/python-control-flow-v2#when-not-to-use-comprehensions","position":63},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl2":"3.6 Advanced Control Flow Patterns"},"type":"lvl2","url":"/python-control-flow-v2#id-3-6-advanced-control-flow-patterns","position":64},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl2":"3.6 Advanced Control Flow Patterns"},"content":"Now let’s explore powerful patterns that appear throughout scientific computing. These aren’t just code tricks — they’re fundamental algorithmic building blocks!","type":"content","url":"/python-control-flow-v2#id-3-6-advanced-control-flow-patterns","position":65},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"Welford’s Algorithm: Numerically Stable Statistics","lvl2":"3.6 Advanced Control Flow Patterns"},"type":"lvl3","url":"/python-control-flow-v2#welfords-algorithm-numerically-stable-statistics","position":66},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"Welford’s Algorithm: Numerically Stable Statistics","lvl2":"3.6 Advanced Control Flow Patterns"},"content":"\n\ndef running_statistics(data_stream):\n    \"\"\"\n    Calculate mean and variance in single pass.\n    Uses Welford's algorithm (1962) for numerical stability.\n    Essential for processing streaming telescope data!\n    \"\"\"\n    n = 0\n    mean = 0.0\n    M2 = 0.0\n    \n    for value in data_stream:\n        n += 1\n        delta = value - mean\n        mean += delta / n\n        delta2 = value - mean\n        M2 += delta * delta2\n    \n    if n < 2:\n        return mean, float('nan')\n    \n    variance = M2 / (n - 1)\n    return mean, variance\n\n# Test with problematic data (large baseline with small variations)\nphotometry = [1e8, 1e8 + 1, 1e8 + 2, 1e8 - 1, 1e8 + 0.5]  \nmean, var = running_statistics(photometry)\nprint(f\"Stable algorithm: mean={mean:.1f}, std={math.sqrt(var):.2f}\")\n\n# The naive approach would lose precision!\nnaive_mean = sum(photometry) / len(photometry)\nnaive_var = sum(x**2 for x in photometry) / len(photometry) - naive_mean**2\nprint(f\"Naive (problematic): mean={naive_mean:.1f}, std={math.sqrt(abs(naive_var)):.2f}\")\n\n💡 Computational Thinking: The Convergence Pattern\n\nPATTERN: Iterative Convergenceinitialize state\niteration_count = 0\n\nwhile not converged and iteration_count < max_iterations:\n    new_state = update(state)\n    converged = check_convergence(state, new_state, tolerance)\n    state = new_state\n    iteration_count += 1\n\nif not converged:\n    handle_failure()\n\nThis pattern appears throughout astrophysics:\n\nKepler’s equation solver (finding true anomaly)\n\nStellar structure integration (hydrostatic equilibrium)\n\nRadiative transfer (temperature iterations)\n\nN-body orbit integration (adaptive timesteps)\n\nMaster this pattern and you’ve mastered half of computational astrophysics!","type":"content","url":"/python-control-flow-v2#welfords-algorithm-numerically-stable-statistics","position":67},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl2":"3.7 Debugging Control Flow"},"type":"lvl2","url":"/python-control-flow-v2#id-3-7-debugging-control-flow","position":68},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl2":"3.7 Debugging Control Flow"},"content":"Logic errors are the hardest bugs because the code runs without crashing but produces wrong results. Let’s build your debugging arsenal!","type":"content","url":"/python-control-flow-v2#id-3-7-debugging-control-flow","position":69},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"Strategic Print Debugging","lvl2":"3.7 Debugging Control Flow"},"type":"lvl3","url":"/python-control-flow-v2#strategic-print-debugging","position":70},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"Strategic Print Debugging","lvl2":"3.7 Debugging Control Flow"},"content":"\n\ndef debug_convergence(initial, target, rate, max_iter=20):\n    \"\"\"\n    Example of strategic debug output.\n    Shows exactly where and why algorithms succeed or fail.\n    \"\"\"\n    \n    current = initial\n    history = []\n    \n    for iteration in range(max_iter):\n        old = current\n        current = current * (1 - rate) + target * rate\n        change = current - old\n        history.append(current)\n        \n        # Strategic output - not everything!\n        if iteration < 3 or iteration % 5 == 0:\n            print(f\"Iter {iteration:2d}: {old:.4f} → {current:.4f} (Δ={change:+.5f})\")\n        \n        # Convergence check\n        if abs(current - target) < 1e-6:\n            print(f\"✓ CONVERGED at iteration {iteration}\")\n            return current\n        \n        # Detect problems early\n        if len(history) > 3:\n            if (history[-1] > history[-2] < history[-3]):\n                print(f\"⚠ OSCILLATION detected\")\n                \n    print(f\"✗ FAILED after {max_iter} iterations\")\n    return current\n\n# Test the algorithm\nprint(\"Testing convergence:\")\nresult = debug_convergence(0, 100, 0.1)\n\n","type":"content","url":"/python-control-flow-v2#strategic-print-debugging","position":71},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"Using Assertions for Validation","lvl2":"3.7 Debugging Control Flow"},"type":"lvl3","url":"/python-control-flow-v2#using-assertions-for-validation","position":72},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"Using Assertions for Validation","lvl2":"3.7 Debugging Control Flow"},"content":"The assert statement helps catch bugs during development:\n\ndef calculate_magnitude_average(magnitudes):\n    \"\"\"Calculate average magnitude with assertions for debugging.\"\"\"\n    \n    # Assertions document and enforce assumptions\n    assert len(magnitudes) > 0, \"Need at least one magnitude\"\n    assert all(isinstance(m, (int, float)) for m in magnitudes), \"All must be numbers\"\n    assert all(0 < m < 30 for m in magnitudes), \"Magnitudes must be reasonable\"\n    \n    # Safe to proceed after assertions\n    return sum(magnitudes) / len(magnitudes)\n\n# Test with good data\ngood_mags = [10.2, 10.5, 10.3]\navg = calculate_magnitude_average(good_mags)\nprint(f\"Average magnitude: {avg:.2f}\")\n\n# WARNING: Assertions can be disabled in production with python -O\n# This means they should NEVER be used for actual validation!\n\n⚠️ Critical Warning: Assertions Are Not for Production!\n\nNever use assertions for user input validation or critical checks! Assertions can be completely disabled when Python runs with optimization (python -O), causing them to be skipped entirely.\n\nWRONG - Don’t do this for user-facing code:def process_user_data(value):\n    assert value > 0  # DANGEROUS! Might not run in production!\n    return math.sqrt(value)\n\nRIGHT - Use explicit validation for production:def process_user_data(value):\n    if value <= 0:\n        raise ValueError(f\"Value must be positive, got {value}\")\n    return math.sqrt(value)\n\nWhen to use assertions:\n\nDocumenting internal assumptions during development\n\nCatching programming errors early (not user errors)\n\nSelf-checks in algorithms (but have a fallback plan)\n\nTest suites and debugging\n\nWhen NOT to use assertions:\n\nValidating user input\n\nChecking file existence or permissions\n\nNetwork availability checks\n\nAny check that must run in production\n\nThink of assertions as “developer notes that can catch bugs” rather than “guards that protect your code.”\n\n🌟 The More You Know: How Kepler Found Over 2,700 Exoplanets\n\nThe Kepler Space Telescope discovered 2,778 confirmed exoplanets (with thousands more candidates) using exactly the control flow patterns you just learned! Here’s the simplified algorithm:\n\nNote: This algorithm is greatly simplified for pedagogical purposes. The actual Kepler pipeline used sophisticated techniques including Fourier transforms, multiple detrending algorithms, and extensive validation checks. However, the control flow patterns shown here — guard clauses, filtering, iteration, and conditional validation — formed the backbone of the real system.def kepler_planet_search(star_id, light_curve):\n    \"\"\"Simplified Kepler planet detection algorithm\"\"\"\n    \n    # Guard clause - data quality check\n    if len(light_curve) < 1000:\n        return None\n    \n    # Remove outliers (cosmic rays, etc.)\n    cleaned = [point for point in light_curve \n               if abs(point - median) < 5 * sigma]\n    \n    # Search for periodic dips\n    best_period = None\n    best_depth = 0\n    \n    for trial_period in range(1, 365):  # Days\n        folded = fold_light_curve(cleaned, trial_period)\n        depth = measure_transit_depth(folded)\n        \n        if depth > best_depth and depth > 3 * noise_level:\n            best_period = trial_period\n            best_depth = depth\n    \n    # Validate as planet (not eclipsing binary)\n    if best_period:\n        if is_v_shaped(folded):  # Binary check\n            return None\n        if depth > 0.5:  # Too deep\n            return None\n            \n        return {'period': best_period, 'depth': best_depth}\n    \n    return None\n\nThis ran on 150,000+ stars for 4 years! Your code uses the same patterns that revealed the universe is full of planets!\n\n🛠️ Debug This! The Telescope Priority Bug\n\nA telescope scheduling system has a subtle bug in its priority logic. Can you find and fix it?def assign_telescope_priority(observation):\n    \"\"\"\n    Assign priority based on object type and time sensitivity.\n    Higher numbers = higher priority.\n    \n    THIS CODE HAS A BUG - find it!\n    \"\"\"\n    magnitude = observation['magnitude']\n    obj_type = observation['type']\n    time_critical = observation['time_critical']\n    \n    # Assign base priority by object type\n    if obj_type == 'supernova':\n        priority = 100\n    elif obj_type == 'variable_star' and magnitude < 12:\n        priority = 70\n    elif obj_type == 'variable_star':  # This has a problem!\n        priority = 50\n    elif obj_type == 'asteroid' and time_critical:\n        priority = 80\n    elif obj_type == 'galaxy':\n        priority = 30\n    else:\n        priority = 10\n    \n    # Boost for bright objects\n    if magnitude < 10:\n        priority += 20\n    \n    return priority\n\n# Test case that reveals the bug:\nobs = {'type': 'variable_star', 'magnitude': 8, 'time_critical': True}\nprint(f\"Priority: {assign_telescope_priority(obs)}\")\n# Expected: 70 + 20 = 90 (bright variable star)\n# Actually gets: 90 (seems right... or is it?)\n\nobs2 = {'type': 'variable_star', 'magnitude': 11.5, 'time_critical': True}\nprint(f\"Priority: {assign_telescope_priority(obs2)}\")\n# Expected: 70 (bright variable star condition)\n# Actually gets: 70 (correct!)\n\nobs3 = {'type': 'variable_star', 'magnitude': 14, 'time_critical': True}\nprint(f\"Priority: {assign_telescope_priority(obs3)}\")\n# Expected: 50 (dim variable star)\n# What does it actually get?\n\nSolution\n\nThe Bug: The elif chain for variable stars has overlapping conditions that aren’t immediately obvious!\n\nLooking at the variable star conditions:\n\nelif obj_type == 'variable_star' and magnitude < 12: → priority = 70\n\nelif obj_type == 'variable_star': → priority = 50\n\nThe second condition can NEVER be reached for bright variable stars because they’re caught by the first condition. However, this actually works correctly by accident! The real issue is that the logic is confusing and fragile.\n\nBetter Design:def assign_telescope_priority_fixed(observation):\n    \"\"\"Fixed version with clearer logic.\"\"\"\n    magnitude = observation['magnitude']\n    obj_type = observation['type']\n    time_critical = observation['time_critical']\n    \n    # Assign base priority by object type\n    if obj_type == 'supernova':\n        priority = 100\n    elif obj_type == 'asteroid' and time_critical:\n        priority = 80\n    elif obj_type == 'variable_star':\n        # Nested logic is clearer for subcategories\n        if magnitude < 12:\n            priority = 70\n        else:\n            priority = 50\n    elif obj_type == 'galaxy':\n        priority = 30\n    else:\n        priority = 10\n    \n    # Boost for bright objects\n    if magnitude < 10:\n        priority += 20\n    \n    return priority\n\nKey Lessons:\n\nOrder elif conditions from most specific to most general\n\nAvoid overlapping conditions in elif chains\n\nConsider using nested if statements for subcategories\n\nThe original code works but is hard to maintain and understand\n\nThis type of subtle logic error is common in real telescope scheduling software and can lead to suboptimal observation planning!","type":"content","url":"/python-control-flow-v2#using-assertions-for-validation","position":73},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl2":"The Variable Star Thread Continues"},"type":"lvl2","url":"/python-control-flow-v2#the-variable-star-thread-continues","position":74},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl2":"The Variable Star Thread Continues"},"content":"Let’s apply our control flow knowledge to extend our variable star analysis from Chapters 1 and 2:\n\n# Chapter 3: Variable Star - Adding Periodicity Detection\nimport json\nimport math\n\n# Create sample data (in real use, load from Chapter 2)\nstar = {\n    'name': 'Delta Cephei',\n    'period': 5.366319,\n    'mag_mean': 3.95,\n    'mag_amp': 0.88,\n    'phase_function': 'sinusoidal'\n}\n\ndef analyze_phase_coverage(times, period, min_coverage=0.6):\n    \"\"\"\n    Check if observations adequately sample the phase space.\n    Critical for period determination accuracy!\n    \"\"\"\n    # Guard clause\n    if not times or period <= 0:\n        return False, \"Invalid input data\"\n    \n    # Calculate phases\n    phases = [(t % period) / period for t in times]\n    \n    # Divide phase space into bins\n    n_bins = 10\n    bins_filled = set()\n    \n    for phase in phases:\n        bin_index = int(phase * n_bins)\n        bins_filled.add(bin_index)\n    \n    coverage = len(bins_filled) / n_bins\n    \n    # Conditional logic for assessment\n    if coverage >= min_coverage:\n        quality = \"good\" if coverage > 0.8 else \"adequate\"\n        return True, f\"Phase coverage {quality}: {coverage:.1%}\"\n    else:\n        return False, f\"Insufficient coverage: {coverage:.1%} < {min_coverage:.1%}\"\n\n# Test with simulated observations\ntest_times = [0.5, 1.2, 2.7, 3.1, 4.8, 5.9, 7.2, 8.5, 9.1, 10.3]\nadequate, message = analyze_phase_coverage(test_times, star['period'])\nprint(f\"Delta Cephei observations: {message}\")\n\n# Save enhanced data for Chapter 4\nstar['last_analysis'] = {\n    'phase_coverage': adequate,\n    'message': message,\n    'n_observations': len(test_times)\n}\n\ntry:\n    with open('variable_star_ch3.json', 'w') as f:\n        json.dump(star, f, indent=2)\n    print(\"✓ Data saved for Chapter 4!\")\nexcept IOError as e:\n    print(f\"✗ Could not save: {e}\")\n\n","type":"content","url":"/python-control-flow-v2#the-variable-star-thread-continues","position":75},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl2":"Practice Exercises"},"type":"lvl2","url":"/python-control-flow-v2#practice-exercises","position":76},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl2":"Practice Exercises"},"content":"Now apply your control flow mastery to real astronomical problems!","type":"content","url":"/python-control-flow-v2#practice-exercises","position":77},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"Exercise 3.1: Phase Dispersion Minimization","lvl2":"Practice Exercises"},"type":"lvl3","url":"/python-control-flow-v2#exercise-3-1-phase-dispersion-minimization","position":78},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"Exercise 3.1: Phase Dispersion Minimization","lvl2":"Practice Exercises"},"content":"Complete Implementation (40-50 lines)def find_period_pdm(times, magnitudes, min_period=0.1, max_period=10.0):\n    \"\"\"\n    Find the period of a variable star using Phase Dispersion Minimization.\n    This is a REAL algorithm used in astronomy!\n    \n    Your implementation should:\n    1. Use nested loops for coarse then fine search\n    2. Apply the convergence pattern from Section 3.6\n    3. Include guard clauses for invalid input\n    4. Use list comprehensions where appropriate\n    \n    Pseudocode to get started:\n    - Validate inputs with guard clauses\n    - Coarse search with 0.1 day steps\n    - Find minimum dispersion period\n    - Refine with 0.01 day steps around minimum\n    - Continue until convergence\n    \"\"\"\n    # Your implementation here\n    pass","type":"content","url":"/python-control-flow-v2#exercise-3-1-phase-dispersion-minimization","position":79},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"Exercise 3.2: Transient Detection Pipeline","lvl2":"Practice Exercises"},"type":"lvl3","url":"/python-control-flow-v2#exercise-3-2-transient-detection-pipeline","position":80},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"Exercise 3.2: Transient Detection Pipeline","lvl2":"Practice Exercises"},"content":"Multi-Part Exercise (30-40 lines total)\n\nPart A: Implement data cleaning\nPart B: Detect variability using Welford’s algorithm\nPart C: Classify transients with elif chainsdef process_survey_data(times, mags, errors):\n    \"\"\"\n    Complete pipeline for transient detection.\n    Uses all control flow patterns from this chapter!\n    \"\"\"\n    # Part A: Clean data (guard clauses, list comprehension)\n    # Part B: Find variables (Welford's algorithm)\n    # Part C: Classify (elif chains)\n    pass","type":"content","url":"/python-control-flow-v2#exercise-3-2-transient-detection-pipeline","position":81},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"Exercise 3.3: Debug the Light Curve Folder","lvl2":"Practice Exercises"},"type":"lvl3","url":"/python-control-flow-v2#exercise-3-3-debug-the-light-curve-folder","position":82},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"Exercise 3.3: Debug the Light Curve Folder","lvl2":"Practice Exercises"},"content":"Find and Fix Three Bugsdef fold_light_curve(times, mags, period):\n    \"\"\"This function has 3 bugs - find and fix them!\"\"\"\n    phases = []\n    folded_mags = []\n    \n    for i in range(len(times)):\n        phase = times[i] / period  # Bug 1: Should use modulo!\n        phases.append(phase)\n        folded_mags.append(mags[i])\n    \n    # Sort by phase\n    for i in range(len(phases)):\n        for j in range(len(phases)):  # Bug 2: j should start at i+1\n            if phases[i] > phases[j]:  # Bug 3: Wrong comparison\n                phases[i], phases[j] = phases[j], phases[i]\n                folded_mags[i], folded_mags[j] = folded_mags[j], folded_mags[i]\n    \n    return phases, folded_mags","type":"content","url":"/python-control-flow-v2#exercise-3-3-debug-the-light-curve-folder","position":83},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl2":"Main Takeaways"},"type":"lvl2","url":"/python-control-flow-v2#main-takeaways","position":84},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl2":"Main Takeaways"},"content":"What an incredible journey you’ve just completed! You’ve transformed from someone who writes code line by line to someone who designs algorithms systematically. This transformation mirrors the evolution every computational scientist goes through, from tentative beginner to confident algorithm designer.\n\nYou started by learning to think in pseudocode, a skill that gives you the power to design before you code. Those three levels of refinement you practiced are your blueprint for success. Every hour you invest in pseudocode saves many hours of debugging. When you design your next algorithm for analyzing galaxy spectra or simulating stellar evolution, you’ll catch logical flaws on paper instead of after hours of computation.\n\nThe complete set of comparison and logical operators you’ve mastered — from simple greater-than checks to complex boolean combinations with and, or, and not — gives you the full vocabulary for expressing any logical condition. You understand that == is dangerous with floats, that is checks identity not equality, and that in elegantly tests membership. These aren’t just syntax details; they’re the building blocks of every data validation, every convergence check, every quality filter you’ll ever write.\n\nYour understanding of conditional statements goes beyond syntax to defensive programming philosophy. Those guard clauses you learned could literally prevent spacecraft crashes or save irreplaceable telescope time. The elif chains you practiced will classify astronomical objects, determine observing strategies, and control instrument settings. Every conditional you write is a decision that shapes how your code responds to the infinite variety of real data.\n\nThe loop patterns you’ve mastered are universal across computational physics. That accumulator pattern using Welford’s algorithm? It’s calculating photometric precision in the TESS pipeline right now. The convergence pattern with safety limits? It’s finding equilibrium in stellar models. The nested loops you practiced? They’re processing CCD images from every major observatory. Whether using for loops to iterate through catalogs, while loops to converge solutions, or list comprehensions to filter data, you now have the full toolkit.\n\nMost importantly, you’ve learned that bugs aren’t failures — they’re learning opportunities. Every infinite loop teaches you about termination conditions. Every off-by-one error reinforces proper indexing. The debugging strategies you’ve developed, from strategic print statements to assertions, will serve you throughout your career. Even the experts at NASA and ESA use these same techniques.\n\nRemember that every major computational achievement relies on these fundamentals. The control flow patterns you’ve learned detected gravitational waves at LIGO, discovered thousands of exoplanets with Kepler, and process images from JWST. You’re not just learning Python syntax — you’re joining a tradition of computational thinking that enables humanity’s greatest discoveries.","type":"content","url":"/python-control-flow-v2#main-takeaways","position":85},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl2":"Definitions"},"type":"lvl2","url":"/python-control-flow-v2#definitions","position":86},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl2":"Definitions"},"content":"Accumulator Pattern: An algorithmic pattern where values are iteratively combined into a running total or aggregate, fundamental to reductions and statistical calculations in astronomical data processing.\n\nAdaptive Refinement: A universal pattern where parameters are adjusted based on quality metrics, with safeguards against infinite refinement, appearing in timestepping, mesh refinement, and optimization throughout computational astrophysics.\n\nand: Logical operator that returns True only if both operands are true, using short-circuit evaluation.\n\nassert: Statement that raises an AssertionError if a condition is false, used for debugging and documenting assumptions during development (not for production validation).\n\nBoolean Logic: The system of true/false values and logical operations (and, or, not) that underlies all conditional execution in programs.\n\nbreak: Statement that immediately exits the current loop, skipping any remaining iterations.\n\nConditional Statement: A control structure (if/elif/else) that executes different code blocks based on whether conditions evaluate to true or false.\n\ncontinue: Statement that skips the rest of the current loop iteration and proceeds to the next iteration.\n\nelif: “Else if” statement that checks an additional condition when the previous if or elif was false.\n\nelse: Clause that executes when all previous if/elif conditions were false, or when a loop completes without breaking.\n\nfor: Loop that iterates over elements in a sequence or iterable object.\n\nGuard Clause: A conditional statement at the beginning of a function that handles special cases or invalid inputs immediately, preventing deep nesting.\n\nif: Statement that executes code only when a specified condition is true.\n\nin: Operator that tests membership in a sequence or collection.\n\nis: Operator that tests object identity (same object in memory), not just equality of values.\n\nList Comprehension: A concise Python syntax for creating lists: [expression for item in iterable if condition].\n\nnot: Logical operator that inverts a boolean value (True becomes False, False becomes True).\n\nor: Logical operator that returns True if at least one operand is true, using short-circuit evaluation.\n\npass: Null statement that does nothing, used as a placeholder where syntax requires a statement.\n\nPseudocode: A human-readable description of an algorithm that focuses on logic and structure without syntactic details.\n\nShort-circuit Evaluation: The behavior where logical operators stop evaluating as soon as the result is determined.\n\nWalrus Operator (:=): Assignment expression operator (Python 3.8+) that assigns a value to a variable as part of an expression, allowing both assignment and testing in a single statement.\n\nwhile: Loop that continues executing as long as a specified condition remains true.","type":"content","url":"/python-control-flow-v2#definitions","position":87},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl2":"Key Takeaways"},"type":"lvl2","url":"/python-control-flow-v2#key-takeaways","position":88},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl2":"Key Takeaways"},"content":"✓ Pseudocode reveals logical flaws before they become bugs — always design before implementing\n\n✓ Master all six comparison operators (>, <, >=, <=, ==, !=) and three logical operators (and, or, not)\n\n✓ Never use == with floating-point numbers; always use tolerance-based comparisons like math.isclose()\n\n✓ The is operator checks identity, not equality — use it for None checks\n\n✓ The in operator elegantly tests membership in sequences or strings\n\n✓ Guard clauses handle special cases first, making main logic clearer\n\n✓ for loops iterate over sequences, while loops continue until a condition becomes false\n\n✓ break exits loops early, continue skips to the next iteration, else runs if loop completes\n\n✓ List comprehensions are faster than loops for simple transformations but become unreadable for complex logic\n\n✓ Short-circuit evaluation in and/or prevents errors and improves performance\n\n✓ The accumulator pattern is fundamental to scientific computing, appearing in all statistical calculations\n\n✓ Always include maximum iteration limits in while loops to prevent infinite loops\n\n✓ Welford’s algorithm solves numerical stability issues in streaming statistics\n\n✓ Use assert statements to document and enforce assumptions during development\n\n✓ Every major astronomical discovery relies on the control flow patterns you’ve learned","type":"content","url":"/python-control-flow-v2#key-takeaways","position":89},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl2":"Quick Reference Tables"},"type":"lvl2","url":"/python-control-flow-v2#quick-reference-tables","position":90},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl2":"Quick Reference Tables"},"content":"Comparison Operators\n\nOperator\n\nDescription\n\nExample\n\n>\n\nGreater than\n\nif magnitude > 6.0:\n\n<\n\nLess than\n\nif redshift < 0.1:\n\n>=\n\nGreater or equal\n\nif snr >= 5.0:\n\n<=\n\nLess or equal\n\nif error <= tolerance:\n\n==\n\nEqual (avoid with floats!)\n\nif status == 'complete':\n\n!=\n\nNot equal\n\nif flag != -999:\n\nLogical Operators\n\nOperator\n\nDescription\n\nExample\n\nand\n\nBoth must be true\n\nif x > 0 and y > 0:\n\nor\n\nAt least one true\n\nif bright or variable:\n\nnot\n\nInverts boolean\n\nif not converged:\n\nSpecial Operators\n\nOperator\n\nDescription\n\nExample\n\nin\n\nMembership test\n\nif 'fits' in filename:\n\nis\n\nIdentity test\n\nif result is None:\n\nis not\n\nNegative identity\n\nif data is not None:\n\n:=\n\nWalrus operator (Python 3.8+)\n\nif (n := len(data)) > 100:\n\nControl Flow Statements\n\nStatement\n\nPurpose\n\nExample\n\nif/elif/else\n\nConditional execution\n\nif mag < 6: visible = True\n\nfor\n\nIterate over sequence\n\nfor star in catalog:\n\nwhile\n\nLoop while condition true\n\nwhile error > tolerance:\n\nbreak\n\nExit loop early\n\nif converged: break\n\ncontinue\n\nSkip to next iteration\n\nif bad_data: continue\n\npass\n\nDo nothing (placeholder)\n\nif not ready: pass\n\nassert\n\nDebug check\n\nassert len(data) > 0\n\nBuilt-in Functions for Loops\n\nFunction\n\nPurpose\n\nExample\n\nrange(n)\n\nGenerate 0 to n-1\n\nfor i in range(10):\n\nrange(start, stop, step)\n\nGenerate with step\n\nfor i in range(0, 10, 2):\n\nenumerate(seq)\n\nGet index and value\n\nfor i, val in enumerate(data):\n\nzip(seq1, seq2)\n\nParallel iteration\n\nfor x, y in zip(xs, ys):\n\nlen(seq)\n\nSequence length\n\nfor i in range(len(data)):\n\nComparison Functions\n\nFunction\n\nPurpose\n\nExample\n\nall(iterable)\n\nAll elements true\n\nif all(x > 0 for x in data):\n\nany(iterable)\n\nAny element true\n\nif any(x < 0 for x in data):\n\nmath.isclose()\n\nSafe float comparison\n\nif math.isclose(a, b):\n\nmath.isfinite()\n\nCheck not inf/nan\n\nif math.isfinite(result):\n\nmath.isnan()\n\nCheck for NaN\n\nif not math.isnan(value):\n\nmath.isinf()\n\nCheck for infinity\n\nif math.isinf(value):\n\nisinstance()\n\nType checking\n\nif isinstance(x, float):\n\nCommon Algorithmic Patterns\n\nPattern\n\nPurpose\n\nStructure\n\nAccumulator\n\nAggregate values\n\ntotal = 0; for x in data: total += x\n\nFilter\n\nSelect subset\n\n[x for x in data if condition(x)]\n\nMap\n\nTransform all\n\n[f(x) for x in data]\n\nSearch\n\nFind first match\n\nfor x in data: if test(x): return x\n\nConvergence\n\nIterate to solution\n\nwhile not converged and n < max:\n\nGuard clause\n\nHandle edge cases\n\nif invalid: return None\n\nSentinel\n\nSignal termination\n\nif value == -999: break","type":"content","url":"/python-control-flow-v2#quick-reference-tables","position":91},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl2":"Python Module & Method Reference (Chapter 3 Additions)"},"type":"lvl2","url":"/python-control-flow-v2#python-module-method-reference-chapter-3-additions","position":92},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl2":"Python Module & Method Reference (Chapter 3 Additions)"},"content":"","type":"content","url":"/python-control-flow-v2#python-module-method-reference-chapter-3-additions","position":93},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"New Built-in Functions","lvl2":"Python Module & Method Reference (Chapter 3 Additions)"},"type":"lvl3","url":"/python-control-flow-v2#new-built-in-functions","position":94},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"New Built-in Functions","lvl2":"Python Module & Method Reference (Chapter 3 Additions)"},"content":"Logical Testing\n\nall(iterable) - Returns True if all elements are true\n\nany(iterable) - Returns True if any element is true\n\nisinstance(obj, type) - Check if object is of specified type\n\nLoop Support\n\nenumerate(iterable, start=0) - Returns index-value pairs\n\nzip(*iterables) - Combines multiple iterables for parallel iteration\n\nrange(start, stop, step) - Generate arithmetic progression","type":"content","url":"/python-control-flow-v2#new-built-in-functions","position":95},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"Control Flow Keywords","lvl2":"Python Module & Method Reference (Chapter 3 Additions)"},"type":"lvl3","url":"/python-control-flow-v2#control-flow-keywords","position":96},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"Control Flow Keywords","lvl2":"Python Module & Method Reference (Chapter 3 Additions)"},"content":"Conditionals\n\nif - Execute block if condition is true\n\nelif - Check additional condition if previous was false\n\nelse - Execute if all previous conditions were false\n\nLoops\n\nfor - Iterate over sequence\n\nwhile - Loop while condition is true\n\nbreak - Exit loop immediately\n\ncontinue - Skip to next iteration\n\nelse - Execute if loop completes without break\n\nOther\n\npass - Null operation placeholder\n\nassert - Raise AssertionError if condition is false","type":"content","url":"/python-control-flow-v2#control-flow-keywords","position":97},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"Operators","lvl2":"Python Module & Method Reference (Chapter 3 Additions)"},"type":"lvl3","url":"/python-control-flow-v2#operators","position":98},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"Operators","lvl2":"Python Module & Method Reference (Chapter 3 Additions)"},"content":"Comparison\n\n>, <, >=, <=, ==, != - Numerical comparisons\n\nis, is not - Identity comparisons\n\nin, not in - Membership testing\n\nLogical\n\nand - Logical AND with short-circuit evaluation\n\nor - Logical OR with short-circuit evaluation\n\nnot - Logical NOT (inversion)","type":"content","url":"/python-control-flow-v2#operators","position":99},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"New Math Module Functions","lvl2":"Python Module & Method Reference (Chapter 3 Additions)"},"type":"lvl3","url":"/python-control-flow-v2#new-math-module-functions","position":100},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"New Math Module Functions","lvl2":"Python Module & Method Reference (Chapter 3 Additions)"},"content":"import math\n\nmath.isclose(a, b, rel_tol=1e-9, abs_tol=0.0) - Safe floating-point comparison\n\nmath.isfinite(x) - Check if neither infinite nor NaN\n\nmath.isnan(x) - Check if value is NaN\n\nmath.isinf(x) - Check if value is infinite","type":"content","url":"/python-control-flow-v2#new-math-module-functions","position":101},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"Debugging Support","lvl2":"Python Module & Method Reference (Chapter 3 Additions)"},"type":"lvl3","url":"/python-control-flow-v2#debugging-support","position":102},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl3":"Debugging Support","lvl2":"Python Module & Method Reference (Chapter 3 Additions)"},"content":"IPython Magic Commands\n\n%debug - Enter debugger after exception\n\n%pdb - Automatic debugger on exceptions\n\nDebugger Commands (when in pdb)\n\np variable - Print variable value\n\npp variable - Pretty-print variable\n\nl - List code around current line\n\nn - Next line\n\ns - Step into function\n\nc - Continue execution\n\nu/d - Move up/down call stack\n\nq - Quit debugger","type":"content","url":"/python-control-flow-v2#debugging-support","position":103},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl2":"Next Chapter Preview"},"type":"lvl2","url":"/python-control-flow-v2#next-chapter-preview","position":104},{"hierarchy":{"lvl1":"Chapter 3: Control Flow & Logic","lvl2":"Next Chapter Preview"},"content":"You’ve conquered control flow — now get ready for the next level! Chapter 4 will reveal how to organize data efficiently using Python’s powerful data structures. You’ll discover when to use lists versus dictionaries versus sets, and more importantly, you’ll understand why these choices can make your algorithms run 100 times faster or 100 times slower.\n\nImagine trying to find a specific star in a catalog of millions. With a list, you’d check each star one by one — taking minutes or hours. With a dictionary, you’ll find it instantly — in microseconds! The data structures you’ll learn next are the difference between simulations that finish in minutes and ones that run for days.\n\nThe control flow patterns you’ve mastered here will operate on the data structures you’ll learn next. Your loops will iterate through dictionaries of astronomical objects. Your conditionals will filter sets of observations. Your comprehensions will transform lists of measurements into meaningful results. Together, control flow and data structures give you the power to handle the massive datasets of modern astronomy — from Gaia’s billion-star catalog to the petabytes of data from the Square Kilometre Array.\n\nGet excited — Chapter 4 is where your code goes from processing dozens of data points to handling millions efficiently!","type":"content","url":"/python-control-flow-v2#next-chapter-preview","position":105},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data"},"type":"lvl1","url":"/python-data-structures-v2","position":0},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data"},"content":"","type":"content","url":"/python-data-structures-v2","position":1},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl2":"Learning Objectives"},"type":"lvl2","url":"/python-data-structures-v2#learning-objectives","position":2},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl2":"Learning Objectives"},"content":"By the end of this chapter, you will be able to:\n\nChoose optimal data structures based on algorithmic requirements and performance constraints\n\nPredict whether operations will be O(1) constant time or O(n) linear time\n\nUnderstand memory layout and why it matters for scientific computing\n\nImplement defensive copying strategies to prevent aliasing bugs in simulations\n\nProfile memory usage and optimize data structure choices for large datasets\n\nDesign data structures that prepare you for vectorized computing\n\nDebug common bugs related to mutability, aliasing, and hashability\n\nApply data structure patterns to real computational physics problems","type":"content","url":"/python-data-structures-v2#learning-objectives","position":3},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl2":"Prerequisites Check"},"type":"lvl2","url":"/python-data-structures-v2#prerequisites-check","position":4},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl2":"Prerequisites Check"},"content":"✅ Before Starting This Chapter\n\nYou can write loops and conditional statements fluently (Chapter 3)\n\nYou understand the difference between assignment and equality (Chapter 2)\n\nYou can use IPython for testing and timing code (Chapter 1)\n\nYou understand floating-point precision issues (Chapter 2)\n\nYou understand defensive programming principles (Chapter 1)\n\nIf any boxes are unchecked, review the indicated chapters first.","type":"content","url":"/python-data-structures-v2#prerequisites-check","position":5},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl2":"Chapter Overview"},"type":"lvl2","url":"/python-data-structures-v2#chapter-overview","position":6},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl2":"Chapter Overview"},"content":"Imagine you’re simulating the interactions between a million particles - whether they’re stars in a galaxy, atoms in a protein, or nodes in a network. Each timestep, you need to find which particles are close enough to interact strongly. With the wrong data structure, this neighbor search could take hours per timestep. With the right one - a spatial hash table or tree structure - it takes seconds. That’s the difference between a simulation finishing in a day versus running for months. This chapter teaches you to make these critical choices that determine whether your code scales to research problems or remains stuck with toy models.\n\nThis chapter transforms you from someone who stores data to someone who orchestrates it strategically for scientific computing. You’ll discover not just that dictionary lookups are fast, but why they’re fast - through hash functions that turn particle positions into array indices. You’ll understand when they might fail - like when hash collisions cluster your data. And you’ll learn how to verify performance yourself - because in computational science, measurement beats assumption every time.\n\nThese concepts directly prepare you for the numerical computing ahead. The memory layout discussions explain why NumPy arrays can be 100× more efficient than Python lists for vector operations. The immutability concepts prepare you for functional programming paradigms used in modern frameworks like JAX, where immutable operations enable automatic differentiation. The performance profiling skills will help you identify bottlenecks whether you’re solving differential equations or analyzing experimental data. By chapter’s end, you’ll think about data organization like a computational scientist, architecting for performance from the start.","type":"content","url":"/python-data-structures-v2#chapter-overview","position":7},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl2":"4.1 What Is a Data Structure?"},"type":"lvl2","url":"/python-data-structures-v2#id-4-1-what-is-a-data-structure","position":8},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl2":"4.1 What Is a Data Structure?"},"content":"data structure\nA way of organizing data in memory to enable efficient access and modification\n\nA data structure is fundamentally about organizing information to match your access patterns. Think about an N-body simulation where particles interact through forces - these particles could represent stars in a galaxy, molecules in a gas, or charges in a plasma. You could store particles in order of creation (like a list), organize them by spatial region for fast neighbor finding (like a dictionary of cells), or track unique particle IDs (like a set). Each choice profoundly affects your simulation’s performance - the difference between O(n²) all-pairs checks and O(n log n) tree-based algorithms.\n\n🌟 The More You Know: The Cassini Spacecraft Memory Crisis\n\nIn 1997, the Cassini spacecraft nearly failed before reaching Saturn due to a memory overflow bug in its attitude control system. The flight software used an inefficient data structure to track thruster firings, allocating new memory for each event without reusing space. After millions of small adjustments during the 7-year journey, the embedded system’s limited memory filled up.\n\nNASA engineers had to upload a patch while Cassini was millions of miles away, switching to a circular buffer data structure that reused memory. According to NASA/JPL’s Cassini Program documentation and lessons learned database (JPL D-18441, 1998), the fix was deployed via Deep Space Network commands, requiring precise timing due to the 84-minute round-trip signal delay. The wrong data structure choice nearly lost a $3.26 billion mission.\n\nThis incident, detailed in “Cassini Attitude Control Flight Software: Lessons Learned” (IEEE Aerospace Conference, 2000), demonstrates that understanding data structures isn’t academic - it’s mission-critical. Modern spacecraft now use formal verification methods to prove memory bounds on their data structures.","type":"content","url":"/python-data-structures-v2#id-4-1-what-is-a-data-structure","position":9},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Building Intuition: Measuring Speed Empirically","lvl2":"4.1 What Is a Data Structure?"},"type":"lvl3","url":"/python-data-structures-v2#building-intuition-measuring-speed-empirically","position":10},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Building Intuition: Measuring Speed Empirically","lvl2":"4.1 What Is a Data Structure?"},"content":"Ready to discover something that will transform how you think about organizing data in your simulations? We’re going to measure the dramatic performance difference between different data structures. This empirical approach - measure first, understand why, then optimize - is exactly how computational scientists approach performance optimization.\n\n# Let's discover why data structure choice matters for simulations!\nimport time\nimport random\n\n# Create test data representing particle IDs in a simulation\nsizes = [100, 1000, 10000, 100000]\n\nprint(\"Searching for a particle NOT in the collection (worst case):\")\nprint(\"=\" * 60)\n\nfor n in sizes:\n    # Simulate particle IDs\n    particle_list = list(range(n))\n    particle_set = set(range(n))\n    \n    # Search for a particle that escaped our simulation boundary\n    escaped_particle = -1\n    \n    # Time list search - what you might naturally try first\n    start = time.perf_counter()\n    found = escaped_particle in particle_list\n    list_time = time.perf_counter() - start\n    \n    # Time set search - the optimized approach\n    start = time.perf_counter()\n    found = escaped_particle in particle_set\n    set_time = time.perf_counter() - start\n    \n    print(f\"Size {n:6d}: List: {list_time*1e6:8.2f} μs, \"\n          f\"Set: {set_time*1e6:8.2f} μs\")\n    if list_time > 0:\n        print(f\"           Set is {list_time/set_time:,.0f}× faster!\")\n\nprint(\"\\nFor collision detection between 100,000 particles:\")\nprint(\"List approach: hours per timestep\")\nprint(\"Set approach: milliseconds per timestep!\")\n\nThis performance difference isn’t just academic - it determines whether your galaxy simulation can evolve for billions of years or gets stuck after a few million. The secret behind this magic is the hash table, which you’re about to understand completely.","type":"content","url":"/python-data-structures-v2#building-intuition-measuring-speed-empirically","position":11},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Understanding Big-O Notation","lvl2":"4.1 What Is a Data Structure?"},"type":"lvl3","url":"/python-data-structures-v2#understanding-big-o-notation","position":12},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Understanding Big-O Notation","lvl2":"4.1 What Is a Data Structure?"},"content":"Now that you’ve witnessed this dramatic performance difference empirically, let’s understand the mathematical pattern behind it. Big-O notation describes how an algorithm’s runtime scales with input size - it’s the language computational scientists use to discuss whether an algorithm is feasible for large-scale simulations.\n\nimport matplotlib.pyplot as plt\nimport math\n\n# Visualize how different algorithms scale for physics simulations\nn = range(1, 101)\nconstant = [1 for _ in n]\nlogarithmic = [math.log(x) for x in n]\nlinear = list(n)\nquadratic = [x**2 for x in n]\n\nplt.figure(figsize=(10, 6))\nplt.plot(n, constant, label='O(1) - Hash table lookup', linewidth=2)\nplt.plot(n, logarithmic, label='O(log n) - Tree-based methods', linewidth=2)\nplt.plot(n, linear, label='O(n) - Direct summation', linewidth=2)\nplt.plot(n, quadratic, label='O(n²) - All-pairs (naive)', linewidth=2)\n\nplt.xlim(0, 100)\nplt.ylim(0, 500)\nplt.xlabel('Number of Particles (thousands)')\nplt.ylabel('Time (arbitrary units)')\nplt.title('Algorithm Scaling: Why Data Structures Matter')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(\"For a million-particle simulation:\")\nprint(f\"  O(1): 1 operation - spatial hash lookup\")\nprint(f\"  O(log n): {math.log2(1e6):.0f} operations - tree traversal\")\nprint(f\"  O(n): 1,000,000 operations - single particle force sum\")\nprint(f\"  O(n²): 1,000,000,000,000 operations - all pairs (impossible!)\")\n\nprint(\"\\nIn astronomy: This enables galaxy simulations with billions of stars\")\nprint(\"In chemistry: This allows protein folding with millions of atoms\")\nprint(\"In climate: This permits global models with millions of grid cells\")\n\nThis graph reveals why algorithmic complexity matters for computational physics. The naive O(n²) approach that checks all particle pairs becomes impossible beyond a few thousand particles. But with smart data structures like trees (O(n log n)) or spatial hashing (O(n)), we can simulate entire galaxies, proteins, or climate systems!\n\n💡 Computational Thinking: The Time-Space Tradeoff in Physics Simulations\n\nThis universal pattern appears throughout computational physics: trading memory for speed.\n\nThe Pattern:\n\nUse more memory → organize data cleverly → enable faster computation\n\nUse less memory → simpler organization → slower computation\n\nReal Physics Examples:\n\nParticle mesh methods: Store density on a grid (memory) to avoid O(n²) particle interactions\n\nNeighbor lists: Store nearby particles (memory) vs O(n²) distance checks for every step\n\nTree codes: Store spatial hierarchy (memory) for O(n log n) force calculation\n\nLookup tables: Store pre-computed values vs recalculating expensive functions\n\nFFT methods: Store complex coefficients for O(n log n) vs O(n²) convolution\n\nIn astronomy, the GADGET cosmology code uses 200 bytes per particle (vs 24 for just position/velocity) to achieve O(n log n) scaling, enabling billion-particle dark matter simulations. This 8× memory cost enables 1000× speedup!","type":"content","url":"/python-data-structures-v2#understanding-big-o-notation","position":13},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl2":"4.2 Lists: Python’s Workhorse Sequence"},"type":"lvl2","url":"/python-data-structures-v2#id-4-2-lists-pythons-workhorse-sequence","position":14},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl2":"4.2 Lists: Python’s Workhorse Sequence"},"content":"Lists are Python’s most versatile data structure - perfect for particle arrays, time series data, or any ordered collection. But here’s something crucial for scientific computing: Python lists don’t store your numbers directly! Understanding this explains why NumPy arrays are so much faster for numerical work.","type":"content","url":"/python-data-structures-v2#id-4-2-lists-pythons-workhorse-sequence","position":15},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"How Lists Really Work in Memory","lvl2":"4.2 Lists: Python’s Workhorse Sequence"},"type":"lvl3","url":"/python-data-structures-v2#how-lists-really-work-in-memory","position":16},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"How Lists Really Work in Memory","lvl2":"4.2 Lists: Python’s Workhorse Sequence"},"content":"Let’s explore how Python actually stores your simulation data in memory. This knowledge will help you understand when to use Python lists versus when you need NumPy arrays.\n\n# Stage 1: Basic memory inspection (10 lines)\nimport sys\n\n# Position of one particle in 3D space\nposition = [1.5e10, 2.3e10, 0.8e10]  # cm (typical scale for solar system)\n\n# The list container itself\nlist_size = sys.getsizeof(position)\nprint(f\"List container: {list_size} bytes\")\n\n# Each float is a full Python object!\nelement_sizes = [sys.getsizeof(x) for x in position]\nprint(f\"Each coordinate: {element_sizes[0]} bytes\")\n\n# Stage 2: Calculate total overhead (10 lines)\n# Total memory footprint\ntotal = list_size + sum(element_sizes)\nprint(f\"Total: {total} bytes for 3 floats!\")\nprint(f\"That's {total/24:.1f}× more than raw floats would need!\")\n\nprint(\"\\nWhy so much memory?\")\nprint(\"- List stores pointers, not values\")\nprint(\"- Each float is a full object with type info\")\nprint(\"- Python must track reference counts\")\nprint(\"\\nThis is why NumPy arrays (Chapter 7) store raw values contiguously!\")\n\nHere’s what’s actually happening in memory when you store particle positions:Python List of Positions:          Objects in Memory:\n┌─────────────────┐               ┌──────────────────┐\n│ size: 3         │               │ float: 1.5e10    │\n│ capacity: 4     │          ┌───>│ type info        │\n├─────────────────┤          │    │ ref count        │\n│ pointer ────────┼──────────┘    └──────────────────┘\n│ pointer ────────┼──────────────>┌──────────────────┐\n│ pointer ────────┼──────┐        │ float: 2.3e10    │\n│ (unused)        │    │          └──────────────────┘\n└─────────────────┘    └────────>┌──────────────────┐\n                                  │ float: 0.8e10    │\n                                  └──────────────────┘\n\nNumPy Array (preview):\n┌────┬────┬────┐\n│1.5 │2.3 │0.8 │  <- Raw values, no pointers!\n└────┴────┴────┘","type":"content","url":"/python-data-structures-v2#how-lists-really-work-in-memory","position":17},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"List Operations: Performance for Simulations","lvl2":"4.2 Lists: Python’s Workhorse Sequence"},"type":"lvl3","url":"/python-data-structures-v2#list-operations-performance-for-simulations","position":18},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"List Operations: Performance for Simulations","lvl2":"4.2 Lists: Python’s Workhorse Sequence"},"content":"Different list operations have vastly different costs - critical knowledge when processing particle data or building spatial data structures.\n\nimport time\n\n# Simulate a growing particle system\nparticles = list(range(100_000))\n\n# Adding particles at the END (common pattern)\nstart = time.perf_counter()\nparticles.append(100_000)  # New particle enters domain\nparticles.pop()            # Remove for fair comparison\nend_time = time.perf_counter() - start\n\n# Adding particles at the BEGINNING (avoid this!)\nstart = time.perf_counter()\nparticles.insert(0, -1)    # Insert at front\nparticles.pop(0)           # Remove from front\nbegin_time = time.perf_counter() - start\n\nprint(f\"Adding particle at END:   {end_time*1e6:.2f} microseconds\")\nprint(f\"Adding particle at START: {begin_time*1e6:.2f} microseconds\")\nprint(f\"\\nBeginning is {begin_time/end_time:.0f}× slower!\")\n\nprint(\"\\nWhy? Inserting at the beginning shifts ALL particles in memory!\")\nprint(\"For particle systems, use collections.deque if you need\")\nprint(\"fast operations at both ends (e.g., boundary conditions).\")\n\n💡 Performance Tip: When Both Ends Matter\n\nIf your simulation needs fast operations at both ends (like particles entering/leaving through boundaries), use collections.deque:from collections import deque\nparticles = deque(maxlen=1000000)  # Efficient at both ends!\n\nThis is particularly useful in molecular dynamics where particles cross periodic boundaries, or in astronomical simulations tracking objects entering and leaving a field of view.","type":"content","url":"/python-data-structures-v2#list-operations-performance-for-simulations","position":19},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"List Growth Strategy: Understanding Dynamic Arrays","lvl2":"4.2 Lists: Python’s Workhorse Sequence"},"type":"lvl3","url":"/python-data-structures-v2#list-growth-strategy-understanding-dynamic-arrays","position":20},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"List Growth Strategy: Understanding Dynamic Arrays","lvl2":"4.2 Lists: Python’s Workhorse Sequence"},"content":"Watch how Python manages memory as your particle system grows. This pattern appears in many languages and understanding it helps you write efficient simulation codes.\n\n# Stage 1: Basic growth observation (12 lines)\nimport sys\n\nparticles = []\nprint(\"Watch Python's list growth strategy:\")\nprint(\"(Critical for understanding simulation performance)\")\nprint(\"Length → Capacity (overallocation)\")\nprint(\"-\" * 40)\n\nfor i in range(10):\n    old_size = sys.getsizeof(particles)\n    particles.append(i)\n    new_size = sys.getsizeof(particles)\n    \n    if new_size != old_size:\n        print(f\"{len(particles):4d} → larger allocation\")\n\n# Stage 2: Calculate exact capacities (15 lines)\nparticles = []\nprevious_size = 0\n\nfor i in range(20):\n    old_size = sys.getsizeof(particles)\n    particles.append(i)\n    new_size = sys.getsizeof(particles)\n    \n    if new_size != old_size:\n        # Calculate capacity from size change\n        capacity = (new_size - sys.getsizeof([])) // 8\n        actual_length = len(particles)\n        overalloc = ((capacity - actual_length) / actual_length * 100 \n                     if actual_length > 0 else 0)\n        print(f\"Length {actual_length:4d} → Capacity {capacity:4d} \"\n              f\"({overalloc:5.1f}% extra)\")\n\nThis growth pattern is why appending to lists is “amortized O(1)” - usually fast, occasionally slow when reallocation happens. For time-critical simulations, pre-allocate your arrays when particle count is known!","type":"content","url":"/python-data-structures-v2#list-growth-strategy-understanding-dynamic-arrays","position":21},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl2":"4.3 Tuples: The Power of Immutability"},"type":"lvl2","url":"/python-data-structures-v2#id-4-3-tuples-the-power-of-immutability","position":22},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl2":"4.3 Tuples: The Power of Immutability"},"content":"immutable\nObjects whose state cannot be modified after creation\n\nWhat if you need to guarantee that initial conditions or physical constants won’t change during your simulation? Enter tuples - Python’s immutable sequences. This isn’t a limitation - it’s protection against an entire category of bugs that plague scientific codes!\n\n⚠️ Common Bug Alert: The Accidental Modification Disaster\n\nOne of the most insidious bugs in computational physics happens when functions unexpectedly modify their inputs. Remember the defensive programming principles from Chapter 1? Here’s why they matter:# THE BUG THAT CORRUPTS YOUR SIMULATION:\ndef evolve_system(state, dt):\n    # Trying to be clever, modifying in-place\n    state[0] += state[1] * dt  # Modifies original!\n    state[1] += calculate_force(state[0]) * dt\n    return state\n\ninitial_state = [1.0, 0.0]  # Position, velocity\nstate_t1 = evolve_system(initial_state, 0.01)\nprint(initial_state)  # [1.01, ...] - Initial conditions corrupted!\n\n# THE FIX: Use tuples to prevent modification\ninitial_state = (1.0, 0.0)  # Tuple!\n# Now modification attempts raise errors immediately!\n\nThis connects directly to Chapter 1’s defensive programming: catch errors early, fail loudly!","type":"content","url":"/python-data-structures-v2#id-4-3-tuples-the-power-of-immutability","position":23},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Understanding Immutability in Physics Simulations","lvl2":"4.3 Tuples: The Power of Immutability"},"type":"lvl3","url":"/python-data-structures-v2#understanding-immutability-in-physics-simulations","position":24},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Understanding Immutability in Physics Simulations","lvl2":"4.3 Tuples: The Power of Immutability"},"content":"Let’s see how immutability protects your simulations and enables powerful optimizations:\n\n# Physical constants should NEVER change during simulation\nclass PhysicsConstants:\n    \"\"\"Demonstrating safe vs unsafe constant storage.\"\"\"\n    \n    # UNSAFE: Mutable list (can be accidentally modified)\n    unsafe_constants = [\n        6.674e-8,   # G (cm³/g/s²)\n        2.998e10,   # c (cm/s)\n        1.381e-16,  # k_B (erg/K)\n    ]\n    \n    # SAFE: Immutable tuple (modification attempts fail loudly)\n    safe_constants = (\n        6.674e-8,   # G (cm³/g/s²)\n        2.998e10,   # c (cm/s)\n        1.381e-16,  # k_B (erg/K)\n    )\n\n# Demonstration of the danger\ndef buggy_calculation(constants):\n    \"\"\"This function has a typo that modifies constants!\"\"\"\n    if isinstance(constants, list):\n        # Oops! Used = instead of == in a complex calculation\n        constants[0] = constants[0] * 1e10  # BUG: Modifies G!\n        return \"Calculated (with hidden corruption)\"\n    else:\n        # With tuple, this would raise an error immediately\n        try:\n            constants[0] = constants[0] * 1e10\n        except TypeError as e:\n            return f\"Caught bug immediately: {e}\"\n\n# Test with both\nprint(\"With list:\", buggy_calculation(PhysicsConstants.unsafe_constants.copy()))\nprint(\"With tuple:\", buggy_calculation(PhysicsConstants.safe_constants))\n\nprint(\"\\nImmutability turns subtle runtime corruption into immediate, obvious errors!\")\n\n","type":"content","url":"/python-data-structures-v2#understanding-immutability-in-physics-simulations","position":25},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Tuples as Dictionary Keys: Caching Expensive Calculations","lvl2":"4.3 Tuples: The Power of Immutability"},"type":"lvl3","url":"/python-data-structures-v2#tuples-as-dictionary-keys-caching-expensive-calculations","position":26},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Tuples as Dictionary Keys: Caching Expensive Calculations","lvl2":"4.3 Tuples: The Power of Immutability"},"content":"Here’s where immutability becomes a superpower for computational physics - tuples can be dictionary keys, enabling powerful memoization patterns for expensive calculations:\n\n# Stage 1: Basic caching concept (15 lines)\n# Cache expensive physics calculations\ncache = {}\ncalculation_count = 0\n\ndef gravitational_force(pos1, pos2, m1, m2):\n    \"\"\"Calculate gravitational force with caching.\"\"\"\n    global calculation_count\n    \n    # Create cache key from positions (must be tuples!)\n    cache_key = (pos1, pos2, m1, m2)\n    \n    if cache_key in cache:\n        print(f\"  Cache hit! Avoided expensive calculation\")\n        return cache[cache_key]\n    \n    calculation_count += 1\n    print(f\"  Computing force (calculation #{calculation_count})\")\n\n# Stage 2: Complete implementation with physics (20 lines)\n# Continue from Stage 1\ndef gravitational_force(pos1, pos2, m1, m2):\n    \"\"\"Calculate gravitational force with automatic caching.\"\"\"\n    global calculation_count\n    \n    cache_key = (pos1, pos2, m1, m2)\n    if cache_key in cache:\n        print(f\"  Cache hit!\")\n        return cache[cache_key]\n    \n    calculation_count += 1\n    print(f\"  Computing (calculation #{calculation_count})\")\n    \n    # Unpack positions\n    x1, y1, z1 = pos1\n    x2, y2, z2 = pos2\n    \n    # Calculate distance\n    dx, dy, dz = x2 - x1, y2 - y1, z2 - z1\n    r = (dx**2 + dy**2 + dz**2) ** 0.5\n    \n    # Gravitational force\n    G = 6.674e-8  # cm³/g/s²\n    F = G * m1 * m2 / r**2\n    \n    cache[cache_key] = F\n    return F\n\n# Stage 3: Demonstrate caching benefits (10 lines)\n# Simulate repeated force calculations in N-body code\nsun_pos = (0.0, 0.0, 0.0)\nearth_pos = (1.496e13, 0.0, 0.0)  # 1 AU in cm\nm_sun = 1.989e33  # grams\nm_earth = 5.972e27  # grams\n\nprint(\"First calculation:\")\nF1 = gravitational_force(sun_pos, earth_pos, m_sun, m_earth)\n\nprint(\"\\nSecond calculation (same positions):\")\nF2 = gravitational_force(sun_pos, earth_pos, m_sun, m_earth)\n\nprint(f\"\\nForce: {F1:.2e} dynes\")\nprint(f\"Calculations performed: {calculation_count}\")\nprint(\"In astronomy: Caching speeds up N-body codes 10-100×!\")\n\n","type":"content","url":"/python-data-structures-v2#tuples-as-dictionary-keys-caching-expensive-calculations","position":27},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl2":"4.4 The Mutable vs Immutable Distinction"},"type":"lvl2","url":"/python-data-structures-v2#id-4-4-the-mutable-vs-immutable-distinction","position":28},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl2":"4.4 The Mutable vs Immutable Distinction"},"content":"Time for one of Python’s most important concepts for scientific computing! Understanding mutability is the key to avoiding mysterious bugs where your simulation state changes unexpectedly. This connects directly to Chapter 1’s defensive programming principles - catching errors early prevents corrupted results.","type":"content","url":"/python-data-structures-v2#id-4-4-the-mutable-vs-immutable-distinction","position":29},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Python’s Reference Model in Physics Simulations","lvl2":"4.4 The Mutable vs Immutable Distinction"},"type":"lvl3","url":"/python-data-structures-v2#pythons-reference-model-in-physics-simulations","position":30},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Python’s Reference Model in Physics Simulations","lvl2":"4.4 The Mutable vs Immutable Distinction"},"content":"Python doesn’t store values in variables - it stores references to objects. This has profound implications for simulation codes:\n\n# Critical concept for simulation state management!\n\nprint(\"With IMMUTABLE objects (safe for constants):\")\nG_constant = 6.674e-8\nG_backup = G_constant\nprint(f\"Initially: G={G_constant:.3e}, backup={G_backup:.3e}\")\nprint(f\"Same object in memory? {G_constant is G_backup}\")\n\nG_backup = 6.674e-7  # Creates NEW object (typo won't affect original)\nprint(f\"After change: G={G_constant:.3e}, backup={G_backup:.3e}\")\nprint(\"Original constant is safe!\")\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"With MUTABLE objects (dangerous for state!):\")\n\n# Particle system state\nparticles = [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]  # Positions\nbackup = particles  # Think you're making a backup?\n\nprint(f\"Initially: same object? {particles is backup}\")\n\n# Modify what you think is the backup\nbackup[0][0] = 999.0  # Trying to test something\n\nprint(f\"Original particles: {particles}\")\nprint(f\"'Backup': {backup}\")\nprint(\"😱 You just corrupted your simulation state!\")\n\nprint(\"\\nLesson: Use copy.deepcopy() for simulation state backups!\")\n\n🌟 The More You Know: The Knight Capital Trading Disaster\n\nOn August 1, 2012, Knight Capital Group lost $440 million in 45 minutes due to a data structure aliasing bug. According to SEC Release No. 70694 (October 16, 2013) and Knight’s own 8-K filing with the SEC on August 2, 2012, the incident stemmed from code deployment inconsistencies.\n\nThe company deployed new trading software to 8 servers but accidentally left old code (called “Power Peg”) on one server. Both old and new systems shared a configuration dictionary. The old code interpreted a flag in this shared dictionary as enabling test mode, while the new code used the same flag to enable live trading.\n\nAs reported by Reuters and Bloomberg at the time, when markets opened, 7 servers correctly processed orders while the 8th server executed everything as real trades. In 45 minutes, the rogue server executed 4 million trades worth $7 billion - roughly 10% of total US equity volume that day.\n\nThe shared mutable state (the configuration dictionary) without proper version control created perfect conditions for disaster. Knight Capital, once the largest trader in US equities, was acquired for a fraction of its value within a week. The incident led to new regulations about algorithmic trading systems and configuration management.\n\nThe lesson for scientific computing: When multiple systems or simulation components share mutable data structures, confusion about state can have catastrophic consequences. Always use defensive copying and immutable configurations where possible!","type":"content","url":"/python-data-structures-v2#pythons-reference-model-in-physics-simulations","position":31},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"The Classic Mutable Default Argument Bug","lvl2":"4.4 The Mutable vs Immutable Distinction"},"type":"lvl3","url":"/python-data-structures-v2#the-classic-mutable-default-argument-bug","position":32},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"The Classic Mutable Default Argument Bug","lvl2":"4.4 The Mutable vs Immutable Distinction"},"content":"This bug is so dangerous that Python linters specifically check for it. It’s particularly insidious in iterative simulations:\n\n# Stage 1: Demonstrate the bug (15 lines)\n# THE BUG: Default mutable created ONCE at function definition!\ndef accumulate_energies_buggy(energy, history=[]):\n    \"\"\"BUGGY: Trying to track energy history.\"\"\"\n    history.append(energy)\n    return history\n\n# Simulate multiple independent runs\nprint(\"Run 1:\")\nrun1 = accumulate_energies_buggy(100)\nrun1 = accumulate_energies_buggy(95)\nprint(f\"  Energy history: {run1}\")\n\nprint(\"\\nRun 2 (should be independent):\")\nrun2 = accumulate_energies_buggy(200)\nprint(f\"  Energy history: {run2}\")\nprint(\"  Contains Run 1 data! Runs are coupled! 😱\")\n\n# Stage 2: Show the fix (15 lines)\nprint(\"THE FIX: Use None sentinel pattern\")\n\ndef accumulate_energies_fixed(energy, history=None):\n    \"\"\"Safe version - creates new list for each run.\"\"\"\n    if history is None:\n        history = []  # Fresh list for each simulation\n    history.append(energy)\n    return history\n\nprint(\"\\nRun 3:\")\nrun3 = accumulate_energies_fixed(100)\nrun3 = accumulate_energies_fixed(95)\nprint(f\"  Energy history: {run3}\")\n\nprint(\"\\nRun 4 (properly independent):\")\nrun4 = accumulate_energies_fixed(200)\nprint(f\"  Energy history: {run4}\")\nprint(\"  Runs are independent! ✅\")\n\n","type":"content","url":"/python-data-structures-v2#the-classic-mutable-default-argument-bug","position":33},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Shallow vs Deep Copies in Grid Simulations","lvl2":"4.4 The Mutable vs Immutable Distinction"},"type":"lvl3","url":"/python-data-structures-v2#shallow-vs-deep-copies-in-grid-simulations","position":34},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Shallow vs Deep Copies in Grid Simulations","lvl2":"4.4 The Mutable vs Immutable Distinction"},"content":"This distinction is critical for grid-based simulations in computational fluid dynamics, stellar atmospheres, or any field-based calculation:\n\n# Stage 1: Set up the problem (12 lines)\nimport copy\n\n# Create a 2D grid for temperature distribution\nprint(\"Simulating heat diffusion on a grid:\")\ngrid = [[20.0, 20.0, 20.0],\n        [20.0, 50.0, 20.0],  # Hot spot in center\n        [20.0, 20.0, 20.0]]\nprint(f\"Initial grid: {grid[1]}\")  # Middle row\n\n# Shallow copy - DANGEROUS for grids!\nprint(\"\\n--- SHALLOW COPY (aliases inner arrays) ---\")\ngrid_next_shallow = grid.copy()\n\n# Stage 2: Show the shallow copy problem (10 lines)\n# Try to update center point for next timestep\ngrid_next_shallow[1][1] = 35.0  # Cooling\n\nprint(f\"Original grid center: {grid[1][1]}°C\")\nprint(f\"Next step grid center: {grid_next_shallow[1][1]}°C\")\nprint(\"😱 Modified both grids! Simulation is corrupted!\")\n\n# Reset for deep copy demo\ngrid = [[20.0, 20.0, 20.0],\n        [20.0, 50.0, 20.0],\n        [20.0, 20.0, 20.0]]\n\n# Stage 3: Show the deep copy solution (12 lines)\n# Deep copy - SAFE for grids!\nprint(\"\\n--- DEEP COPY (independent arrays) ---\")\ngrid_next_deep = copy.deepcopy(grid)\ngrid_next_deep[1][1] = 35.0  # Cooling\n\nprint(f\"Original grid center: {grid[1][1]}°C\")\nprint(f\"Next step grid center: {grid_next_deep[1][1]}°C\")\nprint(\"✅ Grids are independent! Simulation is correct!\")\n\nprint(\"\\nMemory visualization:\")\nprint(\"Shallow: grid → [ref1, ref2, ref3] → same inner arrays\")\nprint(\"Deep:    grid → [ref1, ref2, ref3] → independent arrays\")\n\n🐛 Debug This!\n\nA student’s particle simulation is producing inconsistent results. Here’s their code:def update_particles(particles, forces=[]):\n    \"\"\"Update particle positions based on forces.\"\"\"\n    for i, particle in enumerate(particles):\n        if i >= len(forces):\n            forces.append(calculate_force(particle))\n        particle['velocity'] += forces[i] * dt\n        particle['position'] += particle['velocity'] * dt\n    return particles, forces\n\n# Simulation loop\nparticles = [{'position': [0,0,0], 'velocity': [1,0,0]} for _ in range(10)]\nfor timestep in range(100):\n    particles, forces = update_particles(particles)\n\nFind and fix THREE bugs related to mutable defaults and aliasing:\n\nThe forces=[] default is created once and shared between calls\n\nThe function modifies the original particles list (no defensive copy)\n\nThe forces list grows unbounded, carrying over between timesteps\n\nSolution:def update_particles(particles, forces=None):\n    \"\"\"Update particle positions based on forces.\"\"\"\n    if forces is None:\n        forces = []\n    particles = copy.deepcopy(particles)  # Defensive copy\n    forces_new = []  # Fresh forces each time\n    \n    for particle in particles:\n        force = calculate_force(particle)\n        forces_new.append(force)\n        particle['velocity'] += force * dt\n        particle['position'] += particle['velocity'] * dt\n    return particles, forces_new","type":"content","url":"/python-data-structures-v2#shallow-vs-deep-copies-in-grid-simulations","position":35},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl2":"4.5 Dictionaries: O(1) Lookup Magic for Physics"},"type":"lvl2","url":"/python-data-structures-v2#id-4-5-dictionaries-o-1-lookup-magic-for-physics","position":36},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl2":"4.5 Dictionaries: O(1) Lookup Magic for Physics"},"content":"Now we explore one of computer science’s most elegant inventions - the dictionary! In computational physics, dictionaries are perfect for particle properties, lookup tables, and caching expensive calculations. You’re about to understand exactly how they achieve near-instantaneous lookups regardless of size.","type":"content","url":"/python-data-structures-v2#id-4-5-dictionaries-o-1-lookup-magic-for-physics","position":37},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Organizing Simulation Data with Dictionaries","lvl2":"4.5 Dictionaries: O(1) Lookup Magic for Physics"},"type":"lvl3","url":"/python-data-structures-v2#organizing-simulation-data-with-dictionaries","position":38},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Organizing Simulation Data with Dictionaries","lvl2":"4.5 Dictionaries: O(1) Lookup Magic for Physics"},"content":"Let’s see how dictionaries transform particle management in an N-body simulation:\n\n# Stage 1: Basic dictionary structure (15 lines)\n# N-body simulation: Managing particle properties efficiently\nimport json\n\n# Individual particle data (could have thousands)\nparticles = {\n    'particle_0001': {\n        'mass': 1.989e30,  # grams (0.001 solar masses)\n        'position': [1.5e13, 0.0, 0.0],  # cm\n        'velocity': [0.0, 3.0e6, 0.0],   # cm/s\n        'type': 'star'\n    },\n    'particle_0002': {\n        'mass': 5.972e27,  # grams (Earth mass)\n        'position': [2.5e13, 0.0, 0.0],\n        'velocity': [0.0, 2.5e6, 0.0],\n        'type': 'planet'\n    }\n}\n\n# Stage 2: O(1) lookup demonstration (15 lines)\n# O(1) lookup by particle ID - instant access!\ntarget = 'particle_0002'\nprint(f\"Accessing {target}:\")\nprint(f\"  Mass: {particles[target]['mass']:.2e} g\")\nprint(f\"  Type: {particles[target]['type']}\")\n\n# Organize by type for efficient group operations\nby_type = {}\nfor pid, data in particles.items():\n    ptype = data['type']\n    if ptype not in by_type:\n        by_type[ptype] = []\n    by_type[ptype].append(pid)\n\nprint(f\"\\nParticles by type: {by_type}\")\nprint(\"In astronomy: Perfect for star/planet/dark matter separation\")\n\n","type":"content","url":"/python-data-structures-v2#organizing-simulation-data-with-dictionaries","position":39},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Understanding Hash Tables for Physics Applications","lvl2":"4.5 Dictionaries: O(1) Lookup Magic for Physics"},"type":"lvl3","url":"/python-data-structures-v2#understanding-hash-tables-for-physics-applications","position":40},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Understanding Hash Tables for Physics Applications","lvl2":"4.5 Dictionaries: O(1) Lookup Magic for Physics"},"content":"Let’s demystify how dictionaries achieve O(1) lookup - critical for lookup tables in equation of state calculations:\n\ndef demonstrate_hashing_for_physics():\n    \"\"\"Show how hash tables enable fast lookups in physics codes.\"\"\"\n    \n    # Common lookup table keys in physics simulations\n    physics_keys = [\n        (1.0e6, 1.0e-3),   # (Temperature, Density) for EOS\n        (2.0e6, 2.0e-3),\n        (5.0e6, 1.0e-2),\n        \"H_ionization\",     # Reaction rates\n        \"He_ionization\",\n        \"opacity_table\"     # Opacity types\n    ]\n    \n    print(\"How hash tables accelerate physics lookups:\")\n    print(\"=\" * 60)\n    print(\"Key                  → Hash        → Bucket\")\n    print(\"-\" * 60)\n    \n    for key in physics_keys:\n        hash_value = hash(key)\n        # Simplified bucket calculation\n        bucket = abs(hash_value) % 100\n        if isinstance(key, tuple):\n            key_str = f\"T={key[0]:.0e}, ρ={key[1]:.0e}\"\n        else:\n            key_str = str(key)\n        print(f\"{key_str:20s} → {hash_value:11d} → {bucket:3d}\")\n    \n    print(\"\\nThe O(1) lookup process:\")\n    print(\"1. Hash the key → integer\")\n    print(\"2. Map to bucket index\")\n    print(\"3. Direct array access!\")\n    print(\"\\nThis is why equation of state tables are fast!\")\n\ndemonstrate_hashing_for_physics()\n\n","type":"content","url":"/python-data-structures-v2#understanding-hash-tables-for-physics-applications","position":41},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Dictionary Performance for Particle Lookups","lvl2":"4.5 Dictionaries: O(1) Lookup Magic for Physics"},"type":"lvl3","url":"/python-data-structures-v2#dictionary-performance-for-particle-lookups","position":42},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Dictionary Performance for Particle Lookups","lvl2":"4.5 Dictionaries: O(1) Lookup Magic for Physics"},"content":"Time to see the dramatic performance difference for particle system queries:\n\n# Stage 1: Create test data (10 lines)\nimport time\nimport random\n\n# Create a large particle system\nn_particles = 1_000_000\nprint(f\"Creating system with {n_particles:,} particles...\")\n\n# List approach (what beginners might try)\nparticle_list = [(f\"particle_{i:07d}\", random.uniform(1e27, 1e30)) \n                 for i in range(n_particles)]\n\n# Stage 2: Create dictionary version (8 lines)\n# Dictionary approach (professional solution)\nparticle_dict = {f\"particle_{i:07d}\": random.uniform(1e27, 1e30) \n                 for i in range(n_particles)}\n\n# Search for specific particle\ntarget = \"particle_0500000\"\n\nprint(f\"Finding {target} among {n_particles:,} particles...\")\n\n# Stage 3: Time the lookups (15 lines)\n# Time list search - O(n)\nstart = time.perf_counter()\nfor pid, mass in particle_list:\n    if pid == target:\n        mass_list = mass\n        break\nlist_time = time.perf_counter() - start\n\n# Time dict lookup - O(1)\nstart = time.perf_counter()\nmass_dict = particle_dict[target]\ndict_time = time.perf_counter() - start\n\nprint(f\"\\nList search: {list_time*1000:.3f} ms\")\nprint(f\"Dict lookup: {dict_time*1000:.6f} ms\")\nprint(f\"Dictionary is {list_time/dict_time:,.0f}× faster!\")\nprint(\"\\nFor astronomical catalogs with millions of objects:\")\nprint(\"This difference determines feasibility!\")\n\n💡 Computational Thinking: Memoization in Physics Calculations\n\nThis caching pattern dramatically speeds up physics codes:from functools import lru_cache\n\n@lru_cache(maxsize=10000)\ndef equation_of_state(temperature, density):\n    # Expensive calculation cached automatically\n    return complex_eos_calculation(temperature, density)\n\nReal physics applications:\n\nNuclear reaction rates: Cache temperature-dependent rates\n\nOpacity calculations: Cache (T, ρ, composition) lookups\n\nMolecular dynamics: Cache pairwise interaction potentials\n\nClimate models: Cache radiative transfer calculations\n\nIn astronomy, the MESA stellar evolution code uses extensive caching of nuclear reaction rates, speeding up calculations by >100× compared to recalculating every timestep.","type":"content","url":"/python-data-structures-v2#dictionary-performance-for-particle-lookups","position":43},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl2":"4.6 Sets: Mathematical Operations for Particle Systems"},"type":"lvl2","url":"/python-data-structures-v2#id-4-6-sets-mathematical-operations-for-particle-systems","position":44},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl2":"4.6 Sets: Mathematical Operations for Particle Systems"},"content":"Sets are perfect for tracking unique particles, finding neighbors, and performing mathematical operations on particle groups. They provide O(1) membership testing plus elegant set operations - ideal for computational physics!","type":"content","url":"/python-data-structures-v2#id-4-6-sets-mathematical-operations-for-particle-systems","position":45},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Set Operations in Particle Dynamics","lvl2":"4.6 Sets: Mathematical Operations for Particle Systems"},"type":"lvl3","url":"/python-data-structures-v2#set-operations-in-particle-dynamics","position":46},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Set Operations in Particle Dynamics","lvl2":"4.6 Sets: Mathematical Operations for Particle Systems"},"content":"\n\n# Particle tracking across domain boundaries\n# (Common in parallel simulations with domain decomposition)\n\n# Particles in different processor domains\ndomain_A = {'p001', 'p002', 'p003', 'p004', 'p005'}\ndomain_B = {'p004', 'p005', 'p006', 'p007'}\ndomain_C = {'p003', 'p007', 'p008', 'p009'}\n\n# Particles that need communication (on boundaries)\nboundary_particles = (domain_A & domain_B) | (domain_B & domain_C) | (domain_A & domain_C)\nprint(f\"Boundary particles needing communication: {boundary_particles}\")\n\n# Particles unique to each domain (no communication needed)\ninterior_A = domain_A - domain_B - domain_C\ninterior_B = domain_B - domain_A - domain_C\ninterior_C = domain_C - domain_A - domain_B\nprint(f\"Interior particles (A): {interior_A}\")\n\n# Check particle conservation\nall_particles = domain_A | domain_B | domain_C\nprint(f\"Total unique particles: {len(all_particles)}\")\n\n# Particles in multiple domains (need consistency check)\nduplicated = set()\nall_domains = [domain_A, domain_B, domain_C]\nfor i in range(len(all_domains)):\n    for j in range(i+1, len(all_domains)):\n        duplicated |= (all_domains[i] & all_domains[j])\n\nprint(f\"Particles in multiple domains: {duplicated}\")\nprint(\"\\nSets make domain decomposition bookkeeping trivial!\")\nprint(\"Used in: MPI parallel codes, astronomical survey overlaps\")\n\n","type":"content","url":"/python-data-structures-v2#set-operations-in-particle-dynamics","position":47},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Performance: Sets vs Lists for Neighbor Finding","lvl2":"4.6 Sets: Mathematical Operations for Particle Systems"},"type":"lvl3","url":"/python-data-structures-v2#performance-sets-vs-lists-for-neighbor-finding","position":48},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Performance: Sets vs Lists for Neighbor Finding","lvl2":"4.6 Sets: Mathematical Operations for Particle Systems"},"content":"\n\n# Stage 1: Create test data (10 lines)\nimport time\n\n# Simulate neighbor finding in particle simulation\nn_particles = 100_000\nn_neighbors = 100\n\nprint(f\"Finding neighbors among {n_particles:,} particles...\")\n\n# Create particle IDs\nall_particles = [f\"p_{i:06d}\" for i in range(n_particles)]\nneighbors = [f\"p_{i:06d}\" for i in range(n_neighbors)]\n\n# Stage 2: Time the membership tests (15 lines)\n# Convert to set for fast lookup\nall_particles_set = set(all_particles)\n\n# Test if neighbors exist - List approach\nstart = time.perf_counter()\nfor neighbor in neighbors:\n    found = neighbor in all_particles  # O(n) each time!\nlist_time = time.perf_counter() - start\n\n# Test if neighbors exist - Set approach  \nstart = time.perf_counter()\nfor neighbor in neighbors:\n    found = neighbor in all_particles_set  # O(1) each time!\nset_time = time.perf_counter() - start\n\nprint(f\"\\nChecking {n_neighbors} potential neighbors:\")\nprint(f\"List approach: {list_time*1000:.2f} ms\")\nprint(f\"Set approach:  {set_time*1000:.5f} ms\")\nprint(f\"Set is {list_time/set_time:,.0f}× faster!\")\n\n✅ Check Your Understanding\n\nWhy is checking particle membership O(n) with lists but O(1) with sets?\n\nThink about the search process...\n\nAnswer: Lists must check each element sequentially until finding a match (or reaching the end) - this is O(n) linear time. Sets use hash tables: they compute a hash of the particle ID and jump directly to where it would be stored - this is O(1) constant time.\n\nFor a million-particle simulation, this is the difference between microseconds and seconds per lookup. When you’re checking thousands of particle interactions per timestep, this performance difference determines whether your simulation finishes in hours or weeks!","type":"content","url":"/python-data-structures-v2#performance-sets-vs-lists-for-neighbor-finding","position":49},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl2":"4.7 Memory and Performance for Scientific Computing"},"type":"lvl2","url":"/python-data-structures-v2#id-4-7-memory-and-performance-for-scientific-computing","position":50},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl2":"4.7 Memory and Performance for Scientific Computing"},"content":"Now that we understand how different data structures behave algorithmically with sets, dictionaries, and lists, let’s examine their actual memory footprint and cache performance implications. Understanding memory layout is crucial for scientific computing because it explains why specialized numerical libraries are so much faster than pure Python and will help you write cache-efficient simulation codes.","type":"content","url":"/python-data-structures-v2#id-4-7-memory-and-performance-for-scientific-computing","position":51},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Memory Usage in Particle Systems","lvl2":"4.7 Memory and Performance for Scientific Computing"},"type":"lvl3","url":"/python-data-structures-v2#memory-usage-in-particle-systems","position":52},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Memory Usage in Particle Systems","lvl2":"4.7 Memory and Performance for Scientific Computing"},"content":"Let’s examine memory costs for different data structures in a particle simulation context:\n\n# Stage 1: Basic Memory Measurement (8 lines)\nimport sys\n\ndef measure_particle_memory(n=10000):\n    \"\"\"Compare memory for particle ID storage.\"\"\"\n    ids_list = [f\"p_{i:06d}\" for i in range(n)]\n    ids_set = set(ids_list)\n    \n    print(f\"Storing {n:,} particle IDs:\")\n    print(f\"  List: {sys.getsizeof(ids_list):,} bytes\")\n    print(f\"  Set:  {sys.getsizeof(ids_set):,} bytes\")\n\nmeasure_particle_memory()\n\n# Stage 2: Understanding the Tradeoff (10 lines)\ndef understand_physics_tradeoffs():\n    \"\"\"Memory vs speed tradeoffs in physics codes.\"\"\"\n    # Small particle system\n    n = 100\n    positions = [[i*1.0, 0.0, 0.0] for i in range(n)]\n    \n    list_bytes = sys.getsizeof(positions)\n    # If we used spatial hashing (dict of cells)\n    dict_bytes = sys.getsizeof({i: pos for i, pos in enumerate(positions)})\n    \n    print(f\"100 particles as list: {list_bytes:,} bytes\")\n    print(f\"With spatial hash: ~{dict_bytes:,} bytes\")\n    print(f\"Extra {dict_bytes - list_bytes:,} bytes enables O(1) neighbor finding!\")\n\nunderstand_physics_tradeoffs()\n\n# Stage 3: Complete Memory Profile (12 lines)\ndef profile_simulation_structures(n_particles=1000):\n    \"\"\"Memory comparison for complete particle system.\"\"\"\n    import numpy as np  # Preview of Chapter 7\n    \n    structures = {\n        'List of lists': [[i, i*1.0, 0.0, 0.0] for i in range(n_particles)],\n        'List of tuples': [(i, i*1.0, 0.0, 0.0) for i in range(n_particles)],\n        'Dictionary': {i: [i*1.0, 0.0, 0.0] for i in range(n_particles)},\n        'NumPy (preview)': np.zeros((n_particles, 4))\n    }\n    \n    print(f\"Memory for {n_particles:,} particles (ID, x, y, z):\")\n    print(\"-\" * 50)\n    for name, struct in structures.items():\n        size = sys.getsizeof(struct)\n        per_particle = size / n_particles\n        print(f\"{name:15s}: {size:8,} bytes ({per_particle:.1f} bytes/particle)\")\n\nprofile_simulation_structures()\nprint(\"\\nNotice NumPy's dramatic efficiency - that's Chapter 7!\")\n\n","type":"content","url":"/python-data-structures-v2#memory-usage-in-particle-systems","position":53},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Cache Efficiency in Grid Computations","lvl2":"4.7 Memory and Performance for Scientific Computing"},"type":"lvl3","url":"/python-data-structures-v2#cache-efficiency-in-grid-computations","position":54},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Cache Efficiency in Grid Computations","lvl2":"4.7 Memory and Performance for Scientific Computing"},"content":"Modern CPUs are fast, but memory access is slow. Understanding cache efficiency is critical for grid-based simulations:\n\n# Stage 1: Setup (8 lines)\nimport time\n\n# Demonstrate cache effects in finite difference calculations\nsize = 500  # Grid size for heat equation\ngrid = [[20.0 + 0.1*i*j for j in range(size)] \n        for i in range(size)]\n\nprint(\"Simulating heat diffusion finite differences:\")\nprint(\"(Common in CFD and atmospheric modeling)\")\n\n# Stage 2: Row-major access (10 lines)\n# Row-major access (cache-friendly in Python)\nstart = time.perf_counter()\ntotal = 0.0\nfor i in range(size):\n    for j in range(size):\n        total += grid[i][j]\nrow_time = time.perf_counter() - start\n\nprint(f\"Processing {size}×{size} grid:\")\nprint(f\"Row-major (cache-friendly):   {row_time*1000:.1f} ms\")\n\n# Stage 3: Column-major comparison (12 lines)\n# Column-major access (cache-hostile in Python)\nstart = time.perf_counter()\ntotal = 0.0\nfor j in range(size):\n    for i in range(size):\n        total += grid[i][j]\ncol_time = time.perf_counter() - start\n\nprint(f\"Column-major (cache-hostile): {col_time*1000:.1f} ms\")\nprint(f\"Column-major is {col_time/row_time:.1f}× slower!\")\n\nprint(\"\\nNote: Python's object overhead masks some effects.\")\nprint(\"In NumPy/C/Fortran with contiguous arrays, this difference\")\nprint(\"can be 10-100×! Critical for CFD and climate codes.\")\n\n⚠️ Common Bug Alert: The Iteration Modification Trap\n\nNever modify a collection while iterating - a common bug in particle removal:# THE BUG - Skips particles!\nparticles = [p1, p2, p3, p4, p5]\nfor p in particles:\n    if p.escaped_boundary():\n        particles.remove(p)  # WRONG! Skips next particle\n\n# THE FIX 1: Iterate over copy\nfor p in particles.copy():\n    if p.escaped_boundary():\n        particles.remove(p)\n\n# THE FIX 2: List comprehension (best for physics)\nparticles = [p for p in particles if not p.escaped_boundary()]\n\n# THE FIX 3: Build removal list\nto_remove = []\nfor p in particles:\n    if p.escaped_boundary():\n        to_remove.append(p)\nfor p in to_remove:\n    particles.remove(p)\n\nThis bug has corrupted many molecular dynamics simulations!","type":"content","url":"/python-data-structures-v2#cache-efficiency-in-grid-computations","position":55},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl2":"4.8 Choosing the Right Structure for Physics"},"type":"lvl2","url":"/python-data-structures-v2#id-4-8-choosing-the-right-structure-for-physics","position":56},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl2":"4.8 Choosing the Right Structure for Physics"},"content":"After exploring all options, how do you choose? Here’s your decision framework for computational physics:\n\n✅ Check Your Understanding\n\nFor each physics scenario, what data structure would you choose?\n\nTracking unique particle IDs in collision detection → ?\n\nTime series of energy measurements → ?\n\nCaching expensive equation of state calculations → ?\n\nPhysical constants that must not change → ?\n\nThink before reading...\n\nAnswers:\n\nSet (unique particles, O(1) collision checks)\n\nList (ordered by time, allows duplicates)\n\nDictionary (cache (T,ρ) → pressure with O(1) lookup)\n\nTuple or namedtuple (immutable constants, prevents accidents)","type":"content","url":"/python-data-structures-v2#id-4-8-choosing-the-right-structure-for-physics","position":57},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Performance Quick Reference for Physics","lvl2":"4.8 Choosing the Right Structure for Physics"},"type":"lvl3","url":"/python-data-structures-v2#performance-quick-reference-for-physics","position":58},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Performance Quick Reference for Physics","lvl2":"4.8 Choosing the Right Structure for Physics"},"content":"Operation\n\nList\n\nTuple\n\nDict\n\nSet\n\nAccess by index\n\nO(1)\n\nO(1)\n\nN/A\n\nN/A\n\nSearch for particle\n\nO(n)\n\nO(n)\n\nO(1)*\n\nO(1)\n\nAdd particle\n\nO(1)†\n\nN/A\n\nO(1)†\n\nO(1)†\n\nRemove particle\n\nO(n)\n\nN/A\n\nO(1)\n\nO(1)\n\nMemory (relative)\n\n1×\n\n0.9×\n\n3×\n\n3×\n\nSpatial ordering\n\nYes\n\nYes\n\nNo\n\nNo\n\n* Dict searches by key (particle ID)† Amortized - occasionally O(n) during resize","type":"content","url":"/python-data-structures-v2#performance-quick-reference-for-physics","position":59},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Real Example: Combining Data Structures","lvl2":"4.8 Choosing the Right Structure for Physics"},"type":"lvl3","url":"/python-data-structures-v2#real-example-combining-data-structures","position":60},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Real Example: Combining Data Structures","lvl2":"4.8 Choosing the Right Structure for Physics"},"content":"Let’s see how different data structures work together in a simple particle tracking system:\n\n# Simple example: Tracking particles with multiple data structures\n\n# Constants (tuple - can't be changed accidentally!)\nCONSTANTS = (6.674e-8, 2.998e10, 1.381e-16)  # G, c, k_B\n\n# Particle data (dictionary for O(1) lookup by ID)\nparticles = {\n    'p001': {'mass': 1e30, 'x': 0.0, 'y': 0.0},\n    'p002': {'mass': 2e30, 'x': 10.0, 'y': 0.0},\n    'p003': {'mass': 1.5e30, 'x': 5.0, 'y': 5.0},\n}\n\n# Active particles (set for fast membership testing)\nactive = {'p001', 'p002'}  # p003 is inactive\n\n# Time series measurements (list - ordered by time)\nmeasurements = [\n    (0.0, 100.5),  # (time, energy)\n    (1.0, 99.8),\n    (2.0, 99.1),\n]\n\n# Example: Find total mass of active particles\ntotal_mass = 0\nfor pid in active:  # O(1) membership test for each\n    if pid in particles:  # O(1) lookup\n        total_mass += particles[pid]['mass']\n\nprint(f\"Total active mass: {total_mass:.1e} g\")\nprint(f\"Number of measurements: {len(measurements)}\")\nprint(f\"Constants are protected: type(CONSTANTS) = {type(CONSTANTS).__name__}\")\n\n# This simple combination shows:\n# - Tuple for constants (immutable)\n# - Dictionary for particle data (O(1) lookup)\n# - Set for active particles (O(1) membership)\n# - List for time series (ordered)\n\n","type":"content","url":"/python-data-structures-v2#real-example-combining-data-structures","position":61},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl2":"Practice Exercises"},"type":"lvl2","url":"/python-data-structures-v2#practice-exercises","position":62},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl2":"Practice Exercises"},"content":"","type":"content","url":"/python-data-structures-v2#practice-exercises","position":63},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Exercise 4.1: Particle System Organization","lvl2":"Practice Exercises"},"type":"lvl3","url":"/python-data-structures-v2#exercise-4-1-particle-system-organization","position":64},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Exercise 4.1: Particle System Organization","lvl2":"Practice Exercises"},"content":"Part A: Basic List Implementation (5-10 lines)\n\nFollow these exact steps to create a working particle system:\n\ndef create_particle_system_list():\n    \"\"\"Store particles as list of lists - simple but limited.\"\"\"\n    particles = []\n    \n    # Create 5 test particles (id, mass, x, y, z)\n    for i in range(5):\n        particle = [i, 1.0e30 * (i+1), i*1e13, 0.0, 0.0]\n        particles.append(particle)\n    \n    print(f\"Created {len(particles)} particles\")\n    print(f\"Particle 0: ID={particles[0][0]}, mass={particles[0][1]:.1e} g\")\n    return particles\n\nparticles = create_particle_system_list()\n\nPart B: Convert to Dictionary (10-15 lines)\n\nImprove the design with dictionaries for O(1) lookup:\n\ndef create_particle_system_dict():\n    \"\"\"Store particles in dictionary - better for lookups.\"\"\"\n    particles = {}\n    \n    # Create particles with meaningful structure\n    for i in range(5):\n        particles[f'p{i:03d}'] = {\n            'mass': 1.0e30 * (i+1),  # grams\n            'position': [i*1e13, 0.0, 0.0],  # cm\n            'velocity': [0.0, 2e6, 0.0],  # cm/s\n        }\n    \n    # O(1) lookup by ID!\n    target = 'p002'\n    print(f\"Particle {target}: mass = {particles[target]['mass']:.1e} g\")\n    print(f\"  Position: {particles[target]['position'][0]:.1e} cm\")\n    \n    return particles\n\nparticles = create_particle_system_dict()\nprint(f\"System has {len(particles)} particles with O(1) access\")\n\nPart C: Production Version with Validation (15-20 lines)\n\nAdd error checking and performance measurement:\n\nimport time\n\ndef create_particle_system_professional(n=1000):\n    \"\"\"Production-ready particle system with validation.\"\"\"\n    start = time.perf_counter()\n    \n    particles = {}\n    errors = []\n    \n    for i in range(n):\n        # Validate mass (must be positive)\n        mass = 1.0e30 * (1 + i*0.001)\n        if mass <= 0:\n            errors.append(f\"Particle {i}: invalid mass {mass}\")\n            continue\n            \n        particles[f'p{i:06d}'] = {\n            'mass': mass,\n            'position': [i*1e11, 0.0, 0.0],\n            'velocity': [0.0, 3e6, 0.0],\n            'active': True\n        }\n    \n    elapsed = time.perf_counter() - start\n    \n    print(f\"Created {len(particles)} particles in {elapsed*1000:.1f} ms\")\n    if errors:\n        print(f\"Skipped {len(errors)} invalid particles\")\n    \n    # Verify O(1) access\n    test_time = time.perf_counter()\n    _ = particles['p000500']['mass']\n    access_time = time.perf_counter() - test_time\n    print(f\"Single particle access: {access_time*1e6:.2f} μs\")\n    \n    return particles\n\nsystem = create_particle_system_professional()\n\n","type":"content","url":"/python-data-structures-v2#exercise-4-1-particle-system-organization","position":65},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Exercise 4.2: Collision Detection System","lvl2":"Practice Exercises"},"type":"lvl3","url":"/python-data-structures-v2#exercise-4-2-collision-detection-system","position":66},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Exercise 4.2: Collision Detection System","lvl2":"Practice Exercises"},"content":"Part A: Basic Neighbor Finding (10 lines)\n\ndef find_neighbors_naive(positions, radius):\n    \"\"\"Find particle pairs within radius - O(n²) approach.\"\"\"\n    neighbors = []\n    n = len(positions)\n    \n    for i in range(n):\n        for j in range(i+1, n):  # Avoid double counting\n            dx = positions[i][0] - positions[j][0]\n            dy = positions[i][1] - positions[j][1]\n            r = (dx**2 + dy**2) ** 0.5\n            if r < radius:\n                neighbors.append((i, j))\n    \n    return neighbors\n\n# Test with small system\npos = [[0, 0], [1, 0], [0, 1], [2, 2], [3, 3]]\npairs = find_neighbors_naive(pos, 1.5)\nprint(f\"Found {len(pairs)} neighbor pairs: {pairs}\")\n\nPart B: Use Sets for Efficiency (15 lines)\n\ndef find_neighbors_with_sets(particles, radius):\n    \"\"\"Track unique collision pairs with sets.\"\"\"\n    \n    # Use set to avoid duplicate pairs\n    collision_pairs = set()\n    checked_pairs = set()\n    \n    positions = [(i, p['position']) for i, p in particles.items()]\n    \n    for i, (id1, pos1) in enumerate(positions):\n        for j, (id2, pos2) in enumerate(positions[i+1:], i+1):\n            pair = tuple(sorted([id1, id2]))  # Canonical ordering\n            \n            if pair not in checked_pairs:\n                checked_pairs.add(pair)\n                \n                # Check distance\n                dx = pos1[0] - pos2[0]\n                dy = pos1[1] - pos2[1]\n                r = (dx**2 + dy**2) ** 0.5\n                \n                if r < radius:\n                    collision_pairs.add(pair)\n    \n    print(f\"Checked {len(checked_pairs)} unique pairs\")\n    print(f\"Found {len(collision_pairs)} collision candidates\")\n    return collision_pairs\n\n# Test\ntest_particles = {\n    'p1': {'position': [0, 0, 0]},\n    'p2': {'position': [1e10, 0, 0]},\n    'p3': {'position': [0, 1e10, 0]}\n}\ncollisions = find_neighbors_with_sets(test_particles, 1.5e10)\n\nPart C: Spatial Hashing for O(n) (25 lines)\n\n💡 Hint\n\nDivide space into a grid where each cell is twice the collision radius, then only check particles in adjacent cells. This transforms O(n²) to O(n) by limiting comparisons to nearby particles.\n\ndef find_neighbors_spatial_hash(particles, radius, cell_size=None):\n    \"\"\"Use spatial hashing for O(n) neighbor finding.\"\"\"\n    if cell_size is None:\n        cell_size = radius * 2\n    \n    # Build spatial hash\n    cells = {}\n    for pid, data in particles.items():\n        x, y = data['position'][:2]\n        cell_key = (int(x / cell_size), int(y / cell_size))\n        \n        if cell_key not in cells:\n            cells[cell_key] = set()\n        cells[cell_key].add(pid)\n    \n    # Find neighbors (only check adjacent cells)\n    neighbors = set()\n    for cell_key, pids in cells.items():\n        cx, cy = cell_key\n        \n        # Check this cell and 8 neighbors\n        for dx in [-1, 0, 1]:\n            for dy in [-1, 0, 1]:\n                neighbor_cell = (cx + dx, cy + dy)\n                if neighbor_cell in cells:\n                    for p1 in pids:\n                        for p2 in cells[neighbor_cell]:\n                            if p1 < p2:  # Avoid duplicates\n                                # Check actual distance\n                                pos1 = particles[p1]['position']\n                                pos2 = particles[p2]['position']\n                                r = ((pos1[0]-pos2[0])**2 + \n                                     (pos1[1]-pos2[1])**2) ** 0.5\n                                if r < radius:\n                                    neighbors.add((p1, p2))\n    \n    print(f\"Spatial hash: {len(cells)} cells, {len(neighbors)} pairs\")\n    print(\"This scales as O(n) instead of O(n²)!\")\n    return neighbors\n\n","type":"content","url":"/python-data-structures-v2#exercise-4-2-collision-detection-system","position":67},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Exercise 4.3: Equation of State Cache","lvl2":"Practice Exercises"},"type":"lvl3","url":"/python-data-structures-v2#exercise-4-3-equation-of-state-cache","position":68},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Exercise 4.3: Equation of State Cache","lvl2":"Practice Exercises"},"content":"Part A: Basic EOS Function (10 lines)\n\nclass SimpleEOS:\n    \"\"\"Basic equation of state without caching.\"\"\"\n    \n    def __init__(self):\n        self.calculations = 0\n    \n    def pressure(self, temperature, density):\n        \"\"\"Calculate pressure (ideal gas for simplicity).\"\"\"\n        self.calculations += 1\n        k_B = 1.381e-16  # erg/K\n        m_H = 1.673e-24  # grams (hydrogen mass)\n        \n        # P = ρkT/m for ideal gas\n        P = density * k_B * temperature / m_H\n        return P\n\neos = SimpleEOS()\nP1 = eos.pressure(1e6, 1e-3)\nP2 = eos.pressure(1e6, 1e-3)  # Same calculation repeated!\nprint(f\"Pressure: {P1:.2e} dyne/cm²\")\nprint(f\"Performed {eos.calculations} calculations (wasteful!)\")\n\nPart B: Add Dictionary Cache (15 lines)\n\nclass CachedEOS:\n    \"\"\"EOS with dictionary caching.\"\"\"\n    \n    def __init__(self):\n        self.cache = {}\n        self.calculations = 0\n        self.cache_hits = 0\n    \n    def pressure(self, temperature, density):\n        \"\"\"Calculate pressure with caching.\"\"\"\n        key = (temperature, density)\n        \n        if key in self.cache:\n            self.cache_hits += 1\n            return self.cache[key]\n        \n        # Calculate only if not cached\n        self.calculations += 1\n        k_B = 1.381e-16\n        m_H = 1.673e-24\n        P = density * k_B * temperature / m_H\n        \n        self.cache[key] = P\n        return P\n\neos = CachedEOS()\n# Simulate multiple calls with same parameters\nfor _ in range(5):\n    P = eos.pressure(1e6, 1e-3)\n\nprint(f\"Calculations: {eos.calculations}, Cache hits: {eos.cache_hits}\")\nprint(f\"Saved {eos.cache_hits} expensive calculations!\")\n\nPart C: Advanced Cache with Memory Limit (25 lines)\n\nfrom collections import OrderedDict\n\nclass ProductionEOS:\n    \"\"\"Production-ready EOS with LRU cache and statistics.\"\"\"\n    \n    def __init__(self, cache_size=1000):\n        self.cache = OrderedDict()\n        self.cache_size = cache_size\n        self.calculations = 0\n        self.cache_hits = 0\n        self.evictions = 0\n    \n    def pressure(self, T, rho):\n        \"\"\"Get pressure with automatic caching.\"\"\"\n        key = (round(T, 2), round(rho, 6))  # Round for cache efficiency\n        \n        if key in self.cache:\n            # Move to end (most recent)\n            self.cache.move_to_end(key)\n            self.cache_hits += 1\n            return self.cache[key]\n        \n        # Calculate\n        self.calculations += 1\n        k_B = 1.381e-16\n        m_H = 1.673e-24\n        \n        # More realistic EOS (includes radiation pressure)\n        a = 7.566e-15  # Radiation constant\n        P_gas = rho * k_B * T / m_H\n        P_rad = a * T**4 / 3\n        P_total = P_gas + P_rad\n        \n        # Add to cache\n        self.cache[key] = P_total\n        \n        # Evict oldest if needed\n        if len(self.cache) > self.cache_size:\n            self.cache.popitem(last=False)\n            self.evictions += 1\n        \n        return P_total\n    \n    def stats(self):\n        \"\"\"Report cache performance.\"\"\"\n        total = self.calculations + self.cache_hits\n        if total > 0:\n            hit_rate = self.cache_hits / total * 100\n            print(f\"Cache statistics:\")\n            print(f\"  Size: {len(self.cache)}/{self.cache_size}\")\n            print(f\"  Calculations: {self.calculations}\")\n            print(f\"  Cache hits: {self.cache_hits}\")\n            print(f\"  Hit rate: {hit_rate:.1f}%\")\n            print(f\"  Evictions: {self.evictions}\")\n\n# Test with stellar interior conditions\neos = ProductionEOS(cache_size=100)\n\n# Simulate stellar model with repeated conditions\ntest_conditions = [\n    (1e7, 100), (2e7, 150), (1e7, 100),  # Repeated\n    (3e7, 200), (1e7, 100), (2e7, 150),  # More repeats\n]\n\nfor T, rho in test_conditions:\n    P = eos.pressure(T, rho)\n    print(f\"T={T:.0e} K, ρ={rho} g/cm³ → P={P:.2e} dyne/cm²\")\n\neos.stats()\nprint(\"\\nIn astronomy: Essential for stellar evolution codes!\")\n\n","type":"content","url":"/python-data-structures-v2#exercise-4-3-equation-of-state-cache","position":69},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl2":"Main Takeaways"},"type":"lvl2","url":"/python-data-structures-v2#main-takeaways","position":70},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl2":"Main Takeaways"},"content":"You’ve just mastered concepts that will transform your computational physics from toy problems to research-grade simulations! The journey from understanding simple lists to architecting with dictionaries and sets represents a fundamental shift in how you approach scientific computing. You now see data structures not just as containers, but as carefully chosen tools where the right choice can mean the difference between simulations that finish in hours versus weeks.\n\nThe most profound insight from this chapter is that data structure choice often matters more than algorithm optimization. We saw how switching from a list to a set for particle lookups gave us a 100,000× speedup - no amount of code optimization could achieve that! This is the secret that separates research codes from student projects: professionals spend more time architecting their data organization than writing physics equations. The best physics insight in the world is useless if your code can’t handle realistic problem sizes.\n\nThe mutable versus immutable distinction that seemed abstract at first is actually critical for scientific computing. Every time you use a tuple for physical constants or configuration parameters, you’re preventing bugs that have literally crashed spacecraft and corrupted published results. When you properly use defensive copying for your simulation state, you’re protecting yourself from the aliasing bugs that have plagued computational physics for decades. Remember: in the Cassini example, a simple data structure choice nearly lost a billion-dollar mission.\n\nThe performance principles you’ve learned extend far beyond Python. The cache efficiency concepts explain why codes like LAMMPS and GADGET obsess over memory layout - it’s not premature optimization, it’s the difference between simulating thousands versus millions of atoms. The Big-O notation you’ve mastered is the universal language for discussing whether an algorithm scales to galaxy-sized problems. The hash table concept underlying dictionaries appears in every parallel communication library, every adaptive mesh refinement code, and every spatial indexing scheme you’ll encounter.\n\nMost importantly, you now understand the connection between data structures and numerical methods. That O(n²) all-pairs particle interaction that’s impossible for large systems? It becomes O(n log n) with tree-based structures. The O(n) searching that makes timesteps crawl? It becomes O(1) with spatial hashing. These aren’t just computer science concepts - they’re the difference between simulating a few hundred particles and modeling entire galaxies with billions of stars.\n\nRemember: every data structure makes trade-offs. Lists give you ordering but slow searches. Dictionaries trade memory for lightning-fast lookups. Sets provide uniqueness and mathematical operations but lose ordering. There’s no universally “best” structure - only the best structure for your specific physics problem. As you tackle research simulations, you’ll combine multiple structures: NumPy arrays for number crunching, dictionaries for particle properties, sets for collision detection, and spatial data structures for neighbor finding.\n\nWith this foundation, you’re ready to architect simulations that scale to astronomical proportions. The next chapter on functions and modules will show you how to organize this knowledge into reusable, testable components. After that, NumPy will supercharge your numerical operations using the memory layout principles you now understand. You’re no longer just writing code - you’re engineering solutions to the computational challenges of modern physics!","type":"content","url":"/python-data-structures-v2#main-takeaways","position":71},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl2":"Definitions"},"type":"lvl2","url":"/python-data-structures-v2#definitions","position":72},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl2":"Definitions"},"content":"Aliasing: When two or more variables refer to the same object in memory, causing modifications through one to affect the others.\n\nAmortized O(1): An operation that is usually O(1) but occasionally O(n), where the average over many operations remains O(1).\n\nBig-O Notation: Mathematical notation describing how an algorithm’s runtime scales with input size, focusing on the dominant term.\n\nCache: Small, fast memory close to the CPU that stores recently accessed data for quick retrieval.\n\nData Structure: A way of organizing data in computer memory to enable efficient access and modification.\n\nDeep Copy: Creating a completely independent copy of an object and all objects it contains, recursively.\n\nDictionary: A mutable mapping type storing key-value pairs with O(1) average lookup time using hash tables.\n\nHash Function: A function mapping data of arbitrary size to fixed-size values, enabling fast lookups.\n\nHash Table: The underlying implementation for dictionaries and sets, enabling O(1) average-case lookups.\n\nImmutable: Objects whose state cannot be modified after creation (tuples, strings, numbers).\n\nList: Python’s built-in mutable sequence type that stores references to objects in order.\n\nMemoization: Caching technique storing function results to avoid recalculating for the same inputs.\n\nMutable: Objects whose state can be modified after creation (lists, dictionaries, sets).\n\nNamed Tuple: A tuple subclass allowing element access by name as well as index.\n\nO(1) - Constant Time: An operation whose runtime doesn’t depend on input size.\n\nO(n) - Linear Time: An operation whose runtime grows proportionally with input size.\n\nO(n²) - Quadratic Time: An operation whose runtime grows with the square of input size.\n\nO(log n) - Logarithmic Time: An operation whose runtime grows logarithmically with input size.\n\nO(n log n): Common complexity for efficient sorting algorithms and tree-based operations.\n\nReference: A variable that points to an object in memory rather than containing the value directly.\n\nSet: A mutable collection of unique, unordered elements with O(1) membership testing.\n\nShallow Copy: Creating a new container with references to the same contained objects.\n\nSpatial Hashing: Organizing particles by spatial region for O(1) neighbor finding.\n\nTuple: An immutable sequence type that cannot be changed after creation.","type":"content","url":"/python-data-structures-v2#definitions","position":73},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl2":"Key Takeaways"},"type":"lvl2","url":"/python-data-structures-v2#key-takeaways","position":74},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl2":"Key Takeaways"},"content":"✓ Data structure choice can change performance by factors of 1,000,000× or more for large-scale simulations\n\n✓ Lists are versatile but have O(n) search - use for ordered particle arrays that you’ll vectorize with NumPy\n\n✓ Dictionaries and sets provide O(1) lookup through hash tables - essential for particle lookups and caching\n\n✓ Tuples prevent modification bugs - use for physical constants and configuration parameters\n\n✓ The shallow vs deep copy distinction is critical for grid-based simulations\n\n✓ Python stores references to objects, explaining why NumPy’s contiguous arrays are faster\n\n✓ Memory layout affects cache performance by 2-10× even in Python\n\n✓ Every data structure trades something (memory, speed, flexibility) for something else\n\n✓ Mutable default arguments are dangerous - always use the None sentinel pattern (Chapter 1 callback)\n\n✓ Spatial data structures transform O(n²) physics problems into O(n) or O(n log n)\n\n✓ Sets provide elegant mathematical operations for domain decomposition and particle tracking\n\n✓ Dictionaries enable memoization that can speed up equation of state calculations 100×","type":"content","url":"/python-data-structures-v2#key-takeaways","position":75},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl2":"Python Module & Method Reference"},"type":"lvl2","url":"/python-data-structures-v2#python-module-method-reference","position":76},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl2":"Python Module & Method Reference"},"content":"This reference section catalogs all Python modules, functions, and methods introduced in this chapter. Keep this as a quick lookup guide for your physics simulations.","type":"content","url":"/python-data-structures-v2#python-module-method-reference","position":77},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Standard Library Modules","lvl2":"Python Module & Method Reference"},"type":"lvl3","url":"/python-data-structures-v2#standard-library-modules","position":78},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Standard Library Modules","lvl2":"Python Module & Method Reference"},"content":"time module - High-resolution timing for performance measurementimport time\n\ntime.perf_counter() - Returns float seconds with highest available resolution\n\nUse for timing code segments: start = time.perf_counter()\n\nAlways use for benchmarking (not time.time() which can go backwards!)\n\nsys module - System-specific parameters (expanded from Chapter 1)import sys\n\nsys.getsizeof(object) - Returns memory size of object in bytes\n\nIncludes object overhead but not contained objects\n\nEssential for memory profiling in simulations\n\ncopy module - Create object copies with control over depthimport copy\n\ncopy.copy(x) - Creates shallow copy (new container, same contents)\n\ncopy.deepcopy(x) - Creates deep copy (recursively copies all contents)\n\nCritical for grid simulations to avoid aliasing bugs\n\nUse for simulation state backups\n\nrandom module - Generate random numbers for Monte Carloimport random\n\nrandom.random() - Random float in [0.0, 1.0)\n\nrandom.uniform(a, b) - Random float in [a, b]\n\nrandom.choice(seq) - Random element from sequence\n\nrandom.sample(population, k) - k unique random elements\n\njson module - Save/load structured dataimport json\n\njson.dump(obj, file, indent=2) - Write object to file with formatting\n\njson.load(file) - Read object from file\n\njson.dumps(obj) - Convert object to JSON string\n\njson.loads(string) - Parse JSON string to object\n\nmath module - Mathematical functions (review from Chapter 2)import math\n\nmath.sqrt(x) - Square root (use for distances)\n\nmath.log(x) - Natural logarithm\n\nmath.log2(x) - Base-2 logarithm (for Big-O analysis)","type":"content","url":"/python-data-structures-v2#standard-library-modules","position":79},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Collections Module","lvl2":"Python Module & Method Reference"},"type":"lvl3","url":"/python-data-structures-v2#collections-module","position":80},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Collections Module","lvl2":"Python Module & Method Reference"},"content":"collections module - Specialized container datatypesfrom collections import OrderedDict, Counter, defaultdict, deque, namedtuple\n\nOrderedDict - Dictionary that remembers insertion order\n\n.move_to_end(key, last=True) - Move key to end (or beginning if last=False)\n\n.popitem(last=True) - Remove and return (key, value) pair from end/beginning\n\nUse for LRU caches in physics calculations\n\nCounter - Dictionary subclass for counting hashable objects\n\nCounter(iterable) - Create from iterable\n\n.most_common(n) - Returns n most common (element, count) pairs\n\n.update(iterable) - Add counts from iterable\n\nPerfect for histogram analysis of particle properties\n\ndefaultdict - Dictionary with default value factory\n\ndefaultdict(list) - Missing keys default to empty list\n\ndefaultdict(int) - Missing keys default to 0\n\nEliminates KeyError in particle grouping\n\ndeque - Double-ended queue with O(1) operations at both ends\n\ndeque(maxlen=n) - Fixed-size queue (old elements dropped)\n\n.append(x) / .appendleft(x) - Add to right/left end\n\n.pop() / .popleft() - Remove from right/left end\n\nEssential for boundary conditions in simulations\n\nnamedtuple - Tuple subclass with named fieldsParticle = namedtuple('Particle', ['id', 'mass', 'x', 'y', 'z'])\np = Particle(1, 1.0e30, 0, 0, 0)\nprint(p.mass)  # Access by name","type":"content","url":"/python-data-structures-v2#collections-module","position":81},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Functools Module","lvl2":"Python Module & Method Reference"},"type":"lvl3","url":"/python-data-structures-v2#functools-module","position":82},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Functools Module","lvl2":"Python Module & Method Reference"},"content":"functools module - Higher-order functions and operationsfrom functools import lru_cache\n\n@lru_cache(maxsize=128) - Decorator for automatic memoization@lru_cache(maxsize=1000)\ndef expensive_calculation(T, rho):\n    return complex_physics_calculation(T, rho)\n\nCaches function results automatically\n\nmaxsize=None for unlimited cache\n\n.cache_info() - Returns cache statistics\n\n.cache_clear() - Clear the cache","type":"content","url":"/python-data-structures-v2#functools-module","position":83},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Built-in Functions for Data Structures","lvl2":"Python Module & Method Reference"},"type":"lvl3","url":"/python-data-structures-v2#built-in-functions-for-data-structures","position":84},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Built-in Functions for Data Structures","lvl2":"Python Module & Method Reference"},"content":"Type Checking\n\ntype(obj) - Returns object’s type\n\nisinstance(obj, type) - Check if obj is instance of type\n\nid(obj) - Returns unique identifier (memory address)\n\nobj1 is obj2 - Check if same object (identity)\n\nContainer Operations\n\nlen(container) - Number of elements\n\nx in container - Membership test (O(1) for sets/dicts, O(n) for lists)\n\nsorted(iterable) - Returns new sorted list\n\nreversed(sequence) - Returns reverse iterator\n\nenumerate(iterable, start=0) - Returns (index, value) pairs\n\nzip(iter1, iter2, ...) - Parallel iteration\n\nCopying\n\nlist(iterable) - Create new list from iterable\n\ntuple(iterable) - Create new tuple from iterable\n\nset(iterable) - Create new set from iterable\n\ndict(pairs) - Create dictionary from (key, value) pairs","type":"content","url":"/python-data-structures-v2#built-in-functions-for-data-structures","position":85},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Data Structure Methods Summary","lvl2":"Python Module & Method Reference"},"type":"lvl3","url":"/python-data-structures-v2#data-structure-methods-summary","position":86},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Data Structure Methods Summary","lvl2":"Python Module & Method Reference"},"content":"List Methods (Expanded)\n\n.append(x) - Add to end (amortized O(1))\n\n.extend(iterable) - Add all elements from iterable\n\n.insert(i, x) - Insert at position i (O(n))\n\n.remove(x) - Remove first x (O(n))\n\n.pop(i=-1) - Remove and return element at i\n\n.clear() - Remove all elements\n\n.index(x) - Find position of x (O(n))\n\n.count(x) - Count occurrences of x\n\n.sort() - Sort in-place\n\n.reverse() - Reverse in-place\n\n.copy() - Create shallow copy\n\nDictionary Methods (Expanded)\n\n.get(key, default=None) - Safe access with default\n\n.setdefault(key, default) - Get or set with default\n\n.pop(key, default) - Remove and return value\n\n.popitem() - Remove and return arbitrary (key, value)\n\n.update(other) - Update with other dict/pairs\n\n.keys() - View of keys\n\n.values() - View of values\n\n.items() - View of (key, value) pairs\n\n.clear() - Remove all items\n\n.copy() - Shallow copy\n\nSet Methods (Expanded)\n\n.add(x) - Add element\n\n.remove(x) - Remove element (raises KeyError if missing)\n\n.discard(x) - Remove element (no error if missing)\n\n.pop() - Remove and return arbitrary element\n\n.clear() - Remove all elements\n\n.union(other) or | - Elements in either set\n\n.intersection(other) or & - Elements in both sets\n\n.difference(other) or - - Elements in first but not second\n\n.symmetric_difference(other) or ^ - Elements in either but not both\n\n.update(other) or |= - Add elements from other\n\n.intersection_update(other) or &= - Keep only elements in both\n\n.difference_update(other) or -= - Remove elements in other\n\n.symmetric_difference_update(other) or ^= - Keep elements in either but not both\n\n.issubset(other) - Check if all elements in other\n\n.issuperset(other) - Check if contains all elements of other\n\n.isdisjoint(other) - Check if no elements in common\n\n.copy() - Create shallow copy","type":"content","url":"/python-data-structures-v2#data-structure-methods-summary","position":87},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl2":"Quick Reference Tables"},"type":"lvl2","url":"/python-data-structures-v2#quick-reference-tables","position":88},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl2":"Quick Reference Tables"},"content":"","type":"content","url":"/python-data-structures-v2#quick-reference-tables","position":89},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Data Structure Operations","lvl2":"Quick Reference Tables"},"type":"lvl3","url":"/python-data-structures-v2#data-structure-operations","position":90},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Data Structure Operations","lvl2":"Quick Reference Tables"},"content":"Operation\n\nList\n\nTuple\n\nDict\n\nSet\n\nCreate empty\n\n[]\n\n()\n\n{}\n\nset()\n\nCreate with items\n\n[1,2,3]\n\n(1,2,3)\n\n{'a':1}\n\n{1,2,3}\n\nAdd item\n\n.append(x)\n\nN/A\n\nd[k]=v\n\n.add(x)\n\nRemove item\n\n.remove(x)\n\nN/A\n\ndel d[k]\n\n.remove(x)\n\nCheck membership\n\nx in list\n\nx in tuple\n\nk in dict\n\nx in set\n\nGet by index\n\nlist[i]\n\ntuple[i]\n\nN/A\n\nN/A","type":"content","url":"/python-data-structures-v2#data-structure-operations","position":91},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Performance Quick Reference","lvl2":"Quick Reference Tables"},"type":"lvl3","url":"/python-data-structures-v2#performance-quick-reference","position":92},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Performance Quick Reference","lvl2":"Quick Reference Tables"},"content":"Operation\n\nList\n\nTuple\n\nDict\n\nSet\n\nDeque\n\nAccess by index\n\nO(1)\n\nO(1)\n\nN/A\n\nN/A\n\nO(n)\n\nSearch for value\n\nO(n)\n\nO(n)\n\nO(1)*\n\nO(1)\n\nO(n)\n\nAdd to end\n\nO(1)†\n\nN/A\n\nO(1)†\n\nO(1)†\n\nO(1)\n\nAdd to beginning\n\nO(n)\n\nN/A\n\nO(1)†\n\nO(1)†\n\nO(1)\n\nRemove from end\n\nO(1)\n\nN/A\n\nN/A\n\nN/A\n\nO(1)\n\nRemove from beginning\n\nO(n)\n\nN/A\n\nN/A\n\nN/A\n\nO(1)\n\nRemove specific\n\nO(n)\n\nN/A\n\nO(1)\n\nO(1)\n\nO(n)\n\n* Dict searches by key, not value† Amortized - occasionally O(n) during resize","type":"content","url":"/python-data-structures-v2#performance-quick-reference","position":93},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Common Methods for Physics","lvl2":"Quick Reference Tables"},"type":"lvl3","url":"/python-data-structures-v2#common-methods-for-physics","position":94},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"Common Methods for Physics","lvl2":"Quick Reference Tables"},"content":"Structure\n\nMethod\n\nPhysics Use Case\n\nlist\n\n.append(x)\n\nAdd new particle\n\nlist\n\n.extend(iter)\n\nMerge particle lists\n\ndict\n\n.get(k, default)\n\nSafe parameter lookup\n\ndict\n\n@lru_cache\n\nAutomatic memoization\n\nset\n\n.union(other)\n\nCombine particle domains\n\nset\n\n.intersection(other)\n\nFind boundary particles\n\ndeque\n\n.appendleft()\n\nBoundary conditions\n\nOrderedDict\n\n.move_to_end()\n\nLRU cache management","type":"content","url":"/python-data-structures-v2#common-methods-for-physics","position":95},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"When to Use Each Structure","lvl2":"Quick Reference Tables"},"type":"lvl3","url":"/python-data-structures-v2#when-to-use-each-structure","position":96},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl3":"When to Use Each Structure","lvl2":"Quick Reference Tables"},"content":"Physics Task\n\nBest Choice\n\nWhy\n\nParticle positions\n\nList → NumPy array\n\nVectorizable operations\n\nParticle lookup by ID\n\nDictionary\n\nO(1) access\n\nActive particles\n\nSet\n\nFast membership, uniqueness\n\nPhysical constants\n\nTuple/NamedTuple\n\nImmutable, safe\n\nEOS cache\n\nDictionary/LRU cache\n\nFast lookup by (T,ρ)\n\nCollision candidates\n\nSet\n\nNo duplicates, set operations\n\nTime series\n\nList\n\nOrdered, allows duplicates\n\nSpatial grid\n\nDict of sets\n\nO(1) cell access\n\nBoundary particles\n\nDeque\n\nFast operations at both ends\n\nParticle counts\n\nCounter\n\nAutomatic histogram creation","type":"content","url":"/python-data-structures-v2#when-to-use-each-structure","position":97},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl2":"Next Chapter Preview"},"type":"lvl2","url":"/python-data-structures-v2#next-chapter-preview","position":98},{"hierarchy":{"lvl1":"Chapter 4: Data Structures - Organizing Scientific Data","lvl2":"Next Chapter Preview"},"content":"With data structures mastered, Chapter 5 will explore functions and modules - how to organize code for reusability, testing, and collaboration. You’ll learn how Python’s function model, with first-class functions and closure support, enables powerful patterns like decorators and functional programming techniques. These concepts prepare you for building modular physics engines, creating reusable analysis pipelines, and understanding the functional programming paradigm used in modern frameworks like JAX where functions transform into automatically differentiable computational graphs. Get ready to transform your scripts into professional-quality scientific libraries!","type":"content","url":"/python-data-structures-v2#next-chapter-preview","position":99},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code"},"type":"lvl1","url":"/python-functions-modules-v2","position":0},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code"},"content":"","type":"content","url":"/python-functions-modules-v2","position":1},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl2":"Learning Objectives"},"type":"lvl2","url":"/python-functions-modules-v2#learning-objectives","position":2},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl2":"Learning Objectives"},"content":"By the end of this chapter, you will be able to:\n\nDesign functions as clear contracts with well-defined inputs and outputs\n\nUnderstand Python’s scope rules and how they affect variable access\n\nMaster positional, keyword, and default arguments for flexible interfaces\n\nApply functional programming patterns like map, filter, and lambda functions\n\nCreate and import your own modules for code organization\n\nDocument functions properly using docstrings\n\nRecognize and avoid common function-related bugs\n\nBuild modular, reusable code for scientific applications","type":"content","url":"/python-functions-modules-v2#learning-objectives","position":3},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl2":"Prerequisites Check"},"type":"lvl2","url":"/python-functions-modules-v2#prerequisites-check","position":4},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl2":"Prerequisites Check"},"content":"Before starting this chapter, verify you can:\n\nWrite loops and conditionals fluently (Chapter 3)\n\nChoose appropriate data structures for different tasks (Chapter 4)\n\nHandle floating-point arithmetic safely (Chapter 2)\n\nUse IPython for testing and timing code (Chapter 1)\n\nDesign algorithms with pseudocode (Chapter 3)\n\n# Quick prerequisite check\ndata = [2.5, 3.7, 1.2, 4.8]\nresult = []\nfor value in data:\n    if value > 2.0:\n        result.append(value * 2)\nprint(f\"If you got {result}, you're ready!\")\n# Expected: [5.0, 7.4, 9.6]\n\n","type":"content","url":"/python-functions-modules-v2#prerequisites-check","position":5},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl2":"Chapter Overview"},"type":"lvl2","url":"/python-functions-modules-v2#chapter-overview","position":6},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl2":"Chapter Overview"},"content":"Functions are the fundamental building blocks of organized code. Without functions, you’d be copying and pasting the same code repeatedly, making bugs harder to fix and improvements impossible to maintain. But functions are more than just a way to avoid repetition—they’re how we create abstractions, manage complexity, and build reliable software. Whether you’re calculating statistical measures, simulating physical systems, or processing experimental data, every computational project starts with well-designed functions.\n\nThis chapter teaches you to think about functions as contracts between different parts of your code. When you write a function that calculates energy or performs numerical integration, you’re creating a promise: given valid input, the function will reliably return the correct output. This contract mindset helps you write functions that others (including future you) can trust and use effectively. You’ll learn how to choose between positional and keyword arguments, when to use default values, and how to design interfaces that are both flexible and clear.\n\nWe’ll explore Python’s scope rules, which determine where variables can be accessed, and learn how seemingly simple concepts like default arguments can create subtle bugs that have plagued even major scientific software packages. You’ll discover how Python’s flexible parameter system enables powerful interfaces, and how functional programming concepts prepare you for modern scientific computing frameworks. By the end, you’ll be organizing your code into modules that can be shared, tested, and maintained professionally—essential skills for collaborative scientific research.","type":"content","url":"/python-functions-modules-v2#chapter-overview","position":7},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl2":"5.1 Defining Functions: The Basics"},"type":"lvl2","url":"/python-functions-modules-v2#id-5-1-defining-functions-the-basics","position":8},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl2":"5.1 Defining Functions: The Basics"},"content":"Function\nA reusable block of code that performs a specific task, taking inputs and optionally returning outputs.\n\nA function encapsulates a piece of logic that transforms inputs into outputs. Think of a function as a machine: you feed it raw materials (inputs), it performs some process (the function body), and it produces a product (output). In scientific computing, a function might calculate statistical measures, integrate equations, or transform data—each function performs one clear task that can be tested and trusted.","type":"content","url":"/python-functions-modules-v2#id-5-1-defining-functions-the-basics","position":9},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Your First Function","lvl2":"5.1 Defining Functions: The Basics"},"type":"lvl3","url":"/python-functions-modules-v2#your-first-function","position":10},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Your First Function","lvl2":"5.1 Defining Functions: The Basics"},"content":"Let’s start with something every scientist needs—calculating mean and standard deviation:\n\ndef calculate_mean(values):\n    \"\"\"\n    Calculate the arithmetic mean of a list of values.\n    \n    This is our first function - notice the structure!\n    \"\"\"\n    # Validate inputs early using assert (raises error if condition is False)\n    assert len(values) > 0, \"Cannot calculate mean of empty list\"\n    \n    total = sum(values)\n    count = len(values)\n    mean = total / count\n    return mean\n\n# Using the function\nmeasurements = [23.5, 24.1, 23.8, 24.3, 23.9]\navg = calculate_mean(measurements)\nprint(f\"Mean temperature: {avg:.2f}°C\")\n\n# The assert would raise an error with empty list:\n# empty_mean = calculate_mean([])  # AssertionError: Cannot calculate mean of empty list\n\nLet’s break down exactly how this works:\n\ndef keyword: Tells Python we’re defining a function\n\nFunction name (calculate_mean): Follows snake_case convention, describes what it does\n\nParameters (values): Variables that receive values when function is called\n\nDocstring: Brief description of what the function does (always include this!)\n\nFunction body: Indented code that does the actual work\n\nreturn statement: Sends a value back to whoever called the function\n\nWhen Python executes calculate_mean(measurements), it creates a temporary namespace where values = [23.5, 24.1, ...], runs the function body, and returns the result.\n\n🔍 Check Your Understanding #1\n\nWhat will this code print?def calculate_product(x, y):\n    product = x * y\n    # Oops, forgot the return statement!\n\nresult = calculate_product(3.0, 4.0)\nprint(f\"Product: {result}\")\n\nAnswer\n\nIt prints Product: None. The function calculates product but doesn’t return it. Without an explicit return statement, Python functions return None. This is a common bug in scientific code!\n\nTo fix it:def calculate_product(x, y):\n    product = x * y\n    return product  # Now it returns the value","type":"content","url":"/python-functions-modules-v2#your-first-function","position":11},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Functions Without Return Values","lvl2":"5.1 Defining Functions: The Basics"},"type":"lvl3","url":"/python-functions-modules-v2#functions-without-return-values","position":12},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Functions Without Return Values","lvl2":"5.1 Defining Functions: The Basics"},"content":"Not all functions return values. Some perform actions like printing results or saving data:\n\ndef report_statistics(data, name=\"Dataset\"):\n    \"\"\"Report basic statistics without returning values.\"\"\"\n    mean = sum(data) / len(data)\n    minimum = min(data)\n    maximum = max(data)\n    \n    print(f\"Statistics for {name}:\")\n    print(f\"  Mean: {mean:.3f}\")\n    print(f\"  Range: [{minimum:.3f}, {maximum:.3f}]\")\n    # No return statement - returns None implicitly\n\n# Report some calculations\ntemperatures = [20.1, 21.5, 19.8, 22.3, 20.9]\nreport_statistics(temperatures, \"Temperature (°C)\")\n\n","type":"content","url":"/python-functions-modules-v2#functions-without-return-values","position":13},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Returning Multiple Values","lvl2":"5.1 Defining Functions: The Basics"},"type":"lvl3","url":"/python-functions-modules-v2#returning-multiple-values","position":14},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Returning Multiple Values","lvl2":"5.1 Defining Functions: The Basics"},"content":"Python functions can return multiple values using tuples—perfect for calculations that produce related results:\n\ndef analyze_data(values):\n    \"\"\"\n    Calculate multiple statistics at once.\n    \n    Returns:\n        mean, std_dev, min_val, max_val\n    \"\"\"\n    n = len(values)\n    mean = sum(values) / n\n    \n    # Calculate standard deviation\n    variance = sum((x - mean)**2 for x in values) / n\n    std_dev = variance ** 0.5\n    \n    return mean, std_dev, min(values), max(values)\n\n# Analyze experimental data\ndata = [9.8, 10.1, 9.9, 10.2, 9.7, 10.0]\nmean, std, min_val, max_val = analyze_data(data)\n\nprint(f\"Mean: {mean:.3f}\")\nprint(f\"Std Dev: {std:.3f}\")\nprint(f\"Range: [{min_val}, {max_val}]\")\n\n","type":"content","url":"/python-functions-modules-v2#returning-multiple-values","position":15},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Kinetic Energy in CGS Units","lvl2":"5.1 Defining Functions: The Basics"},"type":"lvl3","url":"/python-functions-modules-v2#kinetic-energy-in-cgs-units","position":16},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Kinetic Energy in CGS Units","lvl2":"5.1 Defining Functions: The Basics"},"content":"Now let’s calculate kinetic energy using CGS units (centimeters, grams, seconds):\n\ndef kinetic_energy_cgs(mass_g, velocity_cms):\n    \"\"\"\n    Calculate kinetic energy in ergs.\n    \n    Parameters:\n        mass_g: mass in grams\n        velocity_cms: velocity in cm/s\n    \n    Returns:\n        energy in ergs (g⋅cm²/s²)\n    \"\"\"\n    energy_ergs = 0.5 * mass_g * velocity_cms**2\n    return energy_ergs\n\n# Example: electron moving at 1% speed of light\nelectron_mass = 9.109e-28  # grams\nc_light = 2.998e10  # speed of light in cm/s\nelectron_velocity = 0.01 * c_light  # 1% of c\nke = kinetic_energy_cgs(electron_mass, electron_velocity)\n\nprint(f\"Electron kinetic energy: {ke:.2e} ergs\")\nprint(f\"In eV: {ke/1.602e-12:.2f} eV\")  # 1 eV = 1.602e-12 ergs\n\n","type":"content","url":"/python-functions-modules-v2#kinetic-energy-in-cgs-units","position":17},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"The Design Process: From Problem to Function","lvl2":"5.1 Defining Functions: The Basics"},"type":"lvl3","url":"/python-functions-modules-v2#the-design-process-from-problem-to-function","position":18},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"The Design Process: From Problem to Function","lvl2":"5.1 Defining Functions: The Basics"},"content":"Before writing any function, design it first. This prevents the common mistake of coding yourself into a corner:\n\n\"\"\"\nDESIGN: Function to validate numerical calculation\n\nPURPOSE: Ensure numerical results are physically reasonable\nINPUT: value, expected_range\nOUTPUT: boolean (True if valid)\nPROCESS:\n    - Check if value is finite\n    - Check if value is within range\n\"\"\"\n\ndef validate_result(value, min_val=None, max_val=None):\n    \"\"\"\n    Validate a numerical result is reasonable.\n    \n    Simple validation - we'll expand this pattern later!\n    \"\"\"\n    import math\n    \n    # Check if finite\n    if not math.isfinite(value):\n        return False\n    \n    # Check bounds if provided\n    if min_val is not None and value < min_val:\n        return False\n    if max_val is not None and value > max_val:\n        return False\n    \n    return True\n\n# Test validation\nenergy = kinetic_energy_cgs(1.0, 100.0)\nprint(f\"Energy = {energy} ergs\")\nprint(f\"Valid (positive)? {validate_result(energy, min_val=0)}\")\nprint(f\"Valid (in range)? {validate_result(energy, 0, 10000)}\")\n\n🌟 Why This Matters: Validation Saves Missions\n\nValidation functions aren’t just good practice—they prevent catastrophic failures. When NASA’s Mars Climate Orbiter was lost in 1999, the root cause was a simple unit mismatch: one team used pound-seconds while another expected Newton-seconds. A validation function checking that thrust values were within expected ranges would have caught this $327 million error immediately.\n\nThis is why every function you write should validate its inputs and outputs. The few extra lines of validation code can save years of work and hundreds of millions of dollars!\n\n💡 Computational Thinking: Function Contract Design\n\nEvery well-designed function follows a contract pattern that applies across all programming:CONTRACT PATTERN:\n1. Preconditions: What must be true before calling\n2. Postconditions: What will be true after calling  \n3. Invariants: What stays unchanged\n4. Side effects: What else happens\n\nExample for kinetic_energy_cgs():\n- Precondition: mass > 0, velocity is numeric\n- Postcondition: returns positive energy value\n- Invariant: input values unchanged\n- Side effects: none (pure function)\n\nThis pattern appears in:\n- Database transactions (ACID properties)\n- API design (REST contracts)\n- Parallel computing (thread safety)\n- Unit testing (test contracts)","type":"content","url":"/python-functions-modules-v2#the-design-process-from-problem-to-function","position":19},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl2":"5.2 Function Arguments In-Depth"},"type":"lvl2","url":"/python-functions-modules-v2#id-5-2-function-arguments-in-depth","position":20},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl2":"5.2 Function Arguments In-Depth"},"content":"Parameter\nA variable in a function definition that receives a value when the function is called.\n\nArgument\nThe actual value passed to a function when calling it.\n\nPython provides flexible ways to handle function parameters. Understanding the distinction between positional arguments, keyword arguments, and default values is crucial for designing clear, flexible interfaces. Let’s explore this flexibility through progressive examples.","type":"content","url":"/python-functions-modules-v2#id-5-2-function-arguments-in-depth","position":21},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Positional vs Keyword Arguments","lvl2":"5.2 Function Arguments In-Depth"},"type":"lvl3","url":"/python-functions-modules-v2#positional-vs-keyword-arguments","position":22},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Positional vs Keyword Arguments","lvl2":"5.2 Function Arguments In-Depth"},"content":"\n\ndef calculate_force(mass_g, acceleration_cms2):\n    \"\"\"\n    Calculate force using Newton's second law in CGS units.\n    \n    Parameters:\n        mass_g: mass in grams\n        acceleration_cms2: acceleration in cm/s²\n    \n    Returns:\n        force in dynes (g⋅cm/s²)\n    \"\"\"\n    force_dynes = mass_g * acceleration_cms2\n    return force_dynes\n\n# Different ways to call the same function\nf1 = calculate_force(10, 980)                        # Positional only (Earth's gravity)\nf2 = calculate_force(mass_g=10, acceleration_cms2=980)  # Keywords (clear!)\nf3 = calculate_force(acceleration_cms2=980, mass_g=10)  # Keywords (any order!)\n\nprint(f\"Positional: {f1:.1f} dynes\")\nprint(f\"Keywords: {f2:.1f} dynes\")  \nprint(f\"Reversed keywords: {f3:.1f} dynes\")\n\n# This would be wrong (arguments reversed):\n# f_wrong = calculate_force(980, 10)  # Would give 9800 instead of 98000\n\n","type":"content","url":"/python-functions-modules-v2#positional-vs-keyword-arguments","position":23},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Default Arguments Make Functions Flexible","lvl2":"5.2 Function Arguments In-Depth"},"type":"lvl3","url":"/python-functions-modules-v2#default-arguments-make-functions-flexible","position":24},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Default Arguments Make Functions Flexible","lvl2":"5.2 Function Arguments In-Depth"},"content":"Default arguments let users omit parameters when standard values suffice:\n\ndef simulate_decay(initial_atoms, half_life_years=5730, time_years=0):\n    \"\"\"\n    Simulate radioactive decay.\n    \n    Parameters:\n        initial_atoms: starting number of atoms\n        half_life_years: half-life in years (default: C-14 = 5730)\n        time_years: elapsed time in years (default: 0)\n    \"\"\"\n    import math\n    remaining = initial_atoms * (0.5 ** (time_years / half_life_years))\n    return remaining\n\n# Different usage patterns\nn0 = 1000000  # One million atoms\n\n# Use all defaults (returns initial count)\nprint(f\"At t=0: {simulate_decay(n0):.0f} atoms\")\n\n# Override just time (uses default half-life for C-14)\nprint(f\"After 5730 years: {simulate_decay(n0, time_years=5730):.0f} atoms\")\n\n# Override all parameters (U-238 example)\nprint(f\"U-238 after 1 billion years: {simulate_decay(n0, 4.5e9, 1e9):.0f} atoms\")\n\n📚 When to Use Different Argument Types\n\nUse Positional Arguments When:\n\nThe meaning is obvious from context (power(base, exponent))\n\nThere are only 1-2 required parameters\n\nThe order is natural and memorable\n\nUse Keyword Arguments When:\n\nThere are many parameters (>3)\n\nParameters are optional\n\nThe meaning isn’t obvious (process(True, False, 5) vs process(verbose=True, cache=False, retries=5))\n\nUse Default Arguments When:\n\nThere’s a sensible standard value\n\nMost calls use the same value\n\nYou want backward compatibility when adding features\n\nBest Practice Progression:\n\nStart with required positional arguments\n\nAdd defaults for optional behaviors\n\nUse keyword-only arguments for clarity (see Advanced section)","type":"content","url":"/python-functions-modules-v2#default-arguments-make-functions-flexible","position":25},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"The Mutable Default Trap","lvl2":"5.2 Function Arguments In-Depth"},"type":"lvl3","url":"/python-functions-modules-v2#the-mutable-default-trap","position":26},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"The Mutable Default Trap","lvl2":"5.2 Function Arguments In-Depth"},"content":"Here’s a critical bug that has appeared in major scientific software:\n\n# THE TRAP - DON'T DO THIS!\ndef add_measurement_buggy(value, data=[]):\n    \"\"\"BUGGY VERSION - default list is shared!\"\"\"\n    data.append(value)\n    return data\n\n# Watch the disaster unfold\nday1 = add_measurement_buggy(23.5)\nprint(f\"Day 1: {day1}\")\n\nday2 = add_measurement_buggy(24.1)  # Surprise!\nprint(f\"Day 2: {day2}\")  # Contains BOTH days!\n\nprint(f\"Same list? {day1 is day2}\")  # True - it's the same object!\n\n⚠️ Common Bug Alert: The Mutable Default Disaster\n\nThis bug has appeared in:\n\nCERN analysis scripts (accumulated all runs’ data)\n\nNASA trajectory calculations (mixed mission parameters)\n\nWeather prediction models (combined different forecasts)\n\nThe symptom: data from previous runs mysteriously appears in new analyses.\n\nThe cause: Python evaluates default arguments once when the function is defined, not each time it’s called.\n\nThe fix: always use None as default for mutable arguments.\n\nSentinel Value\nA special value (like None) used to signal a particular condition, often used as a default for mutable arguments.\n\nHere’s the correct pattern:\n\ndef add_measurement_fixed(value, data=None):\n    \"\"\"CORRECT VERSION - new list each time if needed.\"\"\"\n    if data is None:\n        data = []  # Create new list when not provided\n    data.append(value)\n    return data\n\n# Now it works correctly\nday1 = add_measurement_fixed(23.5)\nday2 = add_measurement_fixed(24.1)\nprint(f\"Day 1: {day1}\")\nprint(f\"Day 2: {day2}\")  # Separate lists!\n\n# Can still provide existing list\ncombined = []\nadd_measurement_fixed(23.5, combined)\nadd_measurement_fixed(24.1, combined)\nprint(f\"Combined: {combined}\")\n\n","type":"content","url":"/python-functions-modules-v2#the-mutable-default-trap","position":27},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Variable Arguments (*args and **kwargs)","lvl2":"5.2 Function Arguments In-Depth"},"type":"lvl3","url":"/python-functions-modules-v2#variable-arguments-args-and-kwargs","position":28},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Variable Arguments (*args and **kwargs)","lvl2":"5.2 Function Arguments In-Depth"},"content":"*args\nCollects extra positional arguments into a tuple.\n\n**kwargs\nCollects extra keyword arguments into a dictionary.\n\ndef calculate_statistics(*values, method='all'):\n    \"\"\"\n    Calculate statistics for any number of values.\n    \n    *args collects positional arguments into a tuple.\n    \"\"\"\n    if not values:\n        return None\n    \n    n = len(values)\n    mean = sum(values) / n\n    \n    if method == 'mean':\n        return mean\n    \n    # Calculate variance and std dev\n    variance = sum((x - mean)**2 for x in values) / n\n    std_dev = variance ** 0.5\n    \n    if method == 'all':\n        return {\n            'n': n,\n            'mean': mean,\n            'std': std_dev,\n            'min': min(values),\n            'max': max(values)\n        }\n\n# Works with any number of arguments!\nprint(calculate_statistics(1, 2, 3, 4, 5))\nprint(f\"Mean only: {calculate_statistics(2.5, 3.7, 1.2, method='mean'):.2f}\")\n\ndef run_experiment(name, **parameters):\n    \"\"\"\n    Run experiment with flexible parameters.\n    \n    **kwargs collects keyword arguments into a dictionary.\n    \"\"\"\n    print(f\"=== Experiment: {name} ===\")\n    \n    # Default parameters\n    defaults = {\n        'temperature': 293.15,  # Kelvin\n        'pressure': 101325,     # Pascals\n        'trials': 10\n    }\n    \n    # Update with user parameters\n    config = defaults.copy()\n    config.update(parameters)\n    \n    print(\"Configuration:\")\n    for key, value in config.items():\n        print(f\"  {key}: {value}\")\n    \n    return f\"Completed {config['trials']} trials\"\n\n# Simple call\nresult = run_experiment(\"Test A\")\n\n# Complex call with many options\nresult = run_experiment(\n    \"Test B\",\n    temperature=350,\n    pressure=200000,\n    trials=50,\n    catalyst=\"Platinum\"\n)\n\n🔍 Check Your Understanding #2\n\nWhat’s wrong with this function definition, and how would you fix it?def process_data(values, scale=1.0, *extra, normalize=True):\n    # Process data with options\n    pass\n\nAnswer\n\nThe parameter order is wrong! Python requires this order:\n\nRegular positional parameters\n\n*args (if any)\n\nKeyword parameters with defaults\n\n**kwargs (if any)\n\nCorrect version:def process_data(values, *extra, scale=1.0, normalize=True):\n    # Now the order is correct\n    pass\n\nThe original would give a SyntaxError because *extra can’t come after a keyword parameter with a default.","type":"content","url":"/python-functions-modules-v2#variable-arguments-args-and-kwargs","position":29},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl2":"5.3 Scope and Namespaces"},"type":"lvl2","url":"/python-functions-modules-v2#id-5-3-scope-and-namespaces","position":30},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl2":"5.3 Scope and Namespaces"},"content":"Scope\n\nThe region of a program where a variable is accessible.\n\nNamespace\n\nA container that holds a set of identifiers and their associated objects.\n\nUnderstanding scope—where variables can be accessed—is crucial for writing bug-free code. Python’s scope rules determine which variables are visible at any point in your program. Without understanding scope, you’ll encounter confusing bugs where variables don’t have the values you expect, or worse, where changing a variable in one place mysteriously affects code elsewhere.","type":"content","url":"/python-functions-modules-v2#id-5-3-scope-and-namespaces","position":31},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"The LEGB Rule","lvl2":"5.3 Scope and Namespaces"},"type":"lvl3","url":"/python-functions-modules-v2#the-legb-rule","position":32},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"The LEGB Rule","lvl2":"5.3 Scope and Namespaces"},"content":"LEGB\n\nLocal, Enclosing, Global, Built-in - Python’s scope resolution order.\n\nPython resolves variable names using the LEGB rule, searching in this order:\n\nLocal: Inside the current function\n\nEnclosing: In the enclosing function (for nested functions)\n\nGlobal: At the top level of the module\n\nBuilt-in: In the built-in namespace (print, len, etc.)\n\n# Demonstrating LEGB with scientific context\nspeed_of_light = 2.998e10  # Global scope (cm/s in CGS)\n\ndef calculate_energy():\n    # Enclosing scope\n    rest_mass = 9.109e-28  # electron mass in grams\n    \n    def relativistic_factor(velocity_cms):\n        # Local scope\n        c = speed_of_light  # Accesses global (cm/s)\n        beta = velocity_cms / c\n        \n        # Local calculation\n        import math\n        if beta >= 1:\n            return float('inf')  # Cannot exceed speed of light\n        gamma = 1 / math.sqrt(1 - beta**2)\n        \n        print(f\"  Inside relativistic: β = {beta:.3f}, γ = {gamma:.3f}\")\n        return gamma\n    \n    # Use inner function\n    v = 0.5 * speed_of_light  # Half light speed (cm/s)\n    gamma = relativistic_factor(v)\n    \n    # Calculate relativistic energy (E = γmc²)\n    energy_ergs = rest_mass * speed_of_light**2 * gamma\n    print(f\"Inside calculate: E = {energy_ergs:.2e} ergs\")\n    return energy_ergs\n\nresult = calculate_energy()\nprint(f\"Global scope: c = {speed_of_light:.2e} cm/s\")\nprint(f\"Final energy: {result:.2e} ergs = {result/1.602e-12:.2f} eV\")\n\n","type":"content","url":"/python-functions-modules-v2#the-legb-rule","position":33},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"The UnboundLocalError Trap","lvl2":"5.3 Scope and Namespaces"},"type":"lvl3","url":"/python-functions-modules-v2#the-unboundlocalerror-trap","position":34},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"The UnboundLocalError Trap","lvl2":"5.3 Scope and Namespaces"},"content":"\n\ncounter = 0  # Global\n\ndef increment_wrong():\n    \"\"\"This will crash - Python sees assignment and assumes local!\"\"\"\n    # counter += 1  # UnboundLocalError!\n    print(\"Uncomment the line above to see the error\")\n\ndef increment_with_global():\n    \"\"\"Explicitly use global variable.\"\"\"\n    global counter\n    counter += 1\n    return counter\n\ndef increment_better(current_count):\n    \"\"\"Best approach - avoid global state entirely.\"\"\"\n    return current_count + 1\n\n# Test the better approach\ncount = 0\ncount = increment_better(count)\ncount = increment_better(count)\nprint(f\"Count: {count}\")\n\n# Global approach (generally avoid)\nincrement_with_global()\nincrement_with_global()\nprint(f\"Global counter: {counter}\")\n\n⚠️ Common Bug Alert: UnboundLocalError\n\nThe error happens because Python sees you’re assigning to counter, assumes it’s local, but then can’t find a local value to increment.\n\nSymptoms:\n\nVariable works fine when reading\n\nCrashes when trying to modify\n\nError message mentions “referenced before assignment”\n\nFix: Either use global keyword or (better) pass the value explicitly.\n\nReal disaster: A climate model that tried to update global temperature but created local variables instead, producing nonsense results for months before discovery.","type":"content","url":"/python-functions-modules-v2#the-unboundlocalerror-trap","position":35},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Closures: Functions That Remember","lvl2":"5.3 Scope and Namespaces"},"type":"lvl3","url":"/python-functions-modules-v2#closures-functions-that-remember","position":36},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Closures: Functions That Remember","lvl2":"5.3 Scope and Namespaces"},"content":"{margin} Closure\nA function that remembers variables from its enclosing scope even after that scope has finished.\n\ndef create_integrator(method='rectangle'):\n    \"\"\"\n    Factory function that creates specialized integrators.\n    \n    The returned function 'remembers' the method.\n    \"\"\"\n    def integrate(func, a, b, n=100):\n        \"\"\"Numerically integrate func from a to b.\"\"\"\n        dx = (b - a) / n\n        total = 0\n        \n        if method == 'rectangle':\n            # Left rectangle rule\n            for i in range(n):\n                x = a + i * dx\n                total += func(x) * dx\n                \n        elif method == 'midpoint':\n            # Midpoint rule (more accurate)\n            for i in range(n):\n                x = a + (i + 0.5) * dx\n                total += func(x) * dx\n        \n        return total\n    \n    # Return the customized function\n    return integrate\n\n# Create specialized integrators\nrect_integrate = create_integrator('rectangle')\nmid_integrate = create_integrator('midpoint')\n\n# Test with x² from 0 to 1 (exact answer = 1/3)\ndef f(x):\n    return x**2\n\nrect_result = rect_integrate(f, 0, 1, 100)\nmid_result = mid_integrate(f, 0, 1, 100)\nexact = 1/3\n\nprint(f\"Rectangle rule: {rect_result:.6f}\")\nprint(f\"Midpoint rule:  {mid_result:.6f}\")\nprint(f\"Exact:          {exact:.6f}\")\nprint(f\"Rectangle error: {abs(rect_result-exact):.6f}\")\nprint(f\"Midpoint error:  {abs(mid_result-exact):.6f}\")\n\n🔍 Check Your Understanding #3\n\nWhat happens if both inner and outer functions define the same variable name?def outer():\n    x = \"outer\"\n    \n    def inner():\n        x = \"inner\"\n        print(f\"Inner sees: {x}\")\n    \n    inner()\n    print(f\"Outer sees: {x}\")\n\nouter()\n\nAnswer\n\nEach function sees its own local variable. The inner function’s x doesn’t affect the outer function’s x. They’re in different namespaces!\n\nOutput:Inner sees: inner\nOuter sees: outer\n\nIf the inner function didn’t define its own x, it would see the outer’s x due to the LEGB rule (finding it in the Enclosing scope).\n\n💡 Computational Thinking: Why Global Variables Are Dangerous\n\nGlobal variables violate the “principle of least surprise” in computing:HIDDEN STATE ANTI-PATTERN:\n- Function behavior depends on external state\n- Can't understand function in isolation\n- Testing requires global setup\n- Parallel processing becomes impossible\n\nReal disaster: Mars Climate Orbiter (1999)\n- Global unit variable (metric vs imperial)\n- One module changed it to imperial\n- Another module read it expecting metric\n- $327 million spacecraft crashed into Mars\n\nBetter pattern: Explicit State Passing\nBAD:  current_units = 'metric'; calculate()\nGOOD: calculate(units='metric')","type":"content","url":"/python-functions-modules-v2#closures-functions-that-remember","position":37},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl2":"5.4 Functional Programming Elements"},"type":"lvl2","url":"/python-functions-modules-v2#id-5-4-functional-programming-elements","position":38},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl2":"5.4 Functional Programming Elements"},"content":"{margin} Side Effect\nAny state change that occurs beyond returning a value from a function, such as modifying global variables or printing output.\n\n{margin} Pure Function\nA function that always returns the same output for the same input with no side effects.\n\nPython supports functional programming—a style that treats computation as the evaluation of mathematical functions. These concepts are essential for modern scientific frameworks like JAX and lead to cleaner, more testable code.","type":"content","url":"/python-functions-modules-v2#id-5-4-functional-programming-elements","position":39},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Lambda Functions","lvl2":"5.4 Functional Programming Elements"},"type":"lvl3","url":"/python-functions-modules-v2#lambda-functions","position":40},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Lambda Functions","lvl2":"5.4 Functional Programming Elements"},"content":"{margin} Lambda\nAn anonymous function defined inline using the lambda keyword.\n\nLambda functions are small, anonymous functions defined inline:\n\n# Traditional function\ndef square(x):\n    return x**2\n\n# Equivalent lambda\nsquare_lambda = lambda x: x**2\n\nprint(f\"Traditional: {square(5)}\")\nprint(f\"Lambda: {square_lambda(5)}\")\n\n# Lambdas shine for sorting and filtering\ndata_points = [\n    {'x': 1.5, 'y': 2.3, 'error': 0.1},\n    {'x': 2.1, 'y': 4.7, 'error': 0.05},\n    {'x': 0.8, 'y': 1.2, 'error': 0.2},\n]\n\n# Sort by x value\nby_x = sorted(data_points, key=lambda p: p['x'])\nprint(\"\\nSorted by x:\")\nfor p in by_x:\n    print(f\"  x={p['x']}, y={p['y']}\")\n\n# Sort by relative error (error/y)\nby_rel_error = sorted(data_points, key=lambda p: p['error']/p['y'])\nprint(\"\\nSorted by relative error:\")\nfor p in by_rel_error:\n    rel_err = p['error']/p['y'] * 100\n    print(f\"  x={p['x']}, relative error={rel_err:.1f}%\")\n\n","type":"content","url":"/python-functions-modules-v2#lambda-functions","position":41},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Map, Filter, and Reduce","lvl2":"5.4 Functional Programming Elements"},"type":"lvl3","url":"/python-functions-modules-v2#map-filter-and-reduce","position":42},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Map, Filter, and Reduce","lvl2":"5.4 Functional Programming Elements"},"content":"These functional tools transform how you process data:\n\nfrom functools import reduce\n\n# Sample measurements in CGS units\nmeasurements_cms = [981, 979, 983, 9900, 980, 982, 978]  # cm/s² (g values)\n\n# FILTER: Remove outliers\nmean_estimate = 980  # cm/s²\ntolerance = 10  # cm/s²\n\ngood_data = list(filter(\n    lambda x: abs(x - mean_estimate) < tolerance,\n    measurements_cms\n))\n\nprint(f\"Original: {measurements_cms}\")\nprint(f\"Filtered: {good_data}\")\nprint(f\"Removed {len(measurements_cms) - len(good_data)} outliers\")\n\n# MAP: Apply calibration factor\ncalibration = 1.002  # 0.2% correction\ncalibrated = list(map(lambda x: x * calibration, good_data))\nprint(f\"Calibrated: {[f'{x:.1f}' for x in calibrated]}\")\n\n# REDUCE: Calculate mean\nmean = reduce(lambda a, b: a + b, calibrated) / len(calibrated)\nprint(f\"Final mean: {mean:.1f} cm/s²\")\n\n🔍 Check Your Understanding #4\n\nRewrite this loop using functional programming:# Square all positive values\nresults = []\nfor x in data:\n    if x > 0:\n        results.append(x**2)\n\nAnswer\n\nTwo equivalent functional approaches:# Using filter and map\nresults = list(map(\n    lambda x: x**2,\n    filter(lambda x: x > 0, data)\n))\n\n# Using list comprehension (more Pythonic)\nresults = [x**2 for x in data if x > 0]\n\nThe list comprehension is generally preferred in Python for readability, but understanding the functional approach prepares you for libraries like JAX.","type":"content","url":"/python-functions-modules-v2#map-filter-and-reduce","position":43},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Functions as First-Class Objects","lvl2":"5.4 Functional Programming Elements"},"type":"lvl3","url":"/python-functions-modules-v2#functions-as-first-class-objects","position":44},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Functions as First-Class Objects","lvl2":"5.4 Functional Programming Elements"},"content":"In Python, functions are objects you can pass around:\n\ndef apply_operator(data, operator_func):\n    \"\"\"\n    Apply any operation to data.\n    \n    operator_func determines the transformation.\n    \"\"\"\n    return [operator_func(x) for x in data]\n\n# Define different operations\ndef grams_to_kilograms(g):\n    \"\"\"Convert grams to kilograms.\"\"\"\n    return g / 1000.0\n\ndef dynes_to_newtons(dynes):\n    \"\"\"Convert dynes to Newtons (1 N = 10⁵ dynes).\"\"\"\n    return dynes / 1e5\n\ndef normalize(x, mean=0, std=1):\n    \"\"\"Standardize value.\"\"\"\n    return (x - mean) / std\n\n# Use different operations\nmasses_g = [100, 200, 300, 400]  # grams\nmasses_kg = apply_operator(masses_g, grams_to_kilograms)\nprint(f\"Grams: {masses_g}\")\nprint(f\"Kilograms: {masses_kg}\")\n\nforces_dynes = [1e5, 2e5, 3e5, 4e5]  # dynes\nforces_n = apply_operator(forces_dynes, dynes_to_newtons)\nprint(f\"\\nDynes: {forces_dynes}\")\nprint(f\"Newtons: {forces_n}\")\n\n# Create custom normalizer with closure\nvalues = [2, 4, 6, 8, 10]\nmean = sum(values) / len(values)\nstd = (sum((x - mean)**2 for x in values) / len(values)) ** 0.5\n\nnormalizer = lambda x: normalize(x, mean, std)\nnormalized = apply_operator(values, normalizer)\nprint(f\"\\nOriginal: {values}\")\nprint(f\"Normalized: {[f'{x:.2f}' for x in normalized]}\")\n\n🌟 Why This Matters: Modern Frameworks Require Functional Thinking\n\nFunctional programming isn’t just academic—it’s essential for modern scientific computing:\n\nJAX (Google’s NumPy replacement) requires pure functions for automatic differentiation\n\nParallel processing works best with stateless functions\n\nTesting is trivial when functions have no side effects\n\nGPU computing maps naturally to functional operations\n\nExample: In JAX, you can automatically differentiate through an entire physics simulation if it’s written functionally. This enables techniques like physics-informed neural networks that would be impossibly complex to implement manually.","type":"content","url":"/python-functions-modules-v2#functions-as-first-class-objects","position":45},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl2":"5.5 Modules and Packages"},"type":"lvl2","url":"/python-functions-modules-v2#id-5-5-modules-and-packages","position":46},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl2":"5.5 Modules and Packages"},"content":"{margin} Module\nA Python file containing definitions and statements that can be imported.\n\n{margin} Package\nA directory containing multiple Python modules and an __init__.py file.\n\nAs your analysis grows from scripts to projects, organization becomes critical. Modules let you organize related functions together and reuse them across projects.","type":"content","url":"/python-functions-modules-v2#id-5-5-modules-and-packages","position":47},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Creating Your First Module","lvl2":"5.5 Modules and Packages"},"type":"lvl3","url":"/python-functions-modules-v2#creating-your-first-module","position":48},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Creating Your First Module","lvl2":"5.5 Modules and Packages"},"content":"📝 Creating Module Files\n\nTo create a module, you need to save Python code in a separate .py file. You can either:\n\nCreate manually: Open a new file in your editor, paste the code, and save as statistics_tools.py\n\nCreate programmatically: Use the code below to generate the file\n\nFor this course, we recommend creating files manually in your editor for better understanding.\n\nSave this as statistics_tools.py in your current directory:\n\n# Method 1: Show the module contents (you would save this in a file)\nmodule_code = '''\n\"\"\"\nstatistics_tools.py - Basic statistical calculations.\n\nThis module provides fundamental statistical functions.\n\"\"\"\n\nimport math\n\ndef mean(data):\n    \"\"\"Calculate arithmetic mean.\"\"\"\n    return sum(data) / len(data)\n\ndef variance(data):\n    \"\"\"Calculate population variance.\"\"\"\n    m = mean(data)\n    return sum((x - m)**2 for x in data) / len(data)\n\ndef std_dev(data):\n    \"\"\"Calculate population standard deviation.\"\"\"\n    return math.sqrt(variance(data))\n\ndef std_error(data):\n    \"\"\"Calculate standard error of the mean.\"\"\"\n    return std_dev(data) / math.sqrt(len(data))\n\n# Module-level code runs on import\nprint(\"Loading statistics_tools module...\")\n\n# Test code that only runs when script is executed directly\nif __name__ == \"__main__\":\n    test_data = [1, 2, 3, 4, 5]\n    print(f\"Test mean: {mean(test_data)}\")\n    print(f\"Test std dev: {std_dev(test_data):.3f}\")\n'''\n\n# Method 2: Programmatically create the file\nwith open('statistics_tools.py', 'w') as f:\n    f.write(module_code)\n\nprint(\"Module file 'statistics_tools.py' has been created in your current directory\")\n\n","type":"content","url":"/python-functions-modules-v2#creating-your-first-module","position":49},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Using Your Module","lvl2":"5.5 Modules and Packages"},"type":"lvl3","url":"/python-functions-modules-v2#using-your-module","position":50},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Using Your Module","lvl2":"5.5 Modules and Packages"},"content":"\n\n# Different import methods\n\n# Method 1: Import entire module\nimport statistics_tools\n\ndata = [10, 12, 11, 13, 10, 11, 12]\nm = statistics_tools.mean(data)\ns = statistics_tools.std_dev(data)\nprint(f\"Method 1 - Mean: {m:.2f}, Std: {s:.2f}\")\n\n# Method 2: Import specific functions\nfrom statistics_tools import mean, std_error\n\nse = std_error(data)\nprint(f\"Method 2 - Standard error: {se:.3f}\")\n\n# Method 3: Import with alias\nimport statistics_tools as stats\n\nvar = stats.variance(data)\nprint(f\"Method 3 - Variance: {var:.3f}\")\n\n","type":"content","url":"/python-functions-modules-v2#using-your-module","position":51},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Building a Physics Module","lvl2":"5.5 Modules and Packages"},"type":"lvl3","url":"/python-functions-modules-v2#building-a-physics-module","position":52},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Building a Physics Module","lvl2":"5.5 Modules and Packages"},"content":"Let’s create a comprehensive module for physics calculations:\n\n# Create physics_cgs.py module with all CGS units\nphysics_module = '''\n\"\"\"\nphysics_cgs.py - Physics calculations in CGS units.\n\nAll calculations use CGS (centimeter-gram-second) units.\n\"\"\"\n\n# Physical constants in CGS\nSPEED_OF_LIGHT = 2.998e10      # cm/s\nPLANCK_CONSTANT = 6.626e-27    # erg⋅s  \nBOLTZMANN = 1.381e-16          # erg/K\nELECTRON_MASS = 9.109e-28      # g\nPROTON_MASS = 1.673e-24        # g\nGRAVITATIONAL_CONSTANT = 6.674e-8  # cm³/(g⋅s²)\n\ndef kinetic_energy(mass_g, velocity_cms):\n    \"\"\"KE = ½mv² in ergs.\"\"\"\n    return 0.5 * mass_g * velocity_cms**2\n\ndef momentum(mass_g, velocity_cms):\n    \"\"\"p = mv in g⋅cm/s.\"\"\"\n    return mass_g * velocity_cms\n\ndef de_broglie_wavelength(mass_g, velocity_cms):\n    \"\"\"λ = h/p in cm.\"\"\"\n    p = momentum(mass_g, velocity_cms)\n    return PLANCK_CONSTANT / p if p != 0 else float('inf')\n\ndef thermal_velocity(temp_k, mass_g):\n    \"\"\"RMS thermal velocity in cm/s.\"\"\"\n    import math\n    return math.sqrt(3 * BOLTZMANN * temp_k / mass_g)\n\ndef photon_energy(wavelength_cm):\n    \"\"\"E = hc/λ in ergs.\"\"\"\n    return PLANCK_CONSTANT * SPEED_OF_LIGHT / wavelength_cm\n\ndef gravitational_force(m1_g, m2_g, distance_cm):\n    \"\"\"F = Gm₁m₂/r² in dynes.\"\"\"\n    if distance_cm == 0:\n        return float('inf')\n    return GRAVITATIONAL_CONSTANT * m1_g * m2_g / distance_cm**2\n\nclass Particle:\n    \"\"\"Simple particle with physics methods.\"\"\"\n    \n    def __init__(self, mass_g, velocity_cms):\n        self.mass = mass_g\n        self.velocity = velocity_cms\n    \n    def kinetic_energy(self):\n        return kinetic_energy(self.mass, self.velocity)\n    \n    def wavelength(self):\n        return de_broglie_wavelength(self.mass, self.velocity)\n'''\n\nwith open('physics_cgs.py', 'w') as f:\n    f.write(physics_module)\n\n# Now use it\nimport physics_cgs\n\n# Electron at 1% light speed (all CGS units)\nv_electron = 0.01 * physics_cgs.SPEED_OF_LIGHT  # cm/s\nke = physics_cgs.kinetic_energy(physics_cgs.ELECTRON_MASS, v_electron)\nwavelength = physics_cgs.de_broglie_wavelength(physics_cgs.ELECTRON_MASS, v_electron)\n\nprint(f\"Electron at 1% c:\")\nprint(f\"  Velocity = {v_electron:.2e} cm/s\")\nprint(f\"  KE = {ke:.2e} ergs\")\nprint(f\"  de Broglie λ = {wavelength:.2e} cm\")\n\n# Test gravitational force (Earth-Moon in CGS)\nearth_mass_g = 5.972e27  # grams\nmoon_mass_g = 7.342e25   # grams  \nearth_moon_distance_cm = 3.844e10  # cm\n\nforce = physics_cgs.gravitational_force(earth_mass_g, moon_mass_g, earth_moon_distance_cm)\nprint(f\"\\nEarth-Moon gravitational force: {force:.2e} dynes\")\n\n","type":"content","url":"/python-functions-modules-v2#building-a-physics-module","position":53},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Import Best Practices","lvl2":"5.5 Modules and Packages"},"type":"lvl3","url":"/python-functions-modules-v2#import-best-practices","position":54},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Import Best Practices","lvl2":"5.5 Modules and Packages"},"content":"\n\n# GOOD: Clear, explicit imports\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom math import sin, cos, pi\n\n# BAD: Wildcard imports pollute namespace\n# from numpy import *  # Adds 600+ names!\n# from math import *   # Conflicts with numpy!\n\n# Example of namespace collision\nimport math\nimport cmath  # Complex math module (introduced in Chapter 2)\n\n# Clear which log we're using\nreal_log = math.log(10)      # Natural log of real number\ncomplex_log = cmath.log(-1)   # Can handle complex numbers\n\nprint(f\"math.log(10) = {real_log:.3f}\")\nprint(f\"cmath.log(-1) = {complex_log}\")  # Returns complex number\n\n# If we had used 'from math import *':\n# log(10)  # Which log? math or cmath? Unclear!\n\n🔍 Check Your Understanding #5\n\nWhy is from module import * dangerous in scientific code?\n\nAnswer\n\nWildcard imports are dangerous because:\n\nName collisions : Multiple libraries have functions with same names (log, sqrt, sin)\n\nHidden overwrites : Later imports silently replace earlier ones\n\nUnclear source : Can’t tell where a function comes from\n\nNamespace pollution : Hundreds of unwanted names\n\nReal example that caused wrong results:from numpy import *    # Has log (natural log)\nfrom math import *     # Also has log, overwrites numpy's\nfrom sympy import *    # Has symbolic log, overwrites again\n\n# Which log is this?\nresult = log(data)  # Could be any of the three!\n\nAlways use explicit imports for clarity and safety!\n\n🌟 The More You Know: How Modular Code Detected Gravitational Waves\n\nOn September 14, 2015, at 09:50:45 UTC, the Laser Interferometer Gravitational-Wave Observatory (LIGO) detected humanity’s first gravitational wave signal, named GW150914. This discovery, which earned the 2017 Nobel Prize in Physics, was only possible because of meticulously modular software architecture.\n\nThe LIGO analysis pipeline consisted of hundreds of independent Python modules, each performing one specific task:# Simplified LIGO pipeline structure\ndef calibrate_strain(raw_data, calibration_data):\n    \"\"\"Convert detector output to strain.\"\"\"\n    # Complex calibration involving transfer functions\n    \ndef remove_lines(strain_data, line_frequencies):\n    \"\"\"Remove power line interference and harmonics.\"\"\"\n    # Notch filters at 60Hz, 120Hz, etc.\n    \ndef whiten_data(strain, psd):\n    \"\"\"Normalize frequency response.\"\"\"\n    # Spectral whitening for uniform sensitivity\n    \ndef matched_filter(data, template_bank):\n    \"\"\"Search for gravitational wave patterns.\"\"\"\n    # Cross-correlation with theoretical waveforms\n\nWhen the first signal appeared, this modular design enabled unprecedented rapid verification. Within minutes, automated pipelines had processed the data through dozens of independent checks. The modularity allowed the 1000+ member collaboration to:\n\nTest each processing step with simulated signals\n\nRun multiple independent analysis pipelines in parallel\n\nSwap alternative algorithms to verify robustness\n\nComplete peer review in record time\n\nThe discovery paper lists over 30 independent software modules, each thoroughly tested and documented. This modular approach—exactly what you’re learning in this chapter—enabled one of physics’ greatest discoveries!\n\n🌟 The More You Know: The $370 Million Integer Overflow\n\nOn June 4, 1996, the maiden flight of Ariane 5 ended in spectacular failure just 37 seconds after liftoff. The rocket veered off course and self-destructed, destroying four Cluster satellites worth $370 million. This disaster teaches us a crucial lesson about function design and validation.\n\nThe root cause traced back to a single function in the inertial reference system, originally written for Ariane 4. This function converted a 64-bit floating-point horizontal velocity value to a 16-bit signed integer. In Ariane 4’s slower flight profile, this worked perfectly. But Ariane 5 was more powerful. At T+37 seconds, the horizontal velocity exceeded 32,767—the maximum value for a 16-bit signed integer.\n\nThe conversion function essentially looked like this conceptually:def convert_velocity(velocity_64bit):\n    # Direct conversion without validation\n    return int(velocity_64bit)  # Assumes it fits in 16 bits!\n\nWhat they needed was:def convert_velocity_safe(velocity_64bit):\n    MAX_INT16 = 32767\n    MIN_INT16 = -32768\n    \n    if velocity_64bit > MAX_INT16:\n        # Handle overflow appropriately\n        log_error(f\"Velocity {velocity_64bit} exceeds 16-bit range\")\n        return MAX_INT16  # Or raise exception with proper handling\n    elif velocity_64bit < MIN_INT16:\n        return MIN_INT16\n        \n    return int(velocity_64bit)\n\nThe overflow caused an exception in both the primary and backup systems (which ran identical code). The main flight computer, suddenly receiving diagnostic data instead of attitude information, interpreted it as actual flight data and commanded a violent course correction. The resulting aerodynamic forces exceeded design limits, triggering automatic self-destruct.\n\nThe investigation revealed four critical lessons about function design:\n\nAlways validate input ranges, especially when converting between data types\n\nNever assume code reuse is safe without checking new operating conditions\n\nDesign for graceful degradation rather than complete failure\n\nTest with realistic data ranges that match actual operating conditions\n\nEvery validation function you write, every bounds check you add, every type conversion you verify—these aren’t bureaucratic overhead. They’re the difference between mission success and catastrophic failure. This disaster led directly to modern software engineering practices that require explicit contracts for all functions, comprehensive range checking, and formal verification of critical code paths.\n\n:class: history\n\nIn 1990, the Hubble Space Telescope launched with a precisely ground mirror that was perfectly wrong. The primary mirror’s shape was off by just 2.2 micrometers—1/50th the width of a human hair—but this tiny error made the $1.5 billion telescope nearly useless for its first three years.\n\nThe cause? A spacing error in the reflective null corrector, the device used to test the mirror during grinding. But the real failure was in the testing software’s validation functions. The quality control program had a function like this:def validate_mirror_test(measurement, reference):\n    \"\"\"Check if mirror measurement matches reference.\"\"\"\n    difference = abs(measurement - reference)\n    threshold = 0.01  # Arbitrary threshold!\n    \n    if difference < threshold:\n        return \"PASS\"\n    else:\n        return \"FAIL\"\n\nThe problem? When two independent tests disagreed, engineers trusted the wrong one. The software’s validation function didn’t:\n\nCheck which test was more reliable\n\nValidate the validation equipment itself\n\nRequire agreement between multiple independent methods\n\nFlag when results were suspiciously perfect\n\nA proper validation would have looked like:def validate_mirror_comprehensive(test1, test2, test3):\n    \"\"\"Require agreement between independent tests.\"\"\"\n    # Check all tests are within tolerance of each other\n    if not all_agree([test1, test2, test3], tolerance=0.001):\n        raise ValueError(\"Independent tests disagree!\")\n    \n    # Check results aren't suspiciously perfect\n    if any(is_too_perfect(test) for test in [test1, test2, test3]):\n        raise ValueError(\"Measurement suspiciously perfect - check equipment!\")\n    \n    # Validate the validators\n    if not validate_test_equipment():\n        raise ValueError(\"Test equipment out of calibration!\")\n    \n    return statistics.mean([test1, test2, test3])\n\nThe incorrect mirror required a dramatic Space Shuttle servicing mission in 1993 to install corrective optics. The lesson? In scientific computing, your validation functions are as critical as your calculations. A thorough validation function—checking multiple independent sources, validating the validators, and flagging suspicious results—would have caught this error on the ground instead of in orbit.\n\nToday, Hubble’s legacy includes not just stunning images and revolutionary science, but also a fundamental lesson: never trust a single test, always validate your validators, and remember that perfect results are often perfectly wrong.\n## 5.6 Documentation and Testing\n\nGood documentation and testing make your functions trustworthy and reusable. Professional scientific code requires both to ensure reproducibility and reliability.\n\n### Documentation Levels\n\n```{code-cell} ipython3\n# Level 1: Minimal (but essential)\ndef quadratic_formula(a, b, c):\n    \"\"\"Solve ax² + bx + c = 0.\"\"\"\n    discriminant = b**2 - 4*a*c\n    if discriminant < 0:\n        return None, None\n    sqrt_disc = discriminant**0.5\n    return (-b + sqrt_disc)/(2*a), (-b - sqrt_disc)/(2*a)\n\n# Level 2: Professional documentation\ndef integrate_trapezoid(x_values, y_values):\n    \"\"\"\n    Integrate using the trapezoidal rule.\n    \n    Parameters\n    ----------\n    x_values : list or array\n        X coordinates of data points (must be sorted)\n    y_values : list or array\n        Y coordinates of data points\n    \n    Returns\n    -------\n    float\n        Approximate integral of y with respect to x\n    \n    Examples\n    --------\n    >>> x = [0, 1, 2, 3]\n    >>> y = [0, 1, 4, 9]  # y = x²\n    >>> integrate_trapezoid(x, y)\n    8.0  # Exact answer is 9\n    \n    Notes\n    -----\n    Uses the composite trapezoidal rule:\n    ∫y dx ≈ Σ(y[i] + y[i+1])/2 * (x[i+1] - x[i])\n    \n    More accurate for smooth functions with\n    many points.\n    \"\"\"\n    if len(x_values) != len(y_values):\n        raise ValueError(\"x and y must have same length\")\n    \n    integral = 0\n    for i in range(len(x_values) - 1):\n        dx = x_values[i + 1] - x_values[i]\n        avg_y = (y_values[i] + y_values[i + 1]) / 2\n        integral += avg_y * dx\n    \n    return integral\n\n# Test the function\nx = [0, 0.5, 1.0, 1.5, 2.0]\ny = [0, 0.25, 1.0, 2.25, 4.0]  # y = x²\nresult = integrate_trapezoid(x, y)\nprint(f\"Integral of x² from 0 to 2: {result:.3f}\")\nprint(f\"Exact answer: {8/3:.3f}\")\nprint(f\"Error: {abs(result - 8/3):.3f}\")\n```\n\n### Testing Your Functions\n\n```{code-cell} ipython3\ndef quadratic_formula(a, b, c):\n    \"\"\"\n    Solve quadratic equation ax² + bx + c = 0.\n    \n    Returns:\n        tuple: (root1, root2) or (None, None) if no real roots\n    \"\"\"\n    import math\n    \n    # Calculate discriminant\n    discriminant = b**2 - 4*a*c\n    \n    # Check if real roots exist\n    if discriminant < 0:\n        return None, None\n    elif discriminant == 0:\n        # One repeated root\n        root = -b / (2*a)\n        return root, root\n    else:\n        # Two distinct roots\n        sqrt_disc = math.sqrt(discriminant)\n        root1 = (-b + sqrt_disc) / (2*a)\n        root2 = (-b - sqrt_disc) / (2*a)\n        return root1, root2\n\ndef test_quadratic():\n    \"\"\"Test quadratic formula solver.\"\"\"\n    \n    # Test 1: Simple case (x² - 5x + 6 = 0)\n    # Roots should be 2 and 3\n    r1, r2 = quadratic_formula(1, -5, 6)\n    assert abs(r1 - 3) < 1e-10 or abs(r1 - 2) < 1e-10\n    assert abs(r2 - 3) < 1e-10 or abs(r2 - 2) < 1e-10\n    print(\"✓ Test 1: Simple roots\")\n    \n    # Test 2: No real roots (x² + 1 = 0)\n    r1, r2 = quadratic_formula(1, 0, 1)\n    assert r1 is None and r2 is None\n    print(\"✓ Test 2: No real roots\")\n    \n    # Test 3: Single root (x² - 2x + 1 = 0)\n    # Root should be 1 (twice)\n    r1, r2 = quadratic_formula(1, -2, 1)\n    assert abs(r1 - 1) < 1e-10\n    assert abs(r2 - 1) < 1e-10\n    print(\"✓ Test 3: Repeated root\")\n    \n    print(\"All tests passed!\")\n\n# Run tests\ntest_quadratic()\n```\n\n### Introduction to Type Hints (Optional Enhancement)\n\n:::{admonition} 📌 Type Hints are Optional\n:class: info\n\nType hints are an **optional** Python feature introduced in Python 3.5. They document expected types but **Python does not enforce them** - they're primarily for documentation and IDE support. \n\n**For this course**: Type hints are not required but can help make your code clearer. Focus on writing correct functions first, then add type hints if desired.\n\nPython supports type hints to document expected types:\n\ndef kinetic_energy_typed(mass_g: float, velocity_cms: float) -> float:\n    \"\"\"\n    Calculate kinetic energy with type hints.\n    \n    The ': float' annotations indicate expected types.\n    The '-> float' indicates the return type.\n    These are documentation only - Python doesn't enforce them!\n    \"\"\"\n    return 0.5 * mass_g * velocity_cms**2\n\n# Type hints help IDEs provide better autocomplete and catch potential errors\nenergy: float = kinetic_energy_typed(9.109e-28, 3e9)\nprint(f\"Energy: {energy:.2e} ergs\")\n\n# Python still allows \"wrong\" types - hints aren't enforced!\n# This works even though we pass integers instead of floats:\nenergy2 = kinetic_energy_typed(1, 100)  # Still works!\nprint(f\"Integer inputs still work: {energy2} ergs\")\n\n# For more complex types, import from typing module\nfrom typing import List, Tuple, Optional\n\ndef analyze_data_typed(values: List[float]) -> Tuple[float, float, Optional[float]]:\n    \"\"\"\n    Analyze data with complex type hints.\n    \n    List[float] means a list of floats\n    Tuple[float, float, Optional[float]] means it returns\n    a tuple with two floats and maybe a third float (or None)\n    \"\"\"\n    if not values:\n        return 0.0, 0.0, None\n    \n    n = len(values)\n    mean = sum(values) / n\n    variance = sum((x - mean)**2 for x in values) / n\n    std_dev = variance ** 0.5\n    \n    sorted_vals = sorted(values)\n    median = sorted_vals[n // 2] if n % 2 == 1 else (sorted_vals[n//2-1] + sorted_vals[n//2]) / 2\n    \n    return mean, std_dev, median\n\n# The IDE now knows exactly what types are returned\nmean_val, std_val, median_val = analyze_data_typed([1.0, 2.0, 3.0, 4.0, 5.0])\nprint(f\"Mean: {mean_val:.1f}, Std: {std_val:.2f}, Median: {median_val:.1f}\")\n\n🛠️ Debug This!\n\nThis function has a subtle bug. Can you spot it?def find_peaks(data, threshold):\n    \"\"\"Find all peaks above threshold - HAS BUG.\"\"\"\n    peaks = []\n    peak_indices = []\n    \n    for i in range(len(data)):\n        if data[i] > threshold:\n            # Check if it's a local maximum\n            if i == 0 or data[i] > data[i-1]:\n                if i == len(data)-1 or data[i] > data[i+1]:\n                    peaks.append(data[i])\n                    peak_indices.append(i)\n    \n    return peaks, peak_indices\n\nAnswer and Fix\n\nBug: The condition data[i] > data[i-1] should be data[i] >= data[i-1] (same for i+1). Otherwise it misses peaks in plateaus where consecutive values are equal.\n\nFix:def find_peaks_fixed(data, threshold):\n    \"\"\"Find all peaks above threshold.\"\"\"\n    peaks = []\n    peak_indices = []\n    \n    for i in range(len(data)):\n        if data[i] > threshold:\n            # Check if it's a local maximum (>= for plateaus)\n            is_peak = True\n            \n            if i > 0 and data[i] < data[i-1]:\n                is_peak = False\n            if i < len(data)-1 and data[i] < data[i+1]:\n                is_peak = False\n                \n            if is_peak:\n                peaks.append(data[i])\n                peak_indices.append(i)\n    \n    return peaks, peak_indices\n\nThis bug appeared in actual spectroscopy software, missing emission lines that had flat tops!","type":"content","url":"/python-functions-modules-v2#import-best-practices","position":55},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl2":"5.7 Performance Considerations"},"type":"lvl2","url":"/python-functions-modules-v2#id-5-7-performance-considerations","position":56},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl2":"5.7 Performance Considerations"},"content":"Understanding function overhead helps you write efficient code. Let’s measure and optimize!","type":"content","url":"/python-functions-modules-v2#id-5-7-performance-considerations","position":57},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Function Call Overhead","lvl2":"5.7 Performance Considerations"},"type":"lvl3","url":"/python-functions-modules-v2#function-call-overhead","position":58},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Function Call Overhead","lvl2":"5.7 Performance Considerations"},"content":"\n\nimport time\n\ndef simple_add(a, b):\n    \"\"\"Minimal work - overhead visible.\"\"\"\n    return a + b\n\ndef complex_calc(x):\n    \"\"\"Heavy computation - overhead negligible.\"\"\"\n    result = 0\n    for i in range(100):\n        result += (x + i) ** 0.5\n    return result / 100\n\n# Measure overhead\nn_calls = 100000\n\n# Simple function\nstart = time.perf_counter()\nfor _ in range(n_calls):\n    simple_add(1.0, 2.0)\nsimple_time = time.perf_counter() - start\n\n# Inline equivalent\nstart = time.perf_counter()\nfor _ in range(n_calls):\n    result = 1.0 + 2.0\ninline_time = time.perf_counter() - start\n\n# Complex function\nstart = time.perf_counter()\nfor _ in range(n_calls // 100):  # Fewer calls (it's slower)\n    complex_calc(1.0)\ncomplex_time = time.perf_counter() - start\n\nprint(\"Function Call Analysis:\")\nprint(f\"Simple function: {simple_time*1000:.1f} ms\")\nprint(f\"Inline addition: {inline_time*1000:.1f} ms\")\nprint(f\"Overhead factor: {simple_time/inline_time:.1f}x\")\nprint(f\"\\nComplex function: {complex_time*1000:.1f} ms\")\nprint(\"\\nLesson: Overhead only matters for trivial operations!\")\n\n","type":"content","url":"/python-functions-modules-v2#function-call-overhead","position":59},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Memoization for Expensive Calculations","lvl2":"5.7 Performance Considerations"},"type":"lvl3","url":"/python-functions-modules-v2#memoization-for-expensive-calculations","position":60},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Memoization for Expensive Calculations","lvl2":"5.7 Performance Considerations"},"content":"{margin} Memoization\nCaching function results to avoid recomputing expensive operations.\n\nfrom functools import lru_cache\n\n# Fibonacci without memoization (exponentially slow!)\ndef fib_slow(n):\n    if n < 2:\n        return n\n    return fib_slow(n-1) + fib_slow(n-2)\n\n# Fibonacci with memoization (linear time!)\n@lru_cache(maxsize=128)\ndef fib_fast(n):\n    if n < 2:\n        return n\n    return fib_fast(n-1) + fib_fast(n-2)\n\n# Compare performance\nimport time\n\nn = 30\n\nstart = time.perf_counter()\nresult_slow = fib_slow(n)\ntime_slow = time.perf_counter() - start\n\nstart = time.perf_counter()\nresult_fast = fib_fast(n)\ntime_fast = time.perf_counter() - start\n\nprint(f\"Fibonacci({n}) = {result_fast}\")\nprint(f\"Without memoization: {time_slow*1000:.1f} ms\")\nprint(f\"With memoization: {time_fast*1000:.3f} ms\")\nprint(f\"Speedup: {time_slow/time_fast:.0f}x\")\n\n# Check cache statistics\nprint(f\"\\nCache info: {fib_fast.cache_info()}\")\n\n","type":"content","url":"/python-functions-modules-v2#memoization-for-expensive-calculations","position":61},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl2":"Practice Exercises"},"type":"lvl2","url":"/python-functions-modules-v2#practice-exercises","position":62},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl2":"Practice Exercises"},"content":"","type":"content","url":"/python-functions-modules-v2#practice-exercises","position":63},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Exercise 5.1: Build Statistical Analysis Functions","lvl2":"Practice Exercises"},"type":"lvl3","url":"/python-functions-modules-v2#exercise-5-1-build-statistical-analysis-functions","position":64},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Exercise 5.1: Build Statistical Analysis Functions","lvl2":"Practice Exercises"},"content":"Create a suite of analysis functions for experimental data:\n\ndef calculate_statistics(data):\n    \"\"\"\n    Calculate comprehensive statistics.\n    \n    TODO: Your implementation should:\n    1. Handle empty lists (return None or raise ValueError)\n    2. Check for None values in the data\n    3. Return a dictionary with: mean, std, min, max, median\n    \n    Example return value:\n    {'mean': 5.2, 'std': 1.3, 'min': 3.1, 'max': 7.8, 'median': 5.0}\n    \"\"\"\n    # Your implementation here\n    # Start with: if not data: return None\n    pass\n\ndef remove_outliers(data, n_sigma=3):\n    \"\"\"\n    Remove points more than n_sigma standard deviations from mean.\n    \n    TODO: Your implementation should:\n    1. Calculate mean and standard deviation\n    2. Keep only values within mean ± n_sigma*std\n    3. Return filtered list\n    \n    Hint: Use the statistics from calculate_statistics()\n    \"\"\"\n    # Your implementation here\n    pass\n\ndef bootstrap_error(data, statistic_func=None, n_samples=1000):\n    \"\"\"\n    Estimate error using bootstrap resampling.\n    \n    TODO (Advanced challenge!):\n    1. Default to mean if no statistic_func provided\n    2. Resample data with replacement n_samples times\n    3. Calculate statistic for each resample\n    4. Return standard deviation of the statistics\n    \n    Hint: Use random.choices(data, k=len(data)) for resampling\n    \"\"\"\n    # Your implementation here\n    pass\n\n# Test with sample data\ntest_data = [9.8, 9.7, 10.1, 9.9, 50.0, 9.8, 10.0, 9.9]  # Note outlier!\nprint(f\"Original data: {test_data}\")\nprint(\"Implement the functions above to analyze this data!\")\n\n# Once implemented, you should be able to:\n# stats = calculate_statistics(test_data)\n# clean_data = remove_outliers(test_data, n_sigma=2)\n# error = bootstrap_error(clean_data)\n\n","type":"content","url":"/python-functions-modules-v2#exercise-5-1-build-statistical-analysis-functions","position":65},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Exercise 5.2: Create a Scientific Module","lvl2":"Practice Exercises"},"type":"lvl3","url":"/python-functions-modules-v2#exercise-5-2-create-a-scientific-module","position":66},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Exercise 5.2: Create a Scientific Module","lvl2":"Practice Exercises"},"content":"Build analysis_tools.py module:\"\"\"\nanalysis_tools.py - Data analysis utilities\n\nTODO: Create this module with:\n1. Constants (confidence levels, etc.)\n2. Statistical functions\n3. Data cleaning functions\n4. Plotting helpers\n5. Module testing in __main__\n\"\"\"\n\n# Your module here","type":"content","url":"/python-functions-modules-v2#exercise-5-2-create-a-scientific-module","position":67},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Exercise 5.3: Variable Star Analysis Functions","lvl2":"Practice Exercises"},"type":"lvl3","url":"/python-functions-modules-v2#exercise-5-3-variable-star-analysis-functions","position":68},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Exercise 5.3: Variable Star Analysis Functions","lvl2":"Practice Exercises"},"content":"Continue building our variable star analysis toolkit:\n\ndef generate_cepheid_data(period_days=5.4, amplitude_mag=0.3, n_points=50):\n    \"\"\"\n    Generate simulated Cepheid variable star data.\n    \n    Parameters:\n        period_days: Period in days\n        amplitude_mag: Amplitude in magnitudes\n        n_points: Number of observations\n    \n    Returns:\n        times, magnitudes, errors (all as lists)\n    \"\"\"\n    import random\n    import math\n    \n    times = []\n    mags = []\n    errors = []\n    \n    for i in range(n_points):\n        # Irregular sampling\n        t = i * period_days / 10 + random.uniform(-0.1, 0.1)\n        \n        # Cepheid light curve (asymmetric - rises quickly, falls slowly)\n        phase = (t % period_days) / period_days\n        \n        if phase < 0.3:\n            # Rising branch (quick brightening)\n            mag = 12.0 - amplitude_mag * (phase / 0.3)\n        else:\n            # Falling branch (slow dimming)\n            mag = 12.0 - amplitude_mag * math.exp(-(phase - 0.3) / 0.4)\n        \n        # Add realistic noise\n        mag += random.gauss(0, 0.02)\n        error = 0.01 + 0.01 * random.random()\n        \n        times.append(t)\n        mags.append(mag)\n        errors.append(error)\n    \n    return times, mags, errors\n\n# Create analysis functions (for you to implement)\ndef find_period_simple(times, mags):\n    \"\"\"\n    TODO: Estimate period from data.\n    Hint: Look for repeating patterns in brightness!\n    \n    One approach:\n    1. Find time between brightness minima\n    2. Average these intervals\n    3. Return estimated period\n    \"\"\"\n    # Your implementation here\n    pass\n\ndef phase_fold(times, mags, period):\n    \"\"\"\n    TODO: Fold data on given period.\n    \n    Algorithm:\n    1. Calculate phase for each time: phase = (time % period) / period\n    2. Sort by phase\n    3. Return phases and corresponding magnitudes\n    \"\"\"\n    # Your implementation here\n    pass\n\n# Generate and analyze data\nt, m, e = generate_cepheid_data()\nprint(f\"Generated {len(t)} observations\")\nprint(f\"Time range: {min(t):.1f} to {max(t):.1f} days\")\nprint(f\"Magnitude range: {min(m):.2f} to {max(m):.2f}\")\nprint(\"\\nImplement the analysis functions to find the period!\")\n\n","type":"content","url":"/python-functions-modules-v2#exercise-5-3-variable-star-analysis-functions","position":69},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl2":"Main Takeaways"},"type":"lvl2","url":"/python-functions-modules-v2#main-takeaways","position":70},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl2":"Main Takeaways"},"content":"Functions transform programming from repetitive scripting into modular, maintainable software engineering. When you encapsulate logic in well-designed functions, you create building blocks that can be tested independently, shared with collaborators, and combined into complex analysis pipelines. The progression from simple functions to modules to packages mirrors how scientific software naturally grows—what starts as a quick calculation evolves into a shared tool used by entire research communities.\n\nThe distinction between positional, keyword, and default arguments gives you the flexibility to design interfaces that are both powerful and intuitive. Positional arguments work well for obvious parameters like power(base, exponent), while keyword arguments with defaults enable complex functions that remain simple for common cases. Understanding when to use each type—and the critical danger of mutable default arguments—prevents the subtle bugs that have plagued major scientific packages.\n\nThe scope rules and namespace concepts you’ve learned explain why variables sometimes behave unexpectedly in complex programs. Understanding the LEGB rule prevents frustrating bugs where variables have unexpected values or modifications in one place affect seemingly unrelated code. The mutable default argument trap demonstrates why understanding Python’s evaluation model is crucial for writing reliable code. These aren’t just academic concepts—they’ve caused real disasters in production systems.\n\nFunctional programming concepts like map, filter, and pure functions prepare you for modern scientific computing frameworks. JAX requires functional style for automatic differentiation, parallel processing works best with stateless functions, and testing becomes trivial when functions have no side effects. The ability to pass functions as arguments and return them from other functions enables powerful patterns like the specialized integrators we created with closures.\n\nThe performance measurements showed that function call overhead only matters for trivial operations in tight loops—exactly where you’ll want to use NumPy’s vectorized operations (Chapter 7) instead. For complex calculations, overhead is negligible compared to computation time. Memoization can provide dramatic speedups when expensive calculations repeat, as often happens in optimization and parameter searching.\n\nLooking forward, the functions you’ve learned to write here form the foundation for object-oriented programming in Chapter 6, where functions become methods attached to objects. The module organization skills prepare you for building larger scientific packages, while the documentation practices ensure your code can be understood and maintained by others. Most importantly, thinking in terms of functional contracts and clear interfaces will make you a better computational scientist, capable of building the robust, efficient tools that modern research demands.","type":"content","url":"/python-functions-modules-v2#main-takeaways","position":71},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl2":"Definitions"},"type":"lvl2","url":"/python-functions-modules-v2#definitions","position":72},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl2":"Definitions"},"content":"argument - The actual value passed to a function when calling it (e.g., in f(5), 5 is an argument)\n\nclosure - A function that remembers variables from its enclosing scope even after that scope has finished executing\n\ndecorator - A function that modifies another function’s behavior without changing its code\n\ndefault argument - A parameter value used when no argument is provided during function call\n\ndocstring - A string literal that appears as the first statement in a function, module, or class to document its purpose\n\nfunction - A reusable block of code that performs a specific task, taking inputs and optionally returning outputs\n\nglobal - A keyword that allows a function to modify a variable in the global scope\n\nkeyword argument - An argument passed to a function by explicitly naming the parameter\n\nlambda - An anonymous function defined inline using the lambda keyword\n\nLEGB - The order Python searches for variables: Local, Enclosing, Global, Built-in\n\nmemoization - Caching function results to avoid recomputing expensive operations\n\nmodule - A Python file containing definitions and statements that can be imported and reused\n\nnamespace - A container that holds a set of identifiers and their associated objects\n\npackage - A directory containing multiple Python modules and an __init__.py file\n\nparameter - A variable in a function definition that receives a value when the function is called\n\npositional argument - An argument passed to a function based on its position in the parameter list\n\npure function - A function that always returns the same output for the same input with no side effects\n\nreturn value - The result that a function sends back to the code that called it\n\nscope - The region of a program where a variable is accessible\n\nside effect - Any state change that occurs beyond returning a value from a function\n\n*args - Syntax for collecting variable positional arguments into a tuple\n\n*kwargs - Syntax for collecting variable keyword arguments into a dictionary","type":"content","url":"/python-functions-modules-v2#definitions","position":73},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl2":"Key Takeaways"},"type":"lvl2","url":"/python-functions-modules-v2#key-takeaways","position":74},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl2":"Key Takeaways"},"content":"Functions are contracts: they promise specific outputs for given inputs\n\nChoose positional arguments for obvious parameters, keyword arguments for optional ones\n\nThe mutable default argument trap occurs because defaults are evaluated once at definition time\n\nAlways use None as a sentinel for mutable default arguments\n\nPython searches for variables using LEGB: Local, Enclosing, Global, Built-in\n\nGlobal variables make code hard to test, debug, and parallelize\n\nLambda functions are useful for simple operations but limited to single expressions\n\nFunctional programming concepts (map, filter, reduce) prepare you for modern frameworks\n\nThe if __name__ == \"__main__\" pattern makes modules both importable and executable\n\nNever use from module import * in production code—it causes namespace pollution\n\nDocstrings are essential for scientific code that others will use and maintain\n\nFunction call overhead matters only in tight loops with trivial operations\n\nMemoization can dramatically speed up expensive repeated calculations\n\nTesting and validation functions are as important as calculation functions\n\nPerformance optimization should follow: algorithm → vectorization → caching → parallelization","type":"content","url":"/python-functions-modules-v2#key-takeaways","position":75},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl2":"Quick Reference Tables"},"type":"lvl2","url":"/python-functions-modules-v2#quick-reference-tables","position":76},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl2":"Quick Reference Tables"},"content":"","type":"content","url":"/python-functions-modules-v2#quick-reference-tables","position":77},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Function Definition Patterns","lvl2":"Quick Reference Tables"},"type":"lvl3","url":"/python-functions-modules-v2#function-definition-patterns","position":78},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Function Definition Patterns","lvl2":"Quick Reference Tables"},"content":"Pattern\n\nSyntax\n\nUse Case\n\nBasic function\n\ndef func(x, y):\n\nSimple operations\n\nPositional only\n\ndef func(a, b, /):\n\nForce positional\n\nDefault arguments\n\ndef func(x, y=10):\n\nOptional parameters\n\nKeyword only\n\ndef func(*, x, y):\n\nForce keywords\n\nVariable args\n\ndef func(*args):\n\nUnknown number of inputs\n\nKeyword args\n\ndef func(**kwargs):\n\nFlexible options\n\nCombined\n\ndef func(a, *args, x=1, **kwargs):\n\nMaximum flexibility\n\nLambda\n\nlambda x: x**2\n\nSimple inline functions","type":"content","url":"/python-functions-modules-v2#function-definition-patterns","position":79},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"When to Use Different Argument Types","lvl2":"Quick Reference Tables"},"type":"lvl3","url":"/python-functions-modules-v2#when-to-use-different-argument-types","position":80},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"When to Use Different Argument Types","lvl2":"Quick Reference Tables"},"content":"Argument Type\n\nWhen to Use\n\nExample\n\nPositional\n\nObvious meaning, 1-3 params\n\npower(2, 3)\n\nKeyword\n\nMany params, optional\n\nplot(data, color='red')\n\nDefault\n\nCommon values\n\nround(3.14, digits=2)\n\n*args\n\nVariable inputs\n\nmaximum(1, 2, 3, 4)\n\n**kwargs\n\nConfiguration options\n\nsetup(debug=True, verbose=False)","type":"content","url":"/python-functions-modules-v2#when-to-use-different-argument-types","position":81},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Module Import Patterns","lvl2":"Quick Reference Tables"},"type":"lvl3","url":"/python-functions-modules-v2#module-import-patterns","position":82},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Module Import Patterns","lvl2":"Quick Reference Tables"},"content":"Pattern\n\nExample\n\nWhen to Use\n\nImport module\n\nimport numpy\n\nUse many functions\n\nImport with alias\n\nimport numpy as np\n\nStandard abbreviations\n\nImport specific\n\nfrom math import sin, cos\n\nFew specific functions\n\nImport all (avoid!)\n\nfrom math import *\n\nNever in production\n\nRelative import\n\nfrom . import module\n\nWithin packages","type":"content","url":"/python-functions-modules-v2#module-import-patterns","position":83},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Common Function Bugs and Fixes","lvl2":"Quick Reference Tables"},"type":"lvl3","url":"/python-functions-modules-v2#common-function-bugs-and-fixes","position":84},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Common Function Bugs and Fixes","lvl2":"Quick Reference Tables"},"content":"Problem\n\nSymptom\n\nFix\n\nMutable default\n\nData persists between calls\n\nUse None sentinel\n\nUnboundLocalError\n\nCan’t modify global\n\nUse global or pass value\n\nMissing return\n\nFunction returns None\n\nAdd return statement\n\nNamespace pollution\n\nName conflicts\n\nAvoid wildcard imports\n\nSlow recursion\n\nExponential time\n\nAdd memoization\n\nType confusion\n\nUnexpected types\n\nAdd type hints/validation","type":"content","url":"/python-functions-modules-v2#common-function-bugs-and-fixes","position":85},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl2":"Next Chapter Preview"},"type":"lvl2","url":"/python-functions-modules-v2#next-chapter-preview","position":86},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl2":"Next Chapter Preview"},"content":"With functions and modules mastered, Chapter 6 introduces Object-Oriented Programming (OOP)—a paradigm that bundles data and behavior together. You’ll learn to create classes that model physical systems naturally: a Particle class with position and velocity attributes, methods to calculate energy and momentum, and special methods that make your objects work seamlessly with Python’s built-in functions.\n\nThe functional programming concepts from this chapter provide essential background for OOP. Methods are just functions attached to objects, and understanding scope prepares you for the self parameter that confuses many beginners. The module organization skills you’ve developed will expand to organizing classes and building object hierarchies. Most importantly, the design thinking you’ve practiced—creating clean interfaces and thinking about contracts—directly applies to designing effective classes that model the complex systems you’ll encounter in computational physics.","type":"content","url":"/python-functions-modules-v2#next-chapter-preview","position":87},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl2":"References"},"type":"lvl2","url":"/python-functions-modules-v2#references","position":88},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl2":"References"},"content":"","type":"content","url":"/python-functions-modules-v2#references","position":89},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Historical Events and Technical Details","lvl2":"References"},"type":"lvl3","url":"/python-functions-modules-v2#historical-events-and-technical-details","position":90},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Historical Events and Technical Details","lvl2":"References"},"content":"Ariane 5 Flight 501 Failure (1996)\n\nLions, J. L. et al. (1996). “ARIANE 5 Flight 501 Failure: Report by the Inquiry Board.” European Space Agency. Paris: ESA.\n\nGleick, J. (1996). “A Bug and A Crash.” The New York Times Magazine, December 1, 1996.\n\nNuseibeh, B. (1997). “Ariane 5: Who Dunnit?” IEEE Software, 14(3), 15-16.\n\nLIGO Gravitational Wave Detection (2015)\n\nAbbott, B. P. et al. (LIGO Scientific Collaboration) (2016). “Observation of Gravitational Waves from a Binary Black Hole Merger.” Physical Review Letters, 116(6), 061102.\n\nAbbott, B. P. et al. (2016). “GW150914: The Advanced LIGO Detectors in the Era of First Discoveries.” Physical Review Letters, 116(13), 131103.\n\nLIGO Scientific Collaboration. (2021). “LIGO Algorithm Library - LALSuite.” Available at: \n\nhttps://​lscsoft​.docs​.ligo​.org​/lalsuite/\n\nLIGO Open Science Center. (2024). “LIGO Open Data.” Available at: \n\nhttps://​www​.gw​-openscience​.org/\n\nHubble Space Telescope Mirror Error (1990)\n\nAllen, L. et al. (1990). “The Hubble Space Telescope Optical Systems Failure Report.” NASA-TM-103443.\n\nChaisson, E. (1994). The Hubble Wars. New York: HarperCollins. ISBN 0-06-017114-6.\n\nLeckrone, D. S. (1995). “The Hubble Space Telescope Servicing Mission.” Astrophysics and Space Science, 226(1), 1-24.\n\nMars Climate Orbiter Loss (1999)\n\nStephenson, A. G. et al. (1999). “Mars Climate Orbiter Mishap Investigation Board Report.” NASA.\n\nOberg, J. (1999). “Why the Mars Probe Went Off Course.” IEEE Spectrum, 36(12), 34-39.","type":"content","url":"/python-functions-modules-v2#historical-events-and-technical-details","position":91},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Python Documentation","lvl2":"References"},"type":"lvl3","url":"/python-functions-modules-v2#python-documentation","position":92},{"hierarchy":{"lvl1":"Chapter 5: Functions & Modules - Building Reusable Scientific Code","lvl3":"Python Documentation","lvl2":"References"},"content":"Python Language Reference\n\nVan Rossum, G., & Drake, F. L. (2024). “Python Language Reference, version 3.12.” Python Software Foundation. Available at: \n\nhttps://​docs​.python​.org​/3​/reference/\n\nScientific Python Resources\n\nHarris, C. R. et al. (2020). “Array programming with NumPy.” Nature, 585(7825), 357-362.\n\nVanderPlas, J. (2016). Python Data Science Handbook. O’Reilly Media. ISBN: 978-1491912058.","type":"content","url":"/python-functions-modules-v2#python-documentation","position":93},{"hierarchy":{"lvl1":"Chapter 6: OOP Fundamentals - Organizing Scientific Code"},"type":"lvl1","url":"/oop-fundamentals-v2","position":0},{"hierarchy":{"lvl1":"Chapter 6: OOP Fundamentals - Organizing Scientific Code"},"content":"","type":"content","url":"/oop-fundamentals-v2","position":1},{"hierarchy":{"lvl1":"Chapter 6: OOP Fundamentals - Organizing Scientific Code","lvl2":"Learning Objectives"},"type":"lvl2","url":"/oop-fundamentals-v2#learning-objectives","position":2},{"hierarchy":{"lvl1":"Chapter 6: OOP Fundamentals - Organizing Scientific Code","lvl2":"Learning Objectives"},"content":"By the end of this chapter, you will be able to:\n\nTransform functions and data into cohesive classes that model scientific concepts\n\nDistinguish between instance and class attributes and choose appropriately for scientific data\n\nCreate methods that operate on object state while understanding the role of self\n\nImplement properties to compute derived values and validate scientific constraints\n\nWrite special methods (__init__, __str__, __repr__) to make objects Pythonic\n\nDebug common OOP errors using introspection tools and error messages\n\nRecognize when OOP improves code organization versus when functions suffice\n\nConnect the transition from procedural to object-oriented thinking in scientific computing","type":"content","url":"/oop-fundamentals-v2#learning-objectives","position":3},{"hierarchy":{"lvl1":"Chapter 6: OOP Fundamentals - Organizing Scientific Code","lvl2":"Prerequisites Check"},"type":"lvl2","url":"/oop-fundamentals-v2#prerequisites-check","position":4},{"hierarchy":{"lvl1":"Chapter 6: OOP Fundamentals - Organizing Scientific Code","lvl2":"Prerequisites Check"},"content":"Before starting this chapter, verify you can:\n\nDefine and call functions with various parameter types (Chapter 5)\n\nUnderstand scope and namespaces (Chapter 5)\n\nWork with dictionaries and their methods (Chapter 4)\n\nCreate and import modules (Chapter 5)\n\nHandle mutable vs immutable objects (Chapter 4)\n\nQuick diagnostic:\n\n# Can you predict what this prints?\ndef modify(data):\n    data['value'] = data['value'] * 2\n    return data\n\nmeasurement = {'value': 100, 'unit': 'K'}\nresult = modify(measurement)\nprint(f\"Original: {measurement['value']}\")  # What value?\nprint(f\"Result: {result['value']}\")         # What value?\n\nIf you said both print “200”, you’re ready! Objects work similarly - they’re mutable and passed by reference, which becomes crucial when methods modify object state.","type":"content","url":"/oop-fundamentals-v2#prerequisites-check","position":5},{"hierarchy":{"lvl1":"Chapter 6: OOP Fundamentals - Organizing Scientific Code","lvl2":"Chapter Overview"},"type":"lvl2","url":"/oop-fundamentals-v2#chapter-overview","position":6},{"hierarchy":{"lvl1":"Chapter 6: OOP Fundamentals - Organizing Scientific Code","lvl2":"Chapter Overview"},"content":"You’ve mastered functions to organize behavior and modules to organize related functions. But what happens when data and the functions that operate on it are inseparable? When tracking particles in a simulation, each particle has position, velocity, and mass, along with methods to update position, calculate kinetic energy, and check collisions. Passing all this data between separate functions becomes error-prone and verbose. This is where Object-Oriented Programming transforms your code from a collection of functions to a model of your problem domain.\n\nObject-Oriented Programming (OOP) isn’t just another way to organize code - it’s a fundamental shift in how we think about programs. Instead of viewing code as a sequence of operations on data, we model it as interactions between objects that combine data and behavior. A thermometer knows its temperature and how to convert units. A dataset knows its values and how to calculate statistics. A simulation particle knows its state and how to evolve. This paradigm mirrors how we naturally think about scientific systems, making complex programs more intuitive and maintainable.\n\nThis chapter introduces OOP’s essential concepts through practical scientific examples. You’ll learn to create classes (blueprints for objects), instantiate objects (specific instances), and define methods (functions attached to objects). We’ll explore how properties provide computed attributes and validation, ensuring your scientific constraints are always satisfied. Most importantly, you’ll develop judgment about when OOP clarifies code (managing stateful systems, modeling entities) versus when it adds unnecessary complexity (simple calculations, stateless transformations). By the end, you’ll understand why NumPy arrays are objects with methods, setting the foundation for leveraging Python’s scientific ecosystem.","type":"content","url":"/oop-fundamentals-v2#chapter-overview","position":7},{"hierarchy":{"lvl1":"Chapter 6: OOP Fundamentals - Organizing Scientific Code","lvl2":"6.1 From Functions to Objects: The Conceptual Leap"},"type":"lvl2","url":"/oop-fundamentals-v2#id-6-1-from-functions-to-objects-the-conceptual-leap","position":8},{"hierarchy":{"lvl1":"Chapter 6: OOP Fundamentals - Organizing Scientific Code","lvl2":"6.1 From Functions to Objects: The Conceptual Leap"},"content":"{margin} Object-Oriented Programming\nA programming paradigm that organizes code around objects (data) and methods (behavior) rather than functions and logic.\n\nLet’s start with a problem you’ve already solved with functions, then transform it into objects to see the difference. In Python, everything is actually an object - even functions and modules! But some objects are more complex than others, and creating your own classes lets you model your specific problem domain.\n\n# Approach 1: Functions with dictionaries (what you know)\ndef create_particle(mass, x, y, vx, vy):\n    \"\"\"Create a particle dictionary.\"\"\"\n    return {\n        'mass': mass,  # grams\n        'x': x, 'y': y,  # cm\n        'vx': vx, 'vy': vy  # cm/s\n    }\n\ndef kinetic_energy(particle):\n    \"\"\"Calculate kinetic energy in ergs.\"\"\"\n    v_squared = particle['vx']**2 + particle['vy']**2\n    return 0.5 * particle['mass'] * v_squared\n\ndef update_position(particle, dt):\n    \"\"\"Update position based on velocity.\"\"\"\n    particle['x'] += particle['vx'] * dt\n    particle['y'] += particle['vy'] * dt\n    return particle\n\n# Using functions\np1 = create_particle(1.0, 0, 0, 10, 5)\nenergy = kinetic_energy(p1)\np1 = update_position(p1, 0.1)\nprint(f\"Energy: {energy:.1f} ergs\")\n\nNow let’s see the same problem with OOP:\n\n# Approach 2: Object-Oriented (what you're learning)\nclass Particle:\n    \"\"\"A particle with position and velocity.\"\"\"\n    \n    def __init__(self, mass, x, y, vx, vy):\n        \"\"\"Initialize particle state.\"\"\"\n        self.mass = mass  # grams\n        self.x = x        # cm\n        self.y = y        # cm\n        self.vx = vx      # cm/s\n        self.vy = vy      # cm/s\n    \n    def kinetic_energy(self):\n        \"\"\"Calculate kinetic energy in ergs.\"\"\"\n        v_squared = self.vx**2 + self.vy**2\n        return 0.5 * self.mass * v_squared\n    \n    def update_position(self, dt):\n        \"\"\"Update position based on velocity.\"\"\"\n        self.x += self.vx * dt\n        self.y += self.vy * dt\n\n# Using objects\np2 = Particle(1.0, 0, 0, 10, 5)\nenergy = p2.kinetic_energy()\np2.update_position(0.1)\nprint(f\"Energy: {energy:.1f} ergs\")\n\nBoth approaches solve the problem, but notice the differences:\n\nOrganization: Data and methods stay together in the class\n\nSyntax: Methods are called on objects (p2.kinetic_energy())\n\nState: The object maintains its own state between method calls\n\nClarity: The object-oriented version reads more naturally\n\n🎭 The More You Know: How Objects Saved the Mars Rover\n\nIn 2004, NASA’s Spirit rover suddenly stopped responding, 18 days into its mission. The cause? Procedural code managing 250+ hardware components through global variables and scattered functions. When flash memory filled up, the initialization functions couldn’t track which subsystems were already started, causing an infinite reboot loop.\n\nThe fix came from JPL engineer Jennifer Trosper, who had warned about this exact scenario. The team remotely patched the rover’s software to use object-oriented design. Each hardware component became an object tracking its own state:class RoverComponent:\n    def __init__(self, name):\n        self.name = name\n        self.initialized = False\n        self.error_count = 0\n    \n    def initialize(self):\n        if not self.initialized:\n            # Safe initialization\n            self.initialized = True\n\nThis simple change - objects knowing their own state - saved a $400 million mission. Spirit went on to operate for 6 years instead of the planned 90 days. When Curiosity launched in 2011, its entire control system used OOP from the start. Each instrument is an object, each motor is an object, even each wheel is an object with its own wear tracking.\n\nYou’re learning the same pattern that keeps billion-dollar spacecraft alive on other planets!","type":"content","url":"/oop-fundamentals-v2#id-6-1-from-functions-to-objects-the-conceptual-leap","position":9},{"hierarchy":{"lvl1":"Chapter 6: OOP Fundamentals - Organizing Scientific Code","lvl2":"6.2 Classes and Objects: Building Blocks"},"type":"lvl2","url":"/oop-fundamentals-v2#id-6-2-classes-and-objects-building-blocks","position":10},{"hierarchy":{"lvl1":"Chapter 6: OOP Fundamentals - Organizing Scientific Code","lvl2":"6.2 Classes and Objects: Building Blocks"},"content":"{margin} Class\nA blueprint or template for creating objects that defines attributes and methods.\n\n{margin} Object\nA specific instance of a class containing data (attributes) and behavior (methods).\n\n{margin} Constructor\nThe __init__ method that initializes new objects when they’re created.\n\nBefore we dive into creating classes, let’s understand why we need them beyond the simple example we just saw. As your programs grow, you face several challenges that classes elegantly solve:\n\nNamespace pollution: Without classes, you might have functions like calculate_star_luminosity(), calculate_planet_mass(), calculate_galaxy_distance() - your namespace becomes cluttered with hundreds of related functions.\n\nData consistency: When data and functions are separate, nothing prevents you from passing a galaxy’s data to a star’s calculation function, potentially causing silent errors or crashes.\n\nCode reusability: With functions alone, similar behaviors must be duplicated. Every object type needs its own set of functions even when the logic is similar.\n\nConceptual clarity: We naturally think of stars, planets, and galaxies as entities with properties and behaviors. Classes let us model this intuition directly in code.\n\nA class is a blueprint for creating objects. An object (or instance) is a specific realization of that blueprint. Think of a class as the concept “thermometer” and objects as specific thermometers in your lab.\n\n# Define a class (blueprint)\nclass Measurement:\n    \"\"\"A scientific measurement with uncertainty.\"\"\"\n    \n    def __init__(self, value, error):\n        \"\"\"Initialize measurement with value and error.\"\"\"\n        self.value = value\n        self.error = error\n    \n    def relative_error(self):\n        \"\"\"Calculate relative error as percentage.\"\"\"\n        if self.value == 0:\n            return float('inf')\n        return abs(self.error / self.value) * 100\n\n# Create objects (instances)\ntemp = Measurement(273.15, 0.1)  # Temperature in Kelvin\n# Pressure in dyne/cm² (CGS unit for pressure)\npressure = Measurement(1.01325e6, 500)  # 1 atm = 1.01325e6 dyne/cm²\n\nprint(f\"Temperature: {temp.value} ± {temp.error} K\")\nprint(f\"Pressure: {pressure.value:.2e} ± {pressure.error} dyne/cm²\")\nprint(f\"Pressure relative error: {pressure.relative_error():.3f}%\")\n\n","type":"content","url":"/oop-fundamentals-v2#id-6-2-classes-and-objects-building-blocks","position":11},{"hierarchy":{"lvl1":"Chapter 6: OOP Fundamentals - Organizing Scientific Code","lvl3":"Understanding self","lvl2":"6.2 Classes and Objects: Building Blocks"},"type":"lvl3","url":"/oop-fundamentals-v2#understanding-self","position":12},{"hierarchy":{"lvl1":"Chapter 6: OOP Fundamentals - Organizing Scientific Code","lvl3":"Understanding self","lvl2":"6.2 Classes and Objects: Building Blocks"},"content":"{margin} self\nThe first parameter of instance methods, referring to the specific object being operated on.\n\nThe self parameter is how each object keeps track of its own data. When you call temp.relative_error(), Python automatically passes temp as the first argument. Here’s what happens behind the scenes:\n\nclass Counter:\n    \"\"\"Demonstrates how self works.\"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize counter at zero.\"\"\"\n        self.count = 0  # Each object gets its own count\n    \n    def increment(self):\n        \"\"\"Increment this counter by one.\"\"\"\n        self.count += 1  # self refers to the specific object\n    \n    def get_count(self):\n        \"\"\"Return current count value.\"\"\"\n        return self.count\n\n# Each object maintains independent state\nc1 = Counter()\nc2 = Counter()\n\nc1.increment()\nc1.increment()\nc2.increment()\n\nprint(f\"Counter 1: {c1.get_count()}\")  # 2\nprint(f\"Counter 2: {c2.get_count()}\")  # 1\n\n# What Python actually does when you call c1.increment():\n# Counter.increment(c1)  # c1 becomes 'self' in the method\n\nThis seemingly simple concept of bundling data with behavior revolutionized programming. Let me tell you how it started...\n\n🎭 The More You Know: How Norwegian Scientists Invented OOP to Model Ships\n\nIn 1962, Norwegian computer scientists Kristen Nygaard and Ole-Johan Dahl faced an impossible problem at the Norwegian Computing Center. They were trying to simulate ship movement through fjords, tracking hundreds of vessels with different properties, behaviors, and interactions. Their FORTRAN code had become an unmaintainable nightmare of global variables and GOTO statements - a single change could break the entire simulation.\n\nTheir revolutionary solution? Create “objects” that bundled ship data with ship behavior. Each ship would know its own position, speed, cargo capacity, and fuel consumption, and could execute its own navigation methods. They called their language Simula (SIMUlation LAnguage), and in 1967, Simula 67 introduced the world to classes, objects, inheritance, and virtual methods - all the concepts you’re learning now.# Simplified version of their concept in modern Python:\nclass Ship:\n    def __init__(self, name, position, cargo):\n        self.name = name\n        self.position = position\n        self.cargo = cargo\n    \n    def navigate_through_fjord(self, fjord_map):\n        # Each ship knows how to navigate based on its own properties\n        pass\n\nThe impact was profound. Alan Kay, who later created Smalltalk, said: “I thought of objects being like biological cells... only able to communicate with messages” (simplified paraphrase of his 1960s realization). This biological metaphor transformed computing. IBM adopted OOP for their systems. C++ brought it to systems programming. Java made it mainstream.\n\nThe irony? Nygaard and Dahl’s supervisor initially rejected their idea, reportedly saying something like “We’re here to solve problems, not create new programming paradigms!” (paraphrased from various historical accounts). That “paradigm” now powers everything from video games to Mars rovers to the Large Hadron Collider’s data analysis. Today, when you create a Particle class or a Galaxy object, you’re using concepts invented to help Norwegian ships navigate safely through narrow fjords!\n\n[Source: Nygaard, K., & Dahl, O. J. (1978). The development of the SIMULA languages. ACM SIGPLAN Notices, 13(8), 245-272.]\n\n⚠️ Common Bug Alert: Forgetting self\n\n# WRONG - Missing self parameter\nclass BadClass:\n    def method():  # Missing self!\n        return \"something\"\n\n# This fails:\n# obj = BadClass()\n# obj.method()  # TypeError: takes 0 arguments but 1 given\n\n# CORRECT - Always include self\nclass GoodClass:\n    def method(self):  # self is required\n        return \"something\"\n\nobj = GoodClass()\nprint(obj.method())  # Works!\n\nThis is probably the most common OOP error. Remember: instance methods ALWAYS need self as their first parameter.","type":"content","url":"/oop-fundamentals-v2#understanding-self","position":13},{"hierarchy":{"lvl1":"Chapter 6: OOP Fundamentals - Organizing Scientific Code","lvl3":"Instance vs Class Attributes","lvl2":"6.2 Classes and Objects: Building Blocks"},"type":"lvl3","url":"/oop-fundamentals-v2#instance-vs-class-attributes","position":14},{"hierarchy":{"lvl1":"Chapter 6: OOP Fundamentals - Organizing Scientific Code","lvl3":"Instance vs Class Attributes","lvl2":"6.2 Classes and Objects: Building Blocks"},"content":"{margin} Instance Attribute\nData unique to each object, defined with self.attribute.\n\n{margin} Class Attribute\nData shared by all instances of a class, defined directly in the class body.\n\n{margin} Encapsulation\nThe bundling of data and methods that operate on that data within a single unit (class).\n\nInstance attributes belong to specific objects. Class attributes are shared by all instances. This bundling of data and methods is called encapsulation - a core principle of OOP:\n\nclass Simulation:\n    \"\"\"Demonstrates instance vs class attributes.\"\"\"\n    \n    # Class attribute - shared by all simulations\n    speed_of_light = 2.998e10  # cm/s (CGS)\n    total_runs = 0\n    \n    def __init__(self, name, particles):\n        \"\"\"Initialize a new simulation.\"\"\"\n        # Instance attributes - unique to each simulation\n        self.name = name\n        self.particles = particles\n        self.time = 0.0\n        # Increment shared counter\n        Simulation.total_runs += 1\n    \n    def advance(self, dt):\n        \"\"\"Advance simulation by dt seconds.\"\"\"\n        self.time += dt\n\n# Create simulations\nsim1 = Simulation(\"Test A\", 1000)\nsim2 = Simulation(\"Test B\", 5000)\n\nprint(f\"Total simulations: {Simulation.total_runs}\")\nprint(f\"Sim1 particles: {sim1.particles}\")\nprint(f\"Sim2 particles: {sim2.particles}\")\nprint(f\"Speed of light: {Simulation.speed_of_light} cm/s\")\n\n🔍 Check Your Understanding\n\nWhat’s the output of this code? Why?\n\nclass DataPoint:\n    count = 0  # Class attribute\n    \n    def __init__(self, value):\n        self.value = value  # Instance attribute\n        DataPoint.count += 1\n\np1 = DataPoint(10)\np2 = DataPoint(20)\np1.count = 100  # What happens here?\n\nprint(f\"p1.count: {p1.count}\")\nprint(f\"p2.count: {p2.count}\")\nprint(f\"DataPoint.count: {DataPoint.count}\")\n\nAnswer\n\nOutput:\n\np1.count: 100\n\np2.count: 2\n\nDataPoint.count: 2\n\nWhen you write p1.count = 100, you create a new instance attribute that shadows the class attribute for p1 only. The class attribute remains unchanged at 2, and p2 still sees the class attribute. This is a common source of confusion - instance attributes can hide class attributes with the same name!\n## 6.3 Methods: Functions Attached to Objects\n\n{margin} **Method**\nA function defined inside a class that operates on instances of that class.\n\nMethods are functions that belong to a class. They can access and modify the object's state through `self`. Let's build up from simple to complex.\n\nNote: In Python, we typically import modules at the top of our files for clarity and efficiency. While Python allows imports anywhere in code, importing at the top makes dependencies clear and avoids repeated import overhead. In these examples, we sometimes import inside methods for pedagogical clarity, but in production code, prefer top-level imports.\n\n```{code-cell} python\nimport math  # Best practice: import at the top\n\n# First: A simple class with basic methods\nclass Sample:\n    \"\"\"Scientific sample with basic operations.\"\"\"\n    \n    def __init__(self, mass_g, volume_cm3):\n        \"\"\"Initialize with mass in grams, volume in cm³.\"\"\"\n        self.mass = mass_g\n        self.volume = volume_cm3\n    \n    def density(self):\n        \"\"\"Calculate density in g/cm³.\"\"\"\n        return self.mass / self.volume\n    \n    def is_denser_than_water(self):\n        \"\"\"Check if denser than water (1 g/cm³).\"\"\"\n        return self.density() > 1.0\n\n# Using simple methods\niron = Sample(7.87, 1.0)  # Iron sample\nprint(f\"Iron density: {iron.density()} g/cm³\")\nprint(f\"Sinks in water: {iron.is_denser_than_water()}\")\n```\n\nNow let's advance to more complex mathematical methods:\n\n```{code-cell} python\nclass Vector2D:\n    \"\"\"A 2D vector for physics calculations.\"\"\"\n    \n    def __init__(self, x, y):\n        \"\"\"Initialize vector components in cm.\"\"\"\n        self.x = x\n        self.y = y\n    \n    def magnitude(self):\n        \"\"\"Calculate vector magnitude (length).\"\"\"\n        return (self.x**2 + self.y**2)**0.5\n    \n    def normalize(self):\n        \"\"\"Scale vector to unit length (magnitude = 1).\"\"\"\n        mag = self.magnitude()\n        if mag > 0:\n            self.x /= mag\n            self.y /= mag\n    \n    def dot(self, other):\n        \"\"\"Calculate dot product with another vector.\"\"\"\n        return self.x * other.x + self.y * other.y\n    \n    def angle_with(self, other):\n        \"\"\"Calculate angle with another vector in radians.\"\"\"\n        dot_product = self.dot(other)\n        mags = self.magnitude() * other.magnitude()\n        if mags == 0:\n            return 0\n        cos_angle = dot_product / mags\n        # Clamp to [-1, 1] to avoid numerical errors\n        cos_angle = max(-1, min(1, cos_angle))\n        return math.acos(cos_angle)\n\n# Using vector methods\nv1 = Vector2D(3, 4)\nv2 = Vector2D(1, 0)\n\nprint(f\"v1 magnitude: {v1.magnitude()} cm\")\nprint(f\"Dot product: {v1.dot(v2)} cm²\")\nprint(f\"Angle: {v1.angle_with(v2):.2f} radians\")\n\nv1.normalize()  # Makes magnitude = 1\nprint(f\"After normalization: ({v1.x:.2f}, {v1.y:.2f})\")\nprint(f\"New magnitude: {v1.magnitude():.2f}\")\n```\n\n:::{admonition} 🔍 Check Your Understanding\n:class: question\n\nWhy does `normalize()` modify the vector in place while `magnitude()` returns a value?\n\n:::{dropdown} Answer\nThis follows a common convention in programming: methods that transform an object modify it in place (like `list.sort()`), while methods that calculate values return them without changing the object. The names hint at this pattern: \"normalize\" is a verb suggesting action on the object, while \"magnitude\" is a noun suggesting a property being measured. This convention helps users predict method behavior from the name alone.\n### Method Types: Instance, Class, and Static\n\n```{code-cell} python\nclass DataProcessor:\n    \"\"\"Demonstrates different method types.\"\"\"\n    \n    version = \"1.0\"\n    \n    def __init__(self, data):\n        \"\"\"Initialize with data to process.\"\"\"\n        self.data = data\n    \n    # Instance method - needs self\n    def process(self):\n        \"\"\"Process this object's data.\"\"\"\n        return sum(self.data) / len(self.data)\n    \n    # Class method - gets class, not instance\n    @classmethod\n    def from_file(cls, filename):\n        \"\"\"Create instance from file.\"\"\"\n        # Simulate file reading\n        data = [1, 2, 3, 4, 5]\n        return cls(data)  # Create new instance\n    \n    # Static method - doesn't need self or cls\n    @staticmethod\n    def validate_data(data):\n        \"\"\"Check if data is valid.\"\"\"\n        return len(data) > 0 and all(isinstance(x, (int, float)) for x in data)\n\n# Using different method types\nprocessor = DataProcessor([10, 20, 30])\nprint(f\"Average: {processor.process()}\")\n\n# Class method creates new instance\n# processor2 = DataProcessor.from_file(\"data.txt\")\n\n# Static method works without instance\nvalid = DataProcessor.validate_data([1, 2, 3])\nprint(f\"Data valid: {valid}\")\n```\n\n:::{admonition} 💡 Computational Thinking Box: Methods as Interface\n:class: tip\n\n**PATTERN: Public Interface vs Private Implementation**\n\nIn scientific software, methods define how objects interact. Think of methods as the object's \"API\" - what it promises to do regardless of internal implementation.\n\n**Public Interface** (what users see):\n- `particle.update_position(dt)`\n- `measurement.get_uncertainty()`\n- `simulation.run_steps(100)`\n\n**Private Implementation** (internal details):\n- How position is stored (Cartesian? polar?)\n- How uncertainty is calculated\n- What algorithm updates the simulation\n\nThis separation allows you to change implementation without breaking code that uses your objects. NumPy arrays exemplify this: `arr.mean()` works the same whether the array is stored in row-major or column-major order, in RAM or memory-mapped.\n\n**Best Practice**: Start method names with underscore (`_`) to indicate internal methods not meant for external use.","type":"content","url":"/oop-fundamentals-v2#instance-vs-class-attributes","position":15},{"hierarchy":{"lvl1":"Chapter 6: OOP Fundamentals - Organizing Scientific Code","lvl2":"6.4 Properties: Smart Attributes"},"type":"lvl2","url":"/oop-fundamentals-v2#id-6-4-properties-smart-attributes","position":16},{"hierarchy":{"lvl1":"Chapter 6: OOP Fundamentals - Organizing Scientific Code","lvl2":"6.4 Properties: Smart Attributes"},"content":"{margin} Property\nA special attribute that executes code when accessed or set, created with the @property decorator.\n\n{margin} Setter\nA property method that validates and sets attribute values, defined with @attribute.setter.\n\nProperties let you compute attributes dynamically and validate data when it’s set. They make objects safer and more intuitive:\n\nclass Circle:\n    \"\"\"Circle with computed properties.\"\"\"\n    \n    def __init__(self, radius):\n        \"\"\"Initialize with radius, using validation.\"\"\"\n        # This now uses the setter for validation!\n        self.radius = radius\n    \n    @property\n    def radius(self):\n        \"\"\"Get radius in cm.\"\"\"\n        return self._radius\n    \n    @radius.setter\n    def radius(self, value):\n        \"\"\"Set radius with validation.\"\"\"\n        if value <= 0:\n            raise ValueError(f\"Radius must be positive, got {value}\")\n        self._radius = value\n    \n    @property\n    def area(self):\n        \"\"\"Computed area in cm² using πr².\"\"\"\n        return math.pi * self._radius**2\n    \n    @property\n    def circumference(self):\n        \"\"\"Computed circumference in cm using 2πr.\"\"\"\n        return 2 * math.pi * self._radius\n\n# Properties look like attributes but run code\ncircle = Circle(5)\nprint(f\"Radius: {circle.radius} cm\")\nprint(f\"Area: {circle.area:.2f} cm²\")\nprint(f\"Circumference: {circle.circumference:.2f} cm\")\n\n# Changing radius automatically updates computed properties\ncircle.radius = 10\nprint(f\"New area: {circle.area:.2f} cm²\")\n\n# Validation prevents invalid states (even in __init__ now!)\ntry:\n    bad_circle = Circle(-5)  # This will raise an error\nexcept ValueError as e:\n    print(f\"Error: {e}\")\n\n","type":"content","url":"/oop-fundamentals-v2#id-6-4-properties-smart-attributes","position":17},{"hierarchy":{"lvl1":"Chapter 6: OOP Fundamentals - Organizing Scientific Code","lvl3":"Properties for Unit Safety","lvl2":"6.4 Properties: Smart Attributes"},"type":"lvl3","url":"/oop-fundamentals-v2#properties-for-unit-safety","position":18},{"hierarchy":{"lvl1":"Chapter 6: OOP Fundamentals - Organizing Scientific Code","lvl3":"Properties for Unit Safety","lvl2":"6.4 Properties: Smart Attributes"},"content":"\n\nclass Temperature:\n    \"\"\"Temperature with automatic unit conversion.\"\"\"\n    \n    def __init__(self, kelvin):\n        \"\"\"Initialize with temperature in Kelvin.\"\"\"\n        self._kelvin = kelvin\n    \n    @property\n    def kelvin(self):\n        \"\"\"Get temperature in Kelvin.\"\"\"\n        return self._kelvin\n    \n    @kelvin.setter\n    def kelvin(self, value):\n        \"\"\"Set temperature with validation.\"\"\"\n        if value < 0:\n            raise ValueError(\"Below absolute zero!\")\n        self._kelvin = value\n    \n    @property\n    def celsius(self):\n        \"\"\"Get temperature in Celsius.\"\"\"\n        return self._kelvin - 273.15\n    \n    @celsius.setter\n    def celsius(self, value):\n        \"\"\"Set temperature in Celsius.\"\"\"\n        # Use kelvin setter for validation\n        self.kelvin = value + 273.15\n    \n    @property\n    def fahrenheit(self):\n        \"\"\"Get temperature in Fahrenheit.\"\"\"\n        return self._kelvin * 9/5 - 459.67\n\n# Same temperature, different units\ntemp = Temperature(300)\nprint(f\"Water at {temp.kelvin:.1f} K\")\nprint(f\"  = {temp.celsius:.1f} °C\")\nprint(f\"  = {temp.fahrenheit:.1f} °F\")\n\n# Set in any unit, always consistent\ntemp.celsius = 100\nprint(f\"Boiling: {temp.kelvin:.1f} K\")\n\n🎭 The More You Know: How Properties Could Have Saved Hubble’s Mirror\n\nIn 1990, the Hubble Space Telescope reached orbit with a catastrophic flaw - its primary mirror was ground to the wrong shape by 2.2 micrometers, about 1/50th the width of a human hair. The error occurred because a measuring device called a null corrector had been assembled incorrectly, with one lens positioned 1.3mm out of place. But here’s the tragic part: the computer software accepting test measurements had no validation. It accepted clearly impossible values without question.\n\nDuring testing, technicians actually got measurements showing the mirror was wrong. But other tests (using the faulty null corrector) showed it was “perfect.” The software happily stored both sets of contradictory data. No validation checks asked: “Why do these measurements disagree by orders of magnitude?” or “Is this curvature physically possible for a mirror this size?”\n\nThe disaster cost NASA over $600 million for the initial fix (the true total including delays and lost science time was much higher). The repair required a daring Space Shuttle mission in 1993 to install COSTAR - essentially giving Hubble “glasses.” But the software fix was equally important. NASA completely rewrote their testing software with aggressive validation:# Simplified version of the validation concept:\nclass MirrorMeasurement:\n    def __init__(self, expected_curvature):\n        self.expected = expected_curvature\n        self._curvature_mm = None\n    \n    @property\n    def curvature_mm(self):\n        return self._curvature_mm\n    \n    @curvature_mm.setter\n    def curvature_mm(self, value):\n        # Physical limits based on mirror specifications\n        if not (2200.0 <= value <= 2400.0):  \n            raise ValueError(f\"Impossible curvature: {value}mm\")\n        \n        # Check against expected value\n        deviation = abs(value - self.expected) / self.expected\n        if deviation > 0.001:  # 0.1% tolerance\n            raise Warning(f\"Curvature {value} deviates {deviation*100:.2f}% from expected\")\n        \n        self._curvature_mm = value\n\nNote: Modern NASA testing actually uses far more sophisticated validation including statistical process control, multiple sensor cross-validation, and machine learning-based anomaly detection - this example shows the core concept.\n\nToday, every NASA mirror goes through validation software that checks measurements at the moment of entry. Properties ensure that impossible values trigger immediate alerts, not billion-dollar disasters. The James Webb Space Telescope, Hubble’s successor, had its mirrors tested with software that validates every measurement against physical constraints, expected ranges, and cross-checks with redundant sensors.\n\nWhen you write validation in your setters, you’re implementing the same safeguards that now protect every space telescope from Hubble’s fate. That simple if value <= 0: raise ValueError() in your code? That’s the pattern that could have saved one of humanity’s greatest scientific instruments from launching half-blind into space!\n\n[Sources: Allen, L. (1990). The Hubble Space Telescope Optical Systems Failure Report. NASA. Simplified technical details for pedagogical purposes.]\n\n🌟 Why This Matters: Mars Climate Orbiter\n\nIn 1999, NASA’s Mars Climate Orbiter burned up in Mars’ atmosphere. The cause? One team used pound-force seconds, another used newton-seconds. The spacecraft’s thrusters fired with 4.45× the intended force.\n\nWhile Python wasn’t used in the 1999 mission (spacecraft used Ada and C++), modern spacecraft software prevents such disasters using property-based validation - a pattern we can now demonstrate in Python:class Thruster:\n    @property\n    def thrust_newtons(self):\n        return self._thrust_n\n    \n    @thrust_newtons.setter\n    def thrust_newtons(self, value):\n        self._thrust_n = value\n    \n    @property\n    def thrust_pounds(self):\n        return self._thrust_n * 0.224809\n    \n    @thrust_pounds.setter  \n    def thrust_pounds(self, value):\n        self._thrust_n = value / 0.224809\n\nProperties ensure units are always consistent internally, regardless of what units the user provides. This pattern is now mandatory in NASA’s modern flight software, preventing the type of error that destroyed the Mars Climate Orbiter.\n\n🔍 Check Your Understanding\n\nWhat happens if you create a property without a setter but try to assign to it?class ReadOnly:\n    @property\n    def value(self):\n        return 42\n\nobj = ReadOnly()\nobj.value = 100  # What happens?\n\nAnswer\n\nYou get an AttributeError: can't set attribute. Properties without setters are read-only. This is actually useful for computed values that should never be directly modified, like the area of a circle (which should only change when the radius changes). This pattern enforces data consistency by preventing invalid states.\n:::{admonition} ⚠️ Common Bug Alert: Property Recursion\n:class: warning\n\n```{code-cell} python\n# WRONG - Infinite recursion!\nclass BadExample:\n    @property\n    def value(self):\n        return self.value  # Calls itself forever!\n\n# CORRECT - Use different internal name\nclass GoodExample:\n    def __init__(self):\n        \"\"\"Initialize with internal storage.\"\"\"\n        self._value = 0  # Underscore prefix\n    \n    @property\n    def value(self):\n        \"\"\"Access the value.\"\"\"\n        return self._value  # Different name\n\nexample = GoodExample()\nprint(f\"Value: {example.value}\")\n```\n\nAlways use a different internal name (usually with underscore) for the actual storage.","type":"content","url":"/oop-fundamentals-v2#properties-for-unit-safety","position":19},{"hierarchy":{"lvl1":"Chapter 6: OOP Fundamentals - Organizing Scientific Code","lvl2":"6.5 Special Methods: Making Objects Pythonic"},"type":"lvl2","url":"/oop-fundamentals-v2#id-6-5-special-methods-making-objects-pythonic","position":20},{"hierarchy":{"lvl1":"Chapter 6: OOP Fundamentals - Organizing Scientific Code","lvl2":"6.5 Special Methods: Making Objects Pythonic"},"content":"{margin} Special Method\nMethods with double underscores (like __init__, __str__) that define object behavior for built-in operations.\n\n{margin} Duck Typing\nPython’s philosophy that an object’s suitability is determined by its methods, not its type. “If it walks like a duck and quacks like a duck, it’s a duck.”\n\nSpecial methods (also called “magic methods” or “dunder methods”) let your objects work with Python’s built-in functions and operators. The term “duck typing” comes from the saying “If it walks like a duck and quacks like a duck, it’s a duck” - meaning Python cares about what an object can do, not what type it is.\n\n🎭 The More You Know: How Python Democratized Programming\n\nIn December 1989, Guido van Rossum was frustrated. Working at CWI (Centrum Wiskunde & Informatica) in Amsterdam on the Amoeba distributed operating system, he found existing languages inadequate. ABC was too rigid and couldn’t be extended. C was too low-level for rapid development. So during the Christmas vacation (he was bored and the office was closed), he started writing his own language, naming it after the British comedy group Monty Python’s Flying Circus.\n\nGuido made a radical decision that would change programming forever: instead of hiding object behavior behind compiler magic like C++ did, Python would expose everything through special methods that anyone could implement. Want your object to work with len()? Just add __len__(). Want it to support addition? Add __add__(). No special compiler support needed - just simple methods with funny names.\n\nThis transparency was revolutionary. In C++, only the compiler could decide what + meant for built-in types. In Python, ANY object could define it:# This wasn't possible in other languages of the time!\nclass Vector:\n    def __add__(self, other):\n        # YOU decide what + means for YOUR objects\n        return Vector(self.x + other.x, self.y + other.y)\n\nv1 + v2  # Calls YOUR __add__ method\n\nThe scientific community immediately saw the implications. Jim Hugunin created Numeric (NumPy’s ancestor) in 1995, using special methods to make arrays feel like native Python objects:# Scientific arrays that felt built-in!\narray1 + array2    # Element-wise addition via __add__\narray[5:10]        # Slicing via __getitem__\nlen(array)         # Size via __len__\nprint(array)       # Readable output via __str__\n\nGuido later reflected: “I wanted Python to be a bridge between the shell and C. I never imagined it would become the language of scientific computing” (paraphrased from various interviews). That bridge was built on special methods - the democratic principle that any object could be a first-class citizen.\n\nAlex Martelli, who would later coin the term “duck typing” for Python’s approach, explained it perfectly: “In Python, you don’t check if it IS-a duck, you check if it QUACKS-like-a duck, WALKS-like-a duck” (2000, comp.lang.python newsgroup). This philosophy meant scientific libraries could create objects that integrated seamlessly with Python’s syntax.\n\nWhen you implement __str__ or __add__, you’re using the democratic principle that made Python the world’s most popular scientific language: your objects are equals with Python’s built-in types. No special privileges needed - just implement the methods, and Python treats your objects as first-class citizens!\n\n[Sources: Van Rossum, G. (1996). Foreword for “Programming Python” (1st ed.). Various interviews compiled. Martelli’s “duck typing” coined circa 2000.]\n\nclass Fraction:\n    \"\"\"A fraction with arithmetic operations.\"\"\"\n    \n    def __init__(self, numerator, denominator):\n        \"\"\"Initialize and reduce fraction.\"\"\"\n        if denominator == 0:\n            raise ValueError(\"Denominator cannot be zero\")\n        self.num = numerator\n        self.den = denominator\n        self._reduce()\n    \n    def _reduce(self):\n        \"\"\"Reduce to lowest terms using GCD.\"\"\"\n        a, b = abs(self.num), abs(self.den)\n        while b:  # Euclidean algorithm for Greatest Common Divisor (GCD)\n            a, b = b, a % b\n        self.num //= a\n        self.den //= a\n    \n    def __str__(self):\n        \"\"\"Human-readable string for print().\"\"\"\n        return f\"{self.num}/{self.den}\"\n    \n    def __repr__(self):\n        \"\"\"Unambiguous string for debugging.\"\"\"\n        return f\"Fraction({self.num}, {self.den})\"\n    \n    def __float__(self):\n        \"\"\"Convert to float.\"\"\"\n        return self.num / self.den\n    \n    def __add__(self, other):\n        \"\"\"Add two fractions: a/b + c/d = (ad+bc)/bd\"\"\"\n        new_num = self.num * other.den + other.num * self.den\n        new_den = self.den * other.den\n        return Fraction(new_num, new_den)\n    \n    def __eq__(self, other):\n        \"\"\"Check equality: a/b == c/d if ad == bc\"\"\"\n        return self.num * other.den == other.num * self.den\n\n# Using special methods\nf1 = Fraction(1, 2)\nf2 = Fraction(1, 3)\n\nprint(f\"f1 = {f1}\")  # Calls __str__\nprint(f\"f1 + f2 = {f1 + f2}\")  # Calls __add__\nprint(f\"f1 as float: {float(f1)}\")  # Calls __float__\nprint(f\"f1 == Fraction(2,4): {f1 == Fraction(2, 4)}\")  # Calls __eq__\n\n","type":"content","url":"/oop-fundamentals-v2#id-6-5-special-methods-making-objects-pythonic","position":21},{"hierarchy":{"lvl1":"Chapter 6: OOP Fundamentals - Organizing Scientific Code","lvl3":"Essential Special Methods","lvl2":"6.5 Special Methods: Making Objects Pythonic"},"type":"lvl3","url":"/oop-fundamentals-v2#essential-special-methods","position":22},{"hierarchy":{"lvl1":"Chapter 6: OOP Fundamentals - Organizing Scientific Code","lvl3":"Essential Special Methods","lvl2":"6.5 Special Methods: Making Objects Pythonic"},"content":"\n\nclass DataSet:\n    \"\"\"Collection that acts like a built-in container.\"\"\"\n    \n    def __init__(self, values=None):\n        \"\"\"Initialize with optional values.\"\"\"\n        self.values = values if values else []\n    \n    def __len__(self):\n        \"\"\"Support len(dataset).\"\"\"\n        return len(self.values)\n    \n    def __getitem__(self, index):\n        \"\"\"Support dataset[index].\"\"\"\n        return self.values[index]\n    \n    def __setitem__(self, index, value):\n        \"\"\"Support dataset[index] = value.\"\"\"\n        self.values[index] = value\n    \n    def __contains__(self, value):\n        \"\"\"Support 'value in dataset'.\"\"\"\n        return value in self.values\n    \n    def __iter__(self):\n        \"\"\"Support for loops.\"\"\"\n        return iter(self.values)\n    \n    def __bool__(self):\n        \"\"\"Support if dataset: (True if not empty).\"\"\"\n        return len(self.values) > 0\n\n# Acts like a built-in container\ndata = DataSet([10, 20, 30, 40, 50])\n\nprint(f\"Length: {len(data)}\")\nprint(f\"First: {data[0]}\")\nprint(f\"Contains 30: {30 in data}\")\nprint(f\"Is non-empty: {bool(data)}\")\n\ndata[1] = 25\nfor value in data:\n    print(value, end=\" \")\n\n💡 Computational Thinking Box: Protocol-Based Design\n\nPATTERN: Duck Typing Through Special Methods\n\n“If it walks like a duck and quacks like a duck, it’s a duck”\n\nPython doesn’t check types - it checks capabilities. Any object implementing the right special methods can be used anywhere:\n\nIterator Protocol:\n\n__iter__() and __next__() → works in for loops\n\nContainer Protocol:\n\n__len__() and __getitem__() → works with len(), indexing\n\nNumeric Protocol:\n\n__add__(), __mul__(), etc. → works with math operators\n\nContext Manager Protocol:\n\n__enter__() and __exit__() → works with ‘with’ statement\n\nThis is why your custom objects can work with built-in functions! A DataSet with __len__ works with len(). A Vector with __add__ works with +. This protocol-based design is central to Python’s flexibility and why scientific libraries integrate so well.\n\nReal-world example: Any object with .shape, .dtype, and __getitem__ can be used where NumPy expects an array-like object.","type":"content","url":"/oop-fundamentals-v2#essential-special-methods","position":23},{"hierarchy":{"lvl1":"Chapter 6: OOP Fundamentals - Organizing Scientific Code","lvl2":"6.6 When to Use Objects vs Functions"},"type":"lvl2","url":"/oop-fundamentals-v2#id-6-6-when-to-use-objects-vs-functions","position":24},{"hierarchy":{"lvl1":"Chapter 6: OOP Fundamentals - Organizing Scientific Code","lvl2":"6.6 When to Use Objects vs Functions"},"content":"Now that you understand HOW to create classes with all their powerful features - properties for validation, special methods for integration, inheritance for code reuse - you need wisdom about WHEN to use them. Not every problem needs objects. Creating unnecessary classes can make code harder to understand, not easier. The art of programming lies in choosing the right tool for the right job.\n\nHere’s how to decide. This distinction is fundamental - in Chapter 7, we’ll explore the “is-a” relationship (inheritance) versus “has-a” relationship (composition) in detail.","type":"content","url":"/oop-fundamentals-v2#id-6-6-when-to-use-objects-vs-functions","position":25},{"hierarchy":{"lvl1":"Chapter 6: OOP Fundamentals - Organizing Scientific Code","lvl3":"Use Objects When:","lvl2":"6.6 When to Use Objects vs Functions"},"type":"lvl3","url":"/oop-fundamentals-v2#use-objects-when","position":26},{"hierarchy":{"lvl1":"Chapter 6: OOP Fundamentals - Organizing Scientific Code","lvl3":"Use Objects When:","lvl2":"6.6 When to Use Objects vs Functions"},"content":"Managing State Over Time\n\nclass RunningStatistics:\n    \"\"\"Maintains statistics as data arrives.\"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize empty statistics.\"\"\"\n        self.count = 0\n        self.sum = 0\n        self.sum_sq = 0\n    \n    def add_value(self, x):\n        \"\"\"Add new value to statistics.\"\"\"\n        self.count += 1\n        self.sum += x\n        self.sum_sq += x**2\n    \n    @property\n    def mean(self):\n        \"\"\"Current mean.\"\"\"\n        return self.sum / self.count if self.count > 0 else 0\n    \n    @property\n    def variance(self):\n        \"\"\"Current variance.\"\"\"\n        if self.count < 2:\n            return 0\n        mean = self.mean\n        return (self.sum_sq - self.count * mean**2) / (self.count - 1)\n\n# Object maintains state between calls\nstats = RunningStatistics()\nfor value in [1, 2, 3, 4, 5]:\n    stats.add_value(value)\n    print(f\"After {value}: mean={stats.mean:.1f}, var={stats.variance:.1f}\")\n\nModeling Real Entities\n\nclass Galaxy:\n    \"\"\"Model a galaxy with properties.\"\"\"\n    \n    def __init__(self, name, distance_mpc, redshift):\n        \"\"\"Initialize galaxy with observed properties.\"\"\"\n        self.name = name\n        self.distance_mpc = distance_mpc  # Megaparsecs\n        self.redshift = redshift\n    \n    def recession_velocity(self):\n        \"\"\"Calculate recession velocity in km/s.\"\"\"\n        c = 3e5  # km/s\n        return c * self.redshift  # Simple approximation\n    \n    def lookback_time_gyr(self):\n        \"\"\"Estimate lookback time in Gyr.\"\"\"\n        # Simplified: t ≈ z / H0\n        H0 = 70  # km/s/Mpc\n        return self.redshift * 1000 / H0\n\nm31 = Galaxy(\"Andromeda\", 0.78, -0.001)  # Blueshifted!\nprint(f\"{m31.name}: v={m31.recession_velocity():.0f} km/s\")\n\n","type":"content","url":"/oop-fundamentals-v2#use-objects-when","position":27},{"hierarchy":{"lvl1":"Chapter 6: OOP Fundamentals - Organizing Scientific Code","lvl3":"Use Functions When:","lvl2":"6.6 When to Use Objects vs Functions"},"type":"lvl3","url":"/oop-fundamentals-v2#use-functions-when","position":28},{"hierarchy":{"lvl1":"Chapter 6: OOP Fundamentals - Organizing Scientific Code","lvl3":"Use Functions When:","lvl2":"6.6 When to Use Objects vs Functions"},"content":"Simple Transformations\n\n# No need for a class here\ndef celsius_to_kelvin(celsius):\n    \"\"\"Convert Celsius to Kelvin.\"\"\"\n    return celsius + 273.15\n\ndef calculate_orbital_period(semi_major_axis_au):\n    \"\"\"Kepler's third law: P² = a³ for period in years.\"\"\"\n    return semi_major_axis_au ** 1.5\n\n# Simple functions are clearer than unnecessary classes\ntemp_k = celsius_to_kelvin(25)\nperiod = calculate_orbital_period(1.0)  # Earth\nprint(f\"Temperature: {temp_k} K\")\nprint(f\"Period: {period} years\")\n\nStateless Operations\n\n# These don't need to remember anything between calls\ndef mean(values):\n    \"\"\"Calculate mean.\"\"\"\n    return sum(values) / len(values)\n\ndef standard_deviation(values):\n    \"\"\"Calculate standard deviation.\"\"\"\n    m = mean(values)\n    variance = sum((x - m)**2 for x in values) / len(values)\n    return variance**0.5\n\ndata = [1, 2, 3, 4, 5]\nprint(f\"Mean: {mean(data)}\")\nprint(f\"Std: {standard_deviation(data):.2f}\")\n\n🌟 Why This Matters: The NumPy Decision\n\nWhen Travis Oliphant designed NumPy in 2005, he faced this exact decision. Should arrays be simple functions operating on data, or objects with methods?\n\nHe chose objects, and it transformed scientific Python:# If NumPy used only functions:\narray = create_array([1, 2, 3])\nmean = calculate_mean(array)\nreshaped = reshape_array(array, (3, 1))\n\n# Because NumPy uses objects:\narray = np.array([1, 2, 3])\nmean = array.mean()\nreshaped = array.reshape(3, 1)\n\nThe object approach won because arrays maintain state (shape, dtype, memory layout) and operations naturally belong to the data. This decision made NumPy intuitive and helped it become the foundation of scientific Python. You’re learning to make the same architectural decisions!\n\n🎭 The More You Know: How OOP United Astronomy’s Warring Packages\n\nBy 2011, Python astronomy had descended into chaos. Every research group had created their own packages with incompatible interfaces. There was PyFITS for reading FITS files, PyWCS for world coordinate systems, vo.table for Virtual Observatory tables, asciitable for text data, cosmolopy for cosmological calculations, and dozens more. Installing a working astronomy environment was a nightmare - each package had different conventions, different dependencies, and different ways of representing the same concepts.\n\nErik Tollerud, a graduate student at UC Irvine, described the situation: “I spent more time converting between data formats than doing science” (paraphrased from development discussions). A coordinate might be represented as a tuple in one package, a list in another, and a custom object in a third. Unit conversions were handled differently everywhere. Even reading a simple FITS file could require three different packages that didn’t talk to each other.\n\nAt the 2011 Python in Astronomy conference, something remarkable happened. Thomas Robitaille, Perry Greenfield, Erik Tollerud, and developers from competing packages made a radical decision: merge everything into one coherent framework using consistent OOP principles. The design philosophy was simple but powerful:\n\nIf it’s an entity with state and behavior, make it a class (SkyCoord for coordinates, Table for data, Quantity for values with units)\n\nIf it’s a simple transformation, keep it a function (unit conversions, mathematical operations)\n\nEverything has units, always (no more Mars Climate Orbiter disasters)\n\nOne obvious way to do things (borrowed from Python’s philosophy)\n\nThe transformation was remarkable. This incompatible mess:# Old way - three packages, incompatible outputs\nimport pyfits\nimport pywcs  \nimport coords\n\ndata = pyfits.getdata('image.fits')  # Returns numpy array\nheader = pyfits.getheader('image.fits')  # Returns header object\nwcs = pywcs.WCS(header)  # Different coordinate object\n# Convert pixel to sky - returns plain numpy array, no units!\nsky = wcs.wcs_pix2sky([[100, 200]], 1)  \n# Now convert to different coordinate system - different package!\ngalactic = coords.Position((sky[0][0], sky[0][1])).galactic()\n\nBecame this unified interface:# Astropy way - one package, consistent OOP\nfrom astropy.io import fits\nfrom astropy.wcs import WCS\nfrom astropy.coordinates import SkyCoord\n\nhdu = fits.open('image.fits')[0]  # Unified HDU object\nwcs = WCS(hdu.header)  # Same package, consistent interface\n# Returns SkyCoord object with units and frame info!\nsky = wcs.pixel_to_world(100, 200)  \ngalactic = sky.galactic  # Simple property access for conversion\n\nThe key insight? Objects should model astronomical concepts the way astronomers think about them. A coordinate isn’t just numbers - it’s a position with a reference frame, epoch, and possibly distance. A table isn’t just an array - it has columns with units, metadata, and masks. A quantity isn’t just a float - it has units that propagate through calculations.\n\nToday, Astropy has over 10 million downloads and is astronomy’s most-used package. The Large Synoptic Survey Telescope, the Event Horizon Telescope that imaged black holes, and the James Webb Space Telescope data pipelines all build on Astropy’s OOP foundation. When you’re deciding whether to use a class or function, you’re making the same architectural decisions that unified an entire scientific community and enabled discoveries like gravitational waves and exoplanets!\n\n[Sources: Robitaille, T., et al. (2013). Astropy: A community Python package for astronomy. Astronomy & Astrophysics, 558, A33. Development history simplified for pedagogical purposes.]","type":"content","url":"/oop-fundamentals-v2#use-functions-when","position":29},{"hierarchy":{"lvl1":"Chapter 6: OOP Fundamentals - Organizing Scientific Code","lvl2":"6.7 Debugging Classes"},"type":"lvl2","url":"/oop-fundamentals-v2#id-6-7-debugging-classes","position":30},{"hierarchy":{"lvl1":"Chapter 6: OOP Fundamentals - Organizing Scientific Code","lvl2":"6.7 Debugging Classes"},"content":"Understanding how to inspect and debug objects is crucial. Python provides powerful introspection tools to examine objects at runtime:\n\nclass Instrument:\n    \"\"\"Scientific instrument for debugging demo.\"\"\"\n    \n    def __init__(self, name, wavelength_nm):\n        \"\"\"Initialize instrument with name and wavelength.\"\"\"\n        self.name = name\n        self.wavelength_nm = wavelength_nm\n        self._calibrated = False\n    \n    def calibrate(self):\n        \"\"\"Calibrate instrument.\"\"\"\n        self._calibrated = True\n        return f\"{self.name} calibrated\"\n\n# Create instrument\nspectrometer = Instrument(\"HARPS\", 500)\n\n# Introspection tools\nprint(f\"Type: {type(spectrometer)}\")\nprint(f\"Class name: {spectrometer.__class__.__name__}\")\nprint(f\"Is Instrument?: {isinstance(spectrometer, Instrument)}\")\n\n# Check attributes\nprint(f\"\\nHas 'calibrate'?: {hasattr(spectrometer, 'calibrate')}\")\nprint(f\"Wavelength: {getattr(spectrometer, 'wavelength_nm', 'N/A')}\")\n\n# List all attributes (filtering out special ones)\nattrs = [a for a in dir(spectrometer) if not a.startswith('_')]\nprint(f\"\\nPublic attributes: {attrs}\")\n\n# Instance __dict__ vs dir()\n# __dict__ shows instance attributes only\n# dir() shows everything the object can access\nprint(f\"\\nInstance __dict__: {vars(spectrometer)}\")\nprint(f\"dir() has {len(dir(spectrometer))} items (includes inherited)\")\nprint(f\"__dict__ has {len(vars(spectrometer))} items (instance only)\")\n\n🐛 Debug This!\n\nThis code has a subtle but critical bug. Can you find it?\n\nclass Observatory:\n    def __init__(self, name, telescopes=[]):  # Bug here!\n        self.name = name\n        self.telescopes = telescopes\n    \n    def add_telescope(self, telescope):\n        \"\"\"Add a telescope to this observatory.\"\"\"\n        self.telescopes.append(telescope)\n\n# Test the code\nkeck = Observatory(\"Keck\")\nkeck.add_telescope(\"Keck I\")\n\nvlt = Observatory(\"VLT\")\nvlt.add_telescope(\"Antu\")\n\nprint(f\"Keck telescopes: {keck.telescopes}\")\nprint(f\"VLT telescopes: {vlt.telescopes}\")  # Unexpected output!\n\nSolution\n\nThe bug is the mutable default argument telescopes=[]. All instances share the same list! When you add a telescope to one observatory, it appears in all of them.\n\nFix:def __init__(self, name, telescopes=None):\n    self.name = name\n    self.telescopes = telescopes if telescopes is not None else []\n\nThis bug has caused real problems in production systems. Always use None as default for mutable arguments, then create a new object in the method.\n## 6.8 Practice Exercises\n\n### Exercise 1: Build a DataPoint Class\n\nLet's start with a fundamental building block for data analysis:\n\n```{code-cell} python\n\"\"\"\nPart A: Basic DataPoint class (5 minutes)\nCreate a class that stores a value with timestamp\n\"\"\"\n\nimport time\n\nclass DataPoint:\n    \"\"\"Single measurement with timestamp.\"\"\"\n    \n    def __init__(self, value, label=\"\"):\n        \"\"\"Initialize with value and optional label.\"\"\"\n        self.value = value\n        self.label = label\n        self.timestamp = time.time()\n    \n    def age_seconds(self):\n        \"\"\"How old is this data point?\"\"\"\n        return time.time() - self.timestamp\n\n# Test it\ndp = DataPoint(42.5, \"temperature\")\nprint(f\"Value: {dp.value}\")\nprint(f\"Label: {dp.label}\")\n# Note: Brief pause to demonstrate timestamp difference\n# In real applications, timestamps naturally differ\ntime.sleep(0.01)  \nprint(f\"Age: {dp.age_seconds():.3f} seconds\")\n```\n\n```{code-cell} python\n\"\"\"\nPart B: Add validation and properties (10 minutes)\nEnhance with properties for safety\n\"\"\"\n\nclass DataPoint:\n    \"\"\"Measurement with validation.\"\"\"\n    \n    def __init__(self, value, error=0, label=\"\"):\n        \"\"\"Initialize with value, error, and label.\"\"\"\n        self.value = value\n        self.error = abs(error)  # Ensure positive\n        self.label = label\n        self.timestamp = time.time()\n    \n    @property\n    def relative_error(self):\n        \"\"\"Relative error as percentage.\"\"\"\n        if self.value == 0:\n            return float('inf')\n        return (self.error / abs(self.value)) * 100\n    \n    @property\n    def is_significant(self):\n        \"\"\"Check if measurement is significant.\"\"\"\n        return self.relative_error < 5.0  # Less than 5% error\n    \n    def __str__(self):\n        \"\"\"Nice string representation.\"\"\"\n        return f\"{self.value} ± {self.error} ({self.label})\"\n\n# Test enhanced version\ndp = DataPoint(100, 2, \"voltage\")\nprint(dp)\nprint(f\"Relative error: {dp.relative_error:.1f}%\")\nprint(f\"Significant?: {dp.is_significant}\")\n```\n\n```{code-cell} python\n\"\"\"\nPart C: Complete DataSeries class (15 minutes)\nContainer for multiple DataPoints with analysis\n\"\"\"\n\nclass DataSeries:\n    \"\"\"Collection of DataPoints with statistics.\"\"\"\n    \n    def __init__(self, name):\n        \"\"\"Initialize empty series with name.\"\"\"\n        self.name = name\n        self._points = []\n    \n    def add_point(self, value, error=0):\n        \"\"\"Add new data point.\"\"\"\n        dp = DataPoint(value, error, self.name)\n        self._points.append(dp)\n    \n    def __len__(self):\n        \"\"\"Number of points.\"\"\"\n        return len(self._points)\n    \n    def __getitem__(self, index):\n        \"\"\"Access points by index.\"\"\"\n        return self._points[index]\n    \n    @property\n    def values(self):\n        \"\"\"Array of values.\"\"\"\n        return [p.value for p in self._points]\n    \n    @property\n    def mean(self):\n        \"\"\"Calculate mean value.\"\"\"\n        if not self._points:\n            return 0\n        return sum(self.values) / len(self._points)\n    \n    @property\n    def std_dev(self):\n        \"\"\"Calculate standard deviation.\"\"\"\n        if len(self._points) < 2:\n            return 0\n        m = self.mean\n        variance = sum((x - m)**2 for x in self.values) / (len(self._points) - 1)\n        return variance**0.5\n    \n    def __str__(self):\n        \"\"\"String representation with statistics.\"\"\"\n        return f\"DataSeries '{self.name}': {len(self)} points, mean={self.mean:.2f}±{self.std_dev:.2f}\"\n\n# Test complete system\nseries = DataSeries(\"Temperature\")\nfor temp in [20.1, 20.5, 19.8, 20.2, 20.0]:\n    series.add_point(temp, error=0.1)\n\nprint(series)\nprint(f\"First point: {series[0]}\")\nprint(f\"Last point: {series[-1]}\")\nprint(f\"All values: {series.values}\")\n```\n\n### Exercise 2: Variable Star Class (Continuing Our Thread)\n\nBuilding on Chapter 5's lightcurve analysis - now we organize our variable star code using OOP:\n\n```{code-cell} python\n\"\"\"\nTransform our functional variable star analysis into OOP\nThis connects directly to what you learned in Chapter 5!\n\"\"\"\n\nclass VariableStar:\n    \"\"\"A variable star with photometric properties.\"\"\"\n    \n    def __init__(self, name, observations):\n        \"\"\"\n        Initialize with observations.\n        observations: list of (time, magnitude, error) tuples\n        \"\"\"\n        self.name = name\n        self.observations = sorted(observations)  # Sort by time\n    \n    @property\n    def mean_magnitude(self):\n        \"\"\"Average magnitude.\"\"\"\n        mags = [obs[1] for obs in self.observations]\n        return sum(mags) / len(mags) if mags else 0\n    \n    @property\n    def amplitude(self):\n        \"\"\"Peak-to-peak amplitude.\"\"\"\n        if not self.observations:\n            return 0\n        mags = [obs[1] for obs in self.observations]\n        return max(mags) - min(mags)\n    \n    @property\n    def time_span(self):\n        \"\"\"Total observation time span in days.\"\"\"\n        if len(self.observations) < 2:\n            return 0\n        times = [obs[0] for obs in self.observations]\n        return max(times) - min(times)\n    \n    def phase_fold(self, period):\n        \"\"\"\n        Fold lightcurve at given period.\n        \n        Phase folding: Maps all times to [0,1] based on period.\n        Like overlaying multiple periods to see the repeating pattern.\n        \n        Visual example:\n        Time:   0 5.4 10.8 16.2 21.6 27.0 ...\n        Phase:  0 0   0    0    0    0    ... (all fold to same phase)\n                ↓ ↓   ↓    ↓    ↓    ↓\n                [────one period────]\n        \n        Used to find periodic signals in variable stars.\n        \"\"\"\n        result = []\n        for time, mag, err in self.observations:\n            phase = (time % period) / period\n            result.append((phase, mag, err))\n        return sorted(result)\n    \n    def __str__(self):\n        \"\"\"String representation with key statistics.\"\"\"\n        return (f\"VariableStar({self.name}): \"\n                f\"{len(self.observations)} obs, \"\n                f\"<m>={self.mean_magnitude:.2f}, \"\n                f\"amp={self.amplitude:.2f}\")\n    \n    def __len__(self):\n        \"\"\"Number of observations.\"\"\"\n        return len(self.observations)\n\n# Test with simulated Cepheid data\nobservations = []\nfor day in range(20):\n    time = day + 0.1 * day  # Non-uniform sampling\n    # Sinusoidal variation\n    phase = 2 * math.pi * time / 5.366  # Delta Cephei period\n    magnitude = 4.0 + 0.5 * math.sin(phase)\n    error = 0.01\n    observations.append((time, magnitude, error))\n\ndelta_cep = VariableStar(\"Delta Cephei\", observations)\nprint(delta_cep)\nprint(f\"Time span: {delta_cep.time_span:.1f} days\")\n\n# Phase fold at known period\nfolded = delta_cep.phase_fold(5.366)\nprint(f\"First folded point: phase={folded[0][0]:.3f}, mag={folded[0][1]:.2f}\")\n```\n\n### Exercise 3: Performance Comparison\n\n```{code-cell} python\n\"\"\"\nCompare OOP vs functional approaches for performance\nUnderstanding the tradeoffs helps you choose wisely\n\"\"\"\n\nimport time\n\n# OOP Approach\nclass ParticleOOP:\n    def __init__(self, x, y, vx, vy):\n        \"\"\"Initialize particle with position and velocity.\"\"\"\n        self.x = x    # cm\n        self.y = y    # cm\n        self.vx = vx  # cm/s\n        self.vy = vy  # cm/s\n    \n    def update(self, dt):\n        \"\"\"Update position based on velocity.\"\"\"\n        self.x += self.vx * dt\n        self.y += self.vy * dt\n    \n    def energy(self):\n        \"\"\"Calculate kinetic energy in ergs (mass = 1g).\"\"\"\n        return 0.5 * (self.vx**2 + self.vy**2)\n\n# Functional Approach\ndef create_particle(x, y, vx, vy):\n    \"\"\"Create particle dictionary.\"\"\"\n    return {'x': x, 'y': y, 'vx': vx, 'vy': vy}\n\ndef update_particle(p, dt):\n    \"\"\"Update particle position.\"\"\"\n    p['x'] += p['vx'] * dt\n    p['y'] += p['vy'] * dt\n    return p\n\ndef particle_energy(p):\n    \"\"\"Calculate particle energy.\"\"\"\n    return 0.5 * (p['vx']**2 + p['vy']**2)\n\n# Performance test with moderate sample for demonstration\nn = 5000  # Moderate size for notebook execution\ndt = 0.01\n\n# OOP timing\nstart = time.perf_counter()\nparticles_oop = [ParticleOOP(i, i, 1, 1) for i in range(n)]\nfor p in particles_oop:\n    p.update(dt)\n    e = p.energy()\noop_time = time.perf_counter() - start\n\n# Functional timing\nstart = time.perf_counter()\nparticles_func = [create_particle(i, i, 1, 1) for i in range(n)]\nfor p in particles_func:\n    update_particle(p, dt)\n    e = particle_energy(p)\nfunc_time = time.perf_counter() - start\n\nprint(f\"OOP approach: {oop_time*1000:.2f} ms\")\nprint(f\"Functional approach: {func_time*1000:.2f} ms\")\nprint(f\"Ratio: {oop_time/func_time:.2f}x\")\n\nprint(\"\\nNote: OOP is often slightly slower due to:\")\nprint(\"- Attribute lookup overhead\")\nprint(\"- Object creation and memory allocation costs\")\nprint(\"\\nHowever, this performance difference only matters when\")\nprint(\"creating millions of objects in tight loops. For most\")\nprint(\"scientific applications, code clarity and maintainability\")\nprint(\"far outweigh these microsecond differences.\")\n```\n\n## Main Takeaways\n\nYou've just made a fundamental leap in how you think about programming. Object-Oriented Programming isn't just a different syntax – it's a different mental model. Instead of thinking \"what operations do I need to perform on this data?\", you now think \"what *is* this thing and what can it *do*?\" This shift from procedural to object-oriented thinking mirrors how we naturally conceptualize scientific systems. A particle isn't just three numbers for position; it's an entity with mass, velocity, and behaviors like moving and colliding.\n\nThe power of OOP becomes clear when managing complexity. That simple Particle class with five attributes and three methods might seem like overkill compared to a dictionary. But when your simulation has thousands of particles, each needing consistent updates, validation, and state tracking, the object-oriented approach prevents the chaos that killed the Mars Climate Orbiter mission. Properties ensure units stay consistent. Methods guarantee state updates follow physical laws. Special methods make your objects work seamlessly with Python's syntax. These aren't just programming conveniences – they're safety mechanisms that prevent billion-dollar disasters.\n\nBut perhaps the most important lesson is knowing when NOT to use objects. Not every function needs to become a method. Not every data structure needs to become a class. Simple calculations should stay as functions. Stateless transformations don't need objects. The art lies in recognizing when you're modeling entities with state and behavior (use classes) versus performing operations on data (use functions). This judgment will develop as you write more code, but now you have the conceptual framework to make these decisions thoughtfully.\n\nLooking ahead, everything in Python's scientific stack builds on these concepts. NumPy arrays are objects with methods like `.mean()` and `.reshape()`. Every Matplotlib plot is an object maintaining state about axes, data, and styling. When you write `array.sum()` or `figure.savefig()`, you're using the same patterns you just learned. More importantly, you can now create your own scientific classes that integrate seamlessly with these tools. You're not just learning to use objects – you're learning to think in objects, and that's a superpower for scientific computing.\n\n## Definitions\n\n**Attribute**: A variable that belongs to an object. Instance attributes are unique to each object; class attributes are shared by all instances.\n\n**Class**: A blueprint or template for creating objects. Defines what attributes and methods objects will have.\n\n**Constructor**: The `__init__` method that initializes new objects when they're created.\n\n**Duck Typing**: Python's philosophy that an object's suitability is determined by its methods and attributes, not its type. \"If it walks like a duck and quacks like a duck, it's a duck.\"\n\n**Encapsulation**: The bundling of data and methods that operate on that data within a single unit (class).\n\n**Instance**: A specific object created from a class. Each instance has its own set of instance attributes.\n\n**Method**: A function defined inside a class that operates on instances of that class.\n\n**Object**: A specific instance of a class containing data (attributes) and behavior (methods).\n\n**Property**: A special attribute that executes code when accessed or set, created with the `@property` decorator.\n\n**Self**: The first parameter of instance methods, referring to the specific object being operated on.\n\n**Setter**: A property method that validates and sets attribute values, defined with `@attribute.setter`.\n\n**Special Method**: Methods with double underscores (like `__init__`, `__str__`) that define object behavior for built-in operations.\n\n**Static Method**: A method that doesn't receive self or cls, defined with `@staticmethod`.\n\n## Key Takeaways\n\n✓ **Classes combine data and behavior** – Objects bundle related attributes and methods, keeping code organized and preventing errors from mismatched data and functions\n\n✓ **The self parameter connects methods to objects** – It's automatically passed to methods and refers to the specific instance being operated on\n\n✓ **Properties provide smart attributes** – Use `@property` for computed values and validation, ensuring data consistency without explicit method calls\n\n✓ **Special methods make objects Pythonic** – Implementing `__str__`, `__len__`, `__add__` lets your objects work naturally with built-in functions and operators\n\n✓ **Instance attributes belong to objects, class attributes are shared** – Choose instance for object-specific data, class for constants and shared state\n\n✓ **Not everything needs to be a class** – Use objects for stateful entities with behavior, functions for simple calculations and transformations\n\n✓ **Properties prevent unit disasters** – Validation in setters catches errors immediately, preventing Mars Climate Orbiter-style catastrophes\n\n✓ **Everything in Python is an object** – Even functions and modules are objects, making Python's object model consistent and powerful\n\n✓ **Duck typing enables flexibility** – Objects work based on capabilities (methods) not types, allowing seamless integration with Python's protocols\n\n## Quick Reference Tables\n\n### Class Definition Syntax\n\n| Element | Syntax | Example |\n|---------|--------|---------|\n| Define class | `class Name:` | `class Particle:` |\n| Constructor | `def __init__(self):` | `def __init__(self, mass):` |\n| Instance attribute | `self.attr = value` | `self.mass = 1.67e-24` |\n| Class attribute | `attr = value` | `SPEED_OF_LIGHT = 3e10` |\n| Instance method | `def method(self):` | `def velocity(self):` |\n| Property getter | `@property` | `@property def energy(self):` |\n| Property setter | `@attr.setter` | `@energy.setter` |\n| Class method | `@classmethod` | `@classmethod def from_file(cls):` |\n| Static method | `@staticmethod` | `@staticmethod def validate():` |\n\n### Essential Special Methods\n\n| Method | Purpose | Called By |\n|--------|---------|-----------|\n| `__init__` | Initialize object | `MyClass()` |\n| `__str__` | Human-readable string | `str(obj)`, `print(obj)` |\n| `__repr__` | Developer string | `repr(obj)` |\n| `__len__` | Get length | `len(obj)` |\n| `__getitem__` | Get by index | `obj[i]` |\n| `__setitem__` | Set by index | `obj[i] = val` |\n| `__contains__` | Check membership | `x in obj` |\n| `__iter__` | Make iterable | `for x in obj` |\n| `__add__` | Addition | `obj1 + obj2` |\n| `__eq__` | Equality test | `obj1 == obj2` |\n| `__bool__` | Truth value | `if obj:`, `bool(obj)` |\n| `__call__` | Make callable | `obj()` |\n\n### Debugging Object Tools\n\n| Function | Purpose | Example |\n|----------|---------|---------|\n| `type(obj)` | Get object's class | `type(particle)` |\n| `isinstance(obj, cls)` | Check if object is instance | `isinstance(p, Particle)` |\n| `hasattr(obj, 'attr')` | Check if attribute exists | `hasattr(p, 'mass')` |\n| `getattr(obj, 'attr')` | Get attribute safely | `getattr(p, 'mass', 0)` |\n| `setattr(obj, 'attr', val)` | Set attribute | `setattr(p, 'mass', 1.0)` |\n| `dir(obj)` | List all accessible attributes | `dir(particle)` |\n| `vars(obj)` or `obj.__dict__` | Get instance attributes only | `vars(particle)` |\n| `help(obj)` | Get documentation | `help(Particle)` |\n\n## References\n\n1. Oliphant, T. E. (2006). *A guide to NumPy* (Vol. 1). USA: Trelgol Publishing. - The design decisions behind NumPy's object-oriented architecture.\n\n2. NASA JPL. (2004). *Mars Exploration Rover Mission: Spirit Anomaly Report*. Jet Propulsion Laboratory. - Documentation of the Spirit rover flash memory issue and software recovery.\n\n3. NASA. (1999). *Mars Climate Orbiter Mishap Investigation Board Phase I Report*. NASA. - Official report on the unit conversion error that destroyed MCO.\n\n4. Allen, L. (1990). *The Hubble Space Telescope Optical Systems Failure Report*. NASA. - Analysis of the mirror aberration and its causes.\n\n5. Van Rossum, G., & Drake, F. L. (2009). *Python 3 Reference Manual*. CreateSpace. - Definitive guide to Python's object model and special methods.\n\n6. Van Rossum, G. (1996). Foreword for \"Programming Python\" (1st ed.). O'Reilly Media. - Early vision for Python's role in computing.\n\n7. Lutz, M. (2013). *Learning Python* (5th ed.). O'Reilly Media. - Comprehensive coverage of Python OOP concepts.\n\n8. Gamma, E., Helm, R., Johnson, R., & Vlissides, J. (1994). *Design Patterns: Elements of Reusable Object-Oriented Software*. Addison-Wesley. - Classic patterns referenced in the protocol discussion.\n\n9. Nygaard, K., & Dahl, O. J. (1978). The development of the SIMULA languages. *ACM SIGPLAN Notices*, 13(8), 245-272. - History of OOP's invention.\n\n10. Kay, A. (1993). The early history of Smalltalk. *ACM SIGPLAN Notices*, 28(3), 69-95. - Influence of Simula on object-oriented thinking.\n\n11. Beazley, D., & Jones, B. K. (2013). *Python Cookbook* (3rd ed.). O'Reilly Media. - Practical recipes for Python OOP and metaprogramming.\n\n12. Ramalho, L. (2015). *Fluent Python*. O'Reilly Media. - Deep dive into Python's object model and special methods.\n\n13. Martelli, A. (2000). *The Python \"Duck Typing\" Principle*. comp.lang.python. - Origin of the duck typing terminology in Python context.\n\n14. Knuth, D. E. (1997). *The Art of Computer Programming, Volume 1: Fundamental Algorithms* (3rd ed.). Addison-Wesley. - Source for the Euclidean algorithm used in GCD calculation.\n\n15. Robitaille, T., et al. (2013). Astropy: A community Python package for astronomy. *Astronomy & Astrophysics*, 558, A33. - The unification of Python astronomy packages.\n\n16. Hugunin, J. (1995). The Numeric Python extensions. *Python Software Foundation Archives*. - Early history of scientific Python.\n\n## Next Chapter Preview\n\nIn Chapter 7: Advanced OOP Patterns, you'll discover how object-oriented programming scales to advanced patterns. You'll master inheritance to model scientific hierarchies (Particle → ChargedParticle → Electron), use composition for complex systems (Telescope HAS-A Mount, Detector, and Filter), and implement context managers for resource safety. We'll explore performance optimization with `__slots__`, advanced special methods for full container behavior, and design patterns from production scientific software. Most importantly, you'll learn when these advanced techniques clarify versus complicate code. The same OOP foundation you just built powers frameworks like Astropy and SciPy – next chapter shows you how to architect at that level!","type":"content","url":"/oop-fundamentals-v2#id-6-7-debugging-classes","position":31},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing"},"type":"lvl1","url":"/python-numpy-fundamentals-v2","position":0},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing"},"content":"","type":"content","url":"/python-numpy-fundamentals-v2","position":1},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl2":"Learning Objectives"},"type":"lvl2","url":"/python-numpy-fundamentals-v2#learning-objectives","position":2},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl2":"Learning Objectives"},"content":"By the end of this chapter, you will be able to:\n\nCreate and manipulate NumPy arrays for efficient numerical computation\n\nApply vectorized operations to eliminate explicit loops and improve performance by 10-100x\n\nMaster array indexing, slicing, and boolean masking for data selection and filtering\n\nUnderstand and leverage broadcasting rules to perform operations on arrays of different shapes\n\nUse essential NumPy functions for scientific computing (linspace, logspace, where, meshgrid)\n\nPerform array transformations including reshaping, stacking, and splitting for data analysis\n\nGenerate random numbers from various distributions for Monte Carlo simulations\n\nApply NumPy to real astronomical calculations with proper CGS units\n\nRecognize when to use NumPy instead of pure Python for numerical tasks","type":"content","url":"/python-numpy-fundamentals-v2#learning-objectives","position":3},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl2":"Prerequisites Check"},"type":"lvl2","url":"/python-numpy-fundamentals-v2#prerequisites-check","position":4},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl2":"Prerequisites Check"},"content":"Before starting this chapter, verify you can:\n\nWork with Python lists and list comprehensions (Chapter 4)\n\nUse the math module for mathematical operations (Chapter 2)\n\nUnderstand functions and return values (Chapter 5)\n\nWork with nested data structures (Chapter 4)\n\nImport and use modules (Chapter 5)\n\nRead and write files (Chapter 6)","type":"content","url":"/python-numpy-fundamentals-v2#prerequisites-check","position":5},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Self-Assessment Diagnostic","lvl2":"Prerequisites Check"},"type":"lvl3","url":"/python-numpy-fundamentals-v2#self-assessment-diagnostic","position":6},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Self-Assessment Diagnostic","lvl2":"Prerequisites Check"},"content":"Test your readiness by predicting these outputs WITHOUT running the code:\n\n# Question 1: What does this produce?\nresult = [x**2 for x in range(5) if x % 2 == 0]\n# Your answer: _______\n\n# Question 2: What's the final value of total?\ntotal = sum([len(str(x)) for x in [10, 100, 1000]])\n# Your answer: _______\n\n# Question 3: What error (if any) occurs here?\nimport math\nvalues = [1, 4, 9]\n# roots = math.sqrt(values)  # Uncomment to test\n# Your answer: _______\n\n# Question 4: Can you understand this nested structure?\nmatrix = [[1, 2], [3, 4]]\nflattened = [item for row in matrix for item in row]\n# Your answer: _______\n\nSelf-Assessment Answers\n\n[0, 4, 16] - squares of even numbers from 0-4\n\n9 - lengths are 2, 3, 4, sum = 9\n\nTypeError - math.sqrt() doesn’t accept lists, only single values\n\n[1, 2, 3, 4] - flattens the 2D list into 1D\n\nIf you got all four correct, you’re ready for NumPy! If not, review the indicated chapters.","type":"content","url":"/python-numpy-fundamentals-v2#self-assessment-diagnostic","position":7},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl2":"Chapter Overview"},"type":"lvl2","url":"/python-numpy-fundamentals-v2#chapter-overview","position":8},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl2":"Chapter Overview"},"content":"You’ve been using Python lists to store collections of numbers, and the math module to perform calculations. But what happens when you need to analyze a million stellar spectra, each with thousands of wavelength points? Or when you need to perform the same calculation on every pixel in a telescope image? Try using a list comprehension on a million-element list, and you’ll be waiting a while. This is where NumPy transforms Python from a general-purpose language into a powerhouse for scientific computing, providing the speed and tools necessary for research-grade computational science.\n\nNumPy, short for Numerical Python, is the foundation upon which the entire scientific Python ecosystem is built. Every plot you’ll make with Matplotlib, every optimization you’ll run with SciPy, every dataframe you’ll analyze with Pandas — they all build on NumPy arrays. But NumPy isn’t just about speed; it’s about expressing mathematical operations naturally. Instead of writing loops to add corresponding elements of two lists, you simply write a + b. Instead of nested loops for matrix multiplication, you write a @ b. This isn’t just convenience — it’s a fundamental shift in how you think about numerical computation, from operating on individual elements to operating on entire arrays at once.\n\nThis chapter introduces you to NumPy’s ndarray (n-dimensional array), the object that makes scientific Python possible. You’ll discover why NumPy arrays are 10-100 times faster than Python lists for numerical operations, and how vectorization eliminates the need for most explicit loops. You’ll master broadcasting, NumPy’s powerful mechanism for operating on arrays of different shapes, which enables elegant solutions to complex problems. Most importantly, you’ll learn to think in arrays — a skill that transforms you from someone who writes code that processes data to someone who writes code that expresses mathematical ideas directly. By the end, you’ll understand why virtually every astronomical software package, from data reduction pipelines to cosmological simulations, is built on NumPy’s foundation.\n\n📚 Essential Resource: NumPy Documentation\n\nThis chapter introduces NumPy’s core concepts, but NumPy is vast! The official documentation at https://​numpy​.org​/doc​/stable/ is your indispensable companion. Throughout your career, you’ll constantly reference it for:\n\nComplete function signatures and parameters\n\nAdvanced broadcasting examples\n\nPerformance optimization tips\n\nSpecialized submodules (random, fft, linalg)\n\nPractice using the documentation NOW: After each new function you learn, look it up in the official docs. Read the parameters, check the examples, and explore related functions. The ability to efficiently navigate documentation is as important as coding itself. Bookmark the NumPy documentation — you’ll use it daily in research!\n\nPro tip: Use the NumPy documentation’s search function to quickly find what you need. Type partial function names or concepts, and it will suggest relevant pages. The “See Also” sections are goldmines for discovering related functionality.","type":"content","url":"/python-numpy-fundamentals-v2#chapter-overview","position":9},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl2":"7.1 From Lists to Arrays: Why NumPy?"},"type":"lvl2","url":"/python-numpy-fundamentals-v2#id-7-1-from-lists-to-arrays-why-numpy","position":10},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl2":"7.1 From Lists to Arrays: Why NumPy?"},"content":"NumPy\nA fundamental package for scientific computing in Python, providing support for large, multi-dimensional arrays and matrices.\n\nndarray\nNumPy’s n-dimensional array object, the core data structure for numerical computation.\n\nContiguous Memory\nData stored in adjacent memory locations, enabling fast access and cache efficiency.\n\nLet’s start with a problem you already know how to solve, then see how NumPy transforms it. Imagine you’re analyzing brightness measurements from a variable star:\n\nimport time\nimport math\n\n# Python list approach (what you know from Chapter 4)\nmagnitudes = [12.3, 12.5, 12.4, 12.7, 12.6] * 20000  # 100,000 measurements\nfluxes = []\n\nstart = time.perf_counter()\nfor mag in magnitudes:\n    flux = 10**(-mag/2.5)  # Convert magnitude to relative flux\n    fluxes.append(flux)\nlist_time = time.perf_counter() - start\n\nprint(f\"List approach: {list_time*1000:.2f} ms\")\nprint(f\"First 5 fluxes: {fluxes[:5]}\")\n\nNow let’s see the NumPy approach:\n\nimport numpy as np  # Standard abbreviation used universally\n\n# NumPy array approach (what you're learning)\nmagnitudes = np.array([12.3, 12.5, 12.4, 12.7, 12.6] * 20000)\n\nstart = time.perf_counter()\nfluxes = 10**(-magnitudes/2.5)  # Operates on entire array at once!\nnumpy_time = time.perf_counter() - start\n\nprint(f\"NumPy approach: {numpy_time*1000:.2f} ms\")\nprint(f\"Speedup: {list_time/numpy_time:.1f}x faster\")\nprint(f\"First 5 fluxes: {fluxes[:5]}\")\n\nThe NumPy version is not only faster but also cleaner — no explicit loop needed! This is called vectorization, and it’s the key to NumPy’s power.","type":"content","url":"/python-numpy-fundamentals-v2#id-7-1-from-lists-to-arrays-why-numpy","position":11},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"NumPy vs. Math Module: A Complete Replacement","lvl2":"7.1 From Lists to Arrays: Why NumPy?"},"type":"lvl3","url":"/python-numpy-fundamentals-v2#numpy-vs-math-module-a-complete-replacement","position":12},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"NumPy vs. Math Module: A Complete Replacement","lvl2":"7.1 From Lists to Arrays: Why NumPy?"},"content":"You’ve been using the math module for operations like math.sin(), math.sqrt(), and math.log(). Good news: NumPy can replace virtually all of math’s functionality while adding array support:\n\n# Math module - works on single values only\nimport math\nx = 2.0\nmath_result = math.sin(x)\nprint(f\"math.sin({x}) = {math_result}\")\n\n# Try with a list - this fails!\n# values = [0, 1, 2, 3]\n# math.sin(values)  # TypeError!\n\n# NumPy - works on single values AND arrays\nx = 2.0\nnumpy_scalar_result = np.sin(x)\nprint(f\"np.sin({x}) = {numpy_scalar_result}\")\n\n# NumPy shines with arrays\nvalues = np.array([0, 1, 2, 3])\nnumpy_array_result = np.sin(values)\nprint(f\"np.sin({values}) = {numpy_array_result}\")\n\n# NumPy has everything math has (and more)\nprint(f\"\\nComparison for x = {x}:\")\nprint(f\"  math.sqrt({x}) = {math.sqrt(x):.6f}\")\nprint(f\"  np.sqrt({x})   = {np.sqrt(x):.6f}\")\nprint(f\"  math.exp({x})  = {math.exp(x):.6f}\")\nprint(f\"  np.exp({x})    = {np.exp(x):.6f}\")\nprint(f\"  math.log10({x}) = {math.log10(x):.6f}\")\nprint(f\"  np.log10({x})   = {np.log10(x):.6f}\")\n\nKey insight: You can generally replace import math with import numpy as np and use NumPy for everything. The only exceptions are a few specialized functions like math.factorial() that don’t have direct NumPy equivalents (though NumPy has scipy.special.factorial() if needed).\n\n🎯 The More You Know: How NumPy Saved Gravitational Wave Astronomy\n\nOn September 14, 2015, at 09:50:45 UTC, the Laser Interferometer Gravitational-Wave Observatory (LIGO) detected gravitational waves for the first time — ripples in spacetime from two black holes colliding 1.3 billion years ago. But this Nobel Prize-winning discovery almost didn’t happen because of a computational bottleneck that NumPy solved.\n\nLIGO’s detectors produce 16,384 samples per second of incredibly noisy data. Detecting a gravitational wave requires comparing this data stream against hundreds of thousands of theoretical waveform templates using matched filtering. In 2009, the original C++ analysis pipeline took 24 hours to analyze just one hour of data — making real-time detection impossible.\n\nKipp Cannon, a LIGO scientist, made a radical decision: rewrite the entire pipeline in Python using NumPy. Critics were horrified. “Python is too slow for production!” they said. But Cannon understood something crucial: NumPy’s vectorized operations call optimized C libraries (BLAS, LAPACK) that are often faster than hand-written C++ code.\n\nThe key transformation was replacing loops with NumPy operations:# Original approach (simplified)\ndef matched_filter_slow(data, template):\n    result = 0\n    for i in range(len(data)):\n        result += data[i] * template[i]\n    return result\n\n# NumPy approach\ndef matched_filter_fast(data, template):\n    return np.dot(data, template)  # Or simply: data @ template\n\nThe NumPy version wasn’t just cleaner — it was 50x faster! NumPy’s FFT implementation (using FFTW under the hood) accelerated frequency-domain operations by another factor of 100. The complete Python/NumPy pipeline could analyze data faster than real-time, searching for gravitational waves as they arrived.\n\nWhen GW150914 (the first detection) arrived, the NumPy-based pipeline identified it as a candidate within 3 minutes. The same analysis would have taken the original C++ code 72 minutes. Those 69 minutes made the difference between a live detection and a historical footnote. The discovery paper acknowledges NumPy explicitly: “The PyCBC analysis pipeline... makes extensive use of NumPy arrays and operations.”\n\nToday, every gravitational wave detection — from colliding neutron stars to intermediate-mass black holes — flows through NumPy arrays. The library you’re learning didn’t just speed up the analysis; it made an entirely new kind of astronomy possible. When you vectorize operations with NumPy, you’re using the same technique that let humanity hear the universe for the first time!","type":"content","url":"/python-numpy-fundamentals-v2#numpy-vs-math-module-a-complete-replacement","position":13},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl2":"7.2 Creating Arrays: Your Scientific Data Containers"},"type":"lvl2","url":"/python-numpy-fundamentals-v2#id-7-2-creating-arrays-your-scientific-data-containers","position":14},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl2":"7.2 Creating Arrays: Your Scientific Data Containers"},"content":"{margin} dtype\nData type of array elements, controlling memory usage and precision.\n\n{margin} CGS Units\nCentimeter-Gram-Second system, traditionally used in astronomy for convenient scaling in stellar physics calculations.\n\nNumPy provides many ways to create arrays, each suited for different scientific tasks:\n\n# From Python lists (most common starting point)\nmeasurements = [23.5, 24.1, 23.8, 24.3]\narr = np.array(measurements)\nprint(f\"From list: {arr}, dtype: {arr.dtype}\")\n\n# Specify data type for memory efficiency\ncounts = np.array([1000, 2000, 1500], dtype=np.int32)\nprint(f\"Integer array: {counts}, dtype: {counts.dtype}\")\n\n# 2D array (matrix) - like an image\nimage_data = np.array([[10, 20, 30],\n                       [40, 50, 60],\n                       [70, 80, 90]])\nprint(f\"2D array shape: {image_data.shape}\")\nprint(f\"2D array:\\n{image_data}\")\n\n","type":"content","url":"/python-numpy-fundamentals-v2#id-7-2-creating-arrays-your-scientific-data-containers","position":15},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Essential Array Creation Functions for Science","lvl2":"7.2 Creating Arrays: Your Scientific Data Containers"},"type":"lvl3","url":"/python-numpy-fundamentals-v2#essential-array-creation-functions-for-science","position":16},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Essential Array Creation Functions for Science","lvl2":"7.2 Creating Arrays: Your Scientific Data Containers"},"content":"These functions are workhorses in scientific computing:\n\n# linspace: Evenly spaced values (inclusive endpoints)\n# Perfect for wavelength grids, time series\nwavelengths_nm = np.linspace(400, 700, 11)  # 11 points from 400 to 700 nm\nprint(f\"Wavelengths (nm): {wavelengths_nm}\")\n\n# Convert to CGS (cm) - standard in stellar atmosphere models\nwavelengths_cm = wavelengths_nm * 1e-7\nprint(f\"Wavelengths (cm): {wavelengths_cm}\")\n\n# logspace: Logarithmically spaced values\n# Essential for frequency grids, stellar masses\nmasses_solar = np.logspace(-1, 2, 4)  # 0.1 to 100 solar masses\nmasses_g = masses_solar * 1.989e33  # Convert to grams (CGS)\nprint(f\"Stellar masses (g): {masses_g:.2e}\")\n\n# arange: Like Python's range but returns array\ntimes_s = np.arange(0, 10, 0.1)  # 0 to 9.9 in 0.1s steps\nprint(f\"Time points: {len(times_s)} samples from {times_s[0]} to {times_s[-1]}\")\n\n# zeros and ones: Initialize arrays\ndark_frame = np.zeros((100, 100))  # 100x100 CCD dark frame\nflat_field = np.ones((100, 100))   # Flat field (perfect response)\nprint(f\"Dark frame shape: {dark_frame.shape}, sum: {dark_frame.sum()}\")\nprint(f\"Flat field shape: {flat_field.shape}, sum: {flat_field.sum()}\")\n\n💡 Computational Thinking Box: Row-Major vs Column-Major\n\nPATTERN: Memory Layout Matters for Performance\n\nNumPy stores arrays in row-major order (C-style) by default, meaning elements in the same row are adjacent in memory. This affects performance dramatically:# Row-major (NumPy default) - fast row operations\nimage = np.zeros((1000, 1000))\n# Accessing image[0, :] is fast (contiguous memory)\n# Accessing image[:, 0] is slower (strided memory)\n\n# For column operations, consider Fortran order\nimage_fortran = np.zeros((1000, 1000), order='F')\n# Now image_fortran[:, 0] is fast\n\nWhy this matters:\n\nProcessing images row-by-row? Use default (C-order)\n\nProcessing spectra in columns? Consider Fortran order\n\nMatrix multiplication? NumPy optimizes automatically\n\nReal impact: The wrong memory order can make your code 10x slower for large arrays!\n\n🔍 Check Your Understanding\n\nWhat’s the memory difference between np.zeros(1000), np.ones(1000), and np.empty(1000)? When would you use each?\n\nAnswer\n\nAll three allocate the same amount of memory (8000 bytes for float64), but:\n\nnp.zeros(): Allocates AND sets all values to 0 (slower, safe)\n\nnp.ones(): Allocates AND sets all values to 1 (slower, safe)\n\nnp.empty(): Only allocates, doesn’t initialize (fastest, dangerous)\n\nUse cases:\n\nzeros()/ones(): When you need initialized values for accumulation or defaults\n\nempty(): ONLY when you’ll immediately overwrite ALL values, like filling with calculated results\n\nExample where empty() is appropriate:result = np.empty(1000)\nfor i in range(1000):\n    result[i] = expensive_calculation(i)  # Overwrites immediately\n### Saving and Loading Arrays (Connection to Chapter 6)\n\nRemember the file I/O concepts from Chapter 6? NumPy extends them for efficient array storage:\n\n```{code-cell} python\n# Save astronomical data in binary format\nflux_data = np.random.normal(1000, 50, size=1000)\nnp.save('observations.npy', flux_data)  # Binary format, fast\n\n# Load for analysis\nloaded_data = np.load('observations.npy')\nprint(f\"Loaded {len(loaded_data)} measurements\")\n\n# For text format (human-readable but slower)\nnp.savetxt('observations.txt', flux_data[:10], fmt='%.2f')\ntext_data = np.loadtxt('observations.txt')\nprint(f\"Text file sample: {text_data[:3]}\")\n```\n\n## 7.3 Random Numbers for Monte Carlo Simulations\n\n{margin} **Monte Carlo**\nA computational technique using random sampling to solve problems that might be deterministic in principle.\n\nScientific computing often requires random data for Monte Carlo simulations, noise modeling, and statistical testing. NumPy's random module provides a comprehensive suite of distributions essential for computational astrophysics:\n\n```{code-cell} python\n# ALWAYS set seed for reproducibility in scientific code!\nnp.random.seed(42)\n\n# Uniform distribution: random positions, phases\n# Generate random sky coordinates\nn_stars = 1000\nra = np.random.uniform(0, 360, n_stars)  # Right Ascension in degrees\ndec = np.random.uniform(-90, 90, n_stars)  # Declination in degrees\nprint(f\"Generated {n_stars} random sky positions\")\nprint(f\"RA range: [{ra.min():.1f}, {ra.max():.1f}]°\")\n\n# Random phases for periodic variables\nphases = np.random.uniform(0, 2*np.pi, n_stars)\nprint(f\"Phase range: [0, 2π]\")\n```\n\n```{code-cell} python\n# Normal (Gaussian) distribution: measurement errors, thermal noise\n# Simulate CCD readout noise\nmean_counts = 1000  # electrons\nread_noise = 10  # electrons RMS\nn_pixels = 10000\n\npixel_values = np.random.normal(mean_counts, read_noise, n_pixels)\nprint(f\"Pixel statistics:\")\nprint(f\"  Mean: {pixel_values.mean():.1f} (expected: {mean_counts})\")\nprint(f\"  Std: {pixel_values.std():.1f} (expected: {read_noise})\")\nprint(f\"  SNR: {pixel_values.mean()/pixel_values.std():.1f}\")\n```\n\n```{code-cell} python\n# Poisson distribution: photon counting, radioactive decay\n# Simulate photon arrival statistics\nmean_photons = 100  # photons per exposure\nn_exposures = 1000\n\nphoton_counts = np.random.poisson(mean_photons, n_exposures)\nprint(f\"Photon counting statistics:\")\nprint(f\"  Mean: {photon_counts.mean():.1f} (expected: {mean_photons})\")\nprint(f\"  Std: {photon_counts.std():.2f} (expected: {np.sqrt(mean_photons):.2f})\")\nprint(f\"  Variance/Mean: {photon_counts.var()/photon_counts.mean():.2f} (should be ~1)\")\n```\n\n### Advanced Distributions for Astrophysics\n\n```{code-cell} python\n# Exponential distribution: time between events, decay processes\n# Simulate time between supernova detections (days)\nmean_interval = 30  # days\nn_events = 100\nintervals = np.random.exponential(mean_interval, n_events)\nprint(f\"Supernova intervals: mean = {intervals.mean():.1f} days\")\n\n# Power-law distribution (using Pareto for x_min=1)\n# Initial Mass Function approximation\nalpha = 2.35  # Salpeter IMF slope\nn_stars = 1000\n# Generate masses from 0.1 to 100 solar masses\nx = np.random.pareto(alpha, n_stars) + 1  # Pareto starts at 1\nmasses = 0.1 * x  # Scale to start at 0.1 solar masses\nmasses = masses[masses < 100]  # Truncate at 100 solar masses\nprint(f\"Generated {len(masses)} stellar masses with power-law distribution\")\n```\n\n```{code-cell} python\n# Multivariate normal: correlated parameters\n# Simulate color-magnitude relationship\nmean = [12, 1.0]  # Mean magnitude, mean color (B-V)\n# Covariance matrix: brighter stars tend to be bluer\ncov = [[1.0, -0.3],   # Variance in mag, covariance\n       [-0.3, 0.25]]  # Covariance, variance in color\n\nn_stars = 500\nmag_color = np.random.multivariate_normal(mean, cov, n_stars)\nmagnitudes = mag_color[:, 0]\ncolors = mag_color[:, 1]\n\ncorrelation = np.corrcoef(magnitudes, colors)[0, 1]\nprint(f\"Generated {n_stars} stars with correlated properties\")\nprint(f\"Correlation coefficient: {correlation:.2f} (expected: ~-0.6)\")\n```\n\n### Random Sampling Techniques\n\n```{code-cell} python\n# Random choice: selecting objects from catalogs\nstar_types = np.array(['O', 'B', 'A', 'F', 'G', 'K', 'M'])\n# Probabilities based on stellar statistics\nprobs = np.array([0.00003, 0.0013, 0.006, 0.03, 0.076, 0.121, 0.765])\nprobs = probs / probs.sum()  # Ensure normalization\n\nn_sample = 1000\nsampled_types = np.random.choice(star_types, n_sample, p=probs)\n\n# Count occurrences\nunique, counts = np.unique(sampled_types, return_counts=True)\nfor star_type, count in zip(unique, counts):\n    print(f\"Type {star_type}: {count/n_sample*100:.1f}%\")\n```\n\n```{code-cell} python\n# Random permutation: bootstrap resampling\n# Original dataset\ndata = np.array([23.5, 24.1, 23.8, 24.3, 23.9])\n\n# Bootstrap resampling for error estimation\nn_bootstrap = 1000\nbootstrap_means = []\n\nfor i in range(n_bootstrap):\n    # Resample with replacement\n    resampled = np.random.choice(data, len(data), replace=True)\n    bootstrap_means.append(resampled.mean())\n\nbootstrap_means = np.array(bootstrap_means)\nprint(f\"Original mean: {data.mean():.2f}\")\nprint(f\"Bootstrap mean: {bootstrap_means.mean():.2f}\")\nprint(f\"Bootstrap std error: {bootstrap_means.std():.3f}\")\n\n# Shuffle for randomization tests\nshuffled = np.random.permutation(data)\nprint(f\"Original: {data}\")\nprint(f\"Shuffled: {shuffled}\")\n```\n\n:::{admonition} 🌟 Why This Matters: Monte Carlo Markov Chain (MCMC) in Cosmology\n:class: info, important\n\nThe random number generation you're learning powers one of modern cosmology's most important techniques: MCMC sampling for parameter estimation. When analyzing the cosmic microwave background or galaxy surveys, we need to explore vast parameter spaces (often 10+ dimensions) to find the best-fit cosmological model.\n\nMCMC uses random walks through parameter space, with each step drawn from distributions like those above. The Planck satellite mission used MCMC with billions of random samples to determine that the universe is 13.8 billion years old, contains 5% ordinary matter, 27% dark matter, and 68% dark energy — all with unprecedented precision.\n\nYour ability to generate and manipulate random numbers with NumPy is the foundation for these universe-spanning discoveries!","type":"content","url":"/python-numpy-fundamentals-v2#essential-array-creation-functions-for-science","position":17},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl2":"7.4 Array Operations: Vectorization Powers"},"type":"lvl2","url":"/python-numpy-fundamentals-v2#id-7-4-array-operations-vectorization-powers","position":18},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl2":"7.4 Array Operations: Vectorization Powers"},"content":"{margin} Vectorization\nPerforming operations on entire arrays at once rather than using explicit loops.\n\n{margin} Universal Functions\nNumPy functions that operate element-wise on arrays, supporting broadcasting and type casting.\n\nThe true power of NumPy lies in vectorized operations — performing calculations on entire arrays without writing loops:\n\n# Basic arithmetic operates element-wise\na = np.array([1, 2, 3, 4])\nb = np.array([10, 20, 30, 40])\n\nprint(f\"a + b = {a + b}\")      # Element-wise addition\nprint(f\"a * b = {a * b}\")      # Element-wise multiplication\nprint(f\"a ** 2 = {a ** 2}\")    # Element-wise power\nprint(f\"b / a = {b / a}\")      # Element-wise division\n\n# Matrix multiplication uses @ operator\nc = np.array([[1, 2], [3, 4]])\nd = np.array([[5, 6], [7, 8]])\nprint(f\"\\nMatrix multiplication (c @ d):\\n{c @ d}\")\n\n# Compare with list approach (verbose and slow)\na_list = [1, 2, 3, 4]\nb_list = [10, 20, 30, 40]\nresult_list = []\nfor i in range(len(a_list)):\n    result_list.append(a_list[i] + b_list[i])\nprint(f\"\\nList addition: {result_list}\")  # Same result, more code!\n\n","type":"content","url":"/python-numpy-fundamentals-v2#id-7-4-array-operations-vectorization-powers","position":19},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Universal Functions (ufuncs): Optimized Operations","lvl2":"7.4 Array Operations: Vectorization Powers"},"type":"lvl3","url":"/python-numpy-fundamentals-v2#universal-functions-ufuncs-optimized-operations","position":20},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Universal Functions (ufuncs): Optimized Operations","lvl2":"7.4 Array Operations: Vectorization Powers"},"content":"NumPy’s universal functions operate element-wise on arrays with optimized C code:\n\n# Trigonometric functions for coordinate transformations\nangles_deg = np.array([0, 30, 45, 60, 90])\nangles_rad = np.deg2rad(angles_deg)  # Convert to radians\n\nsines = np.sin(angles_rad)\ncosines = np.cos(angles_rad)\n\nprint(f\"Angles (deg): {angles_deg}\")\nprint(f\"sin(θ): {sines}\")\nprint(f\"cos(θ): {cosines}\")\nprint(f\"sin²(θ) + cos²(θ): {sines**2 + cosines**2}\")  # Should all be 1!\n\n# Exponential and logarithm for magnitude scales\nmagnitudes = np.array([0, 1, 2, 5, 10])\nflux_ratios = 10**(-magnitudes/2.5)  # Pogson's equation\nprint(f\"\\nMagnitudes: {magnitudes}\")\nprint(f\"Flux ratios: {flux_ratios}\")\n\n# Verify: magnitude difference = -2.5 * log10(flux ratio)\nrecovered_mags = -2.5 * np.log10(flux_ratios)\nprint(f\"Recovered magnitudes: {recovered_mags}\")\n\n","type":"content","url":"/python-numpy-fundamentals-v2#universal-functions-ufuncs-optimized-operations","position":21},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Array Methods: Built-in Analysis","lvl2":"7.4 Array Operations: Vectorization Powers"},"type":"lvl3","url":"/python-numpy-fundamentals-v2#array-methods-built-in-analysis","position":22},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Array Methods: Built-in Analysis","lvl2":"7.4 Array Operations: Vectorization Powers"},"content":"Arrays come with methods for common statistical operations:\n\n# Create sample data: Gaussian with outliers\nnp.random.seed(42)\ndata = np.random.normal(100, 15, 1000)  # Normal dist: mean=100, std=15\ndata[::100] = 200  # Add outliers every 100th point\n\n# Statistical methods\nprint(f\"Mean: {data.mean():.2f}\")\nprint(f\"Median: {np.median(data):.2f}\")  # More robust to outliers\nprint(f\"Std dev: {data.std():.2f}\")\nprint(f\"Min: {data.min():.2f}, Max: {data.max():.2f}\")\n\n# Find outliers\noutliers = data > 150\nn_outliers = outliers.sum()  # True counts as 1\nprint(f\"Number of outliers (>150): {n_outliers}\")\n\n# Clean data by filtering\nclean_data = data[~outliers]  # ~ means NOT\nprint(f\"Clean mean: {clean_data.mean():.2f}\")\nprint(f\"Clean std: {clean_data.std():.2f}\")\n\n⚠️ Common Bug Alert: Integer Division Trap\n\n# DANGER: Integer arrays can cause unexpected results!\ncounts = np.array([100, 200, 300])  # Default type is int\nnormalized = counts / counts.max()\nprint(f\"Normalized (float result): {normalized}\")\n\n# But watch out for integer division in older NumPy or with //\ninteger_div = counts // 2  # Floor division\nprint(f\"Integer division by 2: {integer_div}\")\n\n# Mixed operations preserve precision\ncounts_int = np.array([100, 200, 300], dtype=int)\nscale = 2.5  # float\nscaled = counts_int / scale  # Result is float!\nprint(f\"Int array / float = {scaled}\")\n\n# Best practice: Use float arrays for scientific calculations\ncounts_safe = np.array([100, 200, 300], dtype=float)\nprint(f\"Safe float array: {counts_safe / counts_safe.max()}\")\n\nAlways use float arrays for scientific calculations unless you specifically need integer arithmetic!\n\n💡 Computational Thinking Box: Algorithmic Complexity and Big-O\n\nPATTERN: Understanding Algorithmic Scaling\n\nDifferent approaches to the same problem can have vastly different performance:# O(n) - Linear time with Python loop\ndef sum_squares_loop(arr):\n    total = 0\n    for x in arr:\n        total += x**2\n    return total\n\n# O(n) - Linear time with NumPy (but ~100x faster!)\ndef sum_squares_numpy(arr):\n    return (arr**2).sum()\n\n# O(n²) - Quadratic time for matrix operations\ndef matrix_multiply_loops(A, B):\n    n = len(A)\n    C = [[0]*n for _ in range(n)]\n    for i in range(n):\n        for j in range(n):\n            for k in range(n):\n                C[i][j] += A[i][k] * B[k][j]\n    return C\n\n# O(n^2.37) - Optimized with NumPy (Strassen algorithm)\ndef matrix_multiply_numpy(A, B):\n    return A @ B\n\nReal impact: For a 1000×1000 matrix:\n\nNested loops: ~10 seconds\n\nNumPy: ~10 milliseconds\n\nThat’s the difference between waiting and real-time processing!","type":"content","url":"/python-numpy-fundamentals-v2#array-methods-built-in-analysis","position":23},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl2":"7.5 Indexing and Slicing: Data Selection Mastery"},"type":"lvl2","url":"/python-numpy-fundamentals-v2#id-7-5-indexing-and-slicing-data-selection-mastery","position":24},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl2":"7.5 Indexing and Slicing: Data Selection Mastery"},"content":"{margin} Boolean Masking\nUsing boolean arrays to select elements that meet certain conditions.\n\nNumPy’s indexing extends Python’s list indexing with powerful new capabilities:\n\n# 1D indexing (like lists but more powerful)\nspectrum = np.array([1.0, 1.2, 1.5, 1.3, 1.1, 0.9, 0.8])\nprint(f\"Full spectrum: {spectrum}\")\nprint(f\"First element: {spectrum[0]}\")\nprint(f\"Last element: {spectrum[-1]}\")  # Negative indexing works!\nprint(f\"Middle section: {spectrum[2:5]}\")\n\n# Fancy indexing - select multiple specific indices\nimportant_indices = [0, 2, 4, 6]\nselected = spectrum[important_indices]\nprint(f\"Selected wavelengths: {selected}\")\n\n# 2D indexing - like accessing matrix elements\nimage = np.array([[10, 20, 30],\n                  [40, 50, 60],\n                  [70, 80, 90]])\nprint(f\"2D array:\\n{image}\")\nprint(f\"Element at row 1, col 2: {image[1, 2]}\")  # Note: comma notation!\nprint(f\"Entire row 1: {image[1, :]}\")\nprint(f\"Entire column 2: {image[:, 2]}\")\nprint(f\"Sub-image: \\n{image[0:2, 1:3]}\")\n\n","type":"content","url":"/python-numpy-fundamentals-v2#id-7-5-indexing-and-slicing-data-selection-mastery","position":25},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Boolean Masking: The Power Tool","lvl2":"7.5 Indexing and Slicing: Data Selection Mastery"},"type":"lvl3","url":"/python-numpy-fundamentals-v2#boolean-masking-the-power-tool","position":26},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Boolean Masking: The Power Tool","lvl2":"7.5 Indexing and Slicing: Data Selection Mastery"},"content":"Boolean masking is one of NumPy’s most powerful features for data filtering:\n\n# Stellar catalog example\nstars_mag = np.array([8.2, 12.5, 6.1, 15.3, 9.7, 11.2, 5.5])\nstars_color = np.array([0.5, 1.2, 0.3, 1.8, 0.7, 1.0, 0.2])  # B-V color\n\n# Create boolean masks\nbright_mask = stars_mag < 10  # True where magnitude < 10\nblue_mask = stars_color < 0.6  # True where B-V < 0.6 (blue stars)\n\nprint(f\"Bright stars mask: {bright_mask}\")\nprint(f\"Bright star magnitudes: {stars_mag[bright_mask]}\")\n\n# Combine conditions with & (not 'and'), | (not 'or')\nbright_and_blue = (stars_mag < 10) & (stars_color < 0.6)\nprint(f\"Bright AND blue: {stars_mag[bright_and_blue]}\")\n\n# Count matching objects\nn_bright = bright_mask.sum()  # True = 1, False = 0\nprint(f\"Number of bright stars: {n_bright}\")\n\n","type":"content","url":"/python-numpy-fundamentals-v2#boolean-masking-the-power-tool","position":27},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Essential Array Functions","lvl2":"7.5 Indexing and Slicing: Data Selection Mastery"},"type":"lvl3","url":"/python-numpy-fundamentals-v2#essential-array-functions","position":28},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Essential Array Functions","lvl2":"7.5 Indexing and Slicing: Data Selection Mastery"},"content":"\n\n# where: conditional operations and finding indices\ndata = np.array([1, 5, 3, 8, 2, 9, 4])\nhigh_indices = np.where(data > 5)[0]  # Returns tuple, we want first element\nprint(f\"Indices where data > 5: {high_indices}\")\nprint(f\"Values at those indices: {data[high_indices]}\")\n\n# Conditional replacement\nclipped = np.where(data > 5, 5, data)  # Clip values above 5\nprint(f\"Clipped data: {clipped}\")\n\n# clip: cleaner way to bound values\nclipped_better = np.clip(data, 2, 8)  # Clip to range [2, 8]\nprint(f\"Clipped with np.clip: {clipped_better}\")\n\n# unique: find unique values in catalogs\nstar_types = np.array(['G', 'K', 'M', 'G', 'K', 'G', 'M', 'M', 'K'])\nunique_types, counts = np.unique(star_types, return_counts=True)\nprint(f\"Unique star types: {unique_types}\")\nprint(f\"Counts: {counts}\")\n\n# histogram: bin data for analysis\nmagnitudes = np.random.normal(12, 2, 1000)\nhist, bins = np.histogram(magnitudes, bins=20)\nprint(f\"Histogram has {len(hist)} bins\")\nprint(f\"Bin edges from {bins[0]:.1f} to {bins[-1]:.1f}\")\nprint(f\"Peak bin has {hist.max()} stars\")\n\n🔍 Check Your Understanding\n\nWhat’s the difference between np.linspace(0, 10, 11) and np.arange(0, 11, 1)?\n\nAnswer\n\nBoth create arrays from 0 to 10, but:\n\nnp.linspace(0, 10, 11) creates exactly 11 evenly-spaced points including both endpoints: [0, 1, 2, ..., 10]\n\nnp.arange(0, 11, 1) creates points from 0 up to (but not including) 11 with step 1: [0, 1, 2, ..., 10]\n\nIn this case they’re equivalent, but:\n\nnp.linspace(0, 10, 20) gives 20 points with fractional spacing\n\nnp.arange(0, 10.5, 0.5) gives points with exact 0.5 steps\n\nUse linspace when you need a specific number of points, arange when you need a specific step size.\n:::{admonition} 🌟 Why This Matters: Finding Exoplanets with Boolean Masking\n:class: info, important\n\nThe Kepler Space Telescope discovered over 2,600 exoplanets by monitoring the brightness of 150,000 stars continuously for four years. Finding planets in this data required sophisticated boolean masking with NumPy.\n\nWhen a planet transits its star, the brightness drops by typically 0.01% - 1%. But the data is noisy, with stellar flares, cosmic rays, and instrumental effects. Here's a simplified version of the detection algorithm:\n\n```python\n# Simulated light curve data\ntime = np.linspace(0, 30, 1000)  # 30 days\nflux = np.ones_like(time) + np.random.normal(0, 0.001, 1000)  # Noise\n\n# Add transits every 3.5 days (period), 0.1 day duration, 1% deep\nperiod, duration, depth = 3.5, 0.1, 0.01\nin_transit = ((time % period) < duration)\nflux[in_transit] -= depth\n\n# Detection using boolean masking\nmedian_flux = np.median(flux)\nthreshold = median_flux - 3 * flux.std()  # 3-sigma detection\ntransit_candidates = flux < threshold\nn_transits = np.diff(np.where(transit_candidates)[0] > 1).sum()\n\nprint(f\"Found {n_transits} transit events\")\n```\n\nThis technique, scaled up with more sophisticated statistics, is how we've discovered thousands of worlds orbiting other stars!","type":"content","url":"/python-numpy-fundamentals-v2#essential-array-functions","position":29},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl2":"7.6 Broadcasting: NumPy’s Secret Superpower"},"type":"lvl2","url":"/python-numpy-fundamentals-v2#id-7-6-broadcasting-numpys-secret-superpower","position":30},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl2":"7.6 Broadcasting: NumPy’s Secret Superpower"},"content":"{margin} Broadcasting\nNumPy’s ability to perform operations on arrays of different shapes by automatically expanding dimensions.\n\nBroadcasting allows NumPy to perform operations on arrays of different shapes, eliminating the need for explicit loops or array duplication:\n\n# Simple broadcasting: scalar with array\narr = np.array([1, 2, 3, 4])\nresult = arr + 10  # 10 is \"broadcast\" to [10, 10, 10, 10]\nprint(f\"Array + scalar: {result}\")\n\n# Row vector + column vector = matrix\nrow = np.array([[1, 2, 3]])      # Shape (1, 3)\ncol = np.array([[10], [20], [30]])  # Shape (3, 1)\nmatrix = row + col  # Broadcasting creates (3, 3) result\nprint(f\"Row shape: {row.shape}\")\nprint(f\"Column shape: {col.shape}\")\nprint(f\"Result:\\n{matrix}\")\nprint(f\"Result shape: {matrix.shape}\")\n\n","type":"content","url":"/python-numpy-fundamentals-v2#id-7-6-broadcasting-numpys-secret-superpower","position":31},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Broadcasting Rules Visualization","lvl2":"7.6 Broadcasting: NumPy’s Secret Superpower"},"type":"lvl3","url":"/python-numpy-fundamentals-v2#broadcasting-rules-visualization","position":32},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Broadcasting Rules Visualization","lvl2":"7.6 Broadcasting: NumPy’s Secret Superpower"},"content":"graph TD\n    A[Array A: Shape 3,1] --> D{Compatible?}\n    B[Array B: Shape 1,4] --> D\n    D -->|Yes| E[Result: Shape 3,4]\n    \n    F[Rule 1: Compare dimensions right to left]\n    G[Rule 2: Dimensions compatible if equal or 1]\n    H[Rule 3: Missing dimensions treated as 1]\n    \n    style E fill:#90EE90\n    style D fill:#FFE4B5\n\nBroadcasting follows simple rules:\n\nArrays are compatible if dimensions are equal or one is 1\n\nMissing dimensions are treated as 1\n\nArrays are stretched along dimensions of size 1\n\n# Practical example: Normalize each row of a matrix\ndata = np.array([[1, 2, 3],\n                 [4, 5, 6],\n                 [7, 8, 9]], dtype=float)\n\n# Calculate row means (shape: 3,)\nrow_means = data.mean(axis=1)  # axis=1 means along columns\nprint(f\"Row means: {row_means}\")\n\n# To subtract row means from each row, we need to reshape\nrow_means_reshaped = row_means.reshape(-1, 1)  # Shape: (3, 1)\nnormalized = data - row_means_reshaped  # Broadcasting!\nprint(f\"Normalized data:\\n{normalized}\")\nprint(f\"New row means: {normalized.mean(axis=1)}\")  # Should be ~0\n\n","type":"content","url":"/python-numpy-fundamentals-v2#broadcasting-rules-visualization","position":33},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Real-World Broadcasting: CCD Image Calibration","lvl2":"7.6 Broadcasting: NumPy’s Secret Superpower"},"type":"lvl3","url":"/python-numpy-fundamentals-v2#real-world-broadcasting-ccd-image-calibration","position":34},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Real-World Broadcasting: CCD Image Calibration","lvl2":"7.6 Broadcasting: NumPy’s Secret Superpower"},"content":"\n\n# Flat-field correction for CCD images\nnp.random.seed(42)\n\n# Simulate CCD data\nraw_image = np.random.poisson(1000, size=(100, 100))  # 100x100 pixels\ndark_current = np.full((100, 100), 50)  # Uniform dark current\nflat_field = np.ones((100, 100))\nflat_field[:, :50] = 0.9  # Left half less sensitive\n\n# Calibrate image using broadcasting\ncalibrated = (raw_image - dark_current) / flat_field\n\nprint(f\"Raw image mean: {raw_image.mean():.1f}\")\nprint(f\"Calibrated mean: {calibrated.mean():.1f}\")\nprint(f\"Left half sensitivity: {calibrated[:, :50].mean():.1f}\")\nprint(f\"Right half sensitivity: {calibrated[:, 50:].mean():.1f}\")\n\n⚠️ Common Bug Alert: Broadcasting Shape Mismatch\n\n# Common mistake: incompatible shapes\na = np.array([1, 2, 3])  # Shape: (3,)\nb = np.array([1, 2])     # Shape: (2,)\n\n# This will fail!\ntry:\n    result = a + b\nexcept ValueError as e:\n    print(f\"Error: {e}\")\n\n# Solutions:\n# 1. Pad the shorter array\nb_padded = np.append(b, 0)  # Now shape (3,)\nprint(f\"Padded addition: {a + b_padded}\")\n\n# 2. Use different shapes that broadcast\na_col = a.reshape(-1, 1)  # Shape: (3, 1)\nb_row = b.reshape(1, -1)  # Shape: (1, 2)\nbroadcasted = a_col + b_row  # Shape: (3, 2)\nprint(f\"Broadcasted result:\\n{broadcasted}\")\n\nAlways check shapes when debugging broadcasting errors!","type":"content","url":"/python-numpy-fundamentals-v2#real-world-broadcasting-ccd-image-calibration","position":35},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl2":"7.7 Array Manipulation: Reshaping Your Data"},"type":"lvl2","url":"/python-numpy-fundamentals-v2#id-7-7-array-manipulation-reshaping-your-data","position":36},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl2":"7.7 Array Manipulation: Reshaping Your Data"},"content":"{margin} View\nA new array object that shares data with the original array.\n\n{margin} Copy\nA new array with its own data, independent of the original.\n\nNumPy provides powerful tools for reorganizing array data:\n\n# Reshape: Change dimensions without changing data\ndata = np.arange(12)\nprint(f\"Original: {data}\")\n\n# Reshape to 2D\nmatrix = data.reshape(3, 4)\nprint(f\"As 3x4 matrix:\\n{matrix}\")\n\n# Reshape to 3D\ncube = data.reshape(2, 2, 3)\nprint(f\"As 2x2x3 cube:\\n{cube}\")\n\n# Use -1 to infer dimension\nauto_reshape = data.reshape(3, -1)  # NumPy figures out the 4\nprint(f\"Auto-reshape to 3x?:\\n{auto_reshape}\")\n\n# Flatten back to 1D\nflattened = matrix.flatten()\nprint(f\"Flattened: {flattened}\")\n\n","type":"content","url":"/python-numpy-fundamentals-v2#id-7-7-array-manipulation-reshaping-your-data","position":37},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Stacking and Splitting Arrays","lvl2":"7.7 Array Manipulation: Reshaping Your Data"},"type":"lvl3","url":"/python-numpy-fundamentals-v2#stacking-and-splitting-arrays","position":38},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Stacking and Splitting Arrays","lvl2":"7.7 Array Manipulation: Reshaping Your Data"},"content":"Combining and separating arrays is common in data analysis:\n\n# Stacking arrays\na = np.array([1, 2, 3])\nb = np.array([4, 5, 6])\nc = np.array([7, 8, 9])\n\n# Vertical stack (row-wise)\nvstacked = np.vstack([a, b, c])\nprint(f\"Vertical stack:\\n{vstacked}\")\n\n# Horizontal stack (column-wise)\nhstacked = np.hstack([a, b, c])\nprint(f\"Horizontal stack: {hstacked}\")\n\n# Concatenate (general purpose)\nconcat_axis0 = np.concatenate([a, b, c])  # Default axis=0\nprint(f\"Concatenated: {concat_axis0}\")\n\n# Stack as columns\ncolumn_stack = np.column_stack([a, b, c])\nprint(f\"Column stack:\\n{column_stack}\")\n\n","type":"content","url":"/python-numpy-fundamentals-v2#stacking-and-splitting-arrays","position":39},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Transpose and Axis Manipulation for Coordinate Systems","lvl2":"7.7 Array Manipulation: Reshaping Your Data"},"type":"lvl3","url":"/python-numpy-fundamentals-v2#transpose-and-axis-manipulation-for-coordinate-systems","position":40},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Transpose and Axis Manipulation for Coordinate Systems","lvl2":"7.7 Array Manipulation: Reshaping Your Data"},"content":"\n\n# Transpose swaps axes - useful for coordinate transformations\n# Example: RA/Dec coordinates to X/Y projections\nra_dec = np.array([[10.5, -25.3],   # Star 1: RA, Dec\n                    [15.2, -30.1],   # Star 2: RA, Dec\n                    [20.8, -22.7]])  # Star 3: RA, Dec\n\n# Transpose to get all RAs and all Decs\ncoords_transposed = ra_dec.T\nprint(f\"Original (each row is a star):\\n{ra_dec}\")\nprint(f\"Transposed (row 0 = all RAs, row 1 = all Decs):\\n{coords_transposed}\")\n\nall_ra = coords_transposed[0]\nall_dec = coords_transposed[1]\nprint(f\"All RA values: {all_ra}\")\nprint(f\"All Dec values: {all_dec}\")\n\n💡 Views vs Copies: Memory Efficiency in Large Datasets\n\nPATTERN: Understanding When NumPy Shares Memory\n\nMany NumPy operations return views, not copies, sharing memory with the original. This is crucial when working with large telescope images or survey data:# Real telescope data scenario\nccd_image = np.zeros((4096, 4096), dtype=np.float32)  # 64 MB\n\n# Views - no extra memory used\nquadrant1 = ccd_image[:2048, :2048]       # View: shares memory\nreshaped = ccd_image.reshape(16, 1024, 1024)  # View: same data\ntransposed = ccd_image.T                  # View: different ordering\n\n# Copies - additional memory allocated\nquadrant1_safe = ccd_image[:2048, :2048].copy()  # Copy: +16 MB\nfancy_indexed = ccd_image[[0, 100, 200]]  # Copy: new array\nmasked = ccd_image[ccd_image > 100]       # Copy: filtered data\n\n# Danger: modifying a view changes the original!\nquadrant1 -= 100  # This modifies ccd_image too!\n\n# Safe approach for calibration\nquadrant1_calibrated = quadrant1.copy()\nquadrant1_calibrated -= 100  # Original ccd_image unchanged\n\nOperations returning views:\n\nBasic slicing: arr[2:8]\n\nReshaping: arr.reshape(2, 5)\n\nTranspose: arr.T\n\nOperations returning copies:\n\nFancy indexing: arr[[1, 3, 5]]\n\nBoolean indexing: arr[arr > 5]\n\nArithmetic: arr + 1\n\nFor the Vera Rubin Observatory processing 20 TB nightly, understanding views vs copies can mean the difference between feasible and impossible memory requirements!","type":"content","url":"/python-numpy-fundamentals-v2#transpose-and-axis-manipulation-for-coordinate-systems","position":41},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl2":"7.8 Essential Scientific Functions"},"type":"lvl2","url":"/python-numpy-fundamentals-v2#id-7-8-essential-scientific-functions","position":42},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl2":"7.8 Essential Scientific Functions"},"content":"NumPy provides specialized functions crucial for scientific computing:","type":"content","url":"/python-numpy-fundamentals-v2#id-7-8-essential-scientific-functions","position":43},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Meshgrid: Creating Coordinate Grids","lvl2":"7.8 Essential Scientific Functions"},"type":"lvl3","url":"/python-numpy-fundamentals-v2#meshgrid-creating-coordinate-grids","position":44},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Meshgrid: Creating Coordinate Grids","lvl2":"7.8 Essential Scientific Functions"},"content":"The meshgrid function is essential for evaluating functions on 2D grids — imagine needing to calculate a value at every point on a detector or create synthetic images:\n\n# Create 2D coordinate grids for evaluation\nx = np.linspace(-2, 2, 5)\ny = np.linspace(-1, 1, 3)\nX, Y = np.meshgrid(x, y)\n\nprint(f\"x coordinates:\\n{X}\")\nprint(f\"y coordinates:\\n{Y}\")\n\n# Why meshgrid? It lets us evaluate f(x,y) without loops!\nZ = X**2 + Y**2  # Paraboloid - evaluated at every grid point\nprint(f\"Function values:\\n{Z}\")\n\n# Common use: creating synthetic star images\nx_pixels = np.linspace(0, 10, 100)\ny_pixels = np.linspace(0, 10, 100)\nX_img, Y_img = np.meshgrid(x_pixels, y_pixels)\n\n# Create 2D Gaussian (like a star PSF)\nsigma = 2.0\nstar_x, star_y = 5.0, 5.0  # Star position\npsf = np.exp(-((X_img - star_x)**2 + (Y_img - star_y)**2) / (2 * sigma**2))\nprint(f\"PSF shape: {psf.shape}, peak: {psf.max():.3f}\")\n\n","type":"content","url":"/python-numpy-fundamentals-v2#meshgrid-creating-coordinate-grids","position":45},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Numerical Differentiation and Integration","lvl2":"7.8 Essential Scientific Functions"},"type":"lvl3","url":"/python-numpy-fundamentals-v2#numerical-differentiation-and-integration","position":46},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Numerical Differentiation and Integration","lvl2":"7.8 Essential Scientific Functions"},"content":"\n\n# gradient: numerical differentiation\n# Useful for finding peaks, trends in light curves\ntime = np.linspace(0, 10, 100)\nflux = np.sin(time) + 0.1 * np.random.randn(100)\n\n# Calculate derivative (rate of change)\nflux_gradient = np.gradient(flux, time)\nprint(f\"Maximum rate of increase: {flux_gradient.max():.3f}\")\nprint(f\"Maximum rate of decrease: {flux_gradient.min():.3f}\")\n\n# Find turning points (where derivative ≈ 0)\nturning_points = np.where(np.abs(flux_gradient) < 0.1)[0]\nprint(f\"Found {len(turning_points)} turning points\")\n\n","type":"content","url":"/python-numpy-fundamentals-v2#numerical-differentiation-and-integration","position":47},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Interpolation with interp","lvl2":"7.8 Essential Scientific Functions"},"type":"lvl3","url":"/python-numpy-fundamentals-v2#interpolation-with-interp","position":48},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Interpolation with interp","lvl2":"7.8 Essential Scientific Functions"},"content":"\n\n# Linear interpolation - crucial for spectra, light curves\nwavelengths_measured = np.array([400, 500, 600, 700])  # nm\nflux_measured = np.array([1.0, 1.5, 1.2, 0.8])\n\n# Interpolate to finer grid\nwavelengths_fine = np.linspace(400, 700, 31)\nflux_interpolated = np.interp(wavelengths_fine, \n                              wavelengths_measured, \n                              flux_measured)\n\nprint(f\"Original: {len(wavelengths_measured)} points\")\nprint(f\"Interpolated: {len(wavelengths_fine)} points\")\nprint(f\"Flux at 550 nm: {np.interp(550, wavelengths_measured, flux_measured):.3f}\")\n\n","type":"content","url":"/python-numpy-fundamentals-v2#interpolation-with-interp","position":49},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Fourier Transforms: Frequency Analysis","lvl2":"7.8 Essential Scientific Functions"},"type":"lvl3","url":"/python-numpy-fundamentals-v2#fourier-transforms-frequency-analysis","position":50},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Fourier Transforms: Frequency Analysis","lvl2":"7.8 Essential Scientific Functions"},"content":"\n\n# Create a signal with multiple frequencies\nt = np.linspace(0, 1, 500)  # 1 second, 500 samples\nsignal = np.sin(2 * np.pi * 5 * t)  # 5 Hz\nsignal += 0.5 * np.sin(2 * np.pi * 10 * t)  # 10 Hz\nsignal += 0.2 * np.random.normal(size=t.shape)  # Noise\n\n# Compute FFT\nfft = np.fft.fft(signal)\nfreqs = np.fft.fftfreq(len(t), t[1] - t[0])\n\n# Power spectrum\npower = np.abs(fft)**2\n\n# Find positive frequencies only\npos_mask = freqs > 0\nfreqs_pos = freqs[pos_mask]\npower_pos = power[pos_mask]\n\n# Find peaks (simplified - use scipy.signal.find_peaks for real work)\npeak_freqs = freqs_pos[power_pos > power_pos.max() / 10]\nprint(f\"Detected frequencies: {peak_freqs[:5]} Hz\")\n\n🌟 Why This Matters: The Vera Rubin Observatory’s Data Challenge\n\nThe Vera Rubin Observatory (formerly LSST) will produce 20 TB of data per night — roughly 200,000 images covering the entire visible sky every three days. Using Python lists instead of NumPy arrays would make this impossible:\n\nMemory Requirements:\n\nPython list of floats: 28 bytes per number (object overhead)\n\nNumPy float32 array: 4 bytes per number\n\nFor one 4k×4k image: Lists need 448 MB, NumPy needs 64 MB\n\nPer night: Lists need 1.4 petabytes, NumPy needs 200 TB\n\nProcessing Speed:\n\nDetecting moving asteroids requires comparing images pixel-by-pixel\n\nWith lists: 200 seconds per image pair\n\nWith NumPy: 0.2 seconds per image pair\n\nThat’s the difference between 5 years and 2 days to process one night!\n\nReal-time Alerts:\nNumPy’s vectorization enables the observatory to:\n\nDetect supernovae within 60 seconds of observation\n\nTrack potentially hazardous asteroids in real-time\n\nAlert astronomers worldwide to transient events while they’re still bright\n\nWithout NumPy’s memory efficiency and speed, modern time-domain astronomy would be impossible. The same library you’re learning makes it feasible to monitor the entire universe for changes every night!","type":"content","url":"/python-numpy-fundamentals-v2#fourier-transforms-frequency-analysis","position":51},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl2":"7.9 Performance and Memory Considerations"},"type":"lvl2","url":"/python-numpy-fundamentals-v2#id-7-9-performance-and-memory-considerations","position":52},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl2":"7.9 Performance and Memory Considerations"},"content":"Understanding NumPy’s performance characteristics helps write efficient code:\n\n# Compare performance: loops vs vectorization\nsize = 100000\na = np.random.randn(size)\nb = np.random.randn(size)\n\n# Method 1: Python loop\nstart = time.perf_counter()\nresult_loop = []\nfor i in range(size):\n    result_loop.append(a[i] * b[i] + a[i]**2)\nloop_time = time.perf_counter() - start\n\n# Method 2: NumPy vectorization\nstart = time.perf_counter()\nresult_vector = a * b + a**2\nvector_time = time.perf_counter() - start\n\nprint(f\"Loop time: {loop_time*1000:.2f} ms\")\nprint(f\"Vector time: {vector_time*1000:.2f} ms\")\nprint(f\"Speedup: {loop_time/vector_time:.1f}x\")\n\n# Memory usage comparison\narray_float64 = np.ones(1000000, dtype=np.float64)\narray_float32 = np.ones(1000000, dtype=np.float32)\narray_float16 = np.ones(1000000, dtype=np.float16)\n\nprint(f\"Memory usage for 1 million elements:\")\nprint(f\"float64: {array_float64.nbytes / 1e6:.1f} MB\")\nprint(f\"float32: {array_float32.nbytes / 1e6:.1f} MB\")\nprint(f\"float16: {array_float16.nbytes / 1e6:.1f} MB\")\n\n","type":"content","url":"/python-numpy-fundamentals-v2#id-7-9-performance-and-memory-considerations","position":53},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Performance Comparison Table","lvl2":"7.9 Performance and Memory Considerations"},"type":"lvl3","url":"/python-numpy-fundamentals-v2#performance-comparison-table","position":54},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Performance Comparison Table","lvl2":"7.9 Performance and Memory Considerations"},"content":"Let’s create concrete performance benchmarks you can run on your system:\n\n\"\"\"\nPerformance comparison: Lists vs NumPy\nRun these on your machine - results vary by hardware!\n\"\"\"\nimport time\n\ndef benchmark_operation(name, list_func, numpy_func, size=100000):\n    \"\"\"Benchmark a single operation.\"\"\"\n    # Setup data\n    list_data = list(range(size))\n    numpy_data = np.arange(size, dtype=float)\n    \n    # Time list operation\n    start = time.perf_counter()\n    list_result = list_func(list_data)\n    list_time = time.perf_counter() - start\n    \n    # Time NumPy operation\n    start = time.perf_counter()\n    numpy_result = numpy_func(numpy_data)\n    numpy_time = time.perf_counter() - start\n    \n    speedup = list_time / numpy_time\n    return list_time * 1000, numpy_time * 1000, speedup\n\n# Define operations to benchmark\noperations = {\n    \"Element-wise square\": (\n        lambda x: [i**2 for i in x],\n        lambda x: x**2\n    ),\n    \"Sum all elements\": (\n        lambda x: sum(x),\n        lambda x: x.sum()\n    ),\n    \"Element-wise sqrt\": (\n        lambda x: [i**0.5 for i in x],\n        lambda x: np.sqrt(x)\n    ),\n    \"Find maximum\": (\n        lambda x: max(x),\n        lambda x: x.max()\n    )\n}\n\nprint(\"Performance Comparison (100,000 elements):\")\nprint(\"-\" * 60)\nprint(f\"{'Operation':<20} {'List (ms)':<12} {'NumPy (ms)':<12} {'Speedup':<10}\")\nprint(\"-\" * 60)\n\nfor name, (list_op, numpy_op) in operations.items():\n    list_ms, numpy_ms, speedup = benchmark_operation(name, list_op, numpy_op)\n    print(f\"{name:<20} {list_ms:<12.2f} {numpy_ms:<12.3f} {speedup:<10.1f}x\")\n\n# Matrix multiplication comparison (smaller size due to O(n³) complexity)\ndef matrix_multiply_lists(A, B):\n    \"\"\"Matrix multiplication with nested lists.\"\"\"\n    n = len(A)\n    C = [[0]*n for _ in range(n)]\n    for i in range(n):\n        for j in range(n):\n            for k in range(n):\n                C[i][j] += A[i][k] * B[k][j]\n    return C\n\n# Create test matrices\nn = 100  # 100x100 matrices\nA_list = [[float(i+j) for j in range(n)] for i in range(n)]\nB_list = [[float(i-j) for j in range(n)] for i in range(n)]\nA_numpy = np.array(A_list)\nB_numpy = np.array(B_list)\n\n# Benchmark matrix multiplication\nstart = time.perf_counter()\nC_list = matrix_multiply_lists(A_list, B_list)\nlist_matmul_time = time.perf_counter() - start\n\nstart = time.perf_counter()\nC_numpy = A_numpy @ B_numpy\nnumpy_matmul_time = time.perf_counter() - start\n\nprint(f\"\\nMatrix Multiplication (100×100):\")\nprint(f\"  Nested loops: {list_matmul_time*1000:.1f} ms\")\nprint(f\"  NumPy (@):    {numpy_matmul_time*1000:.3f} ms\")\nprint(f\"  Speedup:      {list_matmul_time/numpy_matmul_time:.0f}x\")\n\n🔍 Check Your Understanding\n\nWhy is np.empty() faster than np.zeros() but potentially dangerous?\n\nAnswer\n\nnp.empty() allocates memory without initializing values, while np.zeros() allocates and sets all values to 0.# empty is faster but contains garbage\nempty_arr = np.empty(5)\nprint(empty_arr)  # Random values from memory!\n\n# zeros is slower but safe\nzero_arr = np.zeros(5)\nprint(zero_arr)  # Guaranteed [0. 0. 0. 0. 0.]\n\nUse np.empty() only when you’ll immediately overwrite all values. Otherwise, you might accidentally use uninitialized data, leading to non-reproducible bugs!\n### NumPy Gotchas: Top 5 Student Mistakes\n\n:::{admonition} ⚠️ Common NumPy Pitfalls\n:class: warning\n\n1. **Integer division with integer arrays**\n   ```python\n   # Wrong: integer division\n   arr = np.array([1, 2, 3])\n   normalized = arr / arr.max()  # Works but be careful with //\n   \n   # Safe: always use floats for science\n   arr = np.array([1, 2, 3], dtype=float)\n   ```\n\n2. **Views modify the original**\n   ```python\n   data = np.arange(10)\n   view = data[2:8]\n   view[0] = 999  # Changes data[2] to 999!\n   ```\n\n3. **Using `and`/`or` instead of `&`/`|`**\n   ```python\n   # Wrong: Python keywords don't work\n   # mask = (arr > 5) and (arr < 10)  # Error!\n   \n   # Right: Use bitwise operators\n   mask = (arr > 5) & (arr < 10)\n   ```\n\n4. **Misunderstanding axis parameter**\n   ```python\n   matrix = np.array([[1, 2], [3, 4]])\n   matrix.sum(axis=0)  # [4, 6] - sum down columns\n   matrix.sum(axis=1)  # [3, 7] - sum across rows\n   ```\n\n5. **Assuming `np.empty()` contains zeros**\n   ```python\n   # Dangerous: contains random memory\n   arr = np.empty(100)\n   # arr might contain huge values!\n   \n   # Safe: explicitly initialize\n   arr = np.zeros(100)\n   ```","type":"content","url":"/python-numpy-fundamentals-v2#performance-comparison-table","position":55},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl2":"7.10 Practice Exercises"},"type":"lvl2","url":"/python-numpy-fundamentals-v2#id-7-10-practice-exercises","position":56},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl2":"7.10 Practice Exercises"},"content":"","type":"content","url":"/python-numpy-fundamentals-v2#id-7-10-practice-exercises","position":57},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Exercise 1: Photometry Analysis Pipeline","lvl2":"7.10 Practice Exercises"},"type":"lvl3","url":"/python-numpy-fundamentals-v2#exercise-1-photometry-analysis-pipeline","position":58},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Exercise 1: Photometry Analysis Pipeline","lvl2":"7.10 Practice Exercises"},"content":"Build a complete photometry analysis system:\n\nPart A: Basic photometry (5 minutes)\n\n# Convert instrumental magnitudes to calibrated values\ninstrumental_mags = np.array([14.2, 15.1, 13.8, 16.3, 14.7])\n\n# Known standard star (for calibration)\nstandard_instrumental = 14.0\nstandard_true = 12.5  # Known true magnitude\n\n# Calculate zero point\nzero_point = standard_true - standard_instrumental\nprint(f\"Zero point: {zero_point}\")\n\n# Calibrate all magnitudes\ncalibrated_mags = instrumental_mags + zero_point\nprint(f\"Calibrated magnitudes: {calibrated_mags}\")\n\n# Convert to flux (relative to mag 0)\nfluxes = 10**(-calibrated_mags / 2.5)\nprint(f\"Relative fluxes: {fluxes}\")\n\n# Calculate signal-to-noise (assuming Poisson noise)\nexposure_time = 300  # seconds\ncounts = fluxes * 1e6 * exposure_time  # Scale to realistic counts\nsnr = np.sqrt(counts)\nprint(f\"SNR: {snr}\")\n\nPart B: Aperture photometry (10 minutes)\n\n# Create synthetic star image\nx = np.arange(50)\ny = np.arange(50)\nX, Y = np.meshgrid(x, y)\n\n# Add three stars at different positions\nstar_x = [15, 35, 25]\nstar_y = [20, 30, 40]\nstar_flux = [1000, 500, 750]\n\nimage = np.zeros((50, 50))\nfor sx, sy, sf in zip(star_x, star_y, star_flux):\n    # Gaussian PSF for each star\n    r_squared = (X - sx)**2 + (Y - sy)**2\n    image += sf * np.exp(-r_squared / (2 * 2**2))\n\n# Add noise\nnp.random.seed(42)\nimage += np.random.normal(0, 5, image.shape)\n\n# Aperture photometry on first star\ncx, cy = star_x[0], star_y[0]\naperture_radius = 5\n\n# Create circular mask\ndistances = np.sqrt((X - cx)**2 + (Y - cy)**2)\naperture_mask = distances <= aperture_radius\n\n# Measure flux\naperture_flux = image[aperture_mask].sum()\naperture_area = aperture_mask.sum()\nbackground = np.median(image[distances > 20])  # Sky background\n\n# Correct for background\ncorrected_flux = aperture_flux - background * aperture_area\nprint(f\"Aperture flux: {aperture_flux:.1f}\")\nprint(f\"Background: {background:.2f} per pixel\")\nprint(f\"Corrected flux: {corrected_flux:.1f}\")\nprint(f\"True flux: {star_flux[0]}\")\nprint(f\"Recovery: {corrected_flux/star_flux[0]*100:.1f}%\")\n\nPart C: Light curve analysis (15 minutes)\n\n# Generate variable star light curve\ntime = np.linspace(0, 10, 500)  # 10 days, 500 observations\nperiod = 1.7  # days\namplitude = 0.5  # magnitudes\n\n# True light curve\nphase = 2 * np.pi * time / period\ntrue_mag = 12.0 + amplitude * np.sin(phase)\n\n# Add realistic noise\nnp.random.seed(42)\nnoise = np.random.normal(0, 0.05, len(time))\nobserved_mag = true_mag + noise\n\n# Period finding using simplified Lomb-Scargle\ntest_periods = np.linspace(0.5, 5, 1000)\nchi_squared = []\n\n# Test each period (simplified algorithm)\nfor test_period in test_periods:\n    # Fold data at test period\n    test_phase = (time % test_period) / test_period\n    \n    # Bin the folded light curve\n    n_bins = 10\n    binned_mags = []\n    for i in range(n_bins):\n        bin_mask = (test_phase >= i/n_bins) & (test_phase < (i+1)/n_bins)\n        if bin_mask.sum() > 0:\n            binned_mags.append(observed_mag[bin_mask].mean())\n    \n    # Calculate scatter (simplified chi-squared)\n    if len(binned_mags) > 1:\n        chi_squared.append(np.std(binned_mags))\n    else:\n        chi_squared.append(np.inf)\n\n# Find best period\nchi_squared = np.array(chi_squared)\nbest_period = test_periods[chi_squared.argmax()]\n\nprint(f\"True period: {period} days\")\nprint(f\"Found period: {best_period:.3f} days\")\n\n# Verify by folding at found period\nfolded_phase = (time % best_period) / best_period\nphase_order = np.argsort(folded_phase)\n\nprint(f\"Period recovery error: {abs(best_period - period)/period * 100:.1f}%\")\n\n","type":"content","url":"/python-numpy-fundamentals-v2#exercise-1-photometry-analysis-pipeline","position":59},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Exercise 2: Spectral Analysis","lvl2":"7.10 Practice Exercises"},"type":"lvl3","url":"/python-numpy-fundamentals-v2#exercise-2-spectral-analysis","position":60},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Exercise 2: Spectral Analysis","lvl2":"7.10 Practice Exercises"},"content":"Analyze stellar spectra with NumPy:\n\n\"\"\"\nPart 1: Generate synthetic spectrum\n\"\"\"\n# Generate wavelength grid\nwavelength = np.linspace(4000, 7000, 1000)  # Angstroms\n\n# Continuum (blackbody approximation)\nT = 5800  # Solar temperature in K\nh = 6.626e-27  # erg*s\nc = 2.998e10  # cm/s\nk = 1.381e-16  # erg/K\nwave_cm = wavelength * 1e-8\n\n# Planck function (simplified)\ncontinuum = 2 * h * c**2 / wave_cm**5\ncontinuum /= np.exp(h * c / (wave_cm * k * T)) - 1\ncontinuum /= continuum.max()  # Normalize\n\n# Add absorption lines\nlines = [4861, 6563]  # H-beta, H-alpha\nfor line_center in lines:\n    line_depth = 0.3\n    line_width = 5  # Angstroms\n    profile = 1 - line_depth * np.exp(-((wavelength - line_center) / line_width)**2)\n    continuum *= profile\n\n# Add noise\nnp.random.seed(42)\nspectrum = continuum + np.random.normal(0, 0.02, len(wavelength))\n\n\"\"\"\nPart 2: Measure equivalent widths\n\"\"\"\nfor line_center in lines:\n    # Select region around line\n    region_mask = np.abs(wavelength - line_center) < 20\n    wave_region = wavelength[region_mask]\n    flux_region = spectrum[region_mask]\n    \n    # Local continuum (linear fit to edges)\n    edge_mask = (np.abs(wave_region - line_center) > 10)\n    continuum_fit = np.polyfit(wave_region[edge_mask], \n                               flux_region[edge_mask], 1)\n    local_continuum = np.polyval(continuum_fit, wave_region)\n    \n    # Equivalent width\n    normalized = flux_region / local_continuum\n    ew = np.trapz(1 - normalized, wave_region)\n    \n    print(f\"Line at {line_center} Å: EW = {ew:.2f} Å\")\n\n\"\"\"\nPart 3: Measure radial velocity\n\"\"\"\n# Find line shift\nreference_line = 6563  # H-alpha rest wavelength\nline_region = np.abs(wavelength - reference_line) < 10\nline_flux = spectrum[line_region]\nline_wave = wavelength[line_region]\n\n# Find minimum (line center)\nobserved_center = line_wave[line_flux.argmin()]\nshift = observed_center - reference_line\nvelocity = (shift / reference_line) * 3e5  # km/s\n\nprint(f\"Observed line center: {observed_center:.2f} Å\")\nprint(f\"Radial velocity: {velocity:.1f} km/s\")\n\n","type":"content","url":"/python-numpy-fundamentals-v2#exercise-2-spectral-analysis","position":61},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Exercise 3: Debug This!","lvl2":"7.10 Practice Exercises"},"type":"lvl3","url":"/python-numpy-fundamentals-v2#exercise-3-debug-this","position":62},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Exercise 3: Debug This!","lvl2":"7.10 Practice Exercises"},"content":"\n\n\"\"\"\nDebug This! Fixed version with explanations\n\"\"\"\n\ndef analyze_galaxy_redshifts(distances_mpc, velocities_km_s):\n    \"\"\"Calculate Hubble constant from galaxy data.\"\"\"\n    # Original bugs and fixes:\n    \n    # BUG 1 FIXED: Ensure proper operation order\n    # Was: hubble = velocities_km_s / distances_mpc (wrong comment)\n    # Fixed: This is actually correct!\n    \n    # BUG 2 FIXED: Using correct numpy function\n    # Was: distances_log = np.log(distances_mpc)  # Wrong base\n    # Fixed: Use log10 for astronomical calculations\n    distances_log = np.log10(distances_mpc)\n    \n    # BUG 3 FIXED: Ensure float division\n    # Was: normalized = velocities_km_s / velocities_km_s.max()\n    # Fixed: Convert to float to avoid integer division issues\n    normalized = velocities_km_s.astype(float) / velocities_km_s.max()\n    \n    # BUG 4 FIXED: Correct axis for 1D array\n    # Was: residuals = velocities_km_s - np.mean(velocities_km_s, axis=1)\n    # Fixed: No axis needed for 1D array\n    residuals = velocities_km_s - np.mean(velocities_km_s)\n    \n    # Remove outliers for robust estimate\n    mask = (velocities_km_s > 0) & (distances_mpc > 0)\n    clean_vel = velocities_km_s[mask]\n    clean_dist = distances_mpc[mask]\n    \n    # Calculate Hubble constant\n    hubble = clean_vel / clean_dist\n    hubble_mean = np.mean(hubble)\n    \n    return hubble_mean, residuals.std()\n\n# Test data\ndistances = np.array([10, 20, 30, 40, 50], dtype=float)  # Mpc\nvelocities = np.array([700, 1400, 2200, 2800, 3500], dtype=float)  # km/s\n\nH0, scatter = analyze_galaxy_redshifts(distances, velocities)\nprint(f\"Hubble constant: {H0:.1f} km/s/Mpc\")\nprint(f\"Velocity scatter: {scatter:.1f} km/s\")\nprint(f\"(Expected H0 ≈ 70 km/s/Mpc for this simplified data)\")\n\n🌟 Why This Matters: The Hubble Tension\n\nThe code above calculates the Hubble constant, one of cosmology’s most important parameters. Different measurement methods give different values (67 vs 73 km/s/Mpc), creating the “Hubble tension” — one of modern cosmology’s biggest mysteries. Your NumPy skills are the foundation for analyzing the data that might resolve this cosmic puzzle!","type":"content","url":"/python-numpy-fundamentals-v2#exercise-3-debug-this","position":63},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl2":"Main Takeaways"},"type":"lvl2","url":"/python-numpy-fundamentals-v2#main-takeaways","position":64},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl2":"Main Takeaways"},"content":"You’ve just acquired the fundamental tool that transforms Python into a scientific computing powerhouse. NumPy isn’t just a faster way to work with numbers — it’s a different way of thinking about computation. Instead of writing loops that process elements one at a time, you now express mathematical operations on entire datasets at once. This vectorized thinking mirrors how we conceptualize scientific problems: we don’t think about individual photons hitting individual pixels; we think about images, spectra, and light curves as coherent wholes. NumPy lets you write code that matches this conceptual model, making your programs both faster and more readable.\n\nThe performance gains you’ve witnessed — often 10-100x speedups — aren’t just convenient; they’re transformative. Calculations that would take hours with Python lists complete in seconds with NumPy arrays. This speed isn’t achieved through complex optimization tricks but through NumPy’s elegant design: contiguous memory storage, vectorized operations that call optimized C libraries, and broadcasting that eliminates redundant data copying. When you used NumPy to process gravitational wave data or search for exoplanet transits, you experienced the same performance that enables real-time astronomical data analysis at observatories worldwide. The Vera Rubin Observatory’s ability to process 20 TB of data nightly, LIGO’s detection of gravitational waves, and Kepler’s discovery of thousands of exoplanets all depend on the vectorized operations you’ve mastered.\n\nBeyond performance, NumPy provides a vocabulary for scientific computing that’s consistent across the entire Python ecosystem. The array indexing, broadcasting rules, and ufuncs you’ve learned aren’t just NumPy features — they’re the standard interface for numerical computation in Python. When you move on to using SciPy for optimization, Matplotlib for visualization, or Pandas for data analysis, you’ll find they all speak NumPy’s language. This consistency means the effort you’ve invested in understanding NumPy pays dividends across every scientific Python library you’ll ever use. You’ve learned to leverage views for memory efficiency, use boolean masking for sophisticated data filtering, and apply broadcasting to solve complex problems elegantly. These aren’t just programming techniques; they’re computational thinking patterns that will shape how you approach every numerical problem.\n\nYou’ve also mastered the random number generation capabilities essential for Monte Carlo simulations — a cornerstone of modern computational astrophysics. From simulating photon counting statistics with Poisson distributions to modeling measurement errors with Gaussian noise, you now have the tools to create realistic synthetic data and perform statistical analyses. The ability to generate random samples from various distributions, perform bootstrap resampling, and create correlated multivariate data will be crucial in your upcoming projects, especially when you tackle Monte Carlo sampling techniques.\n\nLooking ahead, NumPy arrays will be the primary data structure for the rest of your scientific computing journey. Every image you process, every spectrum you analyze, every simulation you run will flow through NumPy arrays. You now have the tools to replace the math module entirely, using NumPy’s functions that work seamlessly on both scalars and arrays. The concepts you’ve mastered — vectorization, broadcasting, boolean masking — aren’t just NumPy features; they’re the foundation of modern computational science. You’re no longer limited by Python’s native capabilities; you have access to the same computational power that enabled the detection of gravitational waves, the discovery of exoplanets, and the imaging of black holes. In the next chapter, you’ll see how NumPy arrays become the canvas for scientific visualization with Matplotlib, where every plot, image, and diagram starts with the arrays you now know how to create and manipulate.","type":"content","url":"/python-numpy-fundamentals-v2#main-takeaways","position":65},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl2":"Definitions"},"type":"lvl2","url":"/python-numpy-fundamentals-v2#definitions","position":66},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl2":"Definitions"},"content":"Array: NumPy’s fundamental data structure, a grid of values all of the same type, indexed by a tuple of integers.\n\nBoolean Masking: Using an array of boolean values to select elements from another array that meet certain conditions.\n\nBroadcasting: NumPy’s mechanism for performing operations on arrays of different shapes by automatically expanding dimensions according to specific rules.\n\nCGS Units: Centimeter-Gram-Second unit system, traditionally used in astronomy and astrophysics for convenient scaling.\n\nContiguous Memory: Data stored in adjacent memory locations, enabling fast access and efficient cache utilization.\n\nCopy: A new array with its own data, independent of the original array’s memory.\n\ndtype: The data type of array elements, determining memory usage and numerical precision (e.g., float64, int32).\n\nMonte Carlo: A computational technique using random sampling to solve problems that might be deterministic in principle.\n\nndarray: NumPy’s n-dimensional array object, the core data structure for numerical computation.\n\nNumPy: Numerical Python, the fundamental package for scientific computing providing support for arrays and mathematical functions.\n\nShape: The dimensions of an array, given as a tuple indicating the size along each axis.\n\nufunc: Universal function, a NumPy function that operates element-wise on arrays, supporting broadcasting and type casting.\n\nUniversal Functions: NumPy functions that operate element-wise on arrays, supporting broadcasting and type casting.\n\nVectorization: Performing operations on entire arrays at once rather than using explicit loops, leveraging optimized C code.\n\nView: A new array object that shares data with the original array, saving memory but linking modifications.","type":"content","url":"/python-numpy-fundamentals-v2#definitions","position":67},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl2":"Key Takeaways"},"type":"lvl2","url":"/python-numpy-fundamentals-v2#key-takeaways","position":68},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl2":"Key Takeaways"},"content":"✓ NumPy arrays are 10-100x faster than Python lists for numerical operations by using contiguous memory and calling optimized C libraries\n\n✓ Vectorization eliminates explicit loops by operating on entire arrays at once, making code both faster and more readable\n\n✓ NumPy can replace the math module entirely while adding array support — use np.sin() instead of math.sin() everywhere\n\n✓ Broadcasting enables operations on different-shaped arrays by automatically expanding dimensions, eliminating data duplication\n\n✓ Boolean masking provides powerful data filtering using conditions to select array elements, essential for data analysis\n\n✓ Essential creation functions like linspace, logspace, and meshgrid are workhorses for scientific computing\n\n✓ Random number generation from various distributions (uniform, normal, Poisson) enables Monte Carlo simulations\n\n✓ Memory layout matters — row-major vs column-major ordering can cause 10x performance differences\n\n✓ Views share memory while copies are independent — understanding this prevents unexpected data modifications\n\n✓ Array methods provide built-in analysis — .mean(), .std(), .max() operate efficiently along specified axes\n\n✓ NumPy is the foundation of scientific Python — every major package (SciPy, Matplotlib, Pandas) builds on NumPy arrays","type":"content","url":"/python-numpy-fundamentals-v2#key-takeaways","position":69},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl2":"Quick Reference Tables"},"type":"lvl2","url":"/python-numpy-fundamentals-v2#quick-reference-tables","position":70},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl2":"Quick Reference Tables"},"content":"","type":"content","url":"/python-numpy-fundamentals-v2#quick-reference-tables","position":71},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Array Creation Functions","lvl2":"Quick Reference Tables"},"type":"lvl3","url":"/python-numpy-fundamentals-v2#array-creation-functions","position":72},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Array Creation Functions","lvl2":"Quick Reference Tables"},"content":"Function\n\nPurpose\n\nExample\n\nnp.array()\n\nCreate from list\n\nnp.array([1, 2, 3])\n\nnp.zeros()\n\nInitialize with 0s\n\nnp.zeros((3, 4))\n\nnp.ones()\n\nInitialize with 1s\n\nnp.ones((2, 3))\n\nnp.empty()\n\nUninitialized (fast)\n\nnp.empty(5)\n\nnp.arange()\n\nLike Python’s range\n\nnp.arange(0, 10, 2)\n\nnp.linspace()\n\nN evenly spaced\n\nnp.linspace(0, 1, 11)\n\nnp.logspace()\n\nLog-spaced values\n\nnp.logspace(0, 3, 4)\n\nnp.meshgrid()\n\n2D coordinate grids\n\nnp.meshgrid(x, y)","type":"content","url":"/python-numpy-fundamentals-v2#array-creation-functions","position":73},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Essential Array Operations","lvl2":"Quick Reference Tables"},"type":"lvl3","url":"/python-numpy-fundamentals-v2#essential-array-operations","position":74},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Essential Array Operations","lvl2":"Quick Reference Tables"},"content":"Operation\n\nDescription\n\nExample\n\n+, -, *, /\n\nElement-wise arithmetic\n\na + b\n\n**\n\nElement-wise power\n\na ** 2\n\n@\n\nMatrix multiplication\n\na @ b\n\n==, !=, <, >\n\nElement-wise comparison\n\na > 5\n\n&, |, ~\n\nBoolean operations\n\n(a > 0) & (a < 10)\n\n.T\n\nTranspose\n\nmatrix.T\n\n.reshape()\n\nChange dimensions\n\narr.reshape(3, 4)\n\n.flatten()\n\nConvert to 1D\n\nmatrix.flatten()","type":"content","url":"/python-numpy-fundamentals-v2#essential-array-operations","position":75},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Statistical Methods","lvl2":"Quick Reference Tables"},"type":"lvl3","url":"/python-numpy-fundamentals-v2#statistical-methods","position":76},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Statistical Methods","lvl2":"Quick Reference Tables"},"content":"Method\n\nDescription\n\nExample\n\n.mean()\n\nAverage\n\narr.mean() or arr.mean(axis=0)\n\n.std()\n\nStandard deviation\n\narr.std()\n\n.min()/.max()\n\nExtrema\n\narr.min()\n\n.sum()\n\nSum elements\n\narr.sum()\n\n.cumsum()\n\nCumulative sum\n\narr.cumsum()\n\n.argmin()/.argmax()\n\nIndex of extrema\n\narr.argmax()\n\nnp.median()\n\nMedian value\n\nnp.median(arr)\n\nnp.percentile()\n\nPercentiles\n\nnp.percentile(arr, 95)","type":"content","url":"/python-numpy-fundamentals-v2#statistical-methods","position":77},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Random Number Functions","lvl2":"Quick Reference Tables"},"type":"lvl3","url":"/python-numpy-fundamentals-v2#random-number-functions","position":78},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Random Number Functions","lvl2":"Quick Reference Tables"},"content":"Function\n\nDistribution\n\nExample\n\nnp.random.uniform()\n\nUniform\n\nnp.random.uniform(0, 1, 1000)\n\nnp.random.normal()\n\nGaussian/Normal\n\nnp.random.normal(0, 1, 1000)\n\nnp.random.poisson()\n\nPoisson\n\nnp.random.poisson(100, 1000)\n\nnp.random.exponential()\n\nExponential\n\nnp.random.exponential(1.0, 1000)\n\nnp.random.choice()\n\nRandom selection\n\nnp.random.choice(arr, 10)\n\nnp.random.permutation()\n\nShuffle array\n\nnp.random.permutation(arr)\n\nnp.random.seed()\n\nSet random seed\n\nnp.random.seed(42)","type":"content","url":"/python-numpy-fundamentals-v2#random-number-functions","position":79},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Common NumPy Functions","lvl2":"Quick Reference Tables"},"type":"lvl3","url":"/python-numpy-fundamentals-v2#common-numpy-functions","position":80},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl3":"Common NumPy Functions","lvl2":"Quick Reference Tables"},"content":"Function\n\nPurpose\n\nMath Equivalent\n\nnp.sin()\n\nSine\n\nmath.sin()\n\nnp.cos()\n\nCosine\n\nmath.cos()\n\nnp.exp()\n\nExponential\n\nmath.exp()\n\nnp.log()\n\nNatural log\n\nmath.log()\n\nnp.log10()\n\nBase-10 log\n\nmath.log10()\n\nnp.sqrt()\n\nSquare root\n\nmath.sqrt()\n\nnp.abs()\n\nAbsolute value\n\nabs()\n\nnp.round()\n\nRound to integer\n\nround()\n\nnp.where()\n\nConditional selection\n\nN/A\n\nnp.clip()\n\nBound values\n\nN/A\n\nnp.unique()\n\nFind unique values\n\nset()\n\nnp.concatenate()\n\nJoin arrays\n\n+ for lists\n\nnp.gradient()\n\nNumerical derivative\n\nN/A","type":"content","url":"/python-numpy-fundamentals-v2#common-numpy-functions","position":81},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl2":"References"},"type":"lvl2","url":"/python-numpy-fundamentals-v2#references","position":82},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl2":"References"},"content":"Harris, C. R., et al. (2020). Array programming with NumPy. Nature, 585(7825), 357-362. - The definitive NumPy paper describing its design and impact.\n\nOliphant, T. E. (2006). A guide to NumPy. USA: Trelgol Publishing. - The original NumPy book by its creator.\n\nvan der Walt, S., Colbert, S. C., & Varoquaux, G. (2011). The NumPy array: a structure for efficient numerical computation. Computing in Science & Engineering, 13(2), 22-30.\n\nThe LIGO Scientific Collaboration. (2016). Observation of gravitational waves from a binary black hole merger. Physical Review Letters, 116(6), 061102. - Acknowledges NumPy’s role in the discovery.\n\nAbbott, B. P., et al. (2016). GW150914: First results from the search for binary black hole coalescence with Advanced LIGO. Physical Review D, 93(12), 122003. - Details the NumPy-based PyCBC pipeline.\n\nCanton, K., et al. (2014). Implementing a search for aligned-spin neutron star-black hole systems with advanced ground based gravitational wave detectors. Physical Review D, 90(8), 082004. - The PyCBC pipeline paper.\n\nBorucki, W. J., et al. (2010). Kepler planet-detection mission: introduction and first results. Science, 327(5968), 977-980. - Kepler mission overview.\n\nIvezić, Ž., et al. (2019). LSST: From science drivers to reference design and anticipated data products. The Astrophysical Journal, 873(2), 111. - Vera Rubin Observatory data challenges.\n\nVirtanen, P., et al. (2020). SciPy 1.0: fundamental algorithms for scientific computing in Python. Nature Methods, 17(3), 261-272. - Shows NumPy’s foundational role.\n\nHunter, J. D. (2007). Matplotlib: A 2D graphics environment. Computing in Science & Engineering, 9(3), 90-95. - Matplotlib’s dependence on NumPy.\n\nMcKinney, W. (2010). Data structures for statistical computing in python. Proceedings of the 9th Python in Science Conference, 445, 51-56. - Pandas built on NumPy.\n\nPedregosa, F., et al. (2011). Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12, 2825-2830. - scikit-learn’s NumPy foundation.\n\nVanderPlas, J. (2016). Python Data Science Handbook. O’Reilly Media. - Comprehensive NumPy coverage for data science.\n\nJohansson, R. (2019). Numerical Python: Scientific Computing and Data Science Applications with Numpy, SciPy and Matplotlib (2nd ed.). Apress.\n\nHubble, E. (1929). A relation between distance and radial velocity among extra-galactic nebulae. Proceedings of the National Academy of Sciences, 15(3), 168-173. - The original Hubble constant paper referenced in exercises.","type":"content","url":"/python-numpy-fundamentals-v2#references","position":83},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl2":"Next Chapter Preview"},"type":"lvl2","url":"/python-numpy-fundamentals-v2#next-chapter-preview","position":84},{"hierarchy":{"lvl1":"Chapter 7: NumPy - The Foundation of Scientific Computing","lvl2":"Next Chapter Preview"},"content":"In Chapter 8: Matplotlib - Visualizing Your Universe, you’ll discover how to transform the NumPy arrays you’ve mastered into publication-quality visualizations. You’ll learn to create everything from simple line plots to complex multi-panel figures displaying astronomical data. Using the same NumPy arrays you’ve been working with, you’ll visualize spectra with proper wavelength scales, create color-magnitude diagrams from stellar catalogs, display telescope images with world coordinate systems, and generate the kinds of plots that appear in research papers. You’ll master customization techniques to control every aspect of your figures, from axis labels with LaTeX formatting to colormaps optimized for astronomical data. The NumPy operations you’ve learned — slicing for zooming into data, masking for highlighting specific objects, and meshgrid for creating coordinate systems — become the foundation for creating compelling scientific visualizations. Most importantly, you’ll understand how NumPy and Matplotlib work together as an integrated system, with NumPy handling the computation and Matplotlib handling the visualization, forming the core workflow that will carry you through your entire career in computational astrophysics!","type":"content","url":"/python-numpy-fundamentals-v2#next-chapter-preview","position":85},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe"},"type":"lvl1","url":"/matplotlib-fundamentals-v1","position":0},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe"},"content":"","type":"content","url":"/matplotlib-fundamentals-v1","position":1},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl2":"Learning Objectives"},"type":"lvl2","url":"/matplotlib-fundamentals-v1#learning-objectives","position":2},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl2":"Learning Objectives"},"content":"By the end of this chapter, you will be able to:\n\nCreate publication-quality scientific plots using Matplotlib’s object-oriented interface\n\nMaster the anatomy of a figure: axes, subplots, labels, and legends\n\nVisualize astronomical data including light curves, spectra, and images\n\nChoose appropriate scaling (linear, log, semilog) for different data relationships\n\nExperiment iteratively to find the most revealing visualization\n\nBuild reusable plotting modules for common astronomical visualizations\n\nApply appropriate color maps and scaling for different types of data\n\nExport figures in various formats with proper resolution and sizing","type":"content","url":"/matplotlib-fundamentals-v1#learning-objectives","position":3},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl2":"Prerequisites Check"},"type":"lvl2","url":"/matplotlib-fundamentals-v1#prerequisites-check","position":4},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl2":"Prerequisites Check"},"content":"Before starting this chapter, verify you can:\n\nCreate and manipulate NumPy arrays (Chapter 7)\n\nPerform array operations and indexing (Chapter 7)\n\nUse array methods like mean(), std(), max() (Chapter 7)\n\nGenerate random numbers with NumPy (Chapter 7)\n\nWork with 2D arrays and meshgrid (Chapter 7)\n\nRead and write files (Chapter 6)","type":"content","url":"/matplotlib-fundamentals-v1#prerequisites-check","position":5},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl3":"Self-Assessment Diagnostic","lvl2":"Prerequisites Check"},"type":"lvl3","url":"/matplotlib-fundamentals-v1#self-assessment-diagnostic","position":6},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl3":"Self-Assessment Diagnostic","lvl2":"Prerequisites Check"},"content":"Test your readiness by predicting the outputs and identifying any issues:\n\nimport numpy as np\n\n# Question 1: What shape will this array have?\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n# Shape of y: _______\n\n# Question 2: What will this boolean mask select?\ndata = np.array([1, 5, 3, 8, 2, 9])\nmask = data > 5\nselected = data[mask]\n# selected contains: _______\n\n# Question 3: What's wrong with this code?\n# matrix = np.array([[1, 2], [3, 4]])\n# result = matrix + [10, 20, 30]  # Will this work?\n# Your answer: _______\n\n# Question 4: What will this create?\nX, Y = np.meshgrid(np.arange(5), np.arange(3))\n# Shape of X and Y: _______\n\nSelf-Assessment Answers\n\nShape of y: (100,) - 1D array with 100 elements\n\nselected contains: [8, 9] - values greater than 5\n\nBroadcasting error - shapes (2,2) and (3,) are incompatible\n\nShape of X and Y: Both are (3, 5) - 2D grids\n\nIf you got all four correct, you’re ready for Matplotlib! If not, review Chapter 7.","type":"content","url":"/matplotlib-fundamentals-v1#self-assessment-diagnostic","position":7},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl2":"Chapter Overview"},"type":"lvl2","url":"/matplotlib-fundamentals-v1#chapter-overview","position":8},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl2":"Chapter Overview"},"content":"Data without visualization is like a telescope without an eyepiece — you might have collected photons from distant galaxies, but you can’t see the universe they reveal. Every major astronomical discovery of the past century has been communicated through carefully crafted visualizations: Hubble’s plot showing the expanding universe (Hubble 1929), Hertzsprung and Russell’s diagram revealing stellar evolution, the power spectrum of the cosmic microwave background proving inflation theory. These weren’t just pretty pictures; they were visual arguments that changed our understanding of the cosmos. Matplotlib (Hunter 2007), the foundational plotting library for scientific Python, gives you the power to create these kinds of transformative visualizations, turning your NumPy arrays into insights that can be shared, published, and understood.\n\nBut here’s what many tutorials won’t tell you: Matplotlib isn’t just a plotting library — it’s an artist’s studio. Like an artist selecting brushes, canvases, and colors, you’ll learn to choose plot types, figure sizes, and colormaps that best express your data’s story. Creating a great visualization isn’t about following rigid rules; it’s about experimentation, iteration, and developing an aesthetic sense for what works. You’ll discover that the difference between a confusing plot and a revealing one often comes down to trying different scales — linear versus logarithmic, different color mappings, or simply adjusting the aspect ratio. This chapter embraces that experimental nature, teaching you not just the mechanics of plotting but the art of visual exploration. You’ll learn to approach each dataset as a unique challenge, trying multiple visualizations until you find the one that makes the patterns jump off the page.\n\nThis chapter introduces Matplotlib’s two main interfaces — pyplot for quick exploration and the object-oriented API for full control — but focuses on the latter because it’s what you’ll use for research. You’ll master the anatomy of a figure, understanding how figures contain axes, how axes contain plots, and how every element can be customized. You’ll learn the astronomical visualization canon: light curves that reveal exoplanets, spectra that encode stellar chemistry, color-magnitude diagrams that map stellar populations, and images that capture the structure of galaxies. Most crucially, you’ll develop visualization taste — the ability to choose the right plot type, scale, and layout to honestly and effectively communicate your scientific findings. By the chapter’s end, you won’t just make plots; you’ll craft visual narratives that can stand alongside those historic diagrams that revolutionized astronomy. And you’ll have built your own library of plotting functions, turning common visualizations into single function calls that encode your hard-won knowledge about what works.\n\n📚 Essential Resource: Matplotlib Documentation\n\nMatplotlib is vast, and this chapter covers the essential ~20% you’ll use 80% of the time. The official documentation at https://​matplotlib​.org/ is your comprehensive reference for:\n\nGallery of examples: \n\nhttps://​matplotlib​.org​/stable​/gallery​/index​.html\n\nDetailed API reference: \n\nhttps://​matplotlib​.org​/stable​/api​/index​.html\n\nTutorials for specific plot types\n\nColormaps reference: \n\nhttps://​matplotlib​.org​/stable​/tutorials​/colors​/colormaps​.html\n\nPro tip: The Matplotlib gallery is incredibly valuable — find a plot similar to what you want, then adapt its code. Every plot in the gallery includes complete, downloadable source code. When you see a beautiful plot in a paper and wonder “How did they do that?”, the gallery often has the answer.\n\nEssential bookmark: The “Anatomy of a Figure” guide at \n\nhttps://​matplotlib​.org​/stable​/tutorials​/introductory​/usage​.html — keep this open while learning!","type":"content","url":"/matplotlib-fundamentals-v1#chapter-overview","position":9},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl2":"8.1 Matplotlib as Your Artistic Medium"},"type":"lvl2","url":"/matplotlib-fundamentals-v1#id-8-1-matplotlib-as-your-artistic-medium","position":10},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl2":"8.1 Matplotlib as Your Artistic Medium"},"content":"pyplot\n\nMatplotlib’s MATLAB-like procedural interface for quick plotting.\n\nObject-Oriented API\n\nMatplotlib’s powerful interface providing full control over every plot element.\n\nFigure\n\nThe overall container for all plot elements, like a canvas.\n\nAxes\n\nThe plotting area within a figure where data is visualized.\n\nExperimentation\n\nThe iterative process of trying different visualizations to find the most revealing representation.\n\nBefore we dive into technical details, let’s establish a fundamental principle: creating visualizations is an inherently creative act. You’re not just displaying data; you’re making countless aesthetic choices that affect how your message is received. Consider these two philosophies:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate some data - a damped oscillation\ntime = np.linspace(0, 10, 1000)\nsignal = np.exp(-time/3) * np.sin(2 * np.pi * time)\n\n# Approach 1: The Default Plot (technically correct but uninspiring)\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\nax1.plot(time, signal)\nax1.set_title('Default: Technically Correct')\n\n# Approach 2: The Artistic Plot (same data, different choices)\nax2.plot(time, signal, color='#2E86AB', linewidth=2, alpha=0.8)\nax2.fill_between(time, signal, alpha=0.2, color='#2E86AB')\nax2.axhline(y=0, color='#A23B72', linestyle='-', linewidth=0.5, alpha=0.5)\nax2.set_xlabel('Time (s)', fontsize=11, fontweight='light')\nax2.set_ylabel('Amplitude', fontsize=11, fontweight='light')\nax2.set_title('Artistic: Same Data, Better Story', fontsize=13, fontweight='bold')\nax2.grid(True, alpha=0.15, linestyle='-', linewidth=0.5)\nax2.spines['top'].set_visible(False)\nax2.spines['right'].set_visible(False)\n\nplt.tight_layout()\nplt.show()\n\nBoth plots show the same data, but the second one makes deliberate choices about color, transparency, and styling that make it more engaging. This is what we mean by being an artist with Matplotlib — every element is under your control, and those choices matter.","type":"content","url":"/matplotlib-fundamentals-v1#id-8-1-matplotlib-as-your-artistic-medium","position":11},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl3":"The Experimentation Mindset","lvl2":"8.1 Matplotlib as Your Artistic Medium"},"type":"lvl3","url":"/matplotlib-fundamentals-v1#the-experimentation-mindset","position":12},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl3":"The Experimentation Mindset","lvl2":"8.1 Matplotlib as Your Artistic Medium"},"content":"Creating effective visualizations requires experimentation. You rarely get it right on the first try. Here’s a realistic workflow:\n\n# Real data: stellar magnitudes with a power-law distribution\nnp.random.seed(42)\nmasses = np.random.pareto(2.35, 1000) + 0.1  # Stellar IMF (Salpeter 1955)\nluminosities = masses ** 3.5  # Mass-luminosity relation\nluminosities += np.random.normal(0, 0.1 * luminosities)  # Add scatter\n\n# Try different visualizations to find what reveals the pattern best\nfig, axes = plt.subplots(2, 3, figsize=(12, 8))\n\n# Attempt 1: Simple scatter\naxes[0, 0].scatter(masses, luminosities, s=1, alpha=0.5)\naxes[0, 0].set_title('Linear Scale: Pattern Hidden')\naxes[0, 0].set_xlabel('Mass [$M_☉$]')\naxes[0, 0].set_ylabel('Luminosity [$L_☉$]')\n\n# Attempt 2: Log-log reveals power law!\naxes[0, 1].loglog(masses, luminosities, '.', markersize=2, alpha=0.5)\naxes[0, 1].set_title('Log-Log: Power Law Revealed!')\naxes[0, 1].set_xlabel('Mass [M_☉]')\naxes[0, 1].set_ylabel('Luminosity [L_☉]')\n\n# Attempt 3: Semilog-x (wrong choice for this data)\naxes[0, 2].semilogx(masses, luminosities, '.', markersize=2, alpha=0.5)\naxes[0, 2].set_title('Semilog-X: Not Helpful Here')\naxes[0, 2].set_xlabel('Mass [M_☉]')\naxes[0, 2].set_ylabel('Luminosity [L_☉]')\n\n# Attempt 4: 2D histogram for density\nh = axes[1, 0].hist2d(np.log10(masses), np.log10(luminosities), \n                       bins=30, cmap='YlOrRd')\naxes[1, 0].set_title('2D Histogram: Shows Density')\naxes[1, 0].set_xlabel('log(Mass)')\naxes[1, 0].set_ylabel('log(Luminosity)')\n\n# Attempt 5: Hexbin for large datasets\naxes[1, 1].hexbin(masses, luminosities, gridsize=20, \n                  xscale='log', yscale='log', cmap='Blues')\naxes[1, 1].set_title('Hexbin: Good for Large N')\naxes[1, 1].set_xlabel('Mass (M☉)')\naxes[1, 1].set_ylabel('Luminosity (L☉)')\n\n# Attempt 6: Contours with scatter overlay\nfrom scipy.stats import gaussian_kde\nxy = np.vstack([np.log10(masses), np.log10(luminosities)])\nz = gaussian_kde(xy)(xy)\nidx = z.argsort()\nx, y, z = np.log10(masses)[idx], np.log10(luminosities)[idx], z[idx]\naxes[1, 2].scatter(x, y, c=z, s=1, cmap='viridis')\naxes[1, 2].set_title('KDE-Colored: Shows Structure')\naxes[1, 2].set_xlabel('log(Mass)')\naxes[1, 2].set_ylabel('log(Luminosity)')\n\nplt.suptitle('Experimentation Reveals the Best Visualization', \n             fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"Key Lesson: The log-log plot immediately reveals the power-law relationship\")\nprint(\"that was completely hidden in the linear plot. Always experiment!\")\n\n🎯 The More You Know: How a Plot Saved Dark Energy\n\nIn 1998, two teams were racing to measure the universe’s deceleration by observing Type Ia supernovae — “standard candles” whose known brightness reveals their distance. The Supernova Cosmology Project, led by Saul Perlmutter, and the High-Z Supernova Search Team, led by Brian Schmidt and Adam Riess, expected to find the expansion slowing down due to gravity.\n\nThe critical moment came not from sophisticated analysis but from visualization choices. The teams tried dozens of ways to plot their data: magnitude versus redshift, distance versus redshift, logarithmic scales, linear scales. Nothing showed a clear pattern. Then Adam Riess had an idea: instead of plotting raw magnitudes, plot the residuals — the difference between observed and expected magnitudes for a matter-only universe (Riess et al. 1998).# Simplified version of the Nobel Prize-winning plot\nz = np.array([0.01, 0.1, 0.3, 0.5, 0.7, 0.9])  # Redshift\n# Expected magnitudes for matter-only universe\nm_expected = 5 * np.log10(z * 3000) + 25  # Simplified\n# Observed magnitudes (dimmer than expected!)\nm_observed = m_expected + 0.25 * z  # Supernovae are dimmer!\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n# Original plot - pattern not obvious\nax1.plot(z, m_observed, 'ro', markersize=8)\nax1.plot(z, m_expected, 'b--', label='Expected (matter only)')\nax1.set_xlabel('Redshift (z)')\nax1.set_ylabel('Magnitude')\nax1.set_title('Original Plot: Hard to See')\nax1.legend()\n\n# Residual plot - acceleration jumps out!\nax2.plot(z, m_observed - m_expected, 'ro', markersize=8)\nax2.axhline(y=0, color='k', linestyle='--', label='No acceleration')\nax2.set_xlabel('Redshift (z)')\nax2.set_ylabel('Δm (observed - expected)')\nax2.set_title('Residual Plot: Universe Accelerating!')\nax2.legend()\n\nThe residual plot made it obvious: supernovae were consistently dimmer than expected. The universe wasn’t just expanding; it was accelerating! This visualization choice — the result of experimentation and artistic judgment — led to a Nobel Prize (Perlmutter et al. 1999). Sometimes the difference between confusion and clarity is how you choose to plot your data.","type":"content","url":"/matplotlib-fundamentals-v1#the-experimentation-mindset","position":13},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl2":"8.2 Anatomy of a Figure"},"type":"lvl2","url":"/matplotlib-fundamentals-v1#id-8-2-anatomy-of-a-figure","position":14},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl2":"8.2 Anatomy of a Figure"},"content":"Understanding Matplotlib’s hierarchy is crucial for controlling your visualizations. Nicolas P. Rougier created the definitive visualization of this anatomy (Rougier 2018), which has become the standard reference:\n\n# The Complete Anatomy of a Matplotlib Figure\n# Copyright (c) 2016 Nicolas P. Rougier - MIT License\n# Adapted from: http://github.com/rougier/figure-anatomy\n# This brilliant visualization shows every component of a Matplotlib figure\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MultipleLocator, FuncFormatter\n\nnp.random.seed(123)\n\n# Create the figure with Rougier's specifications\nX = np.linspace(0.5, 3.5, 100)\nY1 = 3 + np.cos(X)\nY2 = 1 + np.cos(1 + X/0.75)/2\nY3 = np.random.uniform(Y1, Y2, len(X))\n\nfig = plt.figure(figsize=(8, 8), facecolor=\"w\")\nax = fig.add_subplot(1, 1, 1, aspect=1)\n\n# Configure the tick system\ndef minor_tick(x, pos):\n    if not x % 1.0:\n        return \"\"\n    return \"%.2f\" % x\n\nax.xaxis.set_major_locator(MultipleLocator(1.000))\nax.xaxis.set_minor_locator(MultipleLocator(0.250))\nax.yaxis.set_major_locator(MultipleLocator(1.000))\nax.yaxis.set_minor_locator(MultipleLocator(0.250))\nax.xaxis.set_minor_formatter(FuncFormatter(minor_tick))\n\nax.set_xlim(0, 4)\nax.set_ylim(0, 4)\n\nax.tick_params(which='major', width=1.0, length=10)\nax.tick_params(which='minor', width=1.0, length=5, labelsize=10, labelcolor='0.25')\n\n# Add the plot elements\nax.grid(linestyle=\"--\", linewidth=0.5, color='.25', zorder=-10)\nax.plot(X, Y1, c=(0.25, 0.25, 1.00), lw=2, label=\"Blue signal\", zorder=10)\nax.plot(X, Y2, c=(1.00, 0.25, 0.25), lw=2, label=\"Red signal\")\nax.scatter(X, Y3, c='w', edgecolors='black', linewidth=0.5)\n\nax.set_title(\"Anatomy of a figure\", fontsize=20)\nax.set_xlabel(\"X axis label\")\nax.set_ylabel(\"Y axis label\")\nax.legend(frameon=False)\n\n# Add annotations for each component\ndef circle(x, y, radius=0.15):\n    from matplotlib.patches import Circle\n    from matplotlib.patheffects import withStroke\n    circle = Circle((x, y), radius, clip_on=False, zorder=10, linewidth=1,\n                    edgecolor='black', facecolor=(0, 0, 0, .0125),\n                    path_effects=[withStroke(linewidth=5, foreground='w')])\n    ax.add_artist(circle)\n\ndef text(x, y, text):\n    ax.text(x, y, text, backgroundcolor=\"white\",\n            ha='center', va='top', weight='bold', color='blue')\n\n# Label all the components\ncircle(0.50, -0.05)\ntext(0.50, -0.25, \"Minor tick label\")\n\ncircle(4.00, 2.00)\ntext(4.00, 1.80, \"Major tick\")\n\ncircle(1.80, -0.22)\ntext(1.80, -0.4, \"X axis label\")\n\ncircle(1.75, 2.80)\ntext(1.75, 2.60, \"Line\\n(line plot)\")\n\ncircle(3.20, 1.75)\ntext(3.20, 1.55, \"Markers\\n(scatter plot)\")\n\ncircle(3.00, 3.00)\ntext(3.00, 2.80, \"Grid\")\n\ncircle(3.70, 3.75)\ntext(3.70, 3.55, \"Legend\")\n\ncircle(0.5, 0.5)\ntext(0.5, 0.3, \"Axes\")\n\ncircle(-0.3, 0.65)\ntext(-0.3, 0.45, \"Figure\")\n\n# Add the spines annotation\nax.annotate('Spines', xy=(4.0, 0.35), xycoords='data',\n            xytext=(3.3, 0.5), textcoords='data',\n            weight='bold', color='blue',\n            arrowprops=dict(arrowstyle='->', connectionstyle=\"arc3\", color='blue'))\n\nplt.suptitle(\"Credit: Nicolas P. Rougier (http://github.com/rougier/figure-anatomy)\",\n             fontsize=10, family=\"monospace\", color='.5')\nplt.tight_layout()\nplt.show()\n\nThis brilliant visualization by Rougier (2018) shows how every element — from the figure container down to individual tick labels — forms part of Matplotlib’s hierarchical structure. Understanding these relationships is what gives you the power to customize every aspect of your plots.","type":"content","url":"/matplotlib-fundamentals-v1#id-8-2-anatomy-of-a-figure","position":15},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl3":"Working with the Object-Oriented Interface","lvl2":"8.2 Anatomy of a Figure"},"type":"lvl3","url":"/matplotlib-fundamentals-v1#working-with-the-object-oriented-interface","position":16},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl3":"Working with the Object-Oriented Interface","lvl2":"8.2 Anatomy of a Figure"},"content":"Backend\n\nThe rendering engine Matplotlib uses to create and display figures.\n\nNow that you understand the anatomy, let’s see how to manipulate these elements:\n\n# Create a figure with explicit control over components\nfig = plt.figure(figsize=(10, 6))\n\n# Add axes manually to see the structure\n# [left, bottom, width, height] in figure coordinates (0-1)\nax_main = fig.add_axes([0.1, 0.3, 0.7, 0.6])   # Main plot\nax_zoom = fig.add_axes([0.65, 0.6, 0.2, 0.2])  # Inset zoom\nax_residual = fig.add_axes([0.1, 0.05, 0.7, 0.2])  # Residual panel\n\n# Generate data\nx = np.linspace(0, 10, 100)\ny = np.sin(x) * np.exp(-x/10)\ny_model = np.sin(x) * np.exp(-x/10.5)  # Slightly different model\n\n# Main plot\nax_main.plot(x, y, 'ko', markersize=3, alpha=0.5, label='Data')\nax_main.plot(x, y_model, 'r-', linewidth=2, label='Model')\nax_main.set_ylabel('Signal', fontsize=12)\nax_main.legend(loc='upper right')\nax_main.grid(True, alpha=0.3)\n\n# Zoom inset\nzoom_mask = (x > 3) & (x < 5)\nax_zoom.plot(x[zoom_mask], y[zoom_mask], 'ko', markersize=2)\nax_zoom.plot(x[zoom_mask], y_model[zoom_mask], 'r-', linewidth=1)\nax_zoom.set_xlim(3, 5)\nax_zoom.grid(True, alpha=0.3)\nax_zoom.set_title('Zoom', fontsize=9)\n\n# Residuals\nresiduals = y - y_model\nax_residual.scatter(x, residuals, s=5, alpha=0.5, color='blue')\nax_residual.axhline(y=0, color='red', linestyle='--', linewidth=1)\nax_residual.set_xlabel('Time', fontsize=12)\nax_residual.set_ylabel('Residuals', fontsize=10)\nax_residual.grid(True, alpha=0.3)\n\nfig.suptitle('Explicit Control Over Figure Components', fontsize=14, fontweight='bold')\nplt.show()\n\n💡 Computational Thinking Box: Figure Memory Management\n\nPATTERN: Managing Memory with Many Plots\n\nWhen creating many figures in a loop (common when processing astronomical surveys), memory usage can explode if not managed properly:# BAD: Memory leak - figures accumulate!\nfor i in range(100):\n    plt.figure()\n    plt.plot(data[i])\n    plt.savefig(f'plot_{i}.png')\n    # Figure stays in memory!\n\n# GOOD: Explicitly close figures\nfor i in range(100):\n    fig, ax = plt.subplots()\n    ax.plot(data[i])\n    fig.savefig(f'plot_{i}.png')\n    plt.close(fig)  # Free memory!\n\n# BETTER: Use context manager\nfor i in range(100):\n    with plt.subplots() as (fig, ax):\n        ax.plot(data[i])\n        fig.savefig(f'plot_{i}.png')\n    # Automatically closed!\n\nFor large surveys processing thousands of objects, this difference between memory leak and proper management can mean the difference between success and crashing!","type":"content","url":"/matplotlib-fundamentals-v1#working-with-the-object-oriented-interface","position":17},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl2":"8.3 Choosing the Right Scale: Linear, Log, and Everything Between"},"type":"lvl2","url":"/matplotlib-fundamentals-v1#id-8-3-choosing-the-right-scale-linear-log-and-everything-between","position":18},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl2":"8.3 Choosing the Right Scale: Linear, Log, and Everything Between"},"content":"Linear Scale\n\nEqual steps in data correspond to equal distances on the plot.\n\nLogarithmic Scale\n\nEqual multiplicative factors correspond to equal distances on the plot.\n\nPower Law\n\nA relationship where y \\propto x^n appears as a straight line on a log-log plot.\n\nOne of the most important skills in data visualization is choosing the right scale for your axes. The wrong scale can hide patterns; the right scale makes them obvious:\n\n# Generate different types of relationships\nx = np.logspace(-1, 3, 100)  # 0.1 to 1000\n\n# Different mathematical relationships\nlinear = 2 * x + 5\nquadratic = 0.5 * x**2\nexponential = np.exp(x/100)\npower_law = 10 * x**(-1.5)\nlogarithmic = 50 * np.log10(x) + 10\n\n# Create a comprehensive comparison\nfig, axes = plt.subplots(5, 4, figsize=(14, 16))\n\ndatasets = [\n    (linear, 'Linear: y = 2x + 5'),\n    (quadratic, 'Quadratic: y = 0.5x²'),\n    (exponential, 'Exponential: y = e^(x/100)'),\n    (power_law, 'Power Law: y = 10x^(-1.5)'),\n    (logarithmic, 'Logarithmic: y = 50log(x) + 10')\n]\n\nfor i, (data, title) in enumerate(datasets):\n    # Linear-linear\n    axes[i, 0].plot(x, data, 'b-', linewidth=2)\n    axes[i, 0].set_title(f'{title}\\nLinear-Linear')\n    axes[i, 0].grid(True, alpha=0.3)\n    \n    # Log-log\n    axes[i, 1].loglog(x, np.abs(data), 'r-', linewidth=2)\n    axes[i, 1].set_title('Log-Log')\n    axes[i, 1].grid(True, alpha=0.3, which='both')\n    \n    # Semilog-x\n    axes[i, 2].semilogx(x, data, 'g-', linewidth=2)\n    axes[i, 2].set_title('Semilog-X')\n    axes[i, 2].grid(True, alpha=0.3)\n    \n    # Semilog-y\n    axes[i, 3].semilogy(x, np.abs(data), 'm-', linewidth=2)\n    axes[i, 3].set_title('Semilog-Y')\n    axes[i, 3].grid(True, alpha=0.3)\n\n# Highlight which scale reveals linearity\naxes[0, 0].set_facecolor('#E8F4F8')  # Linear shows linear\naxes[2, 3].set_facecolor('#F8E8E8')  # Semilogy shows exponential\naxes[3, 1].set_facecolor('#F8F8E8')  # Loglog shows power law\naxes[4, 2].set_facecolor('#E8F8E8')  # Semilogx shows logarithmic\n\nfig.suptitle('Choose the Scale that Reveals Your Data\\'s Nature', \n             fontsize=16, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"Key insight: The 'right' scale makes relationships linear!\")\nprint(\"- Power laws → log-log\")\nprint(\"- Exponential growth → semilog-y\")\nprint(\"- Logarithmic growth → semilog-x\")\n\n","type":"content","url":"/matplotlib-fundamentals-v1#id-8-3-choosing-the-right-scale-linear-log-and-everything-between","position":19},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl3":"Practical Guidelines for Scale Selection","lvl2":"8.3 Choosing the Right Scale: Linear, Log, and Everything Between"},"type":"lvl3","url":"/matplotlib-fundamentals-v1#practical-guidelines-for-scale-selection","position":20},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl3":"Practical Guidelines for Scale Selection","lvl2":"8.3 Choosing the Right Scale: Linear, Log, and Everything Between"},"content":"\n\n# Real astronomical example: Galaxy luminosity function\nnp.random.seed(42)\n\n# Schechter function parameters (Schechter 1976)\nL_star = 1e10  # Characteristic luminosity\nalpha = -1.25  # Faint-end slope\nphi_star = 0.01  # Normalization\n\n# Generate galaxy luminosities\nL = np.logspace(7, 12, 1000)\nphi = phi_star * (L/L_star)**alpha * np.exp(-L/L_star)\n\n# Add observational scatter\nobserved_phi = phi * np.random.lognormal(0, 0.2, len(phi))\n\nfig, axes = plt.subplots(2, 2, figsize=(10, 10))\n\n# Linear scale - useless for this data\naxes[0, 0].plot(L, observed_phi, '.', markersize=1, alpha=0.5)\naxes[0, 0].set_xlabel('Luminosity (L☉)')\naxes[0, 0].set_ylabel('Φ (Number density)')\naxes[0, 0].set_title('Linear Scale: Cannot See Structure')\n\n# Log-log - reveals power law at faint end\naxes[0, 1].loglog(L, observed_phi, '.', markersize=1, alpha=0.5, label='Data')\naxes[0, 1].loglog(L, phi, 'r-', linewidth=2, label='Schechter Function')\naxes[0, 1].set_xlabel('Luminosity (L☉)')\naxes[0, 1].set_ylabel('Φ (Number density)')\naxes[0, 1].set_title('Log-Log: Reveals Power Law + Exponential Cutoff')\naxes[0, 1].legend()\n\n# Semilog-y - emphasizes exponential cutoff\naxes[1, 0].semilogx(L, observed_phi, '.', markersize=1, alpha=0.5)\naxes[1, 0].semilogx(L, phi, 'r-', linewidth=2)\naxes[1, 0].set_xlabel('Luminosity (L☉)')\naxes[1, 0].set_ylabel('Φ (Number density)')\naxes[1, 0].set_title('Semilog-X: Emphasizes Bright End Cutoff')\n\n# Custom: log-log with ratio to model\nratio = observed_phi / phi\naxes[1, 1].semilogx(L, ratio, '.', markersize=1, alpha=0.5)\naxes[1, 1].axhline(y=1, color='red', linestyle='--', linewidth=2)\naxes[1, 1].set_xlabel('Luminosity (L☉)')\naxes[1, 1].set_ylabel('Observed / Model')\naxes[1, 1].set_title('Residual Plot: Shows Systematic Deviations')\naxes[1, 1].set_ylim(0.1, 10)\n\nfig.suptitle('Galaxy Luminosity Function: Scale Choice Matters', \n             fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n🔍 Check Your Understanding\n\nYou have data showing radioactive decay: counts versus time. Which scale would best reveal the half-life?\n\nAnswer\n\nSemilog-y (linear time, log counts) is the best choice!\n\nRadioactive decay follows N(t) = N₀ * e^(-λt), which becomes:\nlog(N) = log(N₀) - λt\n\nOn a semilog-y plot, this appears as a straight line with slope -λ. The half-life is clearly visible as the time for the counts to drop by half (constant vertical distance on the log scale).t = np.linspace(0, 5, 100)\nN0 = 1000\nhalf_life = 1.5\ndecay_rate = np.log(2) / half_life\ncounts = N0 * np.exp(-decay_rate * t)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n\n# Linear - curve makes half-life hard to read\nax1.plot(t, counts)\nax1.set_title('Linear: Half-life unclear')\n\n# Semilog-y - straight line, half-life obvious\nax2.semilogy(t, counts)\nax2.axhline(y=N0/2, color='r', linestyle='--', label=f't₁/₂ = {half_life}')\nax2.set_title('Semilog-y: Half-life clear!')\nax2.legend()\n## 8.4 Building Your Plotting Toolkit: Reusable Functions\n\nAs you develop as a scientific programmer, you'll find yourself making similar plots repeatedly. Instead of copying and pasting code, build a library of plotting functions that encode your hard-won knowledge about what works:\n\n```{code-cell} ipython3\n# Example: A reusable light curve plotting function\ndef plot_light_curve(time, flux, flux_err=None, period=None, \n                     title=None, figsize=(12, 8)):\n    \"\"\"\n    Create a publication-quality light curve plot with optional phase folding.\n    \n    Parameters\n    ----------\n    time : array-like\n        Time values (days)\n    flux : array-like\n        Flux or magnitude values\n    flux_err : array-like, optional\n        Flux uncertainties\n    period : float, optional\n        Period for phase folding (days)\n    title : str, optional\n        Plot title\n    figsize : tuple, optional\n        Figure size (width, height)\n    \n    Returns\n    -------\n    fig, axes : matplotlib figure and axes objects\n    \"\"\"\n    if period is not None:\n        fig, axes = plt.subplots(2, 2, figsize=figsize)\n        axes = axes.flatten()\n    else:\n        fig, axes = plt.subplots(1, 1, figsize=(figsize[0], figsize[1]/2))\n        axes = [axes]  # Make it iterable\n    \n    # Main light curve\n    ax = axes[0]\n    if flux_err is not None:\n        ax.errorbar(time, flux, yerr=flux_err, fmt='k.', markersize=2,\n                   alpha=0.5, elinewidth=0.5, capsize=0)\n    else:\n        ax.scatter(time, flux, s=1, alpha=0.5, color='black')\n    \n    ax.set_xlabel('Time (days)', fontsize=11)\n    ax.set_ylabel('Relative Flux', fontsize=11)\n    ax.set_title('Light Curve' if title is None else title, fontsize=12)\n    ax.grid(True, alpha=0.3)\n    \n    # If period provided, add phase-folded plot\n    if period is not None:\n        phase = (time % period) / period\n        \n        # Phase folded\n        ax = axes[1]\n        if flux_err is not None:\n            ax.errorbar(phase, flux, yerr=flux_err, fmt='b.', markersize=2,\n                       alpha=0.3, elinewidth=0.5, capsize=0)\n        else:\n            ax.scatter(phase, flux, s=1, alpha=0.3, color='blue')\n        ax.set_xlabel('Phase', fontsize=11)\n        ax.set_ylabel('Relative Flux', fontsize=11)\n        ax.set_title(f'Phase Folded (P = {period:.3f} days)', fontsize=12)\n        ax.set_xlim(0, 1)\n        ax.grid(True, alpha=0.3)\n        \n        # Double-plotted phase folded\n        ax = axes[2]\n        phase_double = np.concatenate([phase, phase + 1])\n        flux_double = np.concatenate([flux, flux])\n        if flux_err is not None:\n            err_double = np.concatenate([flux_err, flux_err])\n            ax.errorbar(phase_double, flux_double, yerr=err_double, \n                       fmt='r.', markersize=2, alpha=0.3, \n                       elinewidth=0.5, capsize=0)\n        else:\n            ax.scatter(phase_double, flux_double, s=1, alpha=0.3, color='red')\n        ax.set_xlabel('Phase', fontsize=11)\n        ax.set_ylabel('Relative Flux', fontsize=11)\n        ax.set_title('Double Phase Plot', fontsize=12)\n        ax.set_xlim(0, 2)\n        ax.grid(True, alpha=0.3)\n        \n        # Binned phase curve\n        ax = axes[3]\n        n_bins = 50\n        phase_bins = np.linspace(0, 1, n_bins + 1)\n        binned_flux = []\n        binned_err = []\n        bin_centers = []\n        \n        for i in range(n_bins):\n            mask = (phase >= phase_bins[i]) & (phase < phase_bins[i+1])\n            if mask.sum() > 0:\n                binned_flux.append(np.median(flux[mask]))\n                binned_err.append(np.std(flux[mask]) / np.sqrt(mask.sum()))\n                bin_centers.append((phase_bins[i] + phase_bins[i+1]) / 2)\n        \n        ax.errorbar(bin_centers, binned_flux, yerr=binned_err,\n                   fmt='go-', markersize=4, linewidth=1, capsize=3)\n        ax.set_xlabel('Phase', fontsize=11)\n        ax.set_ylabel('Relative Flux', fontsize=11)\n        ax.set_title(f'Binned ({n_bins} bins)', fontsize=12)\n        ax.set_xlim(0, 1)\n        ax.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    return fig, axes\n\n# Test the function with synthetic data\nnp.random.seed(42)\ntime = np.linspace(0, 30, 500)\nperiod = 2.3456  # days\nphase = 2 * np.pi * time / period\nflux = 1.0 - 0.01 * np.sin(phase)**2  # Transit-like signal\nflux += np.random.normal(0, 0.002, len(time))  # Add noise\nflux_err = np.ones_like(flux) * 0.002\n\nfig, axes = plot_light_curve(time, flux, flux_err, period=period, \n                            title='Exoplanet Transit Light Curve')\nplt.show()\n\nprint(\"This reusable function encodes best practices:\")\nprint(\"- Automatic phase folding when period is provided\")\nprint(\"- Double phase plot to see continuity\")\nprint(\"- Binned version to see average behavior\")\nprint(\"- Consistent styling throughout\")\n```\n\n### Building a Personal Plotting Library\n\nHere's a template for organizing your plotting functions:\n\n```{code-cell} python\n# my_astro_plots.py - Your personal plotting library\n\ndef plot_spectrum(wavelength, flux, flux_err=None, lines=None, \n                  title=None, figsize=(12, 5)):\n    \"\"\"Plot a stellar spectrum with optional line identification.\"\"\"\n    fig, ax = plt.subplots(figsize=figsize)\n    \n    # Plot spectrum\n    ax.plot(wavelength, flux, 'k-', linewidth=0.8, label='Spectrum')\n    \n    if flux_err is not None:\n        ax.fill_between(wavelength, flux - flux_err, flux + flux_err,\n                        alpha=0.3, color='gray', label='Uncertainty')\n    \n    # Mark spectral lines\n    if lines is not None:\n        for wave, name in lines:\n            ax.axvline(x=wave, color='red', linestyle=':', alpha=0.5)\n            ax.text(wave, ax.get_ylim()[1]*0.95, name, \n                   rotation=90, va='top', ha='right', fontsize=8)\n    \n    ax.set_xlabel('Wavelength (Å)', fontsize=11)\n    ax.set_ylabel('Normalized Flux', fontsize=11)\n    ax.set_title('Spectrum' if title is None else title, fontsize=12)\n    ax.grid(True, alpha=0.3)\n    ax.legend(loc='best')\n    \n    return fig, ax\n\ndef plot_cmd(color, magnitude, title=None, figsize=(8, 10)):\n    \"\"\"Create a color-magnitude diagram.\"\"\"\n    fig, ax = plt.subplots(figsize=figsize)\n    \n    # 2D histogram for density\n    h = ax.hist2d(color, magnitude, bins=50, cmap='YlOrBr', \n                  density=True)\n    \n    ax.set_xlabel('Color (B-V)', fontsize=11)\n    ax.set_ylabel('Absolute Magnitude', fontsize=11)\n    ax.invert_yaxis()  # Astronomical convention\n    ax.set_title('Color-Magnitude Diagram' if title is None else title, \n                fontsize=12)\n    \n    plt.colorbar(h[3], ax=ax, label='Density')\n    \n    return fig, ax\n\ndef plot_finding_chart(ra, dec, image=None, sources=None, \n                       figsize=(10, 10)):\n    \"\"\"Create a finding chart with marked sources.\"\"\"\n    fig, ax = plt.subplots(figsize=figsize, \n                           subplot_kw={'projection': 'rectilinear'})\n    \n    if image is not None:\n        im = ax.imshow(image, cmap='gray_r', origin='lower',\n                      extent=[ra.min(), ra.max(), dec.min(), dec.max()])\n        plt.colorbar(im, ax=ax, label='Intensity')\n    \n    if sources is not None:\n        for source in sources:\n            circle = plt.Circle((source['ra'], source['dec']), \n                               source.get('radius', 0.1),\n                               fill=False, color='red', linewidth=2)\n            ax.add_patch(circle)\n            ax.text(source['ra'], source['dec'] + 0.15, \n                   source.get('name', ''), \n                   ha='center', color='red')\n    \n    ax.set_xlabel('RA (degrees)', fontsize=11)\n    ax.set_ylabel('Dec (degrees)', fontsize=11)\n    ax.set_title('Finding Chart', fontsize=12)\n    ax.invert_xaxis()  # RA increases to the left\n    \n    return fig, ax\n\n# Example usage\nprint(\"Your personal plotting library is ready!\")\nprint(\"Available functions:\")\nprint(\"  - plot_light_curve(): For time series photometry\")\nprint(\"  - plot_spectrum(): For spectroscopic data\") \nprint(\"  - plot_cmd(): For color-magnitude diagrams\")\nprint(\"  - plot_finding_chart(): For sky position plots\")\n```\n\n:::{admonition} 💡 Computational Thinking Box: DRY Principle in Plotting\n:class: tip\n\n**PATTERN: Don't Repeat Yourself (DRY)**\n\nEvery time you copy-paste plotting code, you're creating technical debt. Instead, abstract common patterns into functions:\n\n```python\n# BAD: Copying and modifying\nfig, ax = plt.subplots()\nax.plot(data1)\nax.set_xlabel('Time (s)')\nax.set_ylabel('Flux')\nax.grid(True, alpha=0.3)\n# ... 50 lines later, same code with data2\n\n# GOOD: Reusable function\ndef plot_time_series(data, xlabel='Time (s)', ylabel='Flux', **kwargs):\n    fig, ax = plt.subplots(**kwargs)\n    ax.plot(data)\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n    ax.grid(True, alpha=0.3)\n    return fig, ax\n\n# Now you can customize without repetition\nfig1, ax1 = plot_time_series(data1, ylabel='X-ray Flux')\nfig2, ax2 = plot_time_series(data2, ylabel='Optical Flux')\n```\n\nBenefits:\n- Consistency across all your plots\n- Easy to update style everywhere at once\n- Encode domain knowledge (like inverting magnitude axis)\n- Share with collaborators\n- Build your reputation for quality figures","type":"content","url":"/matplotlib-fundamentals-v1#practical-guidelines-for-scale-selection","position":21},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl2":"8.5 Essential Plot Types for Astronomy"},"type":"lvl2","url":"/matplotlib-fundamentals-v1#id-8-5-essential-plot-types-for-astronomy","position":22},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl2":"8.5 Essential Plot Types for Astronomy"},"content":"{margin} Light Curve\nA plot showing how an object’s brightness varies over time.\n\n{margin} Spectrum\nA plot showing intensity as a function of wavelength or frequency.","type":"content","url":"/matplotlib-fundamentals-v1#id-8-5-essential-plot-types-for-astronomy","position":23},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl3":"Line Plots: Time Series and Spectra","lvl2":"8.5 Essential Plot Types for Astronomy"},"type":"lvl3","url":"/matplotlib-fundamentals-v1#line-plots-time-series-and-spectra","position":24},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl3":"Line Plots: Time Series and Spectra","lvl2":"8.5 Essential Plot Types for Astronomy"},"content":"Line plots are workhorses for astronomical data:\n\n# Generate realistic stellar spectrum\nnp.random.seed(42)\nwavelength = np.linspace(4000, 7000, 1000)  # Angstroms\ncontinuum = 1 - 0.0001 * (wavelength - 5500)**2 / 1e6  # Continuum shape\n\n# Add absorption lines (H-alpha, H-beta, etc.)\nlines = [(6563, 50, 0.3), (4861, 30, 0.25), (4340, 25, 0.2)]  # center, width, depth\nspectrum = continuum.copy()\nfor center, width, depth in lines:\n    spectrum *= (1 - depth * np.exp(-0.5 * ((wavelength - center) / width)**2))\n\n# Add noise\nspectrum += np.random.normal(0, 0.01, len(wavelength))\n\n# Create publication-quality spectrum plot\nfig, ax = plt.subplots(figsize=(12, 5))\n\nax.plot(wavelength, spectrum, 'k-', linewidth=0.8, label='Observed')\nax.axhline(y=1, color='gray', linestyle='--', alpha=0.5, label='Continuum')\n\n# Mark important lines\nfor center, width, depth in lines:\n    ax.axvline(x=center, color='red', linestyle=':', alpha=0.5)\n    ax.text(center, 0.65, f'{center}Å', rotation=90, \n            va='bottom', ha='right', fontsize=9)\n\nax.set_xlabel('Wavelength (Å)', fontsize=12)\nax.set_ylabel('Normalized Flux', fontsize=12)\nax.set_title('Stellar Spectrum with Balmer Lines', fontsize=14)\nax.set_xlim(4000, 7000)\nax.set_ylim(0.6, 1.1)\nax.grid(True, alpha=0.2)\nax.legend(loc='lower right')\n\nplt.tight_layout()\nplt.show()\n\n","type":"content","url":"/matplotlib-fundamentals-v1#line-plots-time-series-and-spectra","position":25},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl3":"Scatter Plots: Correlations and Diagrams","lvl2":"8.5 Essential Plot Types for Astronomy"},"type":"lvl3","url":"/matplotlib-fundamentals-v1#scatter-plots-correlations-and-diagrams","position":26},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl3":"Scatter Plots: Correlations and Diagrams","lvl2":"8.5 Essential Plot Types for Astronomy"},"content":"Scatter plots reveal relationships between variables:\n\n# Create a color-magnitude diagram (CMD)\nnp.random.seed(42)\n\n# Main sequence stars\nn_ms = 500\ncolor_ms = np.random.uniform(-0.3, 2.0, n_ms)\nmag_ms = 4 * color_ms + np.random.normal(0, 0.5, n_ms) + 4\n\n# Red giants\nn_rg = 100\ncolor_rg = np.random.uniform(0.8, 2.0, n_rg)\nmag_rg = np.random.normal(0, 0.5, n_rg)\n\n# White dwarfs\nn_wd = 50\ncolor_wd = np.random.uniform(-0.3, 0.5, n_wd)\nmag_wd = np.random.normal(11, 0.5, n_wd)\n\n# Create the CMD\nfig, ax = plt.subplots(figsize=(8, 10))\n\nax.scatter(color_ms, mag_ms, c='navy', s=10, alpha=0.6, label='Main Sequence')\nax.scatter(color_rg, mag_rg, c='red', s=30, alpha=0.7, label='Red Giants')\nax.scatter(color_wd, mag_wd, c='lightblue', s=20, alpha=0.8, label='White Dwarfs')\n\nax.set_xlabel('B - V Color Index', fontsize=12)\nax.set_ylabel('V Magnitude', fontsize=12)\nax.set_title('Color-Magnitude Diagram\\n(Hertzsprung-Russell Diagram)', fontsize=14)\nax.invert_yaxis()  # Astronomical convention\nax.grid(True, alpha=0.3)\nax.legend(loc='upper left')\n\n# Add annotations\nax.annotate('Turn-off point', xy=(0.6, 6), xytext=(1.2, 7),\n            arrowprops=dict(arrowstyle='->', color='black', alpha=0.5))\n\nplt.tight_layout()\nplt.show()\n\n⚠️ Common Bug Alert: Histogram Binning\n\n# DANGER: Wrong binning can hide or create features!\ndata = np.random.normal(0, 1, 1000)\n\nfig, axes = plt.subplots(1, 3, figsize=(12, 4))\n\n# Too few bins - loses structure\naxes[0].hist(data, bins=5)\naxes[0].set_title('Too Few Bins (5)')\n\n# Just right - shows distribution\naxes[1].hist(data, bins=30)\naxes[1].set_title('Appropriate Bins (30)')\n\n# Too many bins - adds noise\naxes[2].hist(data, bins=200)\naxes[2].set_title('Too Many Bins (200)')\n\nplt.tight_layout()\nplt.show()\n\n# Use Sturges' rule or Freedman-Diaconis rule for automatic binning:\n# bins='auto' uses the maximum of Sturges and Freedman-Diaconis\n\nAlways experiment with bin sizes or use automatic binning algorithms!\n\n🔍 Check Your Understanding\n\nWhat’s the difference between plt.plot() and ax.plot()? When would you use each?\n\nAnswer\n\nplt.plot() uses the pyplot interface and operates on the “current” axes\n\nax.plot() uses the object-oriented interface and explicitly specifies which axes to use\n\nUse plt.plot() for:\n\nQuick, exploratory plots\n\nSimple single-panel figures\n\nInteractive work in Jupyter notebooks\n\nUse ax.plot() for:\n\nMulti-panel figures\n\nPublication-quality plots\n\nAny situation where you need fine control\n\nScripts that generate many figures\n\nIn research, always prefer ax.plot() for reproducibility and control!\n## 8.6 Images and 2D Data Visualization\n\n:::{margin} **Colormap**\nA mapping from data values to colors for visualization.\n\nNormalization\n\nScaling data values to a standard range for display.\n\nAstronomical images require special consideration for display:\n\n# Create synthetic galaxy image\nnp.random.seed(42)\nx = np.linspace(-5, 5, 200)\ny = np.linspace(-5, 5, 200)\nX, Y = np.meshgrid(x, y)\n\n# Exponential disk profile\nR = np.sqrt(X**2 + Y**2)\ndisk = np.exp(-R / 1.5)\n\n# Add spiral arms (simplified)\ntheta = np.arctan2(Y, X)\nspiral = 1 + 0.3 * np.sin(2 * theta - R)\ngalaxy = disk * spiral\n\n# Add noise and background\ngalaxy += np.random.normal(0, 0.02, galaxy.shape)\ngalaxy += 0.1  # Sky background\n\n# Display with different scaling\nfig, axes = plt.subplots(2, 3, figsize=(12, 8))\n\n# Linear scale\nim1 = axes[0, 0].imshow(galaxy, cmap='gray', origin='lower')\naxes[0, 0].set_title('Linear Scale')\nplt.colorbar(im1, ax=axes[0, 0], fraction=0.046)\n\n# Log scale\nfrom matplotlib.colors import LogNorm\ngalaxy_positive = galaxy - galaxy.min() + 1e-3  # Ensure positive\nim2 = axes[0, 1].imshow(galaxy_positive, cmap='gray', \n                         norm=LogNorm(), origin='lower')\naxes[0, 1].set_title('Log Scale')\nplt.colorbar(im2, ax=axes[0, 1], fraction=0.046)\n\n# Histogram equalization (adaptive)\nfrom matplotlib.colors import PowerNorm\nim3 = axes[0, 2].imshow(galaxy, cmap='gray', \n                         norm=PowerNorm(gamma=0.5), origin='lower')\naxes[0, 2].set_title('Power Scale (γ=0.5)')\nplt.colorbar(im3, ax=axes[0, 2], fraction=0.046)\n\n# Different colormaps\nim4 = axes[1, 0].imshow(galaxy, cmap='viridis', origin='lower')\naxes[1, 0].set_title('Viridis Colormap')\nplt.colorbar(im4, ax=axes[1, 0], fraction=0.046)\n\nim5 = axes[1, 1].imshow(galaxy, cmap='hot', origin='lower')\naxes[1, 1].set_title('Hot Colormap')\nplt.colorbar(im5, ax=axes[1, 1], fraction=0.046)\n\nim6 = axes[1, 2].imshow(galaxy, cmap='twilight', origin='lower')\naxes[1, 2].set_title('Twilight Colormap')\nplt.colorbar(im6, ax=axes[1, 2], fraction=0.046)\n\nfor ax in axes.flat:\n    ax.set_xlabel('X (pixels)')\n    ax.set_ylabel('Y (pixels)')\n\nfig.suptitle('Galaxy Image with Different Displays', fontsize=14)\nplt.tight_layout()\nplt.show()\n\n🌟 Why This Matters: Finding Exoplanets in Pixels\n\nThe Kepler Space Telescope (Borucki et al. 2010) discovered over 2,600 exoplanets not through pretty pictures, but through careful analysis of pixel data. Each star was just a few pixels on the CCD, and the challenge was detecting brightness changes of 0.01% buried in noise.\n\nThe key was visualization. The Kepler team developed specialized image displays showing:\n\nThe target pixel file (TPF) - raw pixel values over time\n\nThe optimal aperture - which pixels to sum for photometry\n\nThe background estimation - critical for accurate measurements# Simplified Kepler pixel analysis\n# Create synthetic stellar image with transit\ntime_points = 50\nimage_size = 11\nimages = np.zeros((time_points, image_size, image_size))\n\n# Add star (Gaussian PSF)\nx, y = np.mgrid[0:image_size, 0:image_size]\nstar_x, star_y = 5, 5\nfor t in range(time_points):\n    psf = 1000 * np.exp(-((x - star_x)**2 + (y - star_y)**2) / 4)\n    \n    # Add transit dip\n    if 20 < t < 25:\n        psf *= 0.99  # 1% dip\n    \n    images[t] = psf + np.random.normal(0, 5, (image_size, image_size))\n\n# Optimal aperture (pixels to sum)\naperture = ((x - star_x)**2 + (y - star_y)**2) < 6\n\n# Extract light curve\nlight_curve = [img[aperture].sum() for img in images]\n\nThis pixel-level visualization revealed not just transiting planets, but also asteroseismology signals, stellar rotation, and even reflected light from hot Jupiters. The ability to visualize and understand pixel data literally opened up new worlds!","type":"content","url":"/matplotlib-fundamentals-v1#scatter-plots-correlations-and-diagrams","position":27},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl2":"8.7 Color Theory and Publication Standards"},"type":"lvl2","url":"/matplotlib-fundamentals-v1#id-8-7-color-theory-and-publication-standards","position":28},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl2":"8.7 Color Theory and Publication Standards"},"content":"DPI\n\nDots per inch, determining figure resolution for printing or display.\n\nLaTeX\n\nA typesetting system commonly used for scientific publications, supported by Matplotlib for mathematical notation.\n\nGridSpec\n\nMatplotlib’s flexible system for creating complex subplot layouts.\n\nChoosing appropriate colormaps is crucial for honest data representation (Crameri et al. 2020):\n\n# Demonstrate perceptual uniformity\ndata = np.random.randn(10, 10)\n\nfig, axes = plt.subplots(2, 4, figsize=(14, 7))\n\n# Bad colormaps (not perceptually uniform)\nbad_cmaps = ['jet', 'rainbow', 'nipy_spectral', 'gist_ncar']\nfor ax, cmap in zip(axes[0], bad_cmaps):\n    im = ax.imshow(data, cmap=cmap)\n    ax.set_title(f'{cmap} (Avoid!)')\n    ax.axis('off')\n    plt.colorbar(im, ax=ax, fraction=0.046)\n\n# Good colormaps (perceptually uniform)\ngood_cmaps = ['viridis', 'plasma', 'cividis', 'twilight']\nfor ax, cmap in zip(axes[1], good_cmaps):\n    im = ax.imshow(data, cmap=cmap)\n    ax.set_title(f'{cmap} (Good!)')\n    ax.axis('off')\n    plt.colorbar(im, ax=ax, fraction=0.046)\n\nfig.suptitle('Perceptually Uniform vs Non-Uniform Colormaps', fontsize=14)\nplt.tight_layout()\nplt.show()\n\n⚠️ Common Bug Alert: DPI and Figure Sizes\n\n# Figure size confusion - physical vs pixel size\nfig1, ax1 = plt.subplots(figsize=(6, 4), dpi=100)  # 600x400 pixels\nfig2, ax2 = plt.subplots(figsize=(6, 4), dpi=200)  # 1200x800 pixels\n\nax1.text(0.5, 0.5, f'Size: {fig1.get_size_inches()}\\nDPI: {fig1.dpi}\\n' + \n         f'Pixels: {fig1.get_size_inches()[0]*fig1.dpi:.0f}x' +\n         f'{fig1.get_size_inches()[1]*fig1.dpi:.0f}',\n         transform=ax1.transAxes, ha='center', va='center')\nax1.set_title('100 DPI')\n\nax2.text(0.5, 0.5, f'Size: {fig2.get_size_inches()}\\nDPI: {fig2.dpi}\\n' +\n         f'Pixels: {fig2.get_size_inches()[0]*fig2.dpi:.0f}x' +\n         f'{fig2.get_size_inches()[1]*fig2.dpi:.0f}',\n         transform=ax2.transAxes, ha='center', va='center')\nax2.set_title('200 DPI')\n\nplt.show()\n\n# For publication: typical requirements\n# - ApJ: 300 DPI for print, figure width = 3.5\" (single column) or 7\" (full page)\n# - Nature: 300-600 DPI, maximum width 183mm\n# - Screen: 72-100 DPI is sufficient\n\n🔍 Check Your Understanding\n\nWhy is the ‘jet’ colormap problematic for scientific visualization?\n\nAnswer\n\nThe ‘jet’ colormap has several serious issues (Wong 2011):\n\nNot perceptually uniform: Equal steps in data don’t appear as equal steps in color\n\nCreates false features: Bright bands at yellow and cyan create artificial boundaries\n\nNot colorblind-friendly: Red-green confusion affects ~8% of males\n\nPoor grayscale conversion: Loses information when printed in black and white\n\nExample of the problem:# Linear data appears to have features with jet\nlinear_data = np.outer(np.ones(10), np.arange(100))\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\nax1.imshow(linear_data, cmap='jet', aspect='auto')\nax1.set_title('Jet: False features appear')\nax2.imshow(linear_data, cmap='viridis', aspect='auto')\nax2.set_title('Viridis: Smooth gradient')\n\nAlways use perceptually uniform colormaps like viridis, plasma, or cividis for scientific data!\n## 8.8 Practice Exercises\n\n### Exercise 1: Creating a Complete Astronomical Figure\n\nBuild a publication-quality multi-wavelength light curve:\n\n**Part A: Generate realistic data (5 minutes)**\n\n```{code-cell} ipython3\n# Simulate multi-wavelength observations of a flaring star\nnp.random.seed(42)\n\n# Time array (days)\ntime = np.linspace(0, 30, 300)\n\n# Base stellar brightness (different in each band)\nbase_optical = 12.0  # magnitude\nbase_xray = 1e-12    # erg/s/cm^2\nbase_radio = 10.0    # mJy\n\n# Add periodic variation (rotation)\nperiod = 5.3  # days\nphase = 2 * np.pi * time / period\n\noptical = base_optical - 0.1 * np.sin(phase)\nxray = base_xray * (1 + 0.2 * np.sin(phase + 0.5))\nradio = base_radio * (1 + 0.15 * np.sin(phase - 0.3))\n\n# Add flares at specific times\nflare_times = [8, 15, 22]\nflare_widths = [0.5, 0.3, 0.7]\n\nfor ft, fw in zip(flare_times, flare_widths):\n    flare_profile = np.exp(-0.5 * ((time - ft) / fw)**2)\n    optical -= 0.5 * flare_profile  # Brightening (lower magnitude)\n    xray *= (1 + 10 * flare_profile)  # X-ray enhancement\n    radio *= (1 + 3 * flare_profile)  # Radio enhancement\n\n# Add realistic noise\noptical += np.random.normal(0, 0.02, len(time))\nxray *= (1 + np.random.normal(0, 0.1, len(time)))\nradio *= (1 + np.random.normal(0, 0.05, len(time)))\n\nprint(f\"Generated {len(time)} observations over {time.max():.1f} days\")\nprint(f\"Detected {len(flare_times)} flares\")\n```\n\n**Part B: Create the multi-panel figure (10 minutes)**\n\n```{code-cell} python\n# Create publication-quality figure\nfig = plt.figure(figsize=(10, 8))\n\n# Use GridSpec for custom layout\nfrom matplotlib.gridspec import GridSpec\ngs = GridSpec(4, 1, height_ratios=[2, 2, 2, 1], hspace=0)\n\n# Optical light curve\nax1 = fig.add_subplot(gs[0])\nax1.scatter(time, optical, s=10, alpha=0.6, color='blue')\nax1.set_ylabel('V Magnitude', fontsize=11)\nax1.invert_yaxis()  # Astronomical convention\nax1.grid(True, alpha=0.3)\nax1.set_xlim(0, 30)\nax1.tick_params(labelbottom=False)\n\n# Mark flares\nfor ft in flare_times:\n    ax1.axvline(x=ft, color='red', linestyle='--', alpha=0.5)\nax1.text(0.02, 0.98, 'Optical', transform=ax1.transAxes,\n         fontweight='bold', va='top')\n\n# X-ray light curve (log scale)\nax2 = fig.add_subplot(gs[1], sharex=ax1)\nax2.semilogy(time, xray, 'r-', linewidth=0.8, alpha=0.8)\nax2.set_ylabel('X-ray Flux\\n(erg/s/cm²)', fontsize=11)\nax2.grid(True, alpha=0.3)\nax2.tick_params(labelbottom=False)\nax2.text(0.02, 0.98, 'X-ray', transform=ax2.transAxes,\n         fontweight='bold', va='top')\n\n# Radio light curve\nax3 = fig.add_subplot(gs[2], sharex=ax1)\nax3.plot(time, radio, 'g-', linewidth=1)\nax3.set_ylabel('Radio Flux\\n(mJy)', fontsize=11)\nax3.grid(True, alpha=0.3)\nax3.tick_params(labelbottom=False)\nax3.text(0.02, 0.98, 'Radio', transform=ax3.transAxes,\n         fontweight='bold', va='top')\n\n# Hardness ratio\nax4 = fig.add_subplot(gs[3], sharex=ax1)\nhardness = xray / (xray.mean())  # Normalized X-ray\nax4.plot(time, hardness, 'k-', linewidth=0.8)\nax4.set_ylabel('Hardness\\nRatio', fontsize=11)\nax4.set_xlabel('Time (days)', fontsize=12)\nax4.grid(True, alpha=0.3)\nax4.set_xlim(0, 30)\n\n# Overall title\nfig.suptitle('Multi-wavelength Observations of Flare Star', \n             fontsize=14, fontweight='bold', y=0.995)\n\nplt.show()\n```\n\n**Part C: Export and document (5 minutes)**\n\n```{code-cell} python\n# Create a function to save figures properly\ndef save_publication_figure(fig, basename, formats=['pdf', 'png']):\n    \"\"\"\n    Save figure in multiple formats with proper settings.\n    \n    Parameters\n    ----------\n    fig : matplotlib.figure.Figure\n        Figure to save\n    basename : str\n        Base filename without extension\n    formats : list\n        List of formats to save\n    \"\"\"\n    for fmt in formats:\n        filename = f\"{basename}.{fmt}\"\n        \n        if fmt == 'pdf':\n            fig.savefig(filename, format='pdf', dpi=300,\n                       bbox_inches='tight', transparent=True)\n        elif fmt == 'png':\n            fig.savefig(filename, format='png', dpi=300,\n                       bbox_inches='tight', transparent=False,\n                       facecolor='white')\n        elif fmt == 'svg':\n            fig.savefig(filename, format='svg',\n                       bbox_inches='tight', transparent=True)\n        \n        print(f\"Saved: {filename}\")\n    \n    # Also save the data for reproducibility\n    data_file = f\"{basename}_data.npz\"\n    np.savez(data_file, time=time, optical=optical, \n             xray=xray, radio=radio)\n    print(f\"Saved data: {data_file}\")\n\n# Example usage (commented out to avoid creating files)\n# save_publication_figure(fig, 'flare_star_multiwave')\n\nprint(\"Figure ready for publication!\")\nprint(\"\\nChecklist:\")\nprint(\"✓ All axes labeled with units\")\nprint(\"✓ Multi-wavelength data aligned\")\nprint(\"✓ Flares marked consistently\")\nprint(\"✓ Professional styling applied\")\nprint(\"✓ Ready for ApJ submission (7-inch width)\")\n```\n\n### Exercise 2: Exploring Scaling Effects\n\nUnderstand how different scales reveal different features:\n\n```{code-cell} python\n\"\"\"\nPart 1: Generate power-law distributed data\n\"\"\"\nnp.random.seed(42)\n# Simulate galaxy cluster masses (power law distribution)\nn_galaxies = 5000\nmasses = np.random.pareto(1.5, n_galaxies) + 1  # M ∝ N^(-2.5)\nmasses *= 1e11  # Solar masses\n\n# Add measurement uncertainty\nmasses_observed = masses * np.random.lognormal(0, 0.1, n_galaxies)\n\nprint(f\"Generated {n_galaxies} galaxy masses\")\nprint(f\"Mass range: {masses_observed.min():.2e} to {masses_observed.max():.2e} M☉\")\n```\n\n```{code-cell} python\n\"\"\"\nPart 2: Try different visualizations\n\"\"\"\nfig, axes = plt.subplots(2, 3, figsize=(14, 8))\n\n# Linear histogram - useless for power law\naxes[0, 0].hist(masses_observed, bins=50, alpha=0.7, color='blue')\naxes[0, 0].set_xlabel('Mass (M☉)')\naxes[0, 0].set_ylabel('Count')\naxes[0, 0].set_title('Linear Scale: Useless for Power Law')\n\n# Log-scale x-axis\naxes[0, 1].hist(masses_observed, bins=np.logspace(11, 15, 50), \n                alpha=0.7, color='green')\naxes[0, 1].set_xscale('log')\naxes[0, 1].set_xlabel('Mass (M☉)')\naxes[0, 1].set_ylabel('Count')\naxes[0, 1].set_title('Log X-axis: Better but not ideal')\n\n# Log-log scale reveals power law\naxes[0, 2].hist(masses_observed, bins=np.logspace(11, 15, 50), \n                alpha=0.7, color='red')\naxes[0, 2].set_xscale('log')\naxes[0, 2].set_yscale('log')\naxes[0, 2].set_xlabel('Mass (M☉)')\naxes[0, 2].set_ylabel('Count')\naxes[0, 2].set_title('Log-Log: Power Law Revealed!')\n\n# Cumulative distribution\nsorted_masses = np.sort(masses_observed)\ncumulative = np.arange(1, len(sorted_masses) + 1) / len(sorted_masses)\n\naxes[1, 0].plot(sorted_masses, cumulative, 'b-', linewidth=2)\naxes[1, 0].set_xlabel('Mass (M☉)')\naxes[1, 0].set_ylabel('Cumulative Fraction')\naxes[1, 0].set_title('Linear CDF')\n\naxes[1, 1].loglog(sorted_masses, 1 - cumulative, 'r-', linewidth=2)\naxes[1, 1].set_xlabel('Mass (M☉)')\naxes[1, 1].set_ylabel('Fraction > M')\naxes[1, 1].set_title('Log-Log Complementary CDF: Slope = Power Law Index')\n\n# Rank-frequency plot\nranks = np.arange(1, len(sorted_masses) + 1)\naxes[1, 2].loglog(ranks, sorted_masses[::-1], 'g.', markersize=1)\naxes[1, 2].set_xlabel('Rank')\naxes[1, 2].set_ylabel('Mass (M☉)')\naxes[1, 2].set_title('Rank-Frequency: Alternative Power Law View')\n\nplt.suptitle('Same Data, Different Scales, Different Insights', \n             fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n```\n\n### Exercise 3: Debug This!\n\n```{code-cell} python\n\"\"\"\nDebug This! Find and fix the visualization problems.\n\"\"\"\n\ndef plot_hr_diagram_broken(b_v, abs_mag):\n    \"\"\"This function has several plotting issues. Can you fix them?\"\"\"\n    # BUG 1: Figure too small for publication\n    # fig, ax = plt.subplots(figsize=(4, 3))  \n    fig, ax = plt.subplots(figsize=(8, 10))  # FIXED: Appropriate size\n    \n    # BUG 2: Wrong plot type for scattered data\n    # ax.plot(b_v, abs_mag, 'b-')  \n    ax.scatter(b_v, abs_mag, s=20, alpha=0.6, c=b_v, cmap='RdYlBu_r')  # FIXED\n    \n    # BUG 3: Y-axis not inverted (astronomical convention)\n    # ax.set_ylim(-10, 15)  \n    ax.set_ylim(15, -10)  # FIXED: Inverted for magnitudes\n    \n    # BUG 4: No axis labels or units\n    # ax.set_xlabel('x')\n    # ax.set_ylabel('y')\n    ax.set_xlabel('B - V Color Index', fontsize=12)  # FIXED\n    ax.set_ylabel('Absolute Magnitude (M$_V$)', fontsize=12)  # FIXED\n    \n    # BUG 5: No title\n    ax.set_title('Hertzsprung-Russell Diagram', fontsize=14, fontweight='bold')\n    \n    # BUG 6: No grid for reference\n    ax.grid(True, alpha=0.3)  # FIXED\n    \n    # BUG 7: Using jet colormap\n    # (Already fixed above with RdYlBu_r)\n    \n    # Add colorbar\n    plt.colorbar(ax.collections[0], ax=ax, label='B - V Color')\n    \n    return fig, ax\n\n# Test with synthetic data\nnp.random.seed(42)\nn_stars = 1000\n\n# Main sequence\nb_v_ms = np.random.uniform(-0.3, 2.0, n_stars)\nabs_mag_ms = 4.5 * b_v_ms + np.random.normal(0, 1, n_stars) + 2\n\n# Giants branch\nb_v_gb = np.random.uniform(0.5, 2.0, 200)\nabs_mag_gb = np.random.normal(-1, 1, 200)\n\n# Combine\nb_v_all = np.concatenate([b_v_ms, b_v_gb])\nabs_mag_all = np.concatenate([abs_mag_ms, abs_mag_gb])\n\n# Create fixed plot\nfig, ax = plot_hr_diagram_broken(b_v_all, abs_mag_all)\nplt.show()\n\nprint(\"Fixed issues:\")\nprint(\"✓ Increased figure size for readability\")\nprint(\"✓ Changed from line to scatter plot\")\nprint(\"✓ Inverted y-axis (astronomical convention)\")\nprint(\"✓ Added proper axis labels with units\")\nprint(\"✓ Added descriptive title\")\nprint(\"✓ Added grid for reference\")\nprint(\"✓ Used perceptually uniform colormap\")\n```\n\n:::{admonition} 🌟 Why This Matters: The Hubble Tension Revealed Through Visualization\n:class: info, important\n\nThe \"Hubble tension\" — the discrepancy between different measurements of the universe's expansion rate — wasn't discovered through complex statistics but through careful visualization of measurement uncertainties. When plotting H₀ measurements from different methods (CMB, supernovae, gravitational lensing), the error bars don't overlap, revealing a fundamental problem in our understanding of cosmology. Your ability to create clear, honest visualizations with proper error bars might help resolve one of astronomy's biggest mysteries!","type":"content","url":"/matplotlib-fundamentals-v1#id-8-7-color-theory-and-publication-standards","position":29},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl2":"Main Takeaways"},"type":"lvl2","url":"/matplotlib-fundamentals-v1#main-takeaways","position":30},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl2":"Main Takeaways"},"content":"You’ve now mastered the art and science of data visualization with Matplotlib, but more importantly, you’ve learned to think like a visual artist-scientist. The journey from simple plots to publication-ready figures has taught you that visualization isn’t just about displaying data — it’s about experimentation, iteration, and making deliberate aesthetic choices that enhance scientific communication. You’ve discovered that creating effective visualizations requires trying multiple approaches: different scales (linear, log, semilog), different plot types (scatter, line, histogram), and different visual encodings (color, size, transparency) until you find the combination that makes patterns jump off the page. This experimental mindset, combined with the technical skills you’ve developed, transforms you from someone who makes plots into someone who crafts visual arguments that can change how we understand the universe.\n\nThe distinction between Matplotlib’s two interfaces — pyplot and object-oriented — initially seemed like unnecessary complexity, but you now understand it’s about control and reproducibility. While pyplot suffices for quick exploration, the object-oriented approach gives you the artist’s palette you need for research-quality visualizations. You’ve seen how professional figures require attention to countless details: choosing perceptually uniform colormaps over the problematic jet (Crameri et al. 2020), using appropriate scales to reveal power laws or exponential relationships, properly labeling axes with units, and following astronomical conventions like inverting magnitude axes. These aren’t arbitrary rules but hard-won practices that ensure your visualizations communicate honestly and effectively. The famous anatomy figure by Nicolas P. Rougier (2018) that you studied shows how every element — from figure to axes to individual tick marks — is under your control, waiting for your artistic vision.\n\nMost importantly, you’ve learned to build your own plotting toolkit, creating reusable functions that encode your domain knowledge and aesthetic preferences. Instead of copy-pasting code and creating technical debt, you now write functions like plot_light_curve() or plot_spectrum() that embody best practices and can be shared with collaborators. This approach follows the DRY (Don’t Repeat Yourself) principle, ensuring consistency across all your visualizations while making it easy to update styles globally. These personal plotting libraries become more valuable over time, accumulating the wisdom of what works for different types of astronomical data. Whether you’re phase-folding light curves to find exoplanets, creating color-magnitude diagrams to study stellar populations, or displaying multi-wavelength observations to understand cosmic phenomena, you have both the technical skills and the artistic sensibility to create visualizations that don’t just show data but tell stories.\n\nLooking ahead to robust computing, the visualization skills you’ve developed become essential debugging tools. When your code fails or produces unexpected results, a well-chosen plot can instantly reveal where things went wrong. The ability to quickly visualize intermediate results, check distributions, and compare expected versus actual outputs will make you a more effective debugger and a more reliable scientific programmer. Remember that every great astronomical discovery — from the expanding universe (Hubble 1929) to dark energy (Riess et al. 1998; Perlmutter et al. 1999) to exoplanets (Borucki et al. 2010) — was communicated through a carefully crafted visualization. The skills you’ve developed here put you in that tradition, able to create the kinds of plots that don’t just illustrate findings but reveal new truths about the cosmos.","type":"content","url":"/matplotlib-fundamentals-v1#main-takeaways","position":31},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl2":"Definitions"},"type":"lvl2","url":"/matplotlib-fundamentals-v1#definitions","position":32},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl2":"Definitions"},"content":"Axes: The plotting area within a figure where data is visualized, including the x and y axis, tick marks, and labels.\n\nBackend: The rendering engine Matplotlib uses to create and display figures (e.g., Agg, TkAgg, Qt5Agg).\n\nColormap: A mapping from data values to colors for visualization, critical for representing 2D data and images.\n\nDPI: Dots per inch, determining figure resolution for printing or display, typically 72 for screen and 300+ for print.\n\nExperimentation: The iterative process of trying different visualizations to find the most revealing representation.\n\nFigure: The overall container for all plot elements, like a canvas that holds one or more axes.\n\nGridSpec: Matplotlib’s flexible system for creating complex subplot layouts with varying sizes and positions.\n\nLaTeX: A typesetting system commonly used for scientific publications, supported by Matplotlib for mathematical notation.\n\nLight Curve: A plot showing how an astronomical object’s brightness varies over time.\n\nLinear Scale: Equal steps in data correspond to equal distances on the plot.\n\nLogarithmic Scale: Equal multiplicative factors correspond to equal distances on the plot.\n\nNormalization: Scaling data values to a standard range for display, such as linear, logarithmic, or power scaling.\n\nObject-Oriented API: Matplotlib’s powerful interface providing full control over every plot element through explicit objects.\n\nPower Law: A mathematical relationship where y ∝ x^n, appearing as a straight line on a log-log plot.\n\npyplot: Matplotlib’s MATLAB-like procedural interface for quick plotting using implicit current figure/axes.\n\nSpectrum: A plot showing intensity as a function of wavelength or frequency, fundamental in astronomical spectroscopy.","type":"content","url":"/matplotlib-fundamentals-v1#definitions","position":33},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl2":"Key Takeaways"},"type":"lvl2","url":"/matplotlib-fundamentals-v1#key-takeaways","position":34},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl2":"Key Takeaways"},"content":"✓ Matplotlib is your artistic medium — Every plot is an opportunity for creative expression and experimentation\n\n✓ Always experiment with different scales — Linear, log-log, semilog-x, and semilog-y reveal different patterns in your data\n\n✓ Use the object-oriented interface for research — fig, ax = plt.subplots() gives you explicit control needed for publication\n\n✓ Build reusable plotting functions — Create your own library encoding best practices for common astronomical plots\n\n✓ Choose perceptually uniform colormaps — Use viridis, plasma, or cividis; avoid jet which creates false features\n\n✓ Master the anatomy of figures — Understanding Rougier’s diagram empowers you to customize every element\n\n✓ Different plots for different data — Use scatter for measurements, lines for models, histograms for distributions\n\n✓ Save in appropriate formats — Vector (PDF, SVG) for plots, raster (PNG) for images, both for safety\n\n✓ Follow astronomical conventions — Invert magnitude axes, use proper coordinate systems, follow field standards\n\n✓ Visualization reveals patterns — The right plot can make invisible relationships obvious, leading to discoveries","type":"content","url":"/matplotlib-fundamentals-v1#key-takeaways","position":35},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl2":"Quick Reference Tables"},"type":"lvl2","url":"/matplotlib-fundamentals-v1#quick-reference-tables","position":36},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl2":"Quick Reference Tables"},"content":"","type":"content","url":"/matplotlib-fundamentals-v1#quick-reference-tables","position":37},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl3":"Choosing the Right Scale","lvl2":"Quick Reference Tables"},"type":"lvl3","url":"/matplotlib-fundamentals-v1#choosing-the-right-scale","position":38},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl3":"Choosing the Right Scale","lvl2":"Quick Reference Tables"},"content":"Data Type\n\nBest Scale\n\nWhy\n\nPower law (y ∝ x^n)\n\nlog-log\n\nAppears as straight line\n\nExponential (y ∝ e^x)\n\nsemilog-y\n\nAppears as straight line\n\nLogarithmic (y ∝ log(x))\n\nsemilog-x\n\nAppears as straight line\n\nLinear relationship\n\nlinear\n\nDirect proportionality visible\n\nWide dynamic range\n\nlog\n\nShows all scales equally\n\nMagnitudes\n\nlinear (inverted)\n\nAstronomical convention","type":"content","url":"/matplotlib-fundamentals-v1#choosing-the-right-scale","position":39},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl3":"Essential Plot Types","lvl2":"Quick Reference Tables"},"type":"lvl3","url":"/matplotlib-fundamentals-v1#essential-plot-types","position":40},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl3":"Essential Plot Types","lvl2":"Quick Reference Tables"},"content":"Function\n\nUse Case\n\nExample\n\nax.plot()\n\nContinuous data, models\n\nax.plot(x, y, 'b-')\n\nax.scatter()\n\nDiscrete measurements\n\nax.scatter(x, y, s=20)\n\nax.errorbar()\n\nData with uncertainties\n\nax.errorbar(x, y, yerr=err)\n\nax.loglog()\n\nPower laws\n\nax.loglog(freq, power)\n\nax.semilogy()\n\nExponential growth\n\nax.semilogy(time, counts)\n\nax.semilogx()\n\nLogarithmic relationships\n\nax.semilogx(mass, radius)\n\nax.hist()\n\nDistributions\n\nax.hist(data, bins=30)\n\nax.imshow()\n\n2D arrays, images\n\nax.imshow(image, cmap='viridis')","type":"content","url":"/matplotlib-fundamentals-v1#essential-plot-types","position":41},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl3":"Common Customizations","lvl2":"Quick Reference Tables"},"type":"lvl3","url":"/matplotlib-fundamentals-v1#common-customizations","position":42},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl3":"Common Customizations","lvl2":"Quick Reference Tables"},"content":"Method\n\nPurpose\n\nExample\n\nax.set_xlabel()\n\nLabel x-axis\n\nax.set_xlabel('Time (days)')\n\nax.set_ylabel()\n\nLabel y-axis\n\nax.set_ylabel('Flux (Jy)')\n\nax.set_title()\n\nAdd title\n\nax.set_title('Light Curve')\n\nax.legend()\n\nAdd legend\n\nax.legend(loc='upper right')\n\nax.grid()\n\nAdd grid lines\n\nax.grid(True, alpha=0.3)\n\nax.set_xlim()\n\nSet x-axis limits\n\nax.set_xlim(0, 10)\n\nax.invert_yaxis()\n\nFlip axis\n\nax.invert_yaxis()\n\nax.tick_params()\n\nAdjust ticks\n\nax.tick_params(labelsize=10)","type":"content","url":"/matplotlib-fundamentals-v1#common-customizations","position":43},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl3":"Figure Export Settings","lvl2":"Quick Reference Tables"},"type":"lvl3","url":"/matplotlib-fundamentals-v1#figure-export-settings","position":44},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl3":"Figure Export Settings","lvl2":"Quick Reference Tables"},"content":"Format\n\nUse Case\n\nTypical Settings\n\nPDF\n\nPublication (vector)\n\ndpi=300, bbox_inches='tight'\n\nPNG\n\nWeb, backup (raster)\n\ndpi=150-300, transparent=False\n\nSVG\n\nVector editing\n\nbbox_inches='tight'\n\nEPS\n\nLegacy journals\n\ndpi=300, bbox_inches='tight'","type":"content","url":"/matplotlib-fundamentals-v1#figure-export-settings","position":45},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl2":"References"},"type":"lvl2","url":"/matplotlib-fundamentals-v1#references","position":46},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl2":"References"},"content":"Borucki, W. J., et al. (2010). Kepler planet-detection mission: introduction and first results. Science, 327(5968), 977-980.\n\nCrameri, F., Shephard, G. E., & Heron, P. J. (2020). The misuse of colour in science communication. Nature Communications, 11(1), 1-10.\n\nHubble, E. (1929). A relation between distance and radial velocity among extra-galactic nebulae. Proceedings of the National Academy of Sciences, 15(3), 168-173.\n\nHunter, J. D. (2007). Matplotlib: A 2D graphics environment. Computing in Science & Engineering, 9(3), 90-95.\n\nPerlmutter, S., et al. (1999). Measurements of Ω and Λ from 42 high-redshift supernovae. The Astrophysical Journal, 517(2), 565-586.\n\nRiess, A. G., et al. (1998). Observational evidence from supernovae for an accelerating universe and a cosmological constant. The Astronomical Journal, 116(3), 1009-1038.\n\nRougier, N. P. (2018). Scientific Visualization: Python + Matplotlib. Self-published. Available at: \n\nhttps://​github​.com​/rougier​/scientific​-visualization​-book\n\nRougier, N. P., et al. (2014). Ten simple rules for better figures. PLoS Computational Biology, 10(9), e1003833.\n\nSalpeter, E. E. (1955). The luminosity function and stellar evolution. The Astrophysical Journal, 121, 161-167.\n\nSchechter, P. (1976). An analytic expression for the luminosity function for galaxies. The Astrophysical Journal, 203, 297-306.\n\nTufte, E. R. (2001). The Visual Display of Quantitative Information (2nd ed.). Graphics Press.\n\nVanderPlas, J. (2016). Python Data Science Handbook. O’Reilly Media.\n\nWilke, C. O. (2019). Fundamentals of Data Visualization. O’Reilly Media.\n\nWong, B. (2011). Points of view: Color blindness. Nature Methods, 8(6), 441.\n\nThe Matplotlib Development Team. (2023). Matplotlib Documentation. \n\nhttps://​matplotlib​.org​/stable​/index​.html","type":"content","url":"/matplotlib-fundamentals-v1#references","position":47},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl2":"Next Chapter Preview"},"type":"lvl2","url":"/matplotlib-fundamentals-v1#next-chapter-preview","position":48},{"hierarchy":{"lvl1":"Chapter 8: Matplotlib - Visualizing Your Universe","lvl2":"Next Chapter Preview"},"content":"In Chapter 9: Robust Computing - Writing Reliable Scientific Code, you’ll learn how to transform your scripts from fragile prototypes into robust, reliable tools that can handle the messiness of real astronomical data. You’ll master error handling with try-except blocks, learning to gracefully manage missing files, corrupted data, and numerical edge cases that would otherwise crash your analysis. You’ll discover defensive programming techniques that validate inputs, check assumptions, and fail informatively when something goes wrong. Most importantly, you’ll learn to write code that helps you debug problems quickly — using logging instead of print statements, creating useful error messages, and structuring your code to isolate failures. The visualization skills you’ve developed with Matplotlib will become powerful debugging tools, helping you create diagnostic plots that reveal where your code is failing and why. These skills are essential for research computing, where your code needs to process thousands of files from telescopes, handle incomplete observations, and work with data that’s often messier than textbook examples. You’ll learn that robust code isn’t about preventing all errors — it’s about failing gracefully, recovering when possible, and always giving you enough information to understand what went wrong!","type":"content","url":"/matplotlib-fundamentals-v1#next-chapter-preview","position":49},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices"},"type":"lvl1","url":"/python-robust-computing-orig","position":0},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices"},"content":"","type":"content","url":"/python-robust-computing-orig","position":1},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl2":"Learning Objectives"},"type":"lvl2","url":"/python-robust-computing-orig#learning-objectives","position":2},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl2":"Learning Objectives"},"content":"By the end of this chapter, you will be able to:\n\nRead and interpret Python error messages to diagnose problems efficiently\n\nWrite try/except blocks to handle errors gracefully\n\nValidate inputs to prevent errors before they occur\n\nUse assertions to document and verify assumptions\n\nReplace print statements with proper logging\n\nWrite simple tests to verify your functions work correctly\n\nDebug code systematically using proven strategies\n\nUnderstand how errors propagate through scientific calculations","type":"content","url":"/python-robust-computing-orig#learning-objectives","position":3},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl2":"Prerequisites Check"},"type":"lvl2","url":"/python-robust-computing-orig#prerequisites-check","position":4},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl2":"Prerequisites Check"},"content":"Before starting this chapter, verify you can:\n\n✓ Write and call functions with parameters (Chapter 5)\n\n✓ Work with NumPy arrays (Chapter 7)\n\n✓ Use if/else statements and loops (Chapter 3)\n\n✓ Work with lists and dictionaries (Chapter 4)\n\n✓ Create simple plots with Matplotlib (Chapter 8)","type":"content","url":"/python-robust-computing-orig#prerequisites-check","position":5},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl2":"Chapter Overview"},"type":"lvl2","url":"/python-robust-computing-orig#chapter-overview","position":6},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl2":"Chapter Overview"},"content":"Your code will fail. This isn’t pessimism—it’s reality. The difference between beginners and professionals isn’t that professionals write perfect code. It’s that professionals write code that fails gracefully, tells them what went wrong, and helps them fix problems quickly.\n\nRemember in Chapter 5 when we wrote this simple function?def calculate_mean(values):\n    return sum(values) / len(values)\n\nThis optimistic code assumes values is never empty, always contains numbers, and never has missing data. In Chapter 7, we processed NumPy arrays without checking for NaN (Not a Number) values. In Chapter 8, we plotted data without verifying it was plottable. Real scientific data breaks all these assumptions.\n\nThis chapter transforms that naive code into robust code—code that handles unexpected situations gracefully rather than crashing. You’ll learn techniques that prevented disasters like the Mars Climate Orbiter loss and that catch the kinds of errors that have led to retracted papers. By the end, your functions will validate inputs, your scripts will log their progress, and your errors will guide rather than frustrate you.","type":"content","url":"/python-robust-computing-orig#chapter-overview","position":7},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl2":"9.1 Understanding Error Messages"},"type":"lvl2","url":"/python-robust-computing-orig#id-9-1-understanding-error-messages","position":8},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl2":"9.1 Understanding Error Messages"},"content":"Error messages are structured reports that Python generates when something goes wrong during code execution. They tell you exactly what went wrong and where. Learning to read them transforms debugging from guesswork into detective work.","type":"content","url":"/python-robust-computing-orig#id-9-1-understanding-error-messages","position":9},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Your First Error Message","lvl2":"9.1 Understanding Error Messages"},"type":"lvl3","url":"/python-robust-computing-orig#your-first-error-message","position":10},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Your First Error Message","lvl2":"9.1 Understanding Error Messages"},"content":"Let’s start with a simple error and learn to decode it:# Callback to Chapter 5: Remember our temperature conversion?\ndef celsius_to_fahrenheit(celsius):\n    return celsuis * 9/5 + 32  # Typo: 'celsuis' not 'celsius'\n\n# Try to use it\ntemp = 25\nresult = celsius_to_fahrenheit(temp)\n\nThis produces an error message with three critical parts:Traceback (most recent call last):\n  File \"example.py\", line 6, in <module>\n    result = celsius_to_fahrenheit(temp)\n  File \"example.py\", line 2, in celsius_to_fahrenheit\n    return celsuis * 9/5 + 32\nNameError: name 'celsuis' is not defined\n\nRead error messages from bottom to top:\n\nError Type (bottom line): NameError tells you the category of problem. A NameError specifically means Python encountered a variable name it doesn’t recognize.\n\nError Message: “name ‘celsuis’ is not defined” explains what’s wrong. Python is looking for a variable called ‘celsuis’ but can’t find it in the current namespace (the collection of currently defined variables).\n\nLocation (lines above): Shows exactly where the error occurred. The error happened in the file “example.py” on line 2, inside the function celsius_to_fahrenheit.\n\nCall Stack (traceback): The traceback shows the sequence of function calls that led to the error. Think of it like breadcrumbs showing Python’s path through your code. Each level shows which function called the next, helping you understand how the program reached the error.\n\nThis systematic reading approach works for any error. The fix here is obvious—we typed ‘celsuis’ instead of ‘celsius’.","type":"content","url":"/python-robust-computing-orig#your-first-error-message","position":11},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Common Error Types","lvl2":"9.1 Understanding Error Messages"},"type":"lvl3","url":"/python-robust-computing-orig#common-error-types","position":12},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Common Error Types","lvl2":"9.1 Understanding Error Messages"},"content":"Let’s understand the four exception types you’ll encounter most often. An exception is Python’s way of signaling that something exceptional (unusual) has happened that prevents normal execution:# TypeError: Wrong type for operation\ntext = \"5\"\nresult = text * 2      # Works! Gives \"55\" (string repetition)\nresult = text + 2      # TypeError! Can't add string and number\n\n# Why this matters: Reading data from files often gives strings\n# when you expect numbers, causing TypeErrors in calculations\n\nA TypeError occurs when you try to perform an operation on a value of the wrong type. Python is strongly typed, meaning it doesn’t automatically convert between types like strings and numbers.# ValueError: Right type, wrong value  \nimport math\nmath.sqrt(25)     # Works: 5.0\nmath.sqrt(-25)    # ValueError! Can't take sqrt of negative\n\n# Why this matters: Physical calculations have constraints\n# like non-negative masses or temperatures above absolute zero\n\nA ValueError means the type is correct but the value is inappropriate for the operation. The square root function expects a non-negative number—giving it a negative number is the right type but wrong value.# IndexError: Accessing beyond list bounds\ndata = [10, 20, 30]\nprint(data[2])    # Works: 30 (remember: indexing starts at 0)\nprint(data[3])    # IndexError! No index 3\n\n# Why this matters: Off-by-one errors are incredibly common\n# when processing arrays of scientific data\n\nAn IndexError occurs when you try to access a list element that doesn’t exist. Python uses zero-based indexing, meaning the first element is at index 0, which often causes off-by-one errors.# KeyError: Dictionary key doesn't exist\nsensor = {'id': 'A1', 'temp': 25.3}\nprint(sensor['temp'])       # Works: 25.3\nprint(sensor['pressure'])   # KeyError! No 'pressure' key\n\n# Why this matters: Data files might be missing expected fields\n# or use different naming conventions than expected\n\nA KeyError happens when you try to access a dictionary using a key that doesn’t exist. This is common when processing data files where not all records have the same fields.","type":"content","url":"/python-robust-computing-orig#common-error-types","position":13},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Understanding Error Propagation","lvl2":"9.1 Understanding Error Messages"},"type":"lvl3","url":"/python-robust-computing-orig#understanding-error-propagation","position":14},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Understanding Error Propagation","lvl2":"9.1 Understanding Error Messages"},"content":"Error propagation refers to how errors spread through your program, potentially corrupting results far from the original problem. In scientific computing, understanding how errors cascade through calculations is crucial. One bad value can corrupt your entire analysis:# Demonstration: How one error ruins everything\ndef process_measurements(readings):\n    \"\"\"Show how errors propagate through calculations.\"\"\"\n    \n    # Step 1: Calculate mean (fails if any reading is None)\n    total = sum(readings)  # TypeError here if None in list\n    mean = total / len(readings)\n    \n    # Step 2: Never reached due to error above\n    normalized = [r / mean for r in readings]\n    \n    # Step 3: Never reached either\n    return normalized\n\n# One bad value stops everything\ndata = [23.5, 24.1, None, 23.8]  # None from sensor failure\nresult = process_measurements(data)  # Crashes at sum()\n\nWhen Python encounters an error it can’t handle, it immediately stops execution. This is called raising an exception. The exception travels up through the call stack until it either finds code that handles it or reaches the top level and crashes the program.\n\nVisualization of Error Propagation:Input: [23.5, 24.1, None, 23.8]\n   ↓\nStep 1: sum() → TypeError (can't add None)\n   ✗ CRASH (Exception raised)\nStep 2: normalize → Never executed\nStep 3: return → Never reached\n\nResult: No output, just an error\n\nThis demonstrates the fail-fast principle—it’s better to stop immediately when something’s wrong rather than continue with corrupted data that could produce misleading results.","type":"content","url":"/python-robust-computing-orig#understanding-error-propagation","position":15},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"🔍 Check Your Understanding","lvl2":"9.1 Understanding Error Messages"},"type":"lvl3","url":"/python-robust-computing-orig#id-check-your-understanding","position":16},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"🔍 Check Your Understanding","lvl2":"9.1 Understanding Error Messages"},"content":"What error would this code produce and which line would cause it?temperatures = [20.5, 21.0, \"22.5\", 20.8]\ntotal = 0\nfor temp in temperatures:\n    total = total + temp\naverage = total / len(temperatures)\n\nAnswer\n\nThis produces a TypeError on line 4 (inside the loop). When the loop reaches “22.5”, Python tries to execute total + temp which becomes 41.5 + \"22.5\". You can’t add a number and a string.\n\nThe error message would be:TypeError: unsupported operand type(s) for +: 'float' and 'str'\n\nThis error message tells us that the + operator doesn’t support combining a float and a string. The term “operand” refers to the values being operated on (41.5 and “22.5”), and “unsupported” means Python doesn’t know how to add these different types together.\n\nTo fix it, convert the string to a float:total = total + float(temp)\n\nThis is extremely common when reading data from CSV files where numbers might be stored as strings.","type":"content","url":"/python-robust-computing-orig#id-check-your-understanding","position":17},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl2":"9.2 Handling Errors with Try/Except"},"type":"lvl2","url":"/python-robust-computing-orig#id-9-2-handling-errors-with-try-except","position":18},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl2":"9.2 Handling Errors with Try/Except"},"content":"Sometimes errors are expected. Files might not exist. Network connections might fail. Data might be corrupted. Try/except blocks let your program handle these situations gracefully instead of crashing.","type":"content","url":"/python-robust-computing-orig#id-9-2-handling-errors-with-try-except","position":19},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Basic Try/Except Structure","lvl2":"9.2 Handling Errors with Try/Except"},"type":"lvl3","url":"/python-robust-computing-orig#basic-try-except-structure","position":20},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Basic Try/Except Structure","lvl2":"9.2 Handling Errors with Try/Except"},"content":"A try/except block is a control structure that attempts to execute code and provides alternative behavior if an error occurs:def safe_divide(a, b):\n    \"\"\"Divide two numbers, handling division by zero.\"\"\"\n    try:\n        # The try block contains code that might fail\n        result = a / b\n        return result\n    except ZeroDivisionError:\n        # The except block runs only if this specific error occurs\n        print(f\"Warning: Attempted to divide {a} by zero\")\n        return None\n\n# Use it safely\nprint(safe_divide(10, 2))   # Output: 5.0\nprint(safe_divide(10, 0))   # Output: Warning message, then None\n\nThe try block contains code that might raise an exception. If an exception occurs, Python immediately jumps to the except block that matches the exception type. If no exception occurs, the except block is skipped entirely. This is called exception handling—catching and responding to errors rather than letting them crash your program.","type":"content","url":"/python-robust-computing-orig#basic-try-except-structure","position":21},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Building Robust File Readers","lvl2":"9.2 Handling Errors with Try/Except"},"type":"lvl3","url":"/python-robust-computing-orig#building-robust-file-readers","position":22},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Building Robust File Readers","lvl2":"9.2 Handling Errors with Try/Except"},"content":"File operations are where try/except blocks shine. Files might not exist, you might lack permissions, or the content might be corrupted. Let’s build up a robust file reader step by step:# Step 1: Handle missing files (8 lines)\ndef read_file_basic(filename):\n    \"\"\"First lesson: Handle missing files.\"\"\"\n    try:\n        with open(filename, 'r') as f:\n            return f.read()\n    except FileNotFoundError:\n        print(f\"File {filename} not found\")\n        return None\n\nThis handles the most common file error—the file doesn’t exist. The with statement ensures the file is properly closed even if an error occurs, which is called context management.# Step 2: Add handling for permission errors (12 lines)\ndef read_file_safer(filename):\n    \"\"\"Second lesson: Handle multiple error types.\"\"\"\n    try:\n        with open(filename, 'r') as f:\n            return f.read()\n    except FileNotFoundError:\n        print(f\"File {filename} not found\")\n        return None\n    except PermissionError:\n        print(f\"No permission to read {filename}\")\n        return None\n\nNow we handle two different exceptions. Python checks each except block in order, running the first one that matches the raised exception.# Step 3: Process content safely (18 lines)\ndef read_numbers_from_file(filename):\n    \"\"\"Third lesson: Handle content errors too.\"\"\"\n    try:\n        with open(filename, 'r') as f:\n            text = f.read()\n    except FileNotFoundError:\n        print(f\"File {filename} not found\")\n        return None\n    \n    # Now parse the content safely\n    try:\n        numbers = [float(line) for line in text.strip().split('\\n')]\n        return numbers\n    except ValueError as e:\n        print(f\"Invalid number in file: {e}\")\n        return None\n\nThe as e syntax captures the exception object, allowing us to access its error message. This is useful for debugging because it tells us exactly which value caused the problem.","type":"content","url":"/python-robust-computing-orig#building-robust-file-readers","position":23},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"🎯 Why This Matters: The Mars Climate Orbiter Disaster","lvl2":"9.2 Handling Errors with Try/Except"},"type":"lvl3","url":"/python-robust-computing-orig#id-why-this-matters-the-mars-climate-orbiter-disaster","position":24},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"🎯 Why This Matters: The Mars Climate Orbiter Disaster","lvl2":"9.2 Handling Errors with Try/Except"},"content":"In 1999, NASA lost the $125 million Mars Climate Orbiter because one team used metric units while another used imperial units. The software didn’t validate or handle unit mismatches. A simple check could have saved the mission:def combine_thrust_data(value1, unit1, value2, unit2):\n    \"\"\"What the Mars software should have done.\"\"\"\n    try:\n        if unit1 != unit2:\n            # Raising an exception explicitly signals an error\n            raise ValueError(f\"Unit mismatch: {unit1} vs {unit2}\")\n        return value1 + value2\n    except ValueError as e:\n        # Log the error and halt rather than proceed with bad data\n        print(f\"CRITICAL ERROR: {e}\")\n        print(\"Halting operation for safety\")\n        return None\n\nThe raise statement explicitly creates and throws an exception. This is how you signal that something is wrong in your own code. The disaster illustrates why error handling isn’t bureaucracy—it prevents catastrophes.","type":"content","url":"/python-robust-computing-orig#id-why-this-matters-the-mars-climate-orbiter-disaster","position":25},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"When NOT to Use Try/Except","lvl2":"9.2 Handling Errors with Try/Except"},"type":"lvl3","url":"/python-robust-computing-orig#when-not-to-use-try-except","position":26},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"When NOT to Use Try/Except","lvl2":"9.2 Handling Errors with Try/Except"},"content":"Not all errors should be caught. Programming mistakes should fail loudly so you can fix them. This is an important distinction between expected errors (like missing files) and programming errors (like typos):# BAD: Hiding programming errors\ndef bad_statistics(data):\n    try:\n        mean = sum(data) / len(dta)  # Typo: 'dta' not 'data'\n        return mean\n    except:  # Never use bare except!\n        return 0  # Hides the typo error!\n\nA bare except catches all exceptions, including ones you don’t expect. This is dangerous because it hides programming errors.# GOOD: Only catch specific, expected errors\ndef good_statistics(data):\n    \"\"\"Only handle the error we expect.\"\"\"\n    if len(data) == 0:\n        raise ValueError(\"Cannot calculate mean of empty dataset\")\n    \n    mean = sum(data) / len(data)  # Typo would crash (good!)\n    return mean\n\nThe rule: catch errors you expect and can handle. Let unexpected errors crash so you can fix them. This is called selective exception handling.","type":"content","url":"/python-robust-computing-orig#when-not-to-use-try-except","position":27},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"⚠️ Common Bug Alert: The Silent Except","lvl2":"9.2 Handling Errors with Try/Except"},"type":"lvl3","url":"/python-robust-computing-orig#id-common-bug-alert-the-silent-except","position":28},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"⚠️ Common Bug Alert: The Silent Except","lvl2":"9.2 Handling Errors with Try/Except"},"content":"# THE WORST ANTI-PATTERN IN PYTHON\ntry:\n    result = complex_calculation()\nexcept:\n    result = 0  # Silently returns 0 for ANY error\n\n# This hides critical errors like:\n# - Typos in variable names (NameError)\n# - Missing imports (ImportError)\n# - Out of memory (MemoryError)\n# - Keyboard interrupts (KeyboardInterrupt)\n\nThis anti-pattern (a common but harmful coding pattern) makes debugging nearly impossible because errors disappear silently. Always catch specific exceptions.","type":"content","url":"/python-robust-computing-orig#id-common-bug-alert-the-silent-except","position":29},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl2":"9.3 Validating Inputs"},"type":"lvl2","url":"/python-robust-computing-orig#id-9-3-validating-inputs","position":30},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl2":"9.3 Validating Inputs"},"content":"The best error is one that never happens. Input validation is the practice of checking that data meets expected requirements before processing it. This follows the fail-fast principle—detect problems as early as possible.","type":"content","url":"/python-robust-computing-orig#id-9-3-validating-inputs","position":31},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"💡 Computational Thinking: The Guard Clause Pattern","lvl2":"9.3 Validating Inputs"},"type":"lvl3","url":"/python-robust-computing-orig#id-computational-thinking-the-guard-clause-pattern","position":32},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"💡 Computational Thinking: The Guard Clause Pattern","lvl2":"9.3 Validating Inputs"},"content":"Guard clauses are conditional statements at the beginning of a function that check preconditions and exit early if they’re not met. This pattern creates a clear separation between validation and logic:# Without guard clauses - nested complexity\ndef process_data_nested(data):\n    if data is not None:\n        if len(data) > 0:\n            if all(isinstance(x, (int, float)) for x in data):\n                # Actual work buried in nested ifs\n                return sum(data) / len(data)\n    return None\n\nThis nested structure is hard to read and understand. Each level of indentation adds cognitive load.# With guard clauses - linear flow\ndef process_data_clean(data):\n    # Guards at the top\n    if data is None:\n        return None\n    if len(data) == 0:\n        return None\n    if not all(isinstance(x, (int, float)) for x in data):\n        return None\n    \n    # Main logic clear and unindented\n    return sum(data) / len(data)\n\nGuard clauses create linear code flow—you can read from top to bottom without tracking nested conditions. This pattern reduces cognitive load by handling edge cases first, leaving the main logic clean and readable.","type":"content","url":"/python-robust-computing-orig#id-computational-thinking-the-guard-clause-pattern","position":33},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Building Validation Layer by Layer","lvl2":"9.3 Validating Inputs"},"type":"lvl3","url":"/python-robust-computing-orig#building-validation-layer-by-layer","position":34},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Building Validation Layer by Layer","lvl2":"9.3 Validating Inputs"},"content":"Effective validation checks multiple aspects of data. Let’s build robust validation step by step, each focusing on one aspect:# Layer 1: Check for data existence (6 lines)\ndef validate_not_empty(data):\n    \"\"\"First check: Do we have data?\"\"\"\n    if not data:\n        raise ValueError(\"Cannot process empty data\")\n    return True\n\nThe truthiness check if not data works because empty containers (lists, strings, dicts) evaluate to False in Python. This is the cheapest validation—just checking if data exists.# Layer 2: Check data types (10 lines)\ndef validate_numeric(values):\n    \"\"\"Second check: Is data the right type?\"\"\"\n    for i, val in enumerate(values):\n        if not isinstance(val, (int, float)):\n            raise TypeError(\n                f\"Item {i} is {type(val).__name__}, expected number\"\n            )\n    return True\n\nThe isinstance() function checks if a value is of a specific type or types. The __name__ attribute gives us a human-readable type name for error messages.# Layer 3: Check physical constraints (12 lines)\ndef validate_temperature_kelvin(temps):\n    \"\"\"Third check: Does data make physical sense?\"\"\"\n    for i, temp in enumerate(temps):\n        if temp < 0:\n            raise ValueError(\n                f\"Temperature {temp}K at position {i} \"\n                f\"violates absolute zero\"\n            )\n    return True\n\nDomain validation checks if values make sense in your problem domain. Temperature can’t be below absolute zero (0 Kelvin), masses can’t be negative, probabilities must be between 0 and 1.\n\nNow combine them into a complete validation pipeline:def process_temperature_data(measurements):\n    \"\"\"Complete validation pipeline.\"\"\"\n    # Validate in order of increasing cost\n    validate_not_empty(measurements)      # Cheap check first\n    validate_numeric(measurements)        # Medium cost\n    validate_temperature_kelvin(measurements)  # Expensive last\n    \n    # Now safe to process\n    return {\n        'mean': sum(measurements) / len(measurements),\n        'min': min(measurements),\n        'max': max(measurements)\n    }\n\nThe validation order matters for performance optimization. Check cheap conditions first (like emptiness) before expensive ones (like complex calculations).","type":"content","url":"/python-robust-computing-orig#building-validation-layer-by-layer","position":35},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Validating NumPy Arrays","lvl2":"9.3 Validating Inputs"},"type":"lvl3","url":"/python-robust-computing-orig#validating-numpy-arrays","position":36},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Validating NumPy Arrays","lvl2":"9.3 Validating Inputs"},"content":"NumPy arrays from Chapter 7 need special validation for NaN (Not a Number) and infinity values. NaN represents undefined results (like 0/0), while infinity represents overflow:import numpy as np\n\ndef validate_array(arr):\n    \"\"\"Check array for common problems.\"\"\"\n    # Convert to array if needed (defensive programming)\n    data = np.asarray(arr)\n    \n    # Check size\n    if data.size == 0:\n        raise ValueError(\"Empty array\")\n    \n    # Check for NaN (Not a Number - undefined values)\n    n_nan = np.sum(np.isnan(data))\n    if n_nan > 0:\n        print(f\"Warning: {n_nan} NaN values found\")\n    \n    # Check for infinity (overflow values)\n    n_inf = np.sum(np.isinf(data))\n    if n_inf > 0:\n        raise ValueError(f\"{n_inf} infinite values found\")\n    \n    return data\n\nThe np.isnan() and np.isinf() functions return boolean arrays indicating which elements are NaN or infinite. These special values can corrupt calculations if not handled properly.","type":"content","url":"/python-robust-computing-orig#validating-numpy-arrays","position":37},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Performance Cost of Validation","lvl2":"9.3 Validating Inputs"},"type":"lvl3","url":"/python-robust-computing-orig#performance-cost-of-validation","position":38},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Performance Cost of Validation","lvl2":"9.3 Validating Inputs"},"content":"Validation has a performance cost—it takes time to check conditions. Let’s measure it to understand the tradeoff:import time\nimport numpy as np\n\ndef process_without_validation(data):\n    \"\"\"No safety checks.\"\"\"\n    return np.mean(data)\n\ndef process_with_validation(data):\n    \"\"\"With safety checks.\"\"\"\n    if len(data) == 0:\n        raise ValueError(\"Empty data\")\n    if np.any(np.isnan(data)):\n        raise ValueError(\"Contains NaN\")\n    return np.mean(data)\n\n# Measure the cost\ndata = np.random.randn(1000000)  # 1 million random numbers\n\nstart = time.time()\nfor _ in range(100):\n    process_without_validation(data)\nno_check_time = time.time() - start\n\nstart = time.time()\nfor _ in range(100):\n    process_with_validation(data)\ncheck_time = time.time() - start\n\nprint(f\"Without validation: {no_check_time:.3f}s\")\nprint(f\"With validation: {check_time:.3f}s\")\nprint(f\"Overhead: {(check_time/no_check_time - 1)*100:.1f}%\")\n\n# Typical output:\n# Without validation: 0.123s\n# With validation: 0.145s\n# Overhead: 17.9%\n\nThe ~18% overhead (additional time cost) is worth it for catching errors that could invalidate hours of computation. This is a classic tradeoff—spending a little time upfront to save a lot of time debugging later.","type":"content","url":"/python-robust-computing-orig#performance-cost-of-validation","position":39},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"🔍 Check Your Understanding","lvl2":"9.3 Validating Inputs"},"type":"lvl3","url":"/python-robust-computing-orig#id-check-your-understanding-1","position":40},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"🔍 Check Your Understanding","lvl2":"9.3 Validating Inputs"},"content":"Which validation should come first and why?\n\nChecking if temperature is positive\n\nChecking if list is empty\n\nChecking if values are numbers\n\nAnswer\n\nThe correct order is:\n\nCheck if list is empty (fastest, most fundamental)\n\nCheck if values are numbers (can’t check temperature if not numbers)\n\nCheck if temperature is positive (domain-specific, most expensive)\n\nThis follows the principle of “fail fast with cheapest check first.” An empty list check is O(1) (constant time), type checking is O(n) (linear time), and domain validation might involve complex calculations. By ordering checks from cheapest to most expensive, we minimize the average time spent on validation.\n\nThe dependency order also matters—you can’t check if temperatures are positive if you haven’t verified they’re numbers first!","type":"content","url":"/python-robust-computing-orig#id-check-your-understanding-1","position":41},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl2":"9.4 Using Assertions"},"type":"lvl2","url":"/python-robust-computing-orig#id-9-4-using-assertions","position":42},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl2":"9.4 Using Assertions"},"content":"Assertions are debugging aids that verify assumptions about your program’s state. They’re like scientific hypotheses in your code—statements you believe must be true. Python checks them and alerts you if they’re violated. They’re your safety net during development.","type":"content","url":"/python-robust-computing-orig#id-9-4-using-assertions","position":43},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Assertions vs Validation: Know the Difference","lvl2":"9.4 Using Assertions"},"type":"lvl3","url":"/python-robust-computing-orig#assertions-vs-validation-know-the-difference","position":44},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Assertions vs Validation: Know the Difference","lvl2":"9.4 Using Assertions"},"content":"There’s a critical distinction between validating external input and asserting internal correctness:def analyze_spectrum(wavelengths, intensities):\n    # VALIDATION: Check external inputs\n    if len(wavelengths) == 0:\n        raise ValueError(\"No wavelength data provided\")\n    if len(wavelengths) != len(intensities):\n        raise ValueError(\"Wavelength and intensity arrays must match\")\n    \n    # Process data\n    normalized = intensities / np.max(intensities)\n    \n    # ASSERTION: Verify our logic is correct\n    assert len(normalized) == len(intensities), \"Lost data during normalization!\"\n    assert np.all(normalized <= 1.0), \"Normalization failed!\"\n    \n    return normalized\n\nValidation protects against bad input from external sources (users, files, networks). Assertions catch bugs in your logic—they verify that your code does what you think it does. Assertions can be disabled in production with the -O flag, while validation always runs.","type":"content","url":"/python-robust-computing-orig#assertions-vs-validation-know-the-difference","position":45},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Using Assertions to Document Assumptions","lvl2":"9.4 Using Assertions"},"type":"lvl3","url":"/python-robust-computing-orig#using-assertions-to-document-assumptions","position":46},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Using Assertions to Document Assumptions","lvl2":"9.4 Using Assertions"},"content":"Assertions make your assumptions explicit—they document what you believe to be true at specific points in your code:def find_peak(data):\n    \"\"\"Find the maximum value and its index.\"\"\"\n    # Precondition: what must be true at start\n    assert len(data) > 0, \"Requires non-empty data\"\n    \n    max_val = data[0]\n    max_idx = 0\n    \n    for i, val in enumerate(data[1:], 1):\n        if val > max_val:\n            max_val = val\n            max_idx = i\n    \n    # Postconditions: what we guarantee at end\n    assert 0 <= max_idx < len(data), \"Index out of bounds\"\n    assert data[max_idx] == max_val, \"Index doesn't match value\"\n    \n    return max_idx, max_val\n\nPreconditions are assumptions about input state, while postconditions are guarantees about output state. Together they form a contract—if the preconditions are met, the postconditions will be satisfied.","type":"content","url":"/python-robust-computing-orig#using-assertions-to-document-assumptions","position":47},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Assertions in Numerical Algorithms","lvl2":"9.4 Using Assertions"},"type":"lvl3","url":"/python-robust-computing-orig#assertions-in-numerical-algorithms","position":48},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Assertions in Numerical Algorithms","lvl2":"9.4 Using Assertions"},"content":"Assertions are particularly valuable for checking numerical stability—whether calculations maintain mathematical properties despite floating-point limitations:def normalize_to_unit_range(values):\n    \"\"\"Scale values to [0, 1] range.\"\"\"\n    min_val = min(values)\n    max_val = max(values)\n    \n    # Mathematical requirement\n    assert max_val >= min_val, \"Max less than min!\"\n    \n    if max_val == min_val:\n        # All values identical - special case\n        return [0.5] * len(values)\n    \n    # Normalize using linear transformation\n    range_val = max_val - min_val\n    normalized = [(v - min_val) / range_val for v in values]\n    \n    # Verify our math preserved the mathematical properties\n    assert all(0 <= v <= 1 for v in normalized), \\\n        f\"Normalization produced values outside [0,1]\"\n    \n    return normalized\n\nThe assertion checks that our normalization formula (v - min) / (max - min) actually produces values in [0, 1]. This catches numerical errors that could arise from floating-point arithmetic.","type":"content","url":"/python-robust-computing-orig#assertions-in-numerical-algorithms","position":49},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"🛠️ Debug This!","lvl2":"9.4 Using Assertions"},"type":"lvl3","url":"/python-robust-computing-orig#id-debug-this","position":50},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"🛠️ Debug This!","lvl2":"9.4 Using Assertions"},"content":"This function has a subtle bug that the assertion will catch:def calculate_variance(data):\n    \"\"\"Calculate variance with Bessel's correction.\"\"\"\n    n = len(data)\n    assert n > 1, \"Need at least 2 values for variance\"\n    \n    mean = sum(data) / n\n    squared_diffs = [(x - mean)**2 for x in data]\n    variance = sum(squared_diffs) / (n - 1)\n    \n    # This assertion sometimes fails. Why?\n    assert variance >= 0, f\"Variance {variance} is negative!\"\n    \n    return variance\n\n# Test case that breaks it\ndata = [1e20, 1, 2, 3]\nresult = calculate_variance(data)\n\nBug Explanation and Fix\n\nThe bug is catastrophic cancellation—a form of numerical instability that occurs when subtracting nearly equal floating-point numbers. When data contains values of very different magnitudes (1e20 vs 1), the mean is dominated by the large value. Subtracting this large mean from small values can produce negative squared differences due to floating-point rounding errors.\n\nHere’s what happens:\n\nMean ≈ 2.5e19 (dominated by 1e20)\n\n(1 - 2.5e19)² should be positive\n\nBut floating-point arithmetic loses precision\n\nResult can be slightly negative due to rounding\n\nFix using the numerically stable two-pass algorithm:def calculate_variance_stable(data):\n    n = len(data)\n    assert n > 1, \"Need at least 2 values for variance\"\n    \n    # First pass: accurate mean\n    mean = sum(data) / n\n    \n    # Second pass: stable sum of squares\n    sum_sq = 0\n    for x in data:\n        sum_sq += (x - mean) ** 2\n    \n    variance = sum_sq / (n - 1)\n    \n    # Allow tiny negative values from rounding\n    assert variance >= -1e-10, f\"Numerical error: {variance}\"\n    \n    # Clamp to zero if slightly negative\n    return max(0, variance)\n\nThis demonstrates why assertions are crucial for catching numerical instabilities!","type":"content","url":"/python-robust-computing-orig#id-debug-this","position":51},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl2":"9.5 Logging Instead of Print"},"type":"lvl2","url":"/python-robust-computing-orig#id-9-5-logging-instead-of-print","position":52},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl2":"9.5 Logging Instead of Print"},"content":"Professional code uses logging instead of print statements. Logging is a systematic way to record program events with timestamps, severity levels, and structured output. It’s the difference between scribbled notes and a proper lab notebook.","type":"content","url":"/python-robust-computing-orig#id-9-5-logging-instead-of-print","position":53},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"From Print to Logging: A Transformation","lvl2":"9.5 Logging Instead of Print"},"type":"lvl3","url":"/python-robust-computing-orig#from-print-to-logging-a-transformation","position":54},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"From Print to Logging: A Transformation","lvl2":"9.5 Logging Instead of Print"},"content":"Let’s transform print-based debugging into professional logging:# Before: Using print (what we did in Chapter 5)\ndef process_data_print(data):\n    print(\"Starting processing\")\n    print(f\"Got {len(data)} items\")\n    \n    results = []\n    for item in data:\n        if item < 0:\n            print(f\"Warning: negative value {item}\")\n        results.append(abs(item))\n    \n    print(\"Done\")\n    return results# After: Using logging\nimport logging\n\n# Configure once at program start\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s'\n)\n\ndef process_data_logged(data):\n    logging.info(f\"Starting processing of {len(data)} items\")\n    \n    results = []\n    for i, item in enumerate(data):\n        if item < 0:\n            logging.warning(f\"Negative value {item} at index {i}\")\n        results.append(abs(item))\n    \n    logging.info(f\"Completed: processed {len(results)} items\")\n    return results\n\nThe logging module provides structured output with:\n\nTimestamps showing exactly when events occurred\n\nSeverity levels indicating importance (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n\nConsistent formatting making logs easy to parse\n\nFlexible output to console, files, or network\n\nThe logged output includes all this metadata:2024-11-15 10:23:45 - INFO - Starting processing of 5 items\n2024-11-15 10:23:45 - WARNING - Negative value -2 at index 1\n2024-11-15 10:23:45 - INFO - Completed: processed 5 items","type":"content","url":"/python-robust-computing-orig#from-print-to-logging-a-transformation","position":55},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Logging Levels and When to Use Them","lvl2":"9.5 Logging Instead of Print"},"type":"lvl3","url":"/python-robust-computing-orig#logging-levels-and-when-to-use-them","position":56},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Logging Levels and When to Use Them","lvl2":"9.5 Logging Instead of Print"},"content":"Different severity levels serve different purposes. Think of them like different types of lab notebook entries:import logging\n\ndef analyze_measurement(value, expected_range=(0, 100)):\n    \"\"\"Demonstrate all logging levels.\"\"\"\n    \n    # DEBUG: Detailed information for diagnosing problems\n    logging.debug(f\"Raw input: {value}\")\n    \n    if value < expected_range[0]:\n        # ERROR: Something went wrong that prevents normal operation\n        logging.error(f\"Value {value} below minimum {expected_range[0]}\")\n        return None\n    elif value > expected_range[1]:\n        # WARNING: Something unexpected but not fatal\n        logging.warning(f\"Value {value} above typical maximum\")\n    \n    result = value * 2.54  # Convert to metric\n    \n    # INFO: Normal program flow confirmation\n    logging.info(f\"Converted {value} to {result}\")\n    \n    return result\n\n# Set level to control what's shown\nlogging.getLogger().setLevel(logging.DEBUG)  # See everything\n# logging.getLogger().setLevel(logging.WARNING)  # Only warnings and above\n\nThe logging level acts as a filter—only messages at or above the set level are displayed. This lets you control verbosity without changing code.","type":"content","url":"/python-robust-computing-orig#logging-levels-and-when-to-use-them","position":57},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Logging to Files for Permanent Records","lvl2":"9.5 Logging Instead of Print"},"type":"lvl3","url":"/python-robust-computing-orig#logging-to-files-for-permanent-records","position":58},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Logging to Files for Permanent Records","lvl2":"9.5 Logging Instead of Print"},"content":"For long-running computations, file logging creates permanent records you can analyze later:import logging\n\n# Configure file logging\nlogging.basicConfig(\n    filename='computation.log',\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s'\n)\n\ndef long_computation(data):\n    \"\"\"Simulate a long-running process.\"\"\"\n    logging.info(f\"Starting computation with {len(data)} points\")\n    \n    for i, point in enumerate(data):\n        if i % 1000 == 0:\n            # Progress indicators help track long runs\n            logging.info(f\"Processed {i}/{len(data)} points\")\n        \n        # Actual computation here\n        result = complex_calculation(point)\n        \n        if result is None:\n            logging.error(f\"Failed at point {i}\")\n    \n    logging.info(\"Computation complete\")\n\nFile logs provide an audit trail—a permanent record of what happened during execution. This is invaluable for debugging issues that only appear after hours of computation.","type":"content","url":"/python-robust-computing-orig#logging-to-files-for-permanent-records","position":59},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"📊 Performance Profile: Print vs Logging","lvl2":"9.5 Logging Instead of Print"},"type":"lvl3","url":"/python-robust-computing-orig#id-performance-profile-print-vs-logging","position":60},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"📊 Performance Profile: Print vs Logging","lvl2":"9.5 Logging Instead of Print"},"content":"import time\nimport logging\nimport sys\n\n# Test: Impact of debug output\ndata = list(range(10000))\n\n# Measure print\nstart = time.time()\nfor i in data:\n    if i % 1000 == 0:\n        print(f\"Processing {i}\", file=sys.stderr)\nprint_time = time.time() - start\n\n# Measure logging\nstart = time.time()\nfor i in data:\n    if i % 1000 == 0:\n        logging.info(f\"Processing {i}\")\nlog_time = time.time() - start\n\n# Measure logging with DEBUG level (not shown)\nlogging.getLogger().setLevel(logging.WARNING)\nstart = time.time()\nfor i in data:\n    if i % 1000 == 0:\n        logging.debug(f\"Processing {i}\")  # Not displayed\ndebug_time = time.time() - start\n\nprint(f\"Print time: {print_time:.4f}s\")\nprint(f\"Logging time: {log_time:.4f}s\")\nprint(f\"Silent debug time: {debug_time:.4f}s\")\n\n# Typical output:\n# Print time: 0.0234s\n# Logging time: 0.0275s (17% slower but adds timestamps)\n# Silent debug time: 0.0089s (debug calls still have cost)\n\nKey insight: Logging is slightly slower than print but provides much more value. Even “silent” debug statements (below the current logging level) have a small performance cost because Python still evaluates the arguments.","type":"content","url":"/python-robust-computing-orig#id-performance-profile-print-vs-logging","position":61},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl2":"9.6 Writing Simple Tests"},"type":"lvl2","url":"/python-robust-computing-orig#id-9-6-writing-simple-tests","position":62},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl2":"9.6 Writing Simple Tests"},"content":"Testing is the practice of verifying that code behaves as expected. It isn’t about proving code is perfect—it’s about catching obvious bugs before they waste your time. Think of tests as experimental verification of your code’s hypotheses.","type":"content","url":"/python-robust-computing-orig#id-9-6-writing-simple-tests","position":63},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Your First Test Function","lvl2":"9.6 Writing Simple Tests"},"type":"lvl3","url":"/python-robust-computing-orig#your-first-test-function","position":64},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Your First Test Function","lvl2":"9.6 Writing Simple Tests"},"content":"A test function is code that verifies other code works correctly. Let’s test a function from Chapter 5, now with better structure:def kelvin_to_celsius(kelvin):\n    \"\"\"Convert Kelvin to Celsius.\"\"\"\n    return kelvin - 273.15\n\ndef test_kelvin_to_celsius():\n    \"\"\"Test temperature conversion.\"\"\"\n    \n    # Test 1: Known values (ground truth)\n    assert kelvin_to_celsius(273.15) == 0, \"Freezing point wrong\"\n    assert kelvin_to_celsius(373.15) == 100, \"Boiling point wrong\"\n    \n    # Test 2: Boundary conditions\n    assert kelvin_to_celsius(0) == -273.15, \"Absolute zero wrong\"\n    \n    # Test 3: Round trip (inverse operations)\n    temp_c = 25\n    temp_k = temp_c + 273.15\n    assert kelvin_to_celsius(temp_k) == temp_c, \"Round trip failed\"\n    \n    print(\"✓ All temperature tests passed!\")\n\n# Run the test\ntest_kelvin_to_celsius()\n\nGood tests check multiple aspects:\n\nKnown values: Cases where you know the exact answer\n\nBoundary conditions: Edge cases and limits\n\nRound trips: Operations that should cancel out\n\nProperties: Mathematical relationships that must hold","type":"content","url":"/python-robust-computing-orig#your-first-test-function","position":65},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Testing Properties, Not Just Values","lvl2":"9.6 Writing Simple Tests"},"type":"lvl3","url":"/python-robust-computing-orig#testing-properties-not-just-values","position":66},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Testing Properties, Not Just Values","lvl2":"9.6 Writing Simple Tests"},"content":"Property-based testing verifies that mathematical properties hold regardless of specific values:def test_mean_properties():\n    \"\"\"Test properties that must hold for any mean function.\"\"\"\n    \n    # Property 1: Mean of identical values equals that value\n    same = [42.0] * 10\n    assert calculate_mean(same) == 42.0\n    \n    # Property 2: Mean is within data range\n    data = [1, 2, 3, 4, 5]\n    mean = calculate_mean(data)\n    assert min(data) <= mean <= max(data)\n    \n    # Property 3: Scaling data scales mean (linearity)\n    scaled = [x * 2 for x in data]\n    assert calculate_mean(scaled) == mean * 2\n    \n    # Property 4: Mean of two values is their midpoint\n    assert calculate_mean([10, 20]) == 15\n    \n    print(\"✓ Mean properties verified!\")\n\nProperties are more robust than specific values because they test the underlying mathematics rather than individual cases.","type":"content","url":"/python-robust-computing-orig#testing-properties-not-just-values","position":67},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Testing Edge Cases","lvl2":"9.6 Writing Simple Tests"},"type":"lvl3","url":"/python-robust-computing-orig#testing-edge-cases","position":68},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Testing Edge Cases","lvl2":"9.6 Writing Simple Tests"},"content":"Edge cases are unusual inputs that often reveal bugs. They’re the boundaries and special conditions where code is most likely to fail:def remove_outliers(data, threshold=3):\n    \"\"\"Remove values more than threshold stdevs from mean.\"\"\"\n    if len(data) == 0:\n        return []\n    \n    mean = sum(data) / len(data)\n    variance = sum((x - mean)**2 for x in data) / len(data)\n    stdev = variance ** 0.5\n    \n    if stdev == 0:  # All values identical\n        return data\n    \n    return [x for x in data if abs(x - mean) <= threshold * stdev]\n\ndef test_remove_outliers():\n    \"\"\"Test outlier removal with edge cases.\"\"\"\n    \n    # Normal case\n    data = [1, 2, 3, 100, 4, 5]\n    cleaned = remove_outliers(data)\n    assert 100 not in cleaned\n    assert 3 in cleaned\n    \n    # Edge case 1: Empty list\n    assert remove_outliers([]) == []\n    \n    # Edge case 2: Single value\n    assert remove_outliers([42]) == [42]\n    \n    # Edge case 3: All identical (zero variance)\n    same = [5, 5, 5, 5]\n    assert remove_outliers(same) == same\n    \n    # Edge case 4: Two values far apart\n    two = [0, 1000]\n    result = remove_outliers(two, threshold=1)\n    assert len(result) <= 2  # Might remove one or both\n    \n    print(\"✓ Edge cases handled correctly!\")\n\ntest_remove_outliers()\n\nCommon edge cases to test:\n\nEmpty input: No data at all\n\nSingle element: Minimum valid input\n\nIdentical values: No variation\n\nExtreme values: Very large or small numbers\n\nBoundary values: Exactly at limits","type":"content","url":"/python-robust-computing-orig#testing-edge-cases","position":69},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"🎯 Why This Matters: The Ariane 5 Disaster","lvl2":"9.6 Writing Simple Tests"},"type":"lvl3","url":"/python-robust-computing-orig#id-why-this-matters-the-ariane-5-disaster","position":70},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"🎯 Why This Matters: The Ariane 5 Disaster","lvl2":"9.6 Writing Simple Tests"},"content":"In 1996, the Ariane 5 rocket exploded 37 seconds after launch, destroying $370 million in satellites. The cause? Reused code from Ariane 4 wasn’t tested with Ariane 5’s flight parameters. A single untested edge case—a velocity value that exceeded 16-bit integer limits—caused an integer overflow error.def velocity_to_int16(velocity):\n    \"\"\"What went wrong in Ariane 5.\"\"\"\n    # This should have been tested!\n    assert -32768 <= velocity <= 32767, \\\n        f\"Velocity {velocity} exceeds 16-bit range\"\n    return int(velocity)\n\n# Ariane 4 test (passed)\ntest_velocity_to_int16(25000)  # OK\n\n# Ariane 5 test (never run!)\ntest_velocity_to_int16(40000)  # Would have caught the bug!\n\nTesting with realistic data ranges would have prevented this disaster. The lesson: always test with the actual conditions your code will face, not just convenient test values.","type":"content","url":"/python-robust-computing-orig#id-why-this-matters-the-ariane-5-disaster","position":71},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl2":"9.7 Debugging Strategies"},"type":"lvl2","url":"/python-robust-computing-orig#id-9-7-debugging-strategies","position":72},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl2":"9.7 Debugging Strategies"},"content":"Debugging is the process of finding and fixing errors in code. It’s detective work. Instead of randomly changing code hoping it works, follow a systematic approach that mirrors the scientific method.","type":"content","url":"/python-robust-computing-orig#id-9-7-debugging-strategies","position":73},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"The Scientific Method of Debugging","lvl2":"9.7 Debugging Strategies"},"type":"lvl3","url":"/python-robust-computing-orig#the-scientific-method-of-debugging","position":74},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"The Scientific Method of Debugging","lvl2":"9.7 Debugging Strategies"},"content":"Debugging follows the same process as scientific research—observation, hypothesis, experimentation, and analysis:def demonstrate_debugging_process():\n    \"\"\"Show systematic debugging approach.\"\"\"\n    \n    # THE PROBLEM: Function returns wrong result\n    def buggy_variance(data):\n        \"\"\"Calculate variance (has a bug).\"\"\"\n        mean = sum(data) / len(data)\n        diffs = [x - mean for x in data]\n        squares = [d*d for d in diffs]\n        return sum(squares) / len(data) - 1  # Bug here!\n    \n    # STEP 1: OBSERVE - Identify the symptom\n    test_data = [2, 4, 6]\n    result = buggy_variance(test_data)\n    expected = 4.0  # Known correct answer\n    print(f\"Expected {expected}, got {result}\")  # Wrong!\n    \n    # STEP 2: HYPOTHESIZE - Form theories\n    # Theory 1: Mean calculation wrong?\n    # Theory 2: Squared differences wrong?\n    # Theory 3: Final division wrong?\n    \n    # STEP 3: EXPERIMENT - Test each theory\n    mean = sum(test_data) / len(test_data)\n    print(f\"Mean: {mean}\")  # Correct: 4.0\n    \n    diffs = [x - mean for x in test_data]\n    print(f\"Differences: {diffs}\")  # Correct: [-2, 0, 2]\n    \n    squares = [d*d for d in diffs]\n    print(f\"Squares: {squares}\")  # Correct: [4, 0, 4]\n    \n    # Found it! The bug is here:\n    print(f\"Sum/len: {sum(squares)/len(test_data)}\")  # 2.67\n    print(f\"Sum/len - 1: {sum(squares)/len(test_data) - 1}\")  # 1.67 (wrong!)\n    print(f\"Sum/(len-1): {sum(squares)/(len(test_data)-1)}\")  # 4.0 (correct!)\n    \n    # STEP 4: FIX - Correct the bug\n    def variance_fixed(data):\n        mean = sum(data) / len(data)\n        diffs = [x - mean for x in data]\n        squares = [d*d for d in diffs]\n        return sum(squares) / (len(data) - 1)  # Fixed!\n\ndemonstrate_debugging_process()\n\nThis systematic approach is much more efficient than random changes. By testing hypotheses one at a time, you isolate the problem quickly.","type":"content","url":"/python-robust-computing-orig#the-scientific-method-of-debugging","position":75},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Binary Search Debugging","lvl2":"9.7 Debugging Strategies"},"type":"lvl3","url":"/python-robust-computing-orig#binary-search-debugging","position":76},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Binary Search Debugging","lvl2":"9.7 Debugging Strategies"},"content":"Binary search debugging uses the divide-and-conquer principle to isolate problems in complex code:def complex_calculation(data):\n    \"\"\"A multi-step calculation to debug.\"\"\"\n    # Add checkpoints to bisect the problem\n    \n    # First half of calculation\n    step1 = [x * 2 for x in data]\n    print(f\"After step 1: {step1[:3]}...\")  # Checkpoint 1\n    \n    step2 = [x + 10 for x in step1]\n    print(f\"After step 2: {step2[:3]}...\")  # Checkpoint 2\n    \n    # If error occurs here, problem is in first half\n    # If error occurs below, problem is in second half\n    \n    step3 = [x / 3 for x in step2]\n    print(f\"After step 3: {step3[:3]}...\")  # Checkpoint 3\n    \n    step4 = sum(step3) / len(step3)\n    print(f\"Final result: {step4}\")  # Checkpoint 4\n    \n    return step4\n\nBy adding checkpoints (diagnostic output) at strategic locations, you can quickly determine which section contains the bug. This is much faster than checking every line.","type":"content","url":"/python-robust-computing-orig#binary-search-debugging","position":77},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Debugging Flowchart","lvl2":"9.7 Debugging Strategies"},"type":"lvl3","url":"/python-robust-computing-orig#debugging-flowchart","position":78},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Debugging Flowchart","lvl2":"9.7 Debugging Strategies"},"content":"Start: Code produces wrong output\n    ↓\nCan you reproduce the error?\n    No → Add logging, gather more info\n    Yes ↓\n    \nIs the input what you expected?\n    No → Fix input validation\n    Yes ↓\n    \nAdd checkpoint prints at midpoint\n    ↓\nIs the error before or after midpoint?\n    Before → Check first half\n    After → Check second half\n    ↓\n    \nRepeat bisection until problem isolated\n    ↓\nFound the specific line with the bug\n    ↓\nFix and verify with test case\n    ↓\nEnd: Add regression test to prevent reoccurrence\n\nThis decision tree approach ensures you don’t miss steps and helps you debug efficiently.","type":"content","url":"/python-robust-computing-orig#debugging-flowchart","position":79},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Common Debugging Patterns","lvl2":"9.7 Debugging Strategies"},"type":"lvl3","url":"/python-robust-computing-orig#common-debugging-patterns","position":80},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Common Debugging Patterns","lvl2":"9.7 Debugging Strategies"},"content":"Certain bugs appear repeatedly. Recognizing these patterns speeds debugging:# Pattern 1: The Off-By-One Error\ndef find_median_buggy(sorted_data):\n    \"\"\"Common bug: forgetting 0-indexing.\"\"\"\n    n = len(sorted_data)\n    middle = n // 2\n    return sorted_data[middle]  # Bug: wrong for even-length lists\n\ndef find_median_fixed(sorted_data):\n    \"\"\"Fixed: handle even and odd lengths.\"\"\"\n    n = len(sorted_data)\n    if n % 2 == 1:  # Odd length: single middle value\n        return sorted_data[n // 2]\n    else:  # Even length: average of two middle values\n        return (sorted_data[n//2 - 1] + sorted_data[n//2]) / 2\n\nOff-by-one errors occur when you forget that Python uses zero-based indexing or miscalculate array boundaries.# Pattern 2: The Mutation Surprise\ndef normalize_buggy(data):\n    \"\"\"Bug: modifying input data while reading it.\"\"\"\n    for i in range(len(data)):\n        data[i] = data[i] / max(data)  # Max changes as we modify!\n    return data\n\ndef normalize_fixed(data):\n    \"\"\"Fixed: calculate max first.\"\"\"\n    max_val = max(data)  # Store before modifying\n    return [x / max_val for x in data]  # Create new list\n\nMutation bugs happen when you modify data while still using it, causing unexpected behavior.","type":"content","url":"/python-robust-computing-orig#common-debugging-patterns","position":81},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"🔍 Check Your Understanding","lvl2":"9.7 Debugging Strategies"},"type":"lvl3","url":"/python-robust-computing-orig#id-check-your-understanding-2","position":82},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"🔍 Check Your Understanding","lvl2":"9.7 Debugging Strategies"},"content":"This code should double each value, but sometimes produces wrong results. What’s the bug?def double_values(data):\n    for i in range(len(data)):\n        data[i] *= 2\n    return data\n\n# Test\noriginal = [1, 2, 3]\ndoubled = double_values(original)\nprint(f\"Original: {original}\")\nprint(f\"Doubled: {doubled}\")\n\nAnswer\n\nThe bug is in-place modification—the function modifies the input list directly. After calling double_values(original), both original and doubled point to the same modified list.\n\nOutput:Original: [2, 4, 6]  # Changed!\nDoubled: [2, 4, 6]\n\nThis violates the principle of least surprise—functions shouldn’t modify their inputs unless that’s explicitly their purpose. In Python, lists are mutable (can be changed), and when you pass a list to a function, you’re passing a reference to the same list, not a copy.\n\nFix:def double_values_fixed(data):\n    \"\"\"Create new list without modifying input.\"\"\"\n    return [x * 2 for x in data]\n\n# Or if you must modify in-place, make it clear:\ndef double_values_inplace(data):\n    \"\"\"Modifies data in-place (changes input!).\"\"\"\n    for i in range(len(data)):\n        data[i] *= 2\n    # Don't return anything to signal in-place modification\n\nThis bug is common because Python’s pass-by-object-reference behavior isn’t always intuitive.","type":"content","url":"/python-robust-computing-orig#id-check-your-understanding-2","position":83},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl2":"Practice Exercises"},"type":"lvl2","url":"/python-robust-computing-orig#practice-exercises","position":84},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl2":"Practice Exercises"},"content":"","type":"content","url":"/python-robust-computing-orig#practice-exercises","position":85},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Exercise 9.1: Robust Data Reader","lvl2":"Practice Exercises"},"type":"lvl3","url":"/python-robust-computing-orig#exercise-9-1-robust-data-reader","position":86},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Exercise 9.1: Robust Data Reader","lvl2":"Practice Exercises"},"content":"Create a function that safely reads numeric data from a file. Build it incrementally:# Part A: Handle missing files (5 lines)\ndef read_file_basic(filename):\n    \"\"\"Step 1: Handle missing files.\"\"\"\n    # Your code here\n    pass\n\n# Part B: Parse numbers safely (10 lines)\ndef read_numbers_safe(filename):\n    \"\"\"Step 2: Add number parsing.\"\"\"\n    # Build on Part A\n    pass\n\n# Part C: Skip invalid lines (15 lines)\ndef read_data_file(filename):\n    \"\"\"Step 3: Complete robust reader.\n    \n    Should:\n    - Handle missing files gracefully\n    - Skip invalid lines with warning\n    - Return None if no valid data\n    - Return list of floats if successful\n    \"\"\"\n    # Your complete implementation\n    pass\n\n# Test cases:\n# 1. test_missing.txt (doesn't exist)\n# 2. test_empty.txt (empty file)\n# 3. test_mixed.txt (numbers and text)\n# 4. test_valid.txt (all valid numbers)","type":"content","url":"/python-robust-computing-orig#exercise-9-1-robust-data-reader","position":87},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Exercise 9.2: Validated Statistics Function","lvl2":"Practice Exercises"},"type":"lvl3","url":"/python-robust-computing-orig#exercise-9-2-validated-statistics-function","position":88},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Exercise 9.2: Validated Statistics Function","lvl2":"Practice Exercises"},"content":"Build on Chapter 7 to create a robust statistics calculator:import numpy as np\n\ndef calculate_stats(data):\n    \"\"\"\n    Calculate statistics with full validation.\n    \n    Should:\n    - Validate input is numeric array\n    - Handle empty arrays\n    - Check for NaN and infinity\n    - Warn about outliers (values > 3 std from mean)\n    - Return dict with mean, std, min, max, n_valid\n    \n    Returns None if data cannot be processed.\n    \"\"\"\n    # Your implementation here\n    pass\n\n# Test with:\ntest_cases = [\n    [1, 2, 3, 4, 5],           # Normal\n    [],                         # Empty\n    [1, 2, np.nan, 4],         # Contains NaN\n    [1, 2, 3, 100],            # Contains outlier\n    [1, np.inf, 3],            # Contains infinity\n]","type":"content","url":"/python-robust-computing-orig#exercise-9-2-validated-statistics-function","position":89},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Exercise 9.3: Comprehensive Test Suite","lvl2":"Practice Exercises"},"type":"lvl3","url":"/python-robust-computing-orig#exercise-9-3-comprehensive-test-suite","position":90},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Exercise 9.3: Comprehensive Test Suite","lvl2":"Practice Exercises"},"content":"Write thorough tests for this function:def find_peaks(data, threshold=0):\n    \"\"\"Find local maxima above threshold.\n    \n    A peak is a value greater than both neighbors\n    and above the threshold.\n    \"\"\"\n    if len(data) < 3:\n        return []  # No peaks possible\n    \n    peaks = []\n    for i in range(1, len(data) - 1):\n        if data[i] > threshold:\n            if data[i] > data[i-1] and data[i] > data[i+1]:\n                peaks.append(i)\n    \n    return peaks\n\ndef test_find_peaks():\n    \"\"\"Write comprehensive tests.\n    \n    Should test:\n    - Normal case with clear peaks\n    - No peaks (monotonic data)\n    - All peaks (zigzag data)\n    - Edge cases (empty, single value, two values)\n    - Threshold filtering\n    - Plateau handling (consecutive equal values)\n    \"\"\"\n    # Your tests here\n    pass","type":"content","url":"/python-robust-computing-orig#exercise-9-3-comprehensive-test-suite","position":91},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Exercise 9.4: Debug and Fix","lvl2":"Practice Exercises"},"type":"lvl3","url":"/python-robust-computing-orig#exercise-9-4-debug-and-fix","position":92},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Exercise 9.4: Debug and Fix","lvl2":"Practice Exercises"},"content":"This data processing pipeline has multiple bugs. Find and fix them:def process_sensor_data(readings, calibration_offset):\n    \"\"\"Process sensor readings with calibration.\n    \n    This function has 3 bugs. Find them using debugging\n    techniques from the chapter.\n    \"\"\"\n    \n    # Apply calibration\n    calibrated = []\n    for reading in readings:\n        calibrated.append(reading - calibration_offset)\n    \n    # Remove negative values (physically impossible)\n    valid = []\n    for i in range(len(calibrated)):\n        if calibrated[i] >= 0:\n            valid.append(calibrated[i])\n    \n    # Calculate statistics\n    mean = sum(valid) / len(valid)\n    variance = 0\n    for value in valid:\n        variance += (value - mean) ** 2\n    variance = variance / len(valid) - 1\n    \n    return {\n        'mean': mean,\n        'variance': variance,\n        'n_valid': len(valid),\n        'n_rejected': len(readings) - len(valid)\n    }\n\n# Debug with these test cases:\ntest1 = process_sensor_data([10, 20, 30], 5)\ntest2 = process_sensor_data([1, 2, 3], 10)  # All become negative\ntest3 = process_sensor_data([], 0)  # Empty input","type":"content","url":"/python-robust-computing-orig#exercise-9-4-debug-and-fix","position":93},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl2":"Main Takeaways (Summary)"},"type":"lvl2","url":"/python-robust-computing-orig#main-takeaways-summary","position":94},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl2":"Main Takeaways (Summary)"},"content":"This chapter transformed you from writing hopeful code to creating robust, professional software. Here are the essential concepts you’ve mastered:\n\nError Understanding: You now read error messages systematically from bottom to top, understanding that exceptions are Python’s way of communicating problems. Each error type (TypeError, ValueError, IndexError, KeyError) tells you something specific about what went wrong.\n\nException Handling: You’ve learned to use try/except blocks to gracefully handle expected errors like missing files or invalid input, while letting programming errors crash loudly so you can fix them. The key principle: catch only what you expect and can handle.\n\nInput Validation: You implement the fail-fast principle using guard clauses to check inputs at function boundaries. Validation happens in order of cost (cheap checks first) and catches problems before they corrupt results.\n\nAssertions as Documentation: You use assertions to verify your code’s logic and document assumptions. They’re your safety net during development, catching mathematical impossibilities and numerical instabilities.\n\nProfessional Logging: You’ve replaced print statements with structured logging that provides timestamps, severity levels, and permanent records. This creates an audit trail for debugging long-running computations.\n\nSystematic Testing: You write test functions that verify known values, mathematical properties, and edge cases. Tests prevent regression—old bugs reappearing when you modify code.\n\nScientific Debugging: You approach debugging like a scientist—observing symptoms, forming hypotheses, experimenting to test them, and analyzing results. Binary search debugging helps you quickly isolate problems in complex code.\n\nThe overarching theme: defensive programming. Every technique in this chapter helps you write code that anticipates problems, handles them gracefully, and helps you fix issues quickly when they arise. This is what separates scripts that work once from tools you can trust with your research.","type":"content","url":"/python-robust-computing-orig#main-takeaways-summary","position":95},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl2":"Definitions"},"type":"lvl2","url":"/python-robust-computing-orig#definitions","position":96},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl2":"Definitions"},"content":"Anti-pattern: A common but harmful coding pattern that should be avoided (e.g., bare except clauses).\n\nAssertion: A debugging aid that verifies assumptions about program state; can be disabled in production.\n\nBare except: An except clause without specifying exception type; dangerous because it catches all errors.\n\nBinary search debugging: Debugging technique that isolates problems by repeatedly dividing code in half.\n\nCall stack: The sequence of function calls that led to the current point in execution.\n\nCatastrophic cancellation: Numerical instability from subtracting nearly equal floating-point numbers.\n\nContext management: Ensuring resources (like files) are properly acquired and released using ‘with’ statements.\n\nDefensive programming: Writing code that anticipates and handles potential problems.\n\nDomain validation: Checking if values make sense in your problem domain (e.g., positive temperatures).\n\nEdge case: Unusual or boundary input that often reveals bugs.\n\nError propagation: How errors spread through calculations, potentially corrupting all downstream results.\n\nException: Python’s way of signaling that something exceptional has happened preventing normal execution.\n\nException handling: Catching and responding to errors using try/except blocks.\n\nFail-fast principle: Detecting and reporting problems as early as possible.\n\nGuard clause: Conditional statement at function start that checks preconditions and exits early if not met.\n\nIn-place modification: Changing data directly rather than creating a new copy.\n\nIndexError: Exception raised when accessing a list index that doesn’t exist.\n\nInput validation: Checking that data meets requirements before processing.\n\nInteger overflow: When a number exceeds the maximum value for its type.\n\nKeyError: Exception raised when accessing a dictionary key that doesn’t exist.\n\nLinear code flow: Code structure that can be read top to bottom without nested conditions.\n\nLogging: Systematic recording of program events with timestamps and severity levels.\n\nLogging level: Filter controlling which log messages are displayed (DEBUG, INFO, WARNING, ERROR, CRITICAL).\n\nMutation bug: Error caused by modifying data while still using it.\n\nNameError: Exception raised when referencing an undefined variable.\n\nNamespace: The collection of currently defined variables and their values.\n\nNaN (Not a Number): Special floating-point value representing undefined results.\n\nNumerical stability: Whether calculations maintain accuracy despite floating-point limitations.\n\nOff-by-one error: Common bug from miscalculating array boundaries or forgetting zero-based indexing.\n\nOverhead: Additional time or resource cost of an operation.\n\nPass-by-object-reference: Python’s parameter passing mechanism where functions receive references to objects.\n\nPerformance cost: Time or resources required for an operation.\n\nPostcondition: What a function guarantees about its output state.\n\nPrecondition: What must be true about input for a function to work correctly.\n\nProperty-based testing: Testing mathematical properties rather than specific values.\n\nRaise statement: Explicitly creating and throwing an exception.\n\nRegression: When previously fixed bugs reappear after code changes.\n\nRegression test: Test that ensures old bugs don’t reappear.\n\nRobust code: Code that handles unexpected situations gracefully.\n\nSelective exception handling: Only catching specific, expected exceptions.\n\nTest function: Code that verifies other code works correctly.\n\nTesting: Process of verifying code behaves as expected.\n\nTraceback: Report showing the sequence of function calls leading to an error.\n\nTradeoff: Balancing competing concerns (e.g., safety vs performance).\n\nTry block: Code section that might raise an exception.\n\nTypeError: Exception raised when operation receives wrong type.\n\nValidation: Checking that external input meets requirements.\n\nValueError: Exception raised when operation receives right type but wrong value.\n\nZero-based indexing: Numbering system where first element is at index 0.","type":"content","url":"/python-robust-computing-orig#definitions","position":97},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl2":"Key Takeaways"},"type":"lvl2","url":"/python-robust-computing-orig#key-takeaways","position":98},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl2":"Key Takeaways"},"content":"✅ Error messages are maps to bugs - Read from bottom (what went wrong) to top (where it happened) for quick diagnosis\n\n✅ Try/except handles expected failures - Catch specific exceptions for files, network, and user input; let programming errors crash\n\n✅ Validation is your first defense - Check inputs at function boundaries using the guard clause pattern to fail fast\n\n✅ Assertions verify your logic - Use them to document assumptions and catch mathematical impossibilities during development\n\n✅ Logging provides persistent insight - Replace print with logging for timestamps, severity levels, and permanent records\n\n✅ Tests prevent regression - Simple tests of properties and edge cases catch bugs before they waste hours of debugging\n\n✅ Debugging is systematic science - Follow observe→hypothesize→experiment→fix rather than random changes\n\n✅ Errors propagate and compound - One unhandled error can corrupt entire pipelines; catch problems early\n\n✅ Real disasters come from missing validation - Mars Climate Orbiter, Ariane 5, and other failures were preventable with proper error handling","type":"content","url":"/python-robust-computing-orig#key-takeaways","position":99},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl2":"Quick Reference Tables"},"type":"lvl2","url":"/python-robust-computing-orig#quick-reference-tables","position":100},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl2":"Quick Reference Tables"},"content":"","type":"content","url":"/python-robust-computing-orig#quick-reference-tables","position":101},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Error Types and Meanings","lvl2":"Quick Reference Tables"},"type":"lvl3","url":"/python-robust-computing-orig#error-types-and-meanings","position":102},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Error Types and Meanings","lvl2":"Quick Reference Tables"},"content":"Exception\n\nMeaning\n\nCommon Cause\n\nExample\n\nNameError\n\nVariable undefined\n\nTypo in name\n\nprint(resuIt) not result\n\nTypeError\n\nWrong type\n\nString not number\n\n\"5\" + 2\n\nValueError\n\nInvalid value\n\nOutside range\n\nmath.sqrt(-1)\n\nIndexError\n\nIndex too large\n\nOff-by-one\n\narr[len(arr)]\n\nKeyError\n\nMissing dict key\n\nTypo or absent\n\ndict['temp'] not there\n\nZeroDivisionError\n\nDivision by zero\n\nEmpty dataset\n\nsum([])/len([])\n\nFileNotFoundError\n\nFile missing\n\nWrong path\n\nWrong filename","type":"content","url":"/python-robust-computing-orig#error-types-and-meanings","position":103},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Validation Strategy","lvl2":"Quick Reference Tables"},"type":"lvl3","url":"/python-robust-computing-orig#validation-strategy","position":104},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Validation Strategy","lvl2":"Quick Reference Tables"},"content":"Check Type\n\nCode Pattern\n\nWhen to Use\n\nCost\n\nEmpty check\n\nif not data:\n\nAlways first\n\nO(1)\n\nType check\n\nisinstance(x, type)\n\nMixed inputs\n\nO(1)\n\nRange check\n\nmin <= x <= max\n\nPhysical limits\n\nO(1)\n\nNaN check\n\nnp.isnan(x)\n\nNumerical data\n\nO(n)\n\nUniqueness\n\nlen(set(x)) == len(x)\n\nDuplicates bad\n\nO(n)","type":"content","url":"/python-robust-computing-orig#validation-strategy","position":105},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Logging Best Practices","lvl2":"Quick Reference Tables"},"type":"lvl3","url":"/python-robust-computing-orig#logging-best-practices","position":106},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Logging Best Practices","lvl2":"Quick Reference Tables"},"content":"Level\n\nUse Case\n\nExample Message\n\nDEBUG\n\nVariable values\n\n“Array shape: (100, 50)”\n\nINFO\n\nNormal progress\n\n“Processing file 3 of 10”\n\nWARNING\n\nConcerning but OK\n\n“Low sample size: n=5”\n\nERROR\n\nOperation failed\n\n“Cannot read config file”\n\nCRITICAL\n\nMust stop\n\n“Database connection lost”","type":"content","url":"/python-robust-computing-orig#logging-best-practices","position":107},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Testing Checklist","lvl2":"Quick Reference Tables"},"type":"lvl3","url":"/python-robust-computing-orig#testing-checklist","position":108},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl3":"Testing Checklist","lvl2":"Quick Reference Tables"},"content":"Test Type\n\nWhat to Test\n\nExample\n\nNormal case\n\nCommon usage\n\nValid input range\n\nEdge cases\n\nBoundaries\n\nEmpty, single item\n\nError cases\n\nInvalid input\n\nWrong type, NaN\n\nProperties\n\nMath invariants\n\nMean in [min, max]\n\nRegression\n\nPrevious bugs\n\nSpecific failure case","type":"content","url":"/python-robust-computing-orig#testing-checklist","position":109},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl2":"Next Chapter Preview"},"type":"lvl2","url":"/python-robust-computing-orig#next-chapter-preview","position":110},{"hierarchy":{"lvl1":"Chapter 9: Robust Computing Fundamentals - Error Handling and Best Practices","lvl2":"Next Chapter Preview"},"content":"Now that your code can handle errors gracefully, Chapter 10 will explore reading and writing scientific data formats. You’ll learn to work with CSV files, JSON data, and binary formats like HDF5. The error handling skills from this chapter will be essential when dealing with external data files where formats might be inconsistent, values might be missing, and files might be corrupted.\n\nYou’re building the foundation for robust scientific computing. Your code no longer just works—it works reliably, tells you when something’s wrong, and helps you fix problems quickly. This transformation from hopeful code to professional code is what separates scripts that work once from tools you can trust with your research.","type":"content","url":"/python-robust-computing-orig#next-chapter-preview","position":111},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software"},"type":"lvl1","url":"/python-advanced-oop","position":0},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software"},"content":"","type":"content","url":"/python-advanced-oop","position":1},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl2":"Learning Objectives"},"type":"lvl2","url":"/python-advanced-oop#learning-objectives","position":2},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl2":"Learning Objectives"},"content":"By the end of this chapter, you will be able to:\n\nDesign abstract base classes that enforce scientific interfaces and invariants\n\nImplement advanced inheritance with mixins and multiple inheritance for code reuse\n\nApply metaprogramming techniques including descriptors and metaclasses for domain-specific behavior\n\nUse design patterns (Factory, Observer, Strategy) to solve recurring architectural problems\n\nImplement dataclasses to reduce boilerplate in data-heavy scientific classes\n\nCreate asynchronous code for concurrent instrument control and data acquisition\n\nOptimize memory and performance using __slots__, caching, and profiling techniques\n\nRecognize these patterns in NumPy, SciPy, Matplotlib, and Astropy source code","type":"content","url":"/python-advanced-oop#learning-objectives","position":3},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl2":"Prerequisites Check"},"type":"lvl2","url":"/python-advanced-oop#prerequisites-check","position":4},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl2":"Prerequisites Check"},"content":"Before starting this chapter, verify you can:\n\n✓ Create classes with methods, properties, and special methods (Chapter 6)\n\n✓ Use NumPy arrays and vectorized operations (Chapters 7-8)\n\n✓ Create Matplotlib figures and subplots (Chapter 9)\n\n✓ Work with decorators like @property (Chapter 6)\n\n✓ Understand array broadcasting and ufuncs (Chapter 8)\n\nQuick diagnostic:\n\n# Can you identify the OOP patterns in this NumPy/Matplotlib code?\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Pattern 1: Factory\narr = np.array([1, 2, 3])  # What pattern?\n\n# Pattern 2: Context Manager  \nwith plt.style.context('dark_background'):  # What pattern?\n    fig, ax = plt.subplots()  # Pattern 3: ?\n\n# Pattern 4: Method chaining\nresult = arr.reshape(3, 1).mean(axis=0)  # What pattern?\n\nIf you recognized factory, context manager, factory again, and fluent interface patterns, you’re ready to understand how they work internally!","type":"content","url":"/python-advanced-oop#prerequisites-check","position":5},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl2":"Chapter Overview"},"type":"lvl2","url":"/python-advanced-oop#chapter-overview","position":6},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl2":"Chapter Overview"},"content":"You’ve mastered NumPy’s powerful arrays and created stunning visualizations with Matplotlib. You’ve noticed that NumPy arrays somehow work with every mathematical operator, that Matplotlib figures manage complex state across multiple method calls, and that both libraries seem to magically know how to work together. How does np.array([1,2,3]) + np.array([4,5,6]) know to add element-wise? Why can you pass NumPy arrays directly to Matplotlib functions? How do these libraries coordinate thousands of classes and millions of lines of code while remaining so elegant to use? The answer lies in advanced object-oriented patterns - the architectural principles that make scientific Python possible.\n\nThis chapter reveals the design patterns hidden throughout the scientific Python ecosystem you’ve been using. You’ll discover that NumPy’s universal functions (ufuncs) rely on abstract base classes to ensure every array type supports the same operations. Matplotlib’s figure and axes use mixins to compose dozens of plotting capabilities without code duplication. Astropy’s units system uses descriptors and metaclasses to catch unit errors at assignment time, not after your spacecraft crashes into Mars. These aren’t academic exercises - they’re the exact patterns that power the tools enabling modern astronomical discoveries, from detecting gravitational waves to imaging black holes.\n\nMost importantly, you’ll develop the architectural thinking needed to contribute to these packages or build your own research-grade software. You’ll understand why certain design decisions were made, recognize patterns when reading source code, and know when to apply these techniques in your own work. By the end, you’ll see NumPy, Matplotlib, and Astropy not as black boxes but as masterclasses in software architecture, and you’ll have the skills to architect systems at the same level. This isn’t just about learning advanced Python - it’s about joining the community of developers who build the tools that enable scientific discovery.","type":"content","url":"/python-advanced-oop#chapter-overview","position":7},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl2":"10.1 Abstract Base Classes: Defining Scientific Interfaces"},"type":"lvl2","url":"/python-advanced-oop#id-10-1-abstract-base-classes-defining-scientific-interfaces","position":8},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl2":"10.1 Abstract Base Classes: Defining Scientific Interfaces"},"content":"{margin} Abstract Base Class (ABC)\nA class that cannot be instantiated and defines methods that subclasses must implement.\n\n{margin} Interface\nA contract specifying what methods a class must provide, ensuring compatibility between components.\n\nNow that you’ve used NumPy and Matplotlib extensively, you’ve encountered abstract base classes without realizing it. Every time NumPy performs an operation on different array types, it’s using ABCs to ensure compatibility. Let’s understand how this works.\n\nfrom abc import ABC, abstractmethod\nimport numpy as np\n\n# First, see the problem ABCs solve in astronomy\nclass BadCCDArray:\n    def read_counts(self): \n        return np.array([100, 200, 300])  # photon counts\n\nclass BadIRArray:\n    def get_flux(self):  # Different method name!\n        return np.array([50, 60, 70])  # flux in erg/s/cm²\n\n# Without ABCs, incompatible interfaces cause runtime errors\n# You've seen this problem when different instruments use different formats\n\nNow let’s solve this with ABCs, using CGS units throughout:\n\nfrom abc import ABC, abstractmethod\n\nclass AstronomicalDetector(ABC):\n    \"\"\"Abstract base defining detector interface in CGS units.\"\"\"\n    \n    @abstractmethod\n    def read_frame(self) -> np.ndarray:\n        \"\"\"Read one frame of data in erg/s/cm².\"\"\"\n        pass\n    \n    @abstractmethod\n    def get_noise(self) -> float:\n        \"\"\"Return noise in erg/s/cm².\"\"\"\n        pass\n    \n    @abstractmethod\n    def get_area_cm2(self) -> float:\n        \"\"\"Return detector area in cm².\"\"\"\n        pass\n    \n    # Concrete methods provide shared functionality\n    def photons_to_flux(self, counts, wavelength_cm):\n        \"\"\"Convert photon counts to flux in erg/s/cm².\n        \n        E = hν = hc/λ where:\n        h = 6.626e-27 erg·s (Planck in CGS)\n        c = 2.998e10 cm/s (speed of light in CGS)\n        \"\"\"\n        h = 6.626e-27  # erg·s\n        c = 2.998e10   # cm/s\n        energy_per_photon = h * c / wavelength_cm\n        return counts * energy_per_photon\n\n# Now implementations MUST follow the interface\nclass CCDDetector(AstronomicalDetector):\n    \"\"\"CCD implementation with CGS units.\"\"\"\n    \n    def __init__(self, area_cm2=4.0, gain=1.0):\n        self.area = area_cm2  # typically 2x2 cm for astronomy CCDs\n        self.gain = gain\n        self.read_noise = 5.0  # electrons\n    \n    def read_frame(self) -> np.ndarray:\n        \"\"\"Read CCD frame, return flux in erg/s/cm².\"\"\"\n        # Simulate Poisson photon noise\n        photons = np.random.poisson(1000, size=(1024, 1024))\n        # Convert to flux at 550nm (5.5e-5 cm)\n        return self.photons_to_flux(photons, 5.5e-5)\n    \n    def get_noise(self) -> float:\n        \"\"\"RMS noise in erg/s/cm².\"\"\"\n        # Convert read noise to flux\n        return self.photons_to_flux(self.read_noise, 5.5e-5)\n    \n    def get_area_cm2(self) -> float:\n        \"\"\"CCD area in cm².\"\"\"\n        return self.area\n\nclass InfraredArray(AstronomicalDetector):\n    \"\"\"IR array implementation with CGS units.\"\"\"\n    \n    def __init__(self, area_cm2=1.0):\n        self.area = area_cm2  # smaller pixels for IR\n        self.dark_current = 0.1  # e-/s at 77K\n    \n    def read_frame(self) -> np.ndarray:\n        \"\"\"Read IR frame at 2.2 microns.\"\"\"\n        # IR arrays have different noise characteristics\n        signal = np.random.randn(256, 256) * 10 + 100\n        # Convert to flux at 2.2 microns (2.2e-4 cm)\n        return self.photons_to_flux(signal, 2.2e-4)\n    \n    def get_noise(self) -> float:\n        \"\"\"Dark current noise in erg/s/cm².\"\"\"\n        return self.photons_to_flux(self.dark_current, 2.2e-4)\n    \n    def get_area_cm2(self) -> float:\n        return self.area\n\n# Process any detector uniformly\ndef calculate_sensitivity(detector: AstronomicalDetector) -> float:\n    \"\"\"Calculate sensitivity for any detector in erg/s/cm².\"\"\"\n    signal = detector.read_frame()\n    noise = detector.get_noise()\n    area = detector.get_area_cm2()\n    \n    # Signal-to-noise ratio calculation\n    snr = np.mean(signal) / noise if noise > 0 else 0\n    sensitivity = noise * 3 / area  # 3-sigma detection limit\n    \n    return sensitivity\n\n# Works with ANY detector implementing the interface!\nccd = CCDDetector(area_cm2=4.0)\nir = InfraredArray(area_cm2=1.0)\n\nprint(f\"CCD sensitivity: {calculate_sensitivity(ccd):.2e} erg/s/cm³\")\nprint(f\"IR sensitivity: {calculate_sensitivity(ir):.2e} erg/s/cm³\")\n\n🎯 The More You Know: How ABCs Saved the Event Horizon Telescope\n\nIn April 2019, the world saw the first image of a black hole - that stunning orange ring around M87*. Behind this image was a software nightmare that abstract base classes solved. The Event Horizon Telescope wasn’t one telescope but eight, scattered from Hawaii to Antarctica to Spain, each with completely different hardware, data formats, and recording systems.\n\nThe challenge was staggering. ALMA in Chile produced 16 terabytes per night in FITS format at 230 GHz. The South Pole Telescope recorded 8 TB in Mark5B format at 240 GHz. The IRAM 30-meter used VDIF format. Early integration attempts were disasters - as Katie Bouman’s team discovered, code written for one telescope would crash catastrophically with another’s data.\n\nThe breakthrough came when they defined abstract interfaces that each telescope had to implement:class EHTStation(ABC):\n    @abstractmethod\n    def get_visibility(self, time, baseline_cm):\n        \"\"\"Return visibility in Jy at baseline in cm.\"\"\"\n        pass\n    \n    @abstractmethod\n    def get_uv_coverage(self, hour_angle):\n        \"\"\"Return (u,v) in megalambda.\"\"\"\n        pass\n    \n    @abstractmethod\n    def correct_atmosphere(self, data, pwv_mm):\n        \"\"\"Correct for atmosphere with precipitable water vapor in mm.\"\"\"\n        pass\n\nEach telescope team could implement these methods however they needed. ALMA’s implementation was 5,000 lines handling their 66-dish array correlator. The single-dish stations were just 500 lines. But the correlation pipeline didn’t care - it just called get_visibility() and trusted each station to handle the details.\n\nThis abstraction proved critical during the final week of processing. When they discovered the South Pole Telescope’s hydrogen maser clock was drifting by 100 nanoseconds per day (catastrophic for interferometry where timing must be precise to picoseconds), they only had to fix SPT’s correct_timing() method. The rest of the pipeline, processing petabytes of data, continued running without modification.\n\nThe image of the black hole’s event horizon - that first glimpse of spacetime bent to its breaking point - exists because abstract base classes let eight incompatible telescopes pretend to be one Earth-sized instrument. It’s the same pattern you’re learning, applied to one of astronomy’s greatest achievements!","type":"content","url":"/python-advanced-oop#id-10-1-abstract-base-classes-defining-scientific-interfaces","position":9},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl3":"Advanced ABC Patterns with NumPy Integration","lvl2":"10.1 Abstract Base Classes: Defining Scientific Interfaces"},"type":"lvl3","url":"/python-advanced-oop#advanced-abc-patterns-with-numpy-integration","position":10},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl3":"Advanced ABC Patterns with NumPy Integration","lvl2":"10.1 Abstract Base Classes: Defining Scientific Interfaces"},"content":"\n\nfrom abc import ABC, abstractmethod\nfrom typing import Tuple\nimport numpy as np\n\nclass SpectralAnalyzer(ABC):\n    \"\"\"Advanced ABC using NumPy arrays and CGS units.\"\"\"\n    \n    @property\n    @abstractmethod\n    def wavelength_range_cm(self) -> Tuple[float, float]:\n        \"\"\"Wavelength coverage in cm.\"\"\"\n        pass\n    \n    @abstractmethod\n    def find_lines(self, flux: np.ndarray, \n                   threshold_sigma: float = 3.0) -> np.ndarray:\n        \"\"\"Find emission lines in flux [erg/s/cm²/cm].\n        \n        Returns wavelengths in cm of detected lines.\n        \"\"\"\n        pass\n    \n    def flux_to_photons(self, flux_cgs, wavelength_cm):\n        \"\"\"Convert flux to photon rate using CGS units.\n        \n        flux: erg/s/cm²/cm\n        returns: photons/s/cm²/cm\n        \"\"\"\n        h = 6.626e-27  # erg·s\n        c = 2.998e10   # cm/s\n        return flux_cgs * wavelength_cm / (h * c)\n\nclass OpticalSpectrograph(SpectralAnalyzer):\n    \"\"\"Optical spectrograph with CGS units.\"\"\"\n    \n    @property\n    def wavelength_range_cm(self):\n        # 380-750 nm in cm\n        return (3.8e-5, 7.5e-5)\n    \n    def find_lines(self, flux, threshold_sigma=3.0):\n        \"\"\"Find emission lines using median filtering.\"\"\"\n        # Simple line finding with NumPy\n        median = np.median(flux)\n        std = np.std(flux)\n        peaks = np.where(flux > median + threshold_sigma * std)[0]\n        \n        # Convert indices to wavelengths in cm\n        wave_start, wave_end = self.wavelength_range_cm\n        wavelengths = np.linspace(wave_start, wave_end, len(flux))\n        \n        return wavelengths[peaks]\n\n# Using the abstract interface with NumPy\nspectrograph = OpticalSpectrograph()\nmock_flux = np.random.randn(1000) * 1e-12 + 5e-12  # erg/s/cm²/cm\nlines = spectrograph.find_lines(mock_flux)\nprint(f\"Found {len(lines)} emission lines\")\nprint(f\"Wavelength range: {spectrograph.wavelength_range_cm} cm\")\n\n🔍 Check Your Understanding\n\nWhy does NumPy’s np.asarray() work with so many different input types (lists, tuples, other arrays)? How do ABCs enable this?\n\nAnswer\n\nNumPy uses the Array API protocol (similar to an ABC) that defines what methods an object needs to be “array-like”. Any object implementing __array__() or __array_interface__ can be converted to a NumPy array.\n\nThis is why you can do:\n\nnp.asarray([1,2,3]) - lists work\n\nnp.asarray((1,2,3)) - tuples work\n\nnp.asarray(pandas_series) - Pandas objects work\n\nnp.asarray(astropy_quantity) - Astropy quantities work\n\nEach of these types implements the array protocol differently, but NumPy doesn’t care about the implementation - just that the required methods exist. This is ABC-based design enabling the entire scientific Python ecosystem to interoperate!\n## 10.2 Multiple Inheritance and Mixins\n\n{margin} **Mixin**\nA class providing specific functionality to be inherited by other classes, not meant to stand alone.\n\n{margin} **Diamond Problem**\nWhen a class inherits from two classes that share a common base, creating ambiguity in method resolution.\n\nYou've seen matplotlib axes that can plot lines, scatter points, histograms, images, and dozens of other visualizations. How does one class have so many capabilities without becoming a 10,000-line monster? The answer is mixins.\n\n```{code-cell} python\n# First, understand the diamond problem you've seen in NumPy\nclass Array:\n    def sum(self): return \"Array.sum\"\n\nclass MaskedArray(Array):\n    def sum(self): return \"MaskedArray.sum\"\n\nclass SparseArray(Array):\n    def sum(self): return \"SparseArray.sum\"\n\nclass MaskedSparseArray(MaskedArray, SparseArray):\n    pass  # Which sum() method?\n\n# Method Resolution Order (MRO) solves this\nmsa = MaskedSparseArray()\nprint(f\"Calls: {msa.sum()}\")  # MaskedArray.sum (first parent)\nprint(f\"MRO: {[c.__name__ for c in MaskedSparseArray.__mro__]}\")\n```\n\nNow let's build a telescope control system using mixins with proper CGS units:\n\n```{code-cell} python\nimport time\nimport numpy as np\n\n# Mixins provide specific, reusable functionality\nclass PointingMixin:\n    \"\"\"Add telescope pointing capability (CGS units).\"\"\"\n    \n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.ra_rad = 0.0  # radians\n        self.dec_rad = 0.0  # radians\n    \n    def point_to(self, ra_hours, dec_degrees):\n        \"\"\"Point telescope to coordinates.\n        \n        Args:\n            ra_hours: Right ascension in hours\n            dec_degrees: Declination in degrees\n        \"\"\"\n        self.ra_rad = ra_hours * 15 * np.pi / 180  # hours to radians\n        self.dec_rad = dec_degrees * np.pi / 180    # degrees to radians\n        \n    def get_altitude_cm(self, latitude_rad, lst_hours):\n        \"\"\"Calculate altitude above horizon in cm at zenith.\n        \n        Uses Earth radius = 6.371e8 cm\n        \"\"\"\n        # Simplified altitude calculation\n        h = np.sin(self.dec_rad) * np.sin(latitude_rad)\n        h += np.cos(self.dec_rad) * np.cos(latitude_rad) * \\\n             np.cos(lst_hours * 15 * np.pi / 180 - self.ra_rad)\n        \n        # Convert to height in cm above horizon\n        earth_radius_cm = 6.371e8\n        return np.arcsin(h) * earth_radius_cm\n\nclass PhotometryMixin:\n    \"\"\"Add photometric capability with CGS flux units.\"\"\"\n    \n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.filter_width_cm = 1e-5  # 100 nm default\n    \n    def counts_to_flux(self, counts, exposure_s, area_cm2):\n        \"\"\"Convert counts to flux in erg/s/cm²/Hz.\n        \n        Assumes optical wavelength 550 nm = 5.5e-5 cm\n        \"\"\"\n        wavelength_cm = 5.5e-5\n        c = 2.998e10  # cm/s\n        h = 6.626e-27  # erg·s\n        \n        # Photon energy\n        energy_per_photon = h * c / wavelength_cm\n        \n        # Flux in erg/s/cm²\n        flux = counts * energy_per_photon / (exposure_s * area_cm2)\n        \n        # Convert to per Hz (ν = c/λ)\n        freq_hz = c / wavelength_cm\n        bandwidth_hz = c * self.filter_width_cm / wavelength_cm**2\n        \n        return flux / bandwidth_hz\n\nclass SpectroscopyMixin:\n    \"\"\"Add spectroscopic capability.\"\"\"\n    \n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.resolution = 1000  # R = λ/Δλ\n    \n    def velocity_shift_cm_per_s(self, obs_wavelength_cm, \n                                rest_wavelength_cm):\n        \"\"\"Calculate velocity shift in cm/s using Doppler formula.\n        \n        v = c * (λ_obs - λ_rest) / λ_rest\n        \"\"\"\n        c = 2.998e10  # cm/s\n        return c * (obs_wavelength_cm - rest_wavelength_cm) / rest_wavelength_cm\n\n# Base telescope class\nclass Telescope:\n    \"\"\"Base telescope with aperture in cm.\"\"\"\n    \n    def __init__(self, name, aperture_cm):\n        super().__init__()  # Important for mixin chain!\n        self.name = name\n        self.aperture_cm = aperture_cm\n    \n    def light_gathering_power(self):\n        \"\"\"Light gathering power in cm².\"\"\"\n        return np.pi * (self.aperture_cm / 2)**2\n\n# Combine mixins to create specialized telescopes\nclass OpticalTelescope(Telescope, PointingMixin, PhotometryMixin):\n    \"\"\"Optical telescope with pointing and photometry.\"\"\"\n    pass\n\nclass SpectroscopicTelescope(Telescope, PointingMixin, SpectroscopyMixin):\n    \"\"\"Telescope with spectrograph.\"\"\"\n    pass\n\nclass MultiInstrumentTelescope(Telescope, PointingMixin, \n                               PhotometryMixin, SpectroscopyMixin):\n    \"\"\"Telescope with all capabilities.\"\"\"\n    pass\n\n# Create telescopes with different capabilities\nphoto_scope = OpticalTelescope(\"1m Photometric\", 100)  # 100 cm = 1m\nphoto_scope.point_to(ra_hours=5.5, dec_degrees=45.0)\n\n# Photometry-specific methods available\nflux = photo_scope.counts_to_flux(counts=10000, exposure_s=30, \n                                  area_cm2=photo_scope.light_gathering_power())\nprint(f\"Flux: {flux:.2e} erg/s/cm²/Hz\")\n\n# Spectroscopic telescope has different methods\nspec_scope = SpectroscopicTelescope(\"2m Spectroscopic\", 200)  # 200 cm\nvelocity = spec_scope.velocity_shift_cm_per_s(\n    obs_wavelength_cm=6.565e-5,  # H-alpha observed\n    rest_wavelength_cm=6.563e-5  # H-alpha rest\n)\nprint(f\"Velocity: {velocity/1e5:.1f} km/s\")  # Convert to km/s for display\n```\n\n:::{admonition} 🎯 The More You Know: How Mixins Orchestrate the Large Hadron Collider\n:class: note, story\n\nIn 2008, CERN faced a software catastrophe. The Large Hadron Collider's four main detectors - ATLAS, CMS, ALICE, and LHCb - each the size of apartment buildings, produced different data types from 40 million collisions per second. The original monolithic code had become unmaintainable. Every detector class exceeded 10,000 lines with massive duplication.\n\nBenedikt Hegner led the refactoring to mixins. The problem: detectors shared some capabilities but not others. ATLAS and CMS needed muon tracking. ALICE specialized in heavy ion collisions. All needed timing to 25 nanoseconds. Traditional inheritance would create bizarre hierarchies or duplicate code.\n\nThe solution was elegant mixins:\n\n```python\nclass MuonTrackingMixin:\n    def reconstruct_muon_momentum_gev(self, hits):\n        \"\"\"Track muons through iron, return momentum in GeV/c.\"\"\"\n        # 500 lines of Kalman filtering through magnetic field\n        pass\n\nclass CalorimeterMixin:\n    def measure_energy_gev(self, cells):\n        \"\"\"Measure energy deposition in GeV.\"\"\"\n        # 300 lines of calorimetry in CGS then convert\n        pass\n\nclass PrecisionTimingMixin:\n    def get_bunch_crossing(self, time_ns):\n        \"\"\"Identify which 25ns proton bunch caused event.\"\"\"\n        pass\n\n# Each detector picks its capabilities\nclass ATLAS(Detector, MuonTrackingMixin, CalorimeterMixin, \n           PrecisionTimingMixin, JetReconstructionMixin):\n    pass\n\nclass ALICE(Detector, CalorimeterMixin, PrecisionTimingMixin,\n           HeavyIonMixin):  # No muons but handles lead ions\n    pass\n```\n\nThis design proved its worth during the 2012 Higgs discovery. When unusual tau signatures appeared, they created `TauLeptonMixin` and added it to both ATLAS and CMS in days instead of months. The mixin processed 5 billion events, finding the 125 GeV Higgs boson.\n\nToday, this architecture processes 600 million collisions per second, filtering to 1000 \"interesting\" events. When you use mixins, you're using the pattern that found the Higgs boson!\n\n💡 Computational Thinking Box: Composition vs Inheritance\n\nPATTERN: Choosing Between Inheritance, Mixins, and Composition\n\nAfter using NumPy and Matplotlib, you’ve seen all three patterns:\n\nInheritance (IS-A): “MaskedArray IS-A ndarray”class ma.MaskedArray(np.ndarray):  # NumPy's actual design\n    pass\n\nMixin (CAN-DO): “Axes CAN-DO plotting methods”class Axes(Artist, _AxesBase):  # Matplotlib's actual design\n    # Inherits from mixins providing different capabilities\n    pass\n\nComposition (HAS-A): “Figure HAS-A list of Axes”class Figure:\n    def __init__(self):\n        self.axes = []  # Composition - Figure contains Axes\n\nDecision Framework:\n\nUse inheritance for true specialization (rare)\n\nUse mixins for orthogonal capabilities (common)\n\nUse composition for complex relationships (very common)\n\nWhen in doubt, prefer composition\n\nReal NumPy example: Structured arrays use composition (array HAS-A dtype), not inheritance!","type":"content","url":"/python-advanced-oop#advanced-abc-patterns-with-numpy-integration","position":11},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl2":"10.3 Dataclasses: Modern Python for Scientific Data"},"type":"lvl2","url":"/python-advanced-oop#id-10-3-dataclasses-modern-python-for-scientific-data","position":12},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl2":"10.3 Dataclasses: Modern Python for Scientific Data"},"content":"{margin} Dataclass\nA decorator that automatically generates special methods for classes primarily storing data.\n\nPython 3.7 introduced dataclasses, which eliminate boilerplate for data-heavy scientific classes. You’ve been writing __init__, __repr__, and __eq__ manually - dataclasses generate them automatically.\n\nfrom dataclasses import dataclass, field\nfrom typing import List, Optional\nimport numpy as np\n\n# Before dataclasses - lots of boilerplate\nclass OldStyleObservation:\n    def __init__(self, target: str, ra_deg: float, dec_deg: float,\n                 exposure_s: float, airmass: float, \n                 flux_cgs: Optional[float] = None):\n        self.target = target\n        self.ra_deg = ra_deg\n        self.dec_deg = dec_deg\n        self.exposure_s = exposure_s\n        self.airmass = airmass\n        self.flux_cgs = flux_cgs\n    \n    def __repr__(self):\n        return (f\"OldStyleObservation(target={self.target}, \"\n                f\"ra_deg={self.ra_deg}, ...)\")\n    \n    def __eq__(self, other):\n        return (self.target == other.target and \n                self.ra_deg == other.ra_deg and ...)\n\n# With dataclasses - automatic and clean!\n@dataclass\nclass Observation:\n    \"\"\"Astronomical observation with automatic methods.\"\"\"\n    target: str\n    ra_deg: float\n    dec_deg: float\n    exposure_s: float\n    airmass: float\n    flux_cgs: Optional[float] = None  # erg/s/cm²\n    \n    def __post_init__(self):\n        \"\"\"Validate after auto-generated __init__.\"\"\"\n        if not 0 <= self.ra_deg <= 360:\n            raise ValueError(f\"RA {self.ra_deg} outside [0,360]\")\n        if not -90 <= self.dec_deg <= 90:\n            raise ValueError(f\"Dec {self.dec_deg} outside [-90,90]\")\n        if self.airmass < 1.0:\n            raise ValueError(f\"Airmass {self.airmass} < 1.0\")\n    \n    def extinction_magnitudes(self) -> float:\n        \"\"\"Atmospheric extinction in magnitudes.\n        \n        Uses Rayleigh scattering at sea level.\n        \"\"\"\n        k_extinction = 0.15  # mag/airmass at 550nm\n        return k_extinction * (self.airmass - 1.0)\n\n# Automatic __init__, __repr__, __eq__ and more!\nobs = Observation(\n    target=\"M31\",\n    ra_deg=10.68,\n    dec_deg=41.27,\n    exposure_s=300,\n    airmass=1.2,\n    flux_cgs=1.5e-12\n)\n\nprint(obs)  # Nice automatic __repr__\nprint(f\"Extinction: {obs.extinction_magnitudes():.3f} mag\")\n\n","type":"content","url":"/python-advanced-oop#id-10-3-dataclasses-modern-python-for-scientific-data","position":13},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl3":"Advanced Dataclass Features for Scientific Computing","lvl2":"10.3 Dataclasses: Modern Python for Scientific Data"},"type":"lvl3","url":"/python-advanced-oop#advanced-dataclass-features-for-scientific-computing","position":14},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl3":"Advanced Dataclass Features for Scientific Computing","lvl2":"10.3 Dataclasses: Modern Python for Scientific Data"},"content":"\n\nfrom __future__ import annotations  # Enables forward references\nfrom dataclasses import dataclass, field, asdict\nimport numpy as np\nfrom typing import ClassVar, Optional\n\n@dataclass\nclass SpectralLine:\n    \"\"\"Spectral line with CGS units and validation.\"\"\"\n    \n    # Class variable (shared by all instances)\n    c_cgs: ClassVar[float] = 2.998e10  # cm/s\n    \n    # Instance variables with types\n    wavelength_cm: float\n    flux_cgs: float  # erg/s/cm²\n    width_cm: float\n    name: str = \"Unknown\"\n    \n    # Computed field with default_factory\n    measurements: List[float] = field(default_factory=list)\n    \n    # Field with metadata\n    snr: float = field(default=0.0, metadata={\"unit\": \"dimensionless\"})\n    \n    def __post_init__(self):\n        \"\"\"Calculate SNR after initialization.\"\"\"\n        if self.flux_cgs > 0 and self.width_cm > 0:\n            # Simple SNR estimate\n            self.snr = self.flux_cgs / (self.width_cm * 1e-13)\n    \n    @property\n    def velocity_width_km_s(self) -> float:\n        \"\"\"Line width in velocity space (km/s).\"\"\"\n        return (self.width_cm / self.wavelength_cm) * self.c_cgs / 1e5\n    \n    @property\n    def frequency_hz(self) -> float:\n        \"\"\"Frequency in Hz.\"\"\"\n        return self.c_cgs / self.wavelength_cm\n\n# Create H-alpha line\nh_alpha = SpectralLine(\n    wavelength_cm=6.563e-5,\n    flux_cgs=1e-13,\n    width_cm=1e-7,\n    name=\"H-alpha\"\n)\n\nprint(f\"H-alpha: {h_alpha.frequency_hz:.2e} Hz\")\nprint(f\"Velocity width: {h_alpha.velocity_width_km_s:.1f} km/s\")\nprint(f\"SNR: {h_alpha.snr:.1f}\")\n\n# Convert to dictionary (useful for saving)\nline_dict = asdict(h_alpha)\nprint(f\"As dict: {line_dict}\")\n\n# Frozen (immutable) dataclass for constants\n@dataclass(frozen=True)\nclass PhysicalConstants:\n    \"\"\"Immutable physical constants in CGS.\"\"\"\n    c: float = 2.998e10      # cm/s\n    h: float = 6.626e-27     # erg·s\n    k: float = 1.381e-16     # erg/K\n    m_e: float = 9.109e-28   # g\n    m_p: float = 1.673e-24   # g\n    \n    def photon_energy(self, wavelength_cm: float) -> float:\n        \"\"\"Photon energy in ergs.\"\"\"\n        return self.h * self.c / wavelength_cm\n\n# Constants are immutable\nconstants = PhysicalConstants()\n# constants.c = 3e10  # This would raise FrozenInstanceError\nprint(f\"Photon at 550nm: {constants.photon_energy(5.5e-5):.2e} erg\")\n\n🌟 Why This Matters: Dataclasses in Scientific Python\n\nDataclasses dramatically reduce boilerplate in scientific code. Here’s a real comparison:# Traditional class - lots of boilerplate\nclass Star:\n    def __init__(self, name, ra, dec, magnitude, distance):\n        self.name = name\n        self.ra = ra\n        self.dec = dec\n        self.magnitude = magnitude\n        self.distance = distance\n    \n    def __repr__(self):\n        return (f\"Star(name={self.name!r}, ra={self.ra}, \"\n                f\"dec={self.dec}, magnitude={self.magnitude}, \"\n                f\"distance={self.distance})\")\n    \n    def __eq__(self, other):\n        if not isinstance(other, Star):\n            return NotImplemented\n        return (self.name == other.name and \n                self.ra == other.ra and\n                self.dec == other.dec and\n                self.magnitude == other.magnitude and\n                self.distance == other.distance)\n\n# With dataclass - clean and automatic!\n@dataclass\nclass Star:\n    name: str\n    ra: float\n    dec: float  \n    magnitude: float\n    distance: float\n    # __init__, __repr__, __eq__ all generated automatically!\n\nBenefits in scientific computing:\n\nType hints make scientific APIs self-documenting\n\nasdict() simplifies data export to JSON/HDF5\n\nfrozen=True ensures immutability for coordinate systems\n\nLess code means fewer bugs in data structures\n\nMany modern scientific Python packages are adopting dataclasses for their clarity and reduced maintenance burden. You’re learning patterns that represent the future direction of scientific Python!","type":"content","url":"/python-advanced-oop#advanced-dataclass-features-for-scientific-computing","position":15},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl2":"10.4 Asynchronous Programming for Instrument Control"},"type":"lvl2","url":"/python-advanced-oop#id-10-4-asynchronous-programming-for-instrument-control","position":16},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl2":"10.4 Asynchronous Programming for Instrument Control"},"content":"{margin} Async/Await\nPython’s syntax for asynchronous programming, allowing concurrent operations without threads.\n\n{margin} Coroutine\nA function that can pause and resume execution, enabling cooperative multitasking.\n\nModern observatories control multiple instruments simultaneously. While one CCD reads out (30 seconds), you can slew the telescope (10 seconds) and configure the spectrograph (5 seconds). Async programming enables this parallelism.\n\nimport asyncio\nimport time\nimport numpy as np\n\n# Synchronous version - everything waits\ndef sync_observe(target: str) -> dict:\n    \"\"\"Traditional blocking observation.\"\"\"\n    print(f\"Starting observation of {target}\")\n    \n    # Each step blocks\n    print(\"Slewing telescope...\")\n    time.sleep(2)  # Simulate 2 second slew\n    \n    print(\"Exposing CCD...\")\n    time.sleep(3)  # Simulate 3 second exposure\n    \n    print(\"Reading CCD...\")\n    time.sleep(1)  # Simulate 1 second readout\n    \n    return {\"target\": target, \"counts\": 1000}\n\n# Time the synchronous version\nstart = time.perf_counter()\nresult = sync_observe(\"M31\")\nsync_time = time.perf_counter() - start\nprint(f\"Synchronous time: {sync_time:.1f}s\\n\")\n\nNow the asynchronous version:\n\n# Asynchronous version - operations can overlap\nasync def slew_telescope(ra_deg: float, dec_deg: float):\n    \"\"\"Async telescope slewing.\"\"\"\n    print(f\"Slewing to RA={ra_deg:.1f}, Dec={dec_deg:.1f}\")\n    await asyncio.sleep(2)  # Non-blocking sleep\n    print(\"Slew complete\")\n    return True\n\nasync def expose_ccd(exposure_s: float):\n    \"\"\"Async CCD exposure.\"\"\"\n    print(f\"Starting {exposure_s}s exposure\")\n    await asyncio.sleep(3)  # Simulated exposure\n    \n    # Generate mock data in CGS units\n    photons = np.random.poisson(1000, size=(100, 100))\n    flux_cgs = photons * 3.6e-12  # erg/s/cm² at 550nm\n    \n    print(\"Exposure complete\")\n    return flux_cgs\n\nasync def configure_spectrograph(resolution: int):\n    \"\"\"Async spectrograph configuration.\"\"\"\n    print(f\"Configuring spectrograph R={resolution}\")\n    await asyncio.sleep(1)\n    print(\"Spectrograph ready\")\n    return resolution\n\nasync def async_observe(target: str, ra: float, dec: float):\n    \"\"\"Parallel observation with multiple instruments.\"\"\"\n    print(f\"Starting async observation of {target}\")\n    \n    # Launch operations concurrently!\n    tasks = [\n        slew_telescope(ra, dec),\n        expose_ccd(30.0),\n        configure_spectrograph(5000)\n    ]\n    \n    # Wait for all to complete\n    results = await asyncio.gather(*tasks)\n    \n    return {\n        \"target\": target,\n        \"pointed\": results[0],\n        \"flux\": results[1].mean(),\n        \"resolution\": results[2]\n    }\n\n# Run async version\nasync def main():\n    start = time.perf_counter()\n    result = await async_observe(\"M31\", 10.68, 41.27)\n    async_time = time.perf_counter() - start\n    \n    print(f\"\\nAsync time: {async_time:.1f}s\")\n    print(f\"Speedup: {sync_time/async_time:.1f}x\")\n    print(f\"Mean flux: {result['flux']:.2e} erg/s/cm²\")\n\n# Note: In Jupyter, use await directly\n# In scripts, use asyncio.run(main())\nawait main()\n\n","type":"content","url":"/python-advanced-oop#id-10-4-asynchronous-programming-for-instrument-control","position":17},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl3":"Real-World Async Telescope Control","lvl2":"10.4 Asynchronous Programming for Instrument Control"},"type":"lvl3","url":"/python-advanced-oop#real-world-async-telescope-control","position":18},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl3":"Real-World Async Telescope Control","lvl2":"10.4 Asynchronous Programming for Instrument Control"},"content":"\n\nfrom dataclasses import dataclass\nfrom typing import List, Optional\nimport asyncio\n\n# Note: In Jupyter notebooks, use 'await' directly in cells\n# In Python scripts, use asyncio.run(main())\n# Some Jupyter versions may require nest_asyncio:\n# import nest_asyncio\n# nest_asyncio.apply()\n\n@dataclass\nclass TelescopeState:\n    \"\"\"Current telescope state.\"\"\"\n    ra_deg: float = 0.0\n    dec_deg: float = 0.0\n    filter: str = \"V\"\n    focus_mm: float = 0.0\n    dome_azimuth_deg: float = 0.0\n\nclass AsyncTelescope:\n    \"\"\"Async telescope controller with realistic operations.\"\"\"\n    \n    def __init__(self, name: str):\n        self.name = name\n        self.state = TelescopeState()\n        self.is_tracking = False\n    \n    async def slew_to(self, ra: float, dec: float) -> float:\n        \"\"\"Slew to coordinates, return time taken.\"\"\"\n        # Calculate slew time based on distance\n        distance = np.sqrt((ra - self.state.ra_deg)**2 + \n                          (dec - self.state.dec_deg)**2)\n        slew_time = min(distance / 2.0, 30.0)  # 2 deg/s, max 30s\n        \n        print(f\"{self.name}: Slewing {distance:.1f}° ({slew_time:.1f}s)\")\n        await asyncio.sleep(slew_time)\n        \n        self.state.ra_deg = ra\n        self.state.dec_deg = dec\n        self.is_tracking = True\n        \n        return slew_time\n    \n    async def change_filter(self, filter_name: str):\n        \"\"\"Change filter wheel.\"\"\"\n        if filter_name != self.state.filter:\n            print(f\"{self.name}: Changing to {filter_name} filter\")\n            await asyncio.sleep(5.0)  # Filter wheel rotation\n            self.state.filter = filter_name\n    \n    async def auto_focus(self) -> float:\n        \"\"\"Autofocus routine, return FWHM in arcsec.\"\"\"\n        print(f\"{self.name}: Starting autofocus\")\n        \n        best_focus = 0.0\n        best_fwhm = float('inf')\n        \n        # Simulate focus sweep\n        for focus in np.linspace(-2, 2, 5):\n            await asyncio.sleep(1.0)  # Move focus\n            \n            # Simulate FWHM measurement\n            fwhm = abs(focus) + np.random.random() + 0.8\n            if fwhm < best_fwhm:\n                best_fwhm = fwhm\n                best_focus = focus\n        \n        self.state.focus_mm = best_focus\n        print(f\"{self.name}: Focus complete, FWHM={best_fwhm:.2f}\\\"\")\n        return best_fwhm\n\n# Async observation sequence\nasync def observe_targets(telescope: AsyncTelescope, \n                         targets: List[tuple]):\n    \"\"\"Observe multiple targets efficiently.\"\"\"\n    \n    for name, ra, dec, filter_name in targets:\n        print(f\"\\n--- Observing {name} ---\")\n        \n        # Parallel operations where possible\n        tasks = [\n            telescope.slew_to(ra, dec),\n            telescope.change_filter(filter_name)\n        ]\n        \n        # Slew and filter change happen in parallel\n        await asyncio.gather(*tasks)\n        \n        # Focus if needed (every 5 targets)\n        if np.random.random() < 0.2:\n            await telescope.auto_focus()\n        \n        # Simulate exposure\n        print(f\"Exposing {name} in {filter_name}\")\n        await asyncio.sleep(10.0)\n        \n        print(f\"Completed {name}\")\n\n# Run observation sequence\nasync def night_observations():\n    telescope = AsyncTelescope(\"10m Keck\")\n    \n    targets = [\n        (\"M31\", 10.68, 41.27, \"V\"),\n        (\"M42\", 83.82, -5.39, \"R\"),\n        (\"M51\", 202.47, 47.20, \"B\"),\n    ]\n    \n    start = time.perf_counter()\n    await observe_targets(telescope, targets)\n    total_time = time.perf_counter() - start\n    \n    print(f\"\\nTotal observation time: {total_time:.1f}s\")\n\nawait night_observations()\n\n⚠️ Common Bug Alert: Mixing Sync and Async\n\n# WRONG - Blocking call in async function\nasync def bad_async():\n    print(\"Starting\")\n    time.sleep(1)  # BLOCKS the event loop!\n    print(\"Done\")\n\n# CORRECT - Use async sleep\nasync def good_async():\n    print(\"Starting\")\n    await asyncio.sleep(1)  # Non-blocking\n    print(\"Done\")\n\n# WRONG - Forgetting await\nasync def forgot_await():\n    result = expose_ccd(30)  # Returns coroutine object!\n    # print(result)  # <coroutine object...>\n\n# CORRECT - Use await\nasync def remembered_await():\n    result = await expose_ccd(30)  # Actually runs\n    print(f\"Got flux: {result.mean():.2e}\")\n\nAlways use await with async functions and never use blocking calls in async code!","type":"content","url":"/python-advanced-oop#real-world-async-telescope-control","position":19},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl2":"10.5 Metaclasses and Descriptors"},"type":"lvl2","url":"/python-advanced-oop#id-10-5-metaclasses-and-descriptors","position":20},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl2":"10.5 Metaclasses and Descriptors"},"content":"{margin} Metaclass\nA class whose instances are classes themselves, controlling class creation.\n\n{margin} Descriptor\nAn object that customizes attribute access through __get__ and __set__ methods.\n\nNow that you’ve seen how NumPy dtypes work and how Astropy units prevent disasters, let’s understand the metaclasses and descriptors that make them possible.\n\n# Descriptors for unit-safe astronomy\nclass UnitProperty:\n    \"\"\"Descriptor ensuring CGS units are maintained.\"\"\"\n    \n    def __init__(self, unit_name: str, cgs_conversion: float = 1.0,\n                 min_val: Optional[float] = None):\n        self.unit_name = unit_name\n        self.cgs_conversion = cgs_conversion\n        self.min_val = min_val\n    \n    def __set_name__(self, owner, name):\n        \"\"\"Called when descriptor is assigned to class.\"\"\"\n        self.name = f\"_{name}\"\n        self.public_name = name\n    \n    def __get__(self, obj, objtype=None):\n        \"\"\"Get value in CGS units.\"\"\"\n        if obj is None:\n            return self\n        return getattr(obj, self.name, None)\n    \n    def __set__(self, obj, value):\n        \"\"\"Set value with validation and conversion.\"\"\"\n        # Handle tuple of (value, unit)\n        if isinstance(value, tuple):\n            val, unit = value\n            if unit != self.unit_name:\n                # Convert to CGS\n                if unit == \"km\" and self.unit_name == \"cm\":\n                    val *= 1e5\n                elif unit == \"AU\" and self.unit_name == \"cm\":\n                    val *= 1.496e13\n                else:\n                    raise ValueError(f\"Cannot convert {unit} to {self.unit_name}\")\n        else:\n            val = value\n        \n        # Validate\n        if self.min_val is not None and val < self.min_val:\n            raise ValueError(f\"{self.public_name} = {val} below minimum {self.min_val}\")\n        \n        setattr(obj, self.name, val)\n\nclass CelestialObject:\n    \"\"\"Object with unit-safe properties.\"\"\"\n    \n    # Descriptors enforce CGS units\n    distance = UnitProperty(\"cm\", min_val=0)\n    mass = UnitProperty(\"g\", min_val=0)\n    radius = UnitProperty(\"cm\", min_val=0)\n    temperature = UnitProperty(\"K\", min_val=0)\n    \n    def __init__(self, name: str):\n        self.name = name\n    \n    @property\n    def escape_velocity_cm_s(self) -> float:\n        \"\"\"Escape velocity in cm/s.\"\"\"\n        if self.mass and self.radius:\n            G = 6.674e-8  # cm³/g/s²\n            return np.sqrt(2 * G * self.mass / self.radius)\n        return 0.0\n\n# Unit-safe usage\nearth = CelestialObject(\"Earth\")\nearth.distance = (1, \"AU\")  # Converts to cm automatically\nearth.mass = 5.972e27  # grams\nearth.radius = (6371, \"km\")  # Converts to cm\n\nprint(f\"Earth distance: {earth.distance:.2e} cm\")\nprint(f\"Earth radius: {earth.radius:.2e} cm\")\nprint(f\"Escape velocity: {earth.escape_velocity_cm_s/1e5:.1f} km/s\")\n\n","type":"content","url":"/python-advanced-oop#id-10-5-metaclasses-and-descriptors","position":21},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl3":"Metaclasses for Automatic Registration","lvl2":"10.5 Metaclasses and Descriptors"},"type":"lvl3","url":"/python-advanced-oop#metaclasses-for-automatic-registration","position":22},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl3":"Metaclasses for Automatic Registration","lvl2":"10.5 Metaclasses and Descriptors"},"content":"\n\n# Metaclass for instrument registry (like Astropy's registry)\nclass InstrumentMeta(type):\n    \"\"\"Metaclass that auto-registers instruments.\"\"\"\n    \n    _registry = {}\n    \n    def __new__(mcs, name, bases, namespace):\n        # Create the class\n        cls = super().__new__(mcs, name, bases, namespace)\n        \n        # Register if it has instrument_code\n        if 'instrument_code' in namespace:\n            code = namespace['instrument_code']\n            mcs._registry[code] = cls\n            print(f\"Registered {name} as '{code}'\")\n        \n        return cls\n    \n    @classmethod\n    def create(mcs, code: str, **kwargs):\n        \"\"\"Factory method using registry.\"\"\"\n        if code not in mcs._registry:\n            available = list(mcs._registry.keys())\n            raise ValueError(f\"Unknown instrument '{code}'. \"\n                           f\"Available: {available}\")\n        return mcs._registry[code](**kwargs)\n    \n    @classmethod\n    def list_instruments(mcs):\n        \"\"\"List all registered instruments.\"\"\"\n        return list(mcs._registry.keys())\n\nclass Instrument(metaclass=InstrumentMeta):\n    \"\"\"Base class with metaclass.\"\"\"\n    \n    def __init__(self, **config):\n        self.config = config\n\n# Classes auto-register on definition!\nclass WFC3(Instrument):\n    \"\"\"Hubble Wide Field Camera 3.\"\"\"\n    instrument_code = \"WFC3\"\n    \n    def __init__(self, channel=\"UVIS\", **config):\n        super().__init__(**config)\n        self.channel = channel  # UVIS or IR\n\nclass MIRI(Instrument):\n    \"\"\"JWST Mid-Infrared Instrument.\"\"\"\n    instrument_code = \"MIRI\"\n    \n    def __init__(self, mode=\"imaging\", **config):\n        super().__init__(**config)\n        self.mode = mode\n\nclass HARPS(Instrument):\n    \"\"\"High Accuracy Radial velocity Planet Searcher.\"\"\"\n    instrument_code = \"HARPS\"\n    \n    def __init__(self, resolution=115000, **config):\n        super().__init__(**config)\n        self.resolution = resolution\n\n# Use the registry\nprint(f\"Available: {InstrumentMeta.list_instruments()}\")\n\n# Create instruments by code\nmiri = InstrumentMeta.create(\"MIRI\", mode=\"spectroscopy\")\nharps = InstrumentMeta.create(\"HARPS\")\n\nprint(f\"Created {miri.__class__.__name__} in {miri.mode} mode\")\nprint(f\"HARPS resolution: R={harps.resolution}\")\n\n🔍 Check Your Understanding\n\nHow do NumPy arrays know which operations they support? How can arr + 1 and arr + another_array both work?\n\nAnswer\n\nNumPy uses a combination of metaclasses and descriptors:\n\nMetaclass Registration: When you create array subclasses, NumPy’s metaclass registers which operations they support\n\nDescriptor Protocol: Operations like + call __add__, which uses descriptors to check if the operation is valid\n\nDynamic Dispatch: Based on the types involved, NumPy dispatches to the appropriate implementation\n\nThis is why:\n\nnp.array([1,2,3]) + 1 broadcasts the scalar\n\nnp.array([1,2,3]) + np.array([4,5,6]) adds element-wise\n\nnp.array([1,2,3]) + \"string\" raises TypeError\n\nThe same patterns you just learned power NumPy’s flexibility!\n## 10.6 Design Patterns from Scientific Computing\n\n{margin} **Design Pattern**\nA reusable solution to a commonly occurring problem in software design.\n\nYou've used these patterns throughout NumPy and Matplotlib. Now let's understand how they work.\n\n### Factory Pattern: Creating the Right Object\n\n```{code-cell} python\n# How NumPy's array creation works (simplified)\nclass ArrayFactory:\n    \"\"\"Factory pattern like np.array().\"\"\"\n    \n    @staticmethod\n    def create(data, dtype=None):\n        \"\"\"Create appropriate array type based on input.\"\"\"\n        \n        # Determine appropriate array type\n        if hasattr(data, '__array__'):\n            # Object provides array interface\n            return np.asarray(data)\n        \n        elif isinstance(data, (list, tuple)):\n            # Python sequence\n            if dtype == 'object':\n                return ObjectArray(data)\n            else:\n                return NumericArray(data)\n        \n        elif isinstance(data, str):\n            # String array\n            return StringArray(data)\n        \n        else:\n            raise TypeError(f\"Cannot create array from {type(data)}\")\n\nclass NumericArray:\n    def __init__(self, data):\n        self.data = list(data)\n        print(f\"Created numeric array: {self.data}\")\n\nclass ObjectArray:\n    def __init__(self, data):\n        self.data = data\n        print(f\"Created object array: {self.data}\")\n\nclass StringArray:\n    def __init__(self, data):\n        self.data = data\n        print(f\"Created string array: {self.data}\")\n\n# Factory creates appropriate type\narr1 = ArrayFactory.create([1, 2, 3])\narr2 = ArrayFactory.create(['a', 'b'], dtype='object')\narr3 = ArrayFactory.create(\"hello\")\n```\n\n### Strategy Pattern: Swappable Algorithms\n\n```{code-cell} python\nfrom abc import ABC, abstractmethod\n\n# How scipy.optimize works (simplified)\nclass OptimizationStrategy(ABC):\n    \"\"\"Abstract strategy for optimization.\"\"\"\n    \n    @abstractmethod\n    def minimize(self, func, x0):\n        \"\"\"Minimize function starting from x0.\"\"\"\n        pass\n\nclass GradientDescent(OptimizationStrategy):\n    \"\"\"Gradient descent optimization.\"\"\"\n    \n    def minimize(self, func, x0):\n        x = x0\n        learning_rate = 0.01\n        \n        for i in range(100):\n            # Numerical gradient\n            eps = 1e-8\n            grad = (func(x + eps) - func(x - eps)) / (2 * eps)\n            x = x - learning_rate * grad\n            \n            if abs(grad) < 1e-6:\n                break\n        \n        return x\n\nclass NelderMead(OptimizationStrategy):\n    \"\"\"Simplex optimization (gradient-free).\"\"\"\n    \n    def minimize(self, func, x0):\n        # Simplified Nelder-Mead\n        x = x0\n        step = 0.1\n        \n        for i in range(100):\n            # Try points around current\n            if func(x + step) < func(x):\n                x = x + step\n            elif func(x - step) < func(x):\n                x = x - step\n            else:\n                step *= 0.5\n        \n        return x\n\nclass Optimizer:\n    \"\"\"Context using strategy pattern.\"\"\"\n    \n    def __init__(self, strategy: OptimizationStrategy):\n        self.strategy = strategy\n    \n    def minimize(self, func, x0):\n        \"\"\"Minimize using selected strategy.\"\"\"\n        return self.strategy.minimize(func, x0)\n\n# Test with rosenbrock function\ndef rosenbrock(x):\n    \"\"\"Rosenbrock function minimum at x=1.\"\"\"\n    return (1 - x)**2 + 100 * (0 - x**2)**2\n\n# Try different strategies\nopt1 = Optimizer(GradientDescent())\nresult1 = opt1.minimize(rosenbrock, x0=0.5)\nprint(f\"Gradient descent: x = {result1:.4f}\")\n\nopt2 = Optimizer(NelderMead())\nresult2 = opt2.minimize(rosenbrock, x0=0.5)\nprint(f\"Nelder-Mead: x = {result2:.4f}\")\n```\n\n:::{admonition} 🌟 Why This Matters: Design Patterns in SciPy\n:class: info, important\n\nSciPy uses these exact patterns. When you write:\n\n```python\nfrom scipy import optimize\nresult = optimize.minimize(func, x0, method='BFGS')\n```\n\nYou're using:\n- **Factory Pattern**: `minimize` creates the right optimizer\n- **Strategy Pattern**: 'BFGS' selects the algorithm\n- **Template Method**: Each optimizer follows the same interface\n\nThis design lets you swap between 20+ optimization algorithms by changing one parameter!","type":"content","url":"/python-advanced-oop#metaclasses-for-automatic-registration","position":23},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl2":"10.7 Performance Optimization Techniques"},"type":"lvl2","url":"/python-advanced-oop#id-10-7-performance-optimization-techniques","position":24},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl2":"10.7 Performance Optimization Techniques"},"content":"{margin} __slots__\nClass attribute that restricts instance attributes to a fixed set, saving memory.\n\nUnderstanding performance helps you write efficient code for large-scale astronomical data processing.\n\nimport sys\nimport time\nfrom functools import lru_cache\n\n# Memory optimization with __slots__\nclass RegularStar:\n    \"\"\"Normal class - flexible but memory-hungry.\"\"\"\n    \n    def __init__(self, ra, dec, mag, parallax):\n        self.ra = ra  # degrees\n        self.dec = dec  # degrees  \n        self.mag = mag  # magnitude\n        self.parallax = parallax  # milliarcsec\n\nclass OptimizedStar:\n    \"\"\"Using __slots__ for memory efficiency.\"\"\"\n    \n    __slots__ = ['ra', 'dec', 'mag', 'parallax', '_distance_cache']\n    \n    def __init__(self, ra, dec, mag, parallax):\n        self.ra = ra\n        self.dec = dec\n        self.mag = mag\n        self.parallax = parallax\n        self._distance_cache = None\n    \n    @property\n    def distance_pc(self):\n        \"\"\"Distance in parsecs (cached).\"\"\"\n        if self._distance_cache is None:\n            if self.parallax > 0:\n                self._distance_cache = 1000.0 / self.parallax\n            else:\n                self._distance_cache = float('inf')\n        return self._distance_cache\n\n# Compare memory usage\nregular = RegularStar(10.68, 41.27, 3.44, 24.36)\noptimized = OptimizedStar(10.68, 41.27, 3.44, 24.36)\n\nprint(f\"Regular: {sys.getsizeof(regular.__dict__)} bytes\")\nprint(f\"Optimized: {sys.getsizeof(optimized)} bytes\")\n\n# For Gaia catalog (2 billion stars), the difference matters!\nn_stars = 100000  # Subset for demo\nregular_mem = n_stars * sys.getsizeof(regular.__dict__)\noptimized_mem = n_stars * 56  # Approximate slotted size\n\nprint(f\"\\n{n_stars:,} stars:\")\nprint(f\"Regular: {regular_mem/1e6:.1f} MB\")\nprint(f\"Optimized: {optimized_mem/1e6:.1f} MB\")\nprint(f\"Savings: {(regular_mem-optimized_mem)/1e6:.1f} MB\")\n\n","type":"content","url":"/python-advanced-oop#id-10-7-performance-optimization-techniques","position":25},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl3":"Method Caching for Expensive Computations","lvl2":"10.7 Performance Optimization Techniques"},"type":"lvl3","url":"/python-advanced-oop#method-caching-for-expensive-computations","position":26},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl3":"Method Caching for Expensive Computations","lvl2":"10.7 Performance Optimization Techniques"},"content":"\n\n# LRU cache for expensive calculations\nclass GalaxySpectrum:\n    \"\"\"Galaxy spectrum with expensive computations.\"\"\"\n    \n    def __init__(self, wavelengths_cm, fluxes_cgs):\n        self.wavelengths = wavelengths_cm  # cm\n        self.fluxes = fluxes_cgs  # erg/s/cm²/cm\n    \n    @lru_cache(maxsize=128)\n    def find_redshift(self, template_lines_cm):\n        \"\"\"Find redshift by template matching (expensive).\"\"\"\n        print(\"Computing redshift...\")  # Shows when cache misses\n        \n        # Simplified cross-correlation\n        best_z = 0.0\n        best_corr = -float('inf')\n        \n        for z in np.linspace(0, 0.1, 100):\n            # Shift template lines\n            shifted = [line * (1 + z) for line in template_lines_cm]\n            \n            # Simple correlation metric\n            corr = sum(1 for line in shifted \n                      if any(abs(line - w) < 1e-7 \n                            for w in self.wavelengths))\n            \n            if corr > best_corr:\n                best_corr = corr\n                best_z = z\n        \n        return best_z\n    \n    def velocity_km_s(self, template_lines_cm):\n        \"\"\"Recession velocity from cached redshift.\"\"\"\n        z = self.find_redshift(template_lines_cm)\n        c_km_s = 2.998e5\n        return z * c_km_s\n\n# Test caching\nwavelengths = np.linspace(4e-5, 7e-5, 1000)  # 400-700 nm in cm\nfluxes = np.random.randn(1000) * 1e-13\n\ngalaxy = GalaxySpectrum(wavelengths, fluxes)\n\n# H-alpha and H-beta rest wavelengths in cm\nh_lines = tuple([4.861e-5, 6.563e-5])\n\n# First call - computes\nz1 = galaxy.find_redshift(h_lines)\nprint(f\"Redshift: {z1:.4f}\")\n\n# Second call - cached!\nz2 = galaxy.find_redshift(h_lines)\nprint(f\"Cached: {z2:.4f}\")\n\n# Cache info\nprint(f\"Cache stats: {galaxy.find_redshift.cache_info()}\")\n\n💡 Computational Thinking Box: Profile Before Optimizing\n\nPATTERN: Measure, Don’t Guess\n\nThe Vera Rubin Observatory will process 20TB nightly. They profiled their pipeline and found surprising bottlenecks:import cProfile\nimport pstats\n\ndef process_image(data):\n    # Complex processing\n    pass\n\n# Profile the code\ncProfile.run('process_image(data)', 'profile.stats')\n\n# Analyze results\nstats = pstats.Stats('profile.stats')\nstats.sort_stats('cumulative')\nstats.print_stats(10)  # Top 10 time consumers\n\nResults showed:\n\n60% time in coordinate transformations (not image processing!)\n\n20% in file I/O\n\nOnly 10% in the actual detection algorithm\n\nOne coordinate optimization gave 3x speedup for the entire pipeline!\n\nLesson: Always profile before optimizing. The bottleneck is rarely where you think.","type":"content","url":"/python-advanced-oop#method-caching-for-expensive-computations","position":27},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl2":"10.8 Practice Exercises"},"type":"lvl2","url":"/python-advanced-oop#id-10-8-practice-exercises","position":28},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl2":"10.8 Practice Exercises"},"content":"","type":"content","url":"/python-advanced-oop#id-10-8-practice-exercises","position":29},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl3":"Exercise 1: Build a Multi-Instrument Observatory System","lvl2":"10.8 Practice Exercises"},"type":"lvl3","url":"/python-advanced-oop#exercise-1-build-a-multi-instrument-observatory-system","position":30},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl3":"Exercise 1: Build a Multi-Instrument Observatory System","lvl2":"10.8 Practice Exercises"},"content":"Create a complete observatory system using ABCs, mixins, and async:\n\n\"\"\"\nPart A: Abstract base and mixins (10 minutes)\nDesign flexible instrument system with CGS units\n\"\"\"\n\nfrom abc import ABC, abstractmethod\nimport asyncio\nimport numpy as np\nfrom dataclasses import dataclass\nfrom typing import Optional\n\n# Abstract base for all instruments\nclass ObservatoryInstrument(ABC):\n    \"\"\"Base class for observatory instruments.\"\"\"\n    \n    @abstractmethod\n    async def initialize(self) -> bool:\n        \"\"\"Initialize instrument.\"\"\"\n        pass\n    \n    @abstractmethod\n    async def acquire(self, integration_s: float) -> np.ndarray:\n        \"\"\"Acquire data for given time.\"\"\"\n        pass\n    \n    @abstractmethod\n    def get_sensitivity_cgs(self) -> float:\n        \"\"\"Return sensitivity in erg/s/cm².\"\"\"\n        pass\n\n# Mixins for capabilities\nclass TemperatureControlMixin:\n    \"\"\"Temperature control for cooled instruments.\"\"\"\n    \n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.temperature_k = 293  # Room temp\n        self.target_temp_k = 150  # Target for CCDs\n    \n    async def cool_down(self):\n        \"\"\"Cool to operating temperature.\"\"\"\n        print(f\"Cooling from {self.temperature_k}K to {self.target_temp_k}K\")\n        \n        while self.temperature_k > self.target_temp_k:\n            await asyncio.sleep(0.1)  # Simulate cooling\n            self.temperature_k -= 10\n            \n        print(f\"Reached {self.temperature_k}K\")\n        return True\n\nclass CalibrationMixin:\n    \"\"\"Calibration capability.\"\"\"\n    \n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.dark_frame = None\n        self.flat_frame = None\n    \n    async def take_dark(self, integration_s: float):\n        \"\"\"Take dark frame.\"\"\"\n        print(f\"Taking {integration_s}s dark\")\n        await asyncio.sleep(integration_s)\n        self.dark_frame = np.random.randn(100, 100) * 10\n    \n    async def take_flat(self):\n        \"\"\"Take flat field.\"\"\"\n        print(\"Taking flat field\")\n        await asyncio.sleep(1)\n        self.flat_frame = np.ones((100, 100)) + np.random.randn(100, 100) * 0.01\n\n# Concrete implementations\nclass CCDCamera(ObservatoryInstrument, TemperatureControlMixin, \n                CalibrationMixin):\n    \"\"\"CCD camera with cooling and calibration.\"\"\"\n    \n    def __init__(self, name: str, pixel_size_cm: float = 1.5e-3):\n        super().__init__()\n        self.name = name\n        self.pixel_size = pixel_size_cm  # 15 microns typical\n        self.initialized = False\n    \n    async def initialize(self) -> bool:\n        \"\"\"Initialize CCD with cooling.\"\"\"\n        print(f\"Initializing {self.name}\")\n        \n        # Cool down first\n        await self.cool_down()\n        \n        # Take calibration frames\n        await asyncio.gather(\n            self.take_dark(1.0),\n            self.take_flat()\n        )\n        \n        self.initialized = True\n        return True\n    \n    async def acquire(self, integration_s: float) -> np.ndarray:\n        \"\"\"Acquire CCD frame.\"\"\"\n        if not self.initialized:\n            raise RuntimeError(\"CCD not initialized\")\n        \n        print(f\"Exposing for {integration_s}s\")\n        await asyncio.sleep(integration_s)\n        \n        # Simulate data with Poisson noise\n        signal = np.random.poisson(1000 * integration_s, (100, 100))\n        \n        # Apply calibration if available\n        if self.dark_frame is not None:\n            signal = signal - self.dark_frame\n        if self.flat_frame is not None:\n            signal = signal / self.flat_frame\n        \n        # Convert to flux in erg/s/cm²\n        # Assume 550nm, QE=0.9\n        h = 6.626e-27  # erg·s\n        c = 2.998e10   # cm/s\n        wavelength = 5.5e-5  # cm\n        \n        photon_energy = h * c / wavelength\n        area_per_pixel = self.pixel_size**2\n        \n        flux_cgs = signal * photon_energy / (integration_s * area_per_pixel)\n        \n        return flux_cgs\n    \n    def get_sensitivity_cgs(self) -> float:\n        \"\"\"3-sigma sensitivity in erg/s/cm².\"\"\"\n        # Depends on temperature\n        read_noise = 5 if self.temperature_k < 200 else 20\n        \n        # Convert to flux\n        h = 6.626e-27\n        c = 2.998e10\n        photon_energy = h * c / 5.5e-5\n        \n        return 3 * read_noise * photon_energy / self.pixel_size**2\n\n# Test the system\nasync def test_observatory():\n    ccd = CCDCamera(\"Main CCD\", pixel_size_cm=1.5e-3)\n    \n    # Initialize (cooling + calibration)\n    await ccd.initialize()\n    \n    # Take science frame\n    data = await ccd.acquire(30.0)\n    \n    print(f\"Mean flux: {data.mean():.2e} erg/s/cm²\")\n    print(f\"Sensitivity: {ccd.get_sensitivity_cgs():.2e} erg/s/cm²\")\n\nawait test_observatory()\n\n\"\"\"\nPart B: Complete observatory with async control (15 minutes)\nMultiple instruments operating in parallel\n\"\"\"\n\n@dataclass\nclass ObservationRequest:\n    \"\"\"Request for observation with units.\"\"\"\n    target: str\n    ra_deg: float\n    dec_deg: float\n    exposures_s: list\n    filters: list\n    priority: int = 5\n\nclass Observatory:\n    \"\"\"Complete observatory with multiple instruments.\"\"\"\n    \n    def __init__(self, name: str):\n        self.name = name\n        self.instruments = {}\n        self.queue = []\n        self.completed = []\n    \n    def add_instrument(self, name: str, instrument: ObservatoryInstrument):\n        \"\"\"Register instrument.\"\"\"\n        self.instruments[name] = instrument\n    \n    async def initialize_all(self):\n        \"\"\"Initialize all instruments in parallel.\"\"\"\n        print(f\"Initializing {self.name} observatory\")\n        \n        tasks = [\n            inst.initialize() \n            for inst in self.instruments.values()\n        ]\n        \n        results = await asyncio.gather(*tasks)\n        \n        if all(results):\n            print(\"All instruments ready\")\n        else:\n            print(\"Some instruments failed initialization\")\n    \n    async def observe(self, request: ObservationRequest):\n        \"\"\"Execute observation request.\"\"\"\n        print(f\"\\nObserving {request.target}\")\n        \n        results = {}\n        \n        for exp_time, filter_name in zip(request.exposures_s, \n                                         request.filters):\n            print(f\"  {filter_name}: {exp_time}s\")\n            \n            # Use primary CCD\n            ccd = self.instruments.get('ccd')\n            if ccd:\n                data = await ccd.acquire(exp_time)\n                results[filter_name] = {\n                    'data': data,\n                    'mean_flux': data.mean(),\n                    'peak_flux': data.max()\n                }\n        \n        self.completed.append((request.target, results))\n        return results\n    \n    async def observe_queue(self):\n        \"\"\"Process observation queue.\"\"\"\n        while self.queue:\n            request = self.queue.pop(0)\n            await self.observe(request)\n    \n    def add_request(self, request: ObservationRequest):\n        \"\"\"Add to queue sorted by priority.\"\"\"\n        self.queue.append(request)\n        self.queue.sort(key=lambda r: r.priority, reverse=True)\n\n# Create and run observatory\nasync def full_observatory_demo():\n    # Create observatory\n    obs = Observatory(\"Mauna Kea\")\n    \n    # Add instruments\n    obs.add_instrument('ccd', CCDCamera(\"Primary CCD\"))\n    \n    # Initialize everything\n    await obs.initialize_all()\n    \n    # Queue observations\n    obs.add_request(ObservationRequest(\n        target=\"M31\",\n        ra_deg=10.68,\n        dec_deg=41.27,\n        exposures_s=[30, 60, 60],\n        filters=['B', 'V', 'R'],\n        priority=10\n    ))\n    \n    obs.add_request(ObservationRequest(\n        target=\"Calibration Star\",\n        ra_deg=0.0,\n        dec_deg=0.0,\n        exposures_s=[5, 5],\n        filters=['V', 'R'],\n        priority=15  # Higher priority\n    ))\n    \n    # Process queue (high priority first)\n    await obs.observe_queue()\n    \n    # Summary\n    print(f\"\\nCompleted {len(obs.completed)} observations\")\n    for target, results in obs.completed:\n        print(f\"  {target}: {list(results.keys())}\")\n\nawait full_observatory_demo()\n\n","type":"content","url":"/python-advanced-oop#exercise-1-build-a-multi-instrument-observatory-system","position":31},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl3":"Exercise 2: Design Pattern Implementation","lvl2":"10.8 Practice Exercises"},"type":"lvl3","url":"/python-advanced-oop#exercise-2-design-pattern-implementation","position":32},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl3":"Exercise 2: Design Pattern Implementation","lvl2":"10.8 Practice Exercises"},"content":"Implement key design patterns for astronomical software:\n\n\"\"\"\nImplement Factory, Strategy, and Observer patterns\nfor a data reduction pipeline\n\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom typing import List, Callable\nimport numpy as np\n\n# Strategy pattern for reduction algorithms\nclass ReductionStrategy(ABC):\n    \"\"\"Abstract strategy for data reduction.\"\"\"\n    \n    @abstractmethod\n    def reduce(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"Reduce data using specific algorithm.\"\"\"\n        pass\n\nclass MedianCombine(ReductionStrategy):\n    \"\"\"Median combination strategy.\"\"\"\n    \n    def reduce(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"Median combine along first axis.\"\"\"\n        return np.median(data, axis=0)\n\nclass SigmaClipping(ReductionStrategy):\n    \"\"\"Sigma-clipped mean strategy.\"\"\"\n    \n    def __init__(self, sigma: float = 3.0):\n        self.sigma = sigma\n    \n    def reduce(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"Sigma-clipped mean combination.\"\"\"\n        mean = np.mean(data, axis=0)\n        std = np.std(data, axis=0)\n        \n        # Mask outliers\n        mask = np.abs(data - mean) < self.sigma * std\n        \n        # Recalculate with mask\n        masked_data = np.where(mask, data, np.nan)\n        return np.nanmean(masked_data, axis=0)\n\n# Observer pattern for pipeline monitoring\nclass PipelineObserver(ABC):\n    \"\"\"Abstract observer for pipeline events.\"\"\"\n    \n    @abstractmethod\n    def update(self, event: str, data: dict):\n        \"\"\"Called when pipeline state changes.\"\"\"\n        pass\n\nclass LogObserver(PipelineObserver):\n    \"\"\"Logs pipeline events.\"\"\"\n    \n    def update(self, event: str, data: dict):\n        \"\"\"Log the event.\"\"\"\n        print(f\"[LOG] {event}: {data}\")\n\nclass QualityObserver(PipelineObserver):\n    \"\"\"Monitors data quality.\"\"\"\n    \n    def __init__(self):\n        self.quality_metrics = []\n    \n    def update(self, event: str, data: dict):\n        \"\"\"Check quality metrics.\"\"\"\n        if event == \"reduction_complete\":\n            result = data.get('result')\n            if result is not None:\n                snr = np.mean(result) / np.std(result)\n                self.quality_metrics.append(snr)\n                \n                if snr < 10:\n                    print(f\"[QUALITY WARNING] Low SNR: {snr:.1f}\")\n\n# Factory pattern for pipeline creation\nclass PipelineFactory:\n    \"\"\"Factory for creating appropriate pipelines.\"\"\"\n    \n    @staticmethod\n    def create_pipeline(obs_type: str) -> 'ReductionPipeline':\n        \"\"\"Create pipeline based on observation type.\"\"\"\n        \n        if obs_type == \"imaging\":\n            pipeline = ReductionPipeline()\n            pipeline.set_strategy(MedianCombine())\n            pipeline.attach(LogObserver())\n            return pipeline\n        \n        elif obs_type == \"spectroscopy\":\n            pipeline = ReductionPipeline()\n            pipeline.set_strategy(SigmaClipping(sigma=2.5))\n            pipeline.attach(LogObserver())\n            pipeline.attach(QualityObserver())\n            return pipeline\n        \n        elif obs_type == \"photometry\":\n            pipeline = ReductionPipeline()\n            pipeline.set_strategy(SigmaClipping(sigma=3.0))\n            pipeline.attach(QualityObserver())\n            return pipeline\n        \n        else:\n            raise ValueError(f\"Unknown observation type: {obs_type}\")\n\n# Main pipeline using all patterns\nclass ReductionPipeline:\n    \"\"\"Data reduction pipeline with patterns.\"\"\"\n    \n    def __init__(self):\n        self.strategy: Optional[ReductionStrategy] = None\n        self.observers: List[PipelineObserver] = []\n    \n    def set_strategy(self, strategy: ReductionStrategy):\n        \"\"\"Set reduction strategy.\"\"\"\n        self.strategy = strategy\n        self.notify(\"strategy_changed\", \n                   {\"strategy\": strategy.__class__.__name__})\n    \n    def attach(self, observer: PipelineObserver):\n        \"\"\"Attach observer.\"\"\"\n        self.observers.append(observer)\n    \n    def notify(self, event: str, data: dict):\n        \"\"\"Notify all observers.\"\"\"\n        for observer in self.observers:\n            observer.update(event, data)\n    \n    def process(self, frames: np.ndarray) -> np.ndarray:\n        \"\"\"Process frames using current strategy.\"\"\"\n        if not self.strategy:\n            raise RuntimeError(\"No reduction strategy set\")\n        \n        self.notify(\"reduction_started\", {\"n_frames\": len(frames)})\n        \n        # Apply strategy\n        result = self.strategy.reduce(frames)\n        \n        self.notify(\"reduction_complete\", {\n            \"result_shape\": result.shape,\n            \"result\": result\n        })\n        \n        return result\n\n# Demonstrate all patterns together\nprint(\"=== Design Patterns Demo ===\\n\")\n\n# Create different pipelines using factory\nimaging_pipeline = PipelineFactory.create_pipeline(\"imaging\")\nspectro_pipeline = PipelineFactory.create_pipeline(\"spectroscopy\")\n\n# Generate mock data\nframes = np.random.randn(10, 100, 100) * 100 + 1000\n\n# Process with different pipelines\nprint(\"Imaging Pipeline:\")\nimaging_result = imaging_pipeline.process(frames)\n\nprint(\"\\nSpectroscopy Pipeline:\")\nspectro_result = spectro_pipeline.process(frames)\n\n# Change strategy at runtime\nprint(\"\\nChanging strategy at runtime:\")\nimaging_pipeline.set_strategy(SigmaClipping(sigma=2.0))\nnew_result = imaging_pipeline.process(frames)\n\nprint(\"\\n✅ Patterns demonstrated:\")\nprint(\"  - Factory: Created specialized pipelines\")\nprint(\"  - Strategy: Swappable reduction algorithms\")\nprint(\"  - Observer: Monitoring and logging\")\n\n","type":"content","url":"/python-advanced-oop#exercise-2-design-pattern-implementation","position":33},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl3":"Exercise 3: Performance Optimization Challenge","lvl2":"10.8 Practice Exercises"},"type":"lvl3","url":"/python-advanced-oop#exercise-3-performance-optimization-challenge","position":34},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl3":"Exercise 3: Performance Optimization Challenge","lvl2":"10.8 Practice Exercises"},"content":"\n\n\"\"\"\nDebug This! Can you find and fix the performance issue?\n\"\"\"\n\n# Inefficient implementation\nclass SlowGalaxy:\n    def __init__(self, name, redshift):\n        self.name = name\n        self.redshift = redshift\n    \n    def distance_mpc(self):\n        \"\"\"Calculate distance in Mpc (SLOW!).\"\"\"\n        # Hubble constant\n        H0 = 70  # km/s/Mpc\n        c = 3e5  # km/s\n        \n        # This recalculates every time!\n        return c * self.redshift / H0\n    \n    def luminosity_distance(self):\n        \"\"\"Luminosity distance (calls distance repeatedly).\"\"\"\n        d = self.distance_mpc()\n        \n        # Cosmological correction (simplified)\n        for i in range(1000):  # Unnecessary loop!\n            d = d * (1 + self.redshift * 0.001)\n        \n        return d\n\n# Test slow version\nimport time\n\ngalaxies = [SlowGalaxy(f\"Galaxy{i}\", 0.01 * i) \n           for i in range(100)]\n\nstart = time.perf_counter()\nfor g in galaxies:\n    for _ in range(10):  # Multiple calls\n        d = g.luminosity_distance()\nslow_time = time.perf_counter() - start\n\nprint(f\"Slow version: {slow_time*1000:.1f} ms\")\n\n# SOLUTION: Optimized implementation\nfrom functools import lru_cache\n\nclass FastGalaxy:\n    __slots__ = ['name', 'redshift', '_distance_cache']\n    \n    def __init__(self, name, redshift):\n        self.name = name\n        self.redshift = redshift\n        self._distance_cache = None\n    \n    @property\n    def distance_mpc(self):\n        \"\"\"Cached distance calculation.\"\"\"\n        if self._distance_cache is None:\n            H0 = 70\n            c = 3e5\n            self._distance_cache = c * self.redshift / H0\n        return self._distance_cache\n    \n    @lru_cache(maxsize=1)\n    def luminosity_distance(self):\n        \"\"\"Cached luminosity distance.\"\"\"\n        d = self.distance_mpc\n        \n        # Vectorized calculation instead of loop\n        correction = (1 + self.redshift) ** 1.0\n        \n        return d * correction\n\n# Test fast version\ngalaxies_fast = [FastGalaxy(f\"Galaxy{i}\", 0.01 * i) \n                for i in range(100)]\n\nstart = time.perf_counter()\nfor g in galaxies_fast:\n    for _ in range(10):\n        d = g.luminosity_distance()\nfast_time = time.perf_counter() - start\n\nprint(f\"Fast version: {fast_time*1000:.1f} ms\")\nprint(f\"Speedup: {slow_time/fast_time:.1f}x\")\n\nprint(\"\\n🐛 Performance issues found and fixed:\")\nprint(\"  1. Recalculating distance every call\")\nprint(\"  2. Unnecessary loop in luminosity_distance\")\nprint(\"  3. No caching of expensive calculations\")\nprint(\"  4. Using dict instead of __slots__\")\n\n","type":"content","url":"/python-advanced-oop#exercise-3-performance-optimization-challenge","position":35},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl2":"Main Takeaways"},"type":"lvl2","url":"/python-advanced-oop#main-takeaways","position":36},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl2":"Main Takeaways"},"content":"You’ve completed a transformative journey from using scientific Python packages to understanding their architectural foundations. The abstract base classes, mixins, design patterns, and optimization techniques you’ve mastered aren’t just advanced features - they’re the pillars supporting every major scientific Python package you use. When NumPy seamlessly handles different array types, when Matplotlib coordinates thousands of plot elements, when Astropy prevents unit conversion disasters, they’re using exactly the patterns you now understand. You’ve moved beyond being a consumer of these tools to understanding how they’re built, why they work, and how to contribute at the same level.\n\nThe progression through this chapter mirrors your growth as a computational scientist. You started by understanding how ABCs enabled the Event Horizon Telescope to coordinate incompatible telescopes into one Earth-sized instrument. You discovered how mixins let CERN process billions of particle collisions without code duplication. You learned how async programming enables modern observatories to control dozens of instruments simultaneously. These aren’t just programming techniques - they’re the engineering principles that enable modern scientific discovery. Every gravitational wave detection, every exoplanet discovery, every galaxy survey relies on software architected with these patterns.\n\nThe practical skills you’ve developed prepare you for real research challenges. Dataclasses eliminate the boilerplate that clutters scientific code, letting you focus on science instead of syntax. Async programming enables you to build instrument control systems that maximize precious telescope time. Descriptors and metaclasses let you build domain-specific languages that make unit errors impossible. Performance optimization techniques ensure your code scales from prototype to production, from analyzing one galaxy to processing entire surveys. You now have the tools to build software that not only works but scales to the demands of modern astronomy.\n\nLooking forward, you’re equipped to recognize these patterns throughout the scientific Python ecosystem and apply them in your own work. When you see NumPy’s factory functions creating appropriate array types, you understand the factory pattern at work. When SciPy swaps optimization algorithms, you recognize the strategy pattern. When Matplotlib manages complex figure state, you see context managers and mixins in action. More importantly, you know when to apply these patterns in your own code - using ABCs when defining interfaces for collaboration, mixins when composing behaviors, async when controlling hardware, and optimization techniques when processing large datasets. You’ve transformed from someone who writes scripts to someone who architects systems, ready to build the next generation of tools that will enable tomorrow’s discoveries.","type":"content","url":"/python-advanced-oop#main-takeaways","position":37},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl2":"Definitions"},"type":"lvl2","url":"/python-advanced-oop#definitions","position":38},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl2":"Definitions"},"content":"Abstract Base Class (ABC): A class that cannot be instantiated and defines methods that subclasses must implement, enforcing interfaces.\n\nAsync/Await: Python’s syntax for asynchronous programming, allowing concurrent operations without threads.\n\nContext Manager: An object implementing __enter__ and __exit__ methods for guaranteed resource cleanup with with statements.\n\nCoroutine: A function that can pause and resume execution, enabling cooperative multitasking with async def.\n\nDataclass: A decorator that automatically generates special methods for classes primarily storing data.\n\nDescriptor: An object implementing __get__, __set__, or __delete__ to customize attribute access behavior.\n\nDesign Pattern: A reusable solution to a commonly occurring problem in software design.\n\nDiamond Problem: Ambiguity in method resolution when a class inherits from two classes sharing a common base.\n\nFactory Pattern: A design pattern that creates objects without specifying their exact classes.\n\nGenerator: A function that returns an iterator, yielding values one at a time to conserve memory.\n\nInterface: A contract specifying what methods a class must provide, ensuring compatibility between components.\n\nMetaclass: A class whose instances are classes themselves, controlling class creation and behavior.\n\nMethod Resolution Order (MRO): The order Python searches through classes when looking for methods in inheritance hierarchies.\n\nMixin: A class providing specific functionality to be inherited by other classes, not meant to stand alone.\n\nObserver Pattern: A design pattern where objects notify registered observers of state changes.\n\nProtocol: A structural type system defining what methods/attributes an object must have for duck typing.\n\nStrategy Pattern: A design pattern that defines a family of algorithms and makes them interchangeable.\n\nType Hint: Optional annotations indicating expected types for variables, parameters, and return values.\n\n__slots__: Class attribute that restricts instance attributes to a fixed set, saving memory.","type":"content","url":"/python-advanced-oop#definitions","position":39},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl2":"Key Takeaways"},"type":"lvl2","url":"/python-advanced-oop#key-takeaways","position":40},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl2":"Key Takeaways"},"content":"✓ Abstract base classes enforce interfaces across teams - ABCs enabled the Event Horizon Telescope’s eight incompatible telescopes to work as one, producing the first black hole image\n\n✓ Mixins compose behaviors without code duplication - The LHC uses mixins to give different detectors different capabilities, enabling the Higgs boson discovery\n\n✓ Dataclasses eliminate boilerplate in data-heavy code - Modern Astropy uses dataclasses to reduce code by 30% while improving type safety\n\n✓ Async programming enables parallel instrument control - Modern observatories use async to control multiple instruments simultaneously, maximizing observation efficiency\n\n✓ Descriptors and metaclasses enable domain-specific behavior - Astropy’s units system uses these to catch unit errors at assignment time, preventing Mars Climate Orbiter disasters\n\n✓ Design patterns solve recurring architectural problems - SciPy’s 20+ optimization algorithms use the Strategy pattern, letting you swap algorithms with one parameter\n\n✓ __slots__ saves 40-50% memory for large datasets - Critical for processing Gaia’s 2 billion star catalog or Vera Rubin’s 20TB nightly data\n\n✓ Caching dramatically improves performance - The @lru_cache decorator can provide 10-1000x speedups for expensive calculations\n\n✓ Type hints make scientific code self-documenting - Modern packages use type hints to catch errors early and improve IDE support\n\n✓ Profile before optimizing - The Vera Rubin Observatory found 60% of time was in coordinate transforms, not image processing as expected","type":"content","url":"/python-advanced-oop#key-takeaways","position":41},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl2":"Quick Reference Tables"},"type":"lvl2","url":"/python-advanced-oop#quick-reference-tables","position":42},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl2":"Quick Reference Tables"},"content":"","type":"content","url":"/python-advanced-oop#quick-reference-tables","position":43},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl3":"Design Patterns in Scientific Python","lvl2":"Quick Reference Tables"},"type":"lvl3","url":"/python-advanced-oop#design-patterns-in-scientific-python","position":44},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl3":"Design Patterns in Scientific Python","lvl2":"Quick Reference Tables"},"content":"Pattern\n\nProblem Solved\n\nNumPy/SciPy Example\n\nYour Use Case\n\nFactory\n\nCreate right object type\n\nnp.array() creates appropriate array\n\nDifferent detector types\n\nStrategy\n\nSwap algorithms\n\nscipy.optimize.minimize(method=...)\n\nReduction algorithms\n\nObserver\n\nEvent notification\n\nMatplotlib figure updates\n\nPipeline monitoring\n\nSingleton\n\nSingle instance\n\nNumPy’s random state\n\nHardware controllers\n\nTemplate\n\nAlgorithm skeleton\n\nAll SciPy optimizers\n\nAnalysis pipelines","type":"content","url":"/python-advanced-oop#design-patterns-in-scientific-python","position":45},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl3":"Performance Optimization Techniques","lvl2":"Quick Reference Tables"},"type":"lvl3","url":"/python-advanced-oop#performance-optimization-techniques","position":46},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl3":"Performance Optimization Techniques","lvl2":"Quick Reference Tables"},"content":"Technique\n\nWhen to Use\n\nTypical Improvement\n\nExample\n\n__slots__\n\nMany small objects\n\n40-50% memory\n\nStar catalogs\n\n@lru_cache\n\nRepeated calculations\n\n10-1000x speed\n\nRedshift finding\n\nAsync\n\nI/O or hardware control\n\n2-10x throughput\n\nTelescope control\n\nVectorization\n\nNumerical operations\n\n10-100x speed\n\nArray operations\n\nDataclasses\n\nData containers\n\n30% less code\n\nObservations","type":"content","url":"/python-advanced-oop#performance-optimization-techniques","position":47},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl3":"Async Best Practices","lvl2":"Quick Reference Tables"},"type":"lvl3","url":"/python-advanced-oop#async-best-practices","position":48},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl3":"Async Best Practices","lvl2":"Quick Reference Tables"},"content":"Pattern\n\nUse Case\n\nExample\n\nasyncio.gather()\n\nRun tasks in parallel\n\nMultiple exposures\n\nasyncio.create_task()\n\nFire and forget\n\nBackground monitoring\n\nasync with\n\nAsync context managers\n\nInstrument connections\n\nasyncio.Queue\n\nProducer-consumer\n\nData pipeline\n\nasyncio.sleep()\n\nNon-blocking delay\n\nHardware timing","type":"content","url":"/python-advanced-oop#async-best-practices","position":49},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl3":"Memory Comparison (per object)","lvl2":"Quick Reference Tables"},"type":"lvl3","url":"/python-advanced-oop#memory-comparison-per-object","position":50},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl3":"Memory Comparison (per object)","lvl2":"Quick Reference Tables"},"content":"Implementation\n\nMemory Usage\n\n1M Objects\n\nRegular class\n\n~296 bytes\n\n296 MB\n\nWith __slots__\n\n~56 bytes\n\n56 MB\n\nNamedtuple\n\n~72 bytes\n\n72 MB\n\nDataclass\n\n~296 bytes\n\n296 MB\n\nDataclass + slots\n\n~56 bytes\n\n56 MB","type":"content","url":"/python-advanced-oop#memory-comparison-per-object","position":51},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl2":"References"},"type":"lvl2","url":"/python-advanced-oop#references","position":52},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl2":"References"},"content":"van Rossum, G., & Warsaw, B. (2001). PEP 3119 – Introducing Abstract Base Classes. Python Enhancement Proposals. \n\nhttps://​www​.python​.org​/dev​/peps​/pep​-3119/ - The original proposal that introduced ABCs to Python.\n\nEvent Horizon Telescope Collaboration. (2019). First M87 Event Horizon Telescope Results. I. The Shadow of the Supermassive Black Hole. The Astrophysical Journal Letters, 875(1), L1. - The paper describing the first black hole image and the software coordination required.\n\nBouman, K. L., et al. (2016). Computational Imaging for VLBI Image Reconstruction. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 913-922. - Details on the software architecture that enabled the EHT collaboration.\n\nHegner, B., et al. (2014). The CMS Software Architecture and Framework. Journal of Physics: Conference Series, 513(5), 052017. - Describes the mixin-based architecture used at CERN for the Large Hadron Collider.\n\nSmith, E. (2018). PEP 557 – Data Classes. Python Enhancement Proposals. \n\nhttps://​www​.python​.org​/dev​/peps​/pep​-0557/ - The proposal that introduced dataclasses to Python 3.7.\n\nvan Rossum, G., Lehtosalo, J., & Langa, Ł. (2014). PEP 484 – Type Hints. Python Enhancement Proposals. \n\nhttps://​www​.python​.org​/dev​/peps​/pep​-0484/ - Introduction of type hints to Python.\n\nLevkivskyi, I. (2019). PEP 563 – Postponed Evaluation of Annotations. Python Enhancement Proposals. \n\nhttps://​www​.python​.org​/dev​/peps​/pep​-0563/ - The from __future__ import annotations feature.\n\nGamma, E., Helm, R., Johnson, R., & Vlissides, J. (1994). Design Patterns: Elements of Reusable Object-Oriented Software. Addison-Wesley. - The classic “Gang of Four” book defining design patterns including Factory, Observer, and Strategy.\n\nSelivanov, Y. (2015). PEP 492 – Coroutines with async and await syntax. Python Enhancement Proposals. \n\nhttps://​www​.python​.org​/dev​/peps​/pep​-0492/ - Introduction of async/await to Python.\n\nReefe, M., et al. (2022). An asynchronous object-oriented approach to the automation of the 0.8-meter George Mason University campus telescope in Python. arXiv preprint arXiv:2206.01780. - Real-world application of async OOP for telescope control.\n\nRamalho, L. (2022). Fluent Python (2nd ed.). O’Reilly Media. - Comprehensive coverage of Python’s advanced OOP features including metaclasses and descriptors.\n\nBeazley, D., & Jones, B. K. (2013). Python Cookbook (3rd ed.). O’Reilly Media. - Practical recipes for advanced Python patterns and optimization techniques.\n\nAstropy Collaboration. (2022). The Astropy Project: Sustaining and Growing a Community-oriented Open-source Project and the Latest Major Release (v5.0) of the Core Package. The Astrophysical Journal, 935(2), 167. - Current state of Astropy’s architecture.\n\nHarris, C. R., et al. (2020). Array programming with NumPy. Nature, 585(7825), 357-362. - NumPy’s design philosophy and architecture.\n\nVirtanen, P., et al. (2020). SciPy 1.0: fundamental algorithms for scientific computing in Python. Nature Methods, 17(3), 261-272. - SciPy’s architectural decisions including the Strategy pattern for optimization.\n\nLanga, Ł. (2021). asyncio — Asynchronous I/O. Python Documentation. \n\nhttps://​docs​.python​.org​/3​/library​/asyncio​.html - Official Python asyncio documentation.\n\nMartin, R. C. (2017). Clean Architecture: A Craftsman’s Guide to Software Structure and Design. Prentice Hall. - Principles of software architecture applicable to scientific computing.\n\nvan der Walt, S., et al. (2014). scikit-image: image processing in Python. PeerJ, 2, e453. - Example of ABC usage in scientific Python packages.\n\nMcKinney, W. (2017). Python for Data Analysis (2nd ed.). O’Reilly Media. - Practical patterns for scientific data processing.\n\nPrice-Whelan, A. M., & Bonaca, A. (2018). Off the Beaten Path: Gaia Reveals GD-1 Stars outside of the Main Stream. The Astrophysical Journal Letters, 863(2), L20. - The paper referenced in Astropy’s Data Carpentry curriculum.\n\nNygaard, K., & Dahl, O. J. (1978). The development of the SIMULA languages. ACM SIGPLAN Notices, 13(8), 245-272. - Historical origin of object-oriented programming concepts.\n\nMeinel, C., & Andres, M. (2018). pyobs: A Python Framework for Robotic Observatories. Proceedings of the SPIE, 10707, 107072P. - Modern telescope control software architecture.\n\nPython Software Foundation. (2024). Abstract Base Classes for Containers. \n\nhttps://​docs​.python​.org​/3​/library​/collections​.abc​.html - Documentation for Python’s built-in ABCs.\n\nTollerud, E., et al. (2023). Astropy Affiliated Packages. The Astropy Project. \n\nhttps://​www​.astropy​.org​/affiliated/ - Overview of packages using Astropy’s architectural patterns.\n\nBeazley, D. (2016). Python Concurrency From the Ground Up. PyCon 2015. \n\nhttps://​www​.youtube​.com​/watch​?v​=​MCs5OvhV9S4 - Deep dive into Python’s concurrency models including asyncio.","type":"content","url":"/python-advanced-oop#references","position":53},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl2":"Next Chapter Preview"},"type":"lvl2","url":"/python-advanced-oop#next-chapter-preview","position":54},{"hierarchy":{"lvl1":"Chapter 10: Advanced OOP Patterns - Architecting Scientific Software","lvl2":"Next Chapter Preview"},"content":"In Chapter 11, you’ll apply everything you’ve learned to build complete scientific applications. You’ll create a data reduction pipeline using the design patterns from this chapter, implement a real-time observation scheduler with async programming, and build a catalog cross-matching system that scales to billions of objects using the optimization techniques you’ve mastered. You’ll see how ABCs, mixins, and dataclasses combine to create professional-grade astronomical software. Most importantly, you’ll work through a complete research project from raw data to publication-ready results, using the same architectural patterns that power modern astronomical discoveries. The advanced patterns you’ve learned here become the foundation for building software that enables real science!","type":"content","url":"/python-advanced-oop#next-chapter-preview","position":55},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy"},"type":"lvl1","url":"/scipy","position":0},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy"},"content":"","type":"content","url":"/scipy","position":1},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy","lvl2":"Learning Objectives"},"type":"lvl2","url":"/scipy#learning-objectives","position":2},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy","lvl2":"Learning Objectives"},"content":"By the end of this chapter, you will:\n\nMaster interpolation and integration for astronomical data\n\nSolve ODEs for orbital dynamics and stellar evolution\n\nApply optimization algorithms to fitting problems\n\nUse signal processing for time series and spectra\n\nImplement statistical tests for data analysis\n\nApply special functions in astrophysical calculations\n\nLeverage sparse matrices for large-scale problems","type":"content","url":"/scipy#learning-objectives","position":3},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy","lvl2":"Introduction: SciPy’s Role in Astronomy"},"type":"lvl2","url":"/scipy#introduction-scipys-role-in-astronomy","position":4},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy","lvl2":"Introduction: SciPy’s Role in Astronomy"},"content":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import constants, special, integrate, optimize, interpolate, signal, stats\nfrom scipy.sparse import csr_matrix, linalg as sparse_linalg\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef why_scipy():\n    \"\"\"Demonstrate SciPy's value for astronomical computations.\"\"\"\n    \n    print(\"SciPy provides optimized algorithms for:\")\n    print(\"1. Physical constants\")\n    print(f\"   Speed of light: {constants.c:.0f} m/s\")\n    print(f\"   Gravitational constant: {constants.G:.4e} m³/kg/s²\")\n    print(f\"   Stefan-Boltzmann: {constants.Stefan_Boltzmann:.4e} W/m²/K⁴\")\n    \n    print(\"\\n2. Special functions\")\n    # Bessel functions for diffraction patterns\n    x = np.linspace(0, 20, 100)\n    airy_pattern = (2 * special.j1(x) / x)**2\n    print(f\"   Airy disk calculation uses Bessel function J₁\")\n    \n    print(\"\\n3. Optimized numerical algorithms\")\n    print(\"   - Integration: adaptive quadrature, ODE solvers\")\n    print(\"   - Optimization: least squares, MCMC\")\n    print(\"   - Signal processing: filters, FFT, wavelets\")\n    print(\"   - Statistics: distributions, tests, correlations\")\n\nwhy_scipy()","type":"content","url":"/scipy#introduction-scipys-role-in-astronomy","position":5},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy","lvl2":"Interpolation for Astronomical Data"},"type":"lvl2","url":"/scipy#interpolation-for-astronomical-data","position":6},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy","lvl2":"Interpolation for Astronomical Data"},"content":"","type":"content","url":"/scipy#interpolation-for-astronomical-data","position":7},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy","lvl3":"1D and 2D Interpolation","lvl2":"Interpolation for Astronomical Data"},"type":"lvl3","url":"/scipy#id-1d-and-2d-interpolation","position":8},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy","lvl3":"1D and 2D Interpolation","lvl2":"Interpolation for Astronomical Data"},"content":"def interpolation_examples():\n    \"\"\"Interpolation methods for astronomical data.\"\"\"\n    \n    # 1D Spectral interpolation\n    print(\"1. Spectral Line Interpolation\")\n    \n    # Original spectrum (low resolution)\n    wave_obs = np.array([4000, 4500, 5000, 5500, 6000, 6500, 7000])\n    flux_obs = np.array([0.8, 0.85, 0.9, 0.95, 1.0, 0.92, 0.88])\n    \n    # Different interpolation methods\n    wave_high = np.linspace(4000, 7000, 1000)\n    \n    # Linear interpolation\n    interp_linear = interpolate.interp1d(wave_obs, flux_obs, kind='linear')\n    flux_linear = interp_linear(wave_high)\n    \n    # Cubic spline\n    interp_cubic = interpolate.interp1d(wave_obs, flux_obs, kind='cubic')\n    flux_cubic = interp_cubic(wave_high)\n    \n    # Akima spline (less oscillation)\n    from scipy.interpolate import Akima1DInterpolator\n    interp_akima = Akima1DInterpolator(wave_obs, flux_obs)\n    flux_akima = interp_akima(wave_high)\n    \n    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n    \n    for ax, flux, title in zip(axes, \n                               [flux_linear, flux_cubic, flux_akima],\n                               ['Linear', 'Cubic', 'Akima']):\n        ax.plot(wave_high, flux, 'b-', linewidth=1, label='Interpolated')\n        ax.plot(wave_obs, flux_obs, 'ro', markersize=6, label='Observed')\n        ax.set_xlabel('Wavelength [Å]')\n        ax.set_ylabel('Flux')\n        ax.set_title(f'{title} Interpolation')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # 2D Image interpolation\n    print(\"\\n2. PSF Interpolation Across Detector\")\n    \n    # PSF measurements at specific positions\n    x_psf = np.array([0, 512, 1024, 0, 512, 1024, 0, 512, 1024])\n    y_psf = np.array([0, 0, 0, 512, 512, 512, 1024, 1024, 1024])\n    fwhm_psf = np.array([2.1, 2.0, 2.2, 2.0, 1.9, 2.1, 2.2, 2.1, 2.3])\n    \n    # Interpolate across entire detector\n    x_grid = np.arange(0, 1025, 32)\n    y_grid = np.arange(0, 1025, 32)\n    X_grid, Y_grid = np.meshgrid(x_grid, y_grid)\n    \n    # Different 2D interpolation methods\n    from scipy.interpolate import griddata\n    \n    # Linear interpolation\n    fwhm_linear = griddata((x_psf, y_psf), fwhm_psf, \n                          (X_grid, Y_grid), method='linear')\n    \n    # Cubic interpolation\n    fwhm_cubic = griddata((x_psf, y_psf), fwhm_psf, \n                         (X_grid, Y_grid), method='cubic')\n    \n    # Radial basis function\n    from scipy.interpolate import Rbf\n    rbf = Rbf(x_psf, y_psf, fwhm_psf, function='thin_plate')\n    fwhm_rbf = rbf(X_grid, Y_grid)\n    \n    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n    \n    for ax, data, title in zip(axes, \n                               [fwhm_linear, fwhm_cubic, fwhm_rbf],\n                               ['Linear', 'Cubic', 'RBF']):\n        im = ax.contourf(X_grid, Y_grid, data, levels=20, cmap='viridis')\n        ax.scatter(x_psf, y_psf, c='red', s=50, marker='x')\n        ax.set_xlabel('X [pixels]')\n        ax.set_ylabel('Y [pixels]')\n        ax.set_title(f'{title} PSF Interpolation')\n        plt.colorbar(im, ax=ax, label='FWHM [pixels]')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return wave_high, flux_cubic\n\nwavelengths, spectrum = interpolation_examples()","type":"content","url":"/scipy#id-1d-and-2d-interpolation","position":9},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy","lvl3":"Extrapolation and Edge Effects","lvl2":"Interpolation for Astronomical Data"},"type":"lvl3","url":"/scipy#extrapolation-and-edge-effects","position":10},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy","lvl3":"Extrapolation and Edge Effects","lvl2":"Interpolation for Astronomical Data"},"content":"def extrapolation_pitfalls():\n    \"\"\"Demonstrate dangers of extrapolation.\"\"\"\n    \n    # Observed data (limited range)\n    redshift = np.array([0.1, 0.3, 0.5, 0.7, 0.9])\n    distance = np.array([450, 1400, 2200, 3100, 4000])  # Mpc\n    \n    # Try to extrapolate\n    z_range = np.linspace(0, 2, 100)\n    \n    # Different extrapolation approaches\n    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n    \n    # Linear extrapolation (dangerous!)\n    f_linear = interpolate.interp1d(redshift, distance, \n                                   kind='linear', fill_value='extrapolate')\n    d_linear = f_linear(z_range)\n    \n    axes[0].plot(z_range, d_linear, 'b-', label='Linear extrap')\n    axes[0].plot(redshift, distance, 'ro', markersize=8, label='Data')\n    axes[0].axvspan(1.0, 2.0, alpha=0.2, color='red', label='Danger zone')\n    axes[0].set_xlabel('Redshift')\n    axes[0].set_ylabel('Distance [Mpc]')\n    axes[0].set_title('Linear Extrapolation (BAD)')\n    axes[0].legend()\n    \n    # Polynomial fit (also dangerous!)\n    poly_coef = np.polyfit(redshift, distance, 3)\n    d_poly = np.polyval(poly_coef, z_range)\n    \n    axes[1].plot(z_range, d_poly, 'g-', label='Polynomial')\n    axes[1].plot(redshift, distance, 'ro', markersize=8, label='Data')\n    axes[1].axvspan(1.0, 2.0, alpha=0.2, color='red')\n    axes[1].set_xlabel('Redshift')\n    axes[1].set_title('Polynomial Extrapolation (WORSE)')\n    axes[1].legend()\n    \n    # Physical model (better)\n    from scipy.integrate import quad\n    \n    def luminosity_distance(z, H0=70, Om0=0.3, OL0=0.7):\n        \"\"\"Calculate luminosity distance using cosmology.\"\"\"\n        c = 3e5  # km/s\n        \n        def integrand(zp):\n            return 1 / np.sqrt(Om0*(1+zp)**3 + OL0)\n        \n        integral, _ = quad(integrand, 0, z)\n        return c/H0 * (1+z) * integral\n    \n    d_model = [luminosity_distance(z) for z in z_range]\n    \n    axes[2].plot(z_range, d_model, 'k-', label='ΛCDM model')\n    axes[2].plot(redshift, distance, 'ro', markersize=8, label='Data')\n    axes[2].set_xlabel('Redshift')\n    axes[2].set_title('Physical Model (GOOD)')\n    axes[2].legend()\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"Lesson: Use physical models, not blind extrapolation!\")\n\nextrapolation_pitfalls()","type":"content","url":"/scipy#extrapolation-and-edge-effects","position":11},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy","lvl2":"Integration Techniques"},"type":"lvl2","url":"/scipy#integration-techniques","position":12},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy","lvl2":"Integration Techniques"},"content":"","type":"content","url":"/scipy#integration-techniques","position":13},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy","lvl3":"Numerical Integration Methods","lvl2":"Integration Techniques"},"type":"lvl3","url":"/scipy#numerical-integration-methods","position":14},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy","lvl3":"Numerical Integration Methods","lvl2":"Integration Techniques"},"content":"def integration_showcase():\n    \"\"\"Compare integration methods for astronomical applications.\"\"\"\n    \n    # 1. Integrate stellar spectrum to get bolometric luminosity\n    print(\"1. Bolometric Luminosity Calculation\")\n    \n    # Planck function\n    def planck(wavelength, T):\n        \"\"\"Planck function in wavelength space.\"\"\"\n        h = constants.h\n        c = constants.c\n        k = constants.k\n        \n        wavelength = wavelength * 1e-9  # Convert nm to m\n        \n        with np.errstate(over='ignore'):\n            B = (2*h*c**2/wavelength**5) / (np.exp(h*c/(wavelength*k*T)) - 1)\n        \n        return B\n    \n    # Integrate over all wavelengths\n    T_star = 5778  # Solar temperature\n    \n    # Method 1: Fixed quadrature\n    from scipy.integrate import quad\n    L_quad, error = quad(lambda lam: planck(lam, T_star), 10, 100000)\n    \n    # Method 2: Adaptive quadrature with specified tolerance\n    from scipy.integrate import quad_vec\n    wavelengths = np.logspace(1, 5, 1000)  # 10 nm to 100 μm\n    integrand = planck(wavelengths, T_star)\n    L_trapz = np.trapz(integrand, wavelengths)\n    \n    # Method 3: Romberg integration\n    from scipy.integrate import romberg\n    L_romberg = romberg(lambda lam: planck(lam, T_star), 10, 100000)\n    \n    print(f\"  Quadrature: L = {L_quad:.3e} W/m²/sr\")\n    print(f\"  Trapezoid:  L = {L_trapz:.3e} W/m²/sr\")\n    print(f\"  Romberg:    L = {L_romberg:.3e} W/m²/sr\")\n    print(f\"  Stefan-Boltzmann: σT⁴ = {constants.Stefan_Boltzmann * T_star**4:.3e} W/m²\")\n    \n    # 2. Mass enclosed in galaxy profile\n    print(\"\\n2. Galaxy Mass Profile\")\n    \n    def density_profile(r, rho0=1e7, rs=10):\n        \"\"\"NFW dark matter density profile.\"\"\"\n        x = r / rs\n        return rho0 / (x * (1 + x)**2)\n    \n    def mass_enclosed(R, rho0=1e7, rs=10):\n        \"\"\"Integrate density to get enclosed mass.\"\"\"\n        \n        def integrand(r):\n            return 4 * np.pi * r**2 * density_profile(r, rho0, rs)\n        \n        mass, _ = quad(integrand, 0, R)\n        return mass\n    \n    radii = np.logspace(0, 3, 50)  # 1 to 1000 kpc\n    masses = [mass_enclosed(R) for R in radii]\n    \n    # Analytical solution for NFW\n    def mass_nfw_analytical(R, rho0=1e7, rs=10):\n        \"\"\"Analytical NFW enclosed mass.\"\"\"\n        x = R / rs\n        return 4 * np.pi * rho0 * rs**3 * (np.log(1+x) - x/(1+x))\n    \n    masses_analytical = [mass_nfw_analytical(R) for R in radii]\n    \n    fig, ax = plt.subplots(figsize=(8, 5))\n    ax.loglog(radii, masses, 'b-', linewidth=2, label='Numerical')\n    ax.loglog(radii, masses_analytical, 'r--', linewidth=2, label='Analytical')\n    ax.set_xlabel('Radius [kpc]')\n    ax.set_ylabel('Enclosed Mass [M☉]')\n    ax.set_title('NFW Profile Mass Integration')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    plt.show()\n\nintegration_showcase()","type":"content","url":"/scipy#numerical-integration-methods","position":15},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy","lvl3":"Multi-dimensional Integration","lvl2":"Integration Techniques"},"type":"lvl3","url":"/scipy#multi-dimensional-integration","position":16},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy","lvl3":"Multi-dimensional Integration","lvl2":"Integration Techniques"},"content":"def multidimensional_integration():\n    \"\"\"Multi-dimensional integration for complex geometries.\"\"\"\n    \n    # Calculate volume and mass of triaxial galaxy\n    print(\"Triaxial Galaxy Mass Calculation\")\n    \n    def galaxy_density(x, y, z, a=20, b=10, c=5, rho0=1e8):\n        \"\"\"Density in triaxial galaxy.\"\"\"\n        r_ell = np.sqrt((x/a)**2 + (y/b)**2 + (z/c)**2)\n        if r_ell > 1:\n            return 0\n        return rho0 * np.exp(-r_ell)\n    \n    # Method 1: Triple integral\n    from scipy.integrate import tplquad\n    \n    mass, error = tplquad(\n        galaxy_density,\n        -5, 5,  # z limits\n        lambda z: -10, lambda z: 10,  # y limits\n        lambda z, y: -20, lambda z, y: 20  # x limits\n    )\n    \n    print(f\"  Total mass (tplquad): {mass:.3e} M☉\")\n    print(f\"  Relative error: {error/mass:.3e}\")\n    \n    # Method 2: Monte Carlo integration\n    def monte_carlo_integrate(n_samples=100000):\n        \"\"\"Monte Carlo integration for complex shapes.\"\"\"\n        # Sample in bounding box\n        x = np.random.uniform(-20, 20, n_samples)\n        y = np.random.uniform(-10, 10, n_samples)\n        z = np.random.uniform(-5, 5, n_samples)\n        \n        # Evaluate density\n        densities = np.array([galaxy_density(xi, yi, zi) \n                             for xi, yi, zi in zip(x, y, z)])\n        \n        # Volume of bounding box\n        volume = 40 * 20 * 10\n        \n        # Monte Carlo estimate\n        mass_mc = volume * np.mean(densities)\n        error_mc = volume * np.std(densities) / np.sqrt(n_samples)\n        \n        return mass_mc, error_mc\n    \n    mass_mc, error_mc = monte_carlo_integrate()\n    print(f\"  Total mass (Monte Carlo): {mass_mc:.3e} ± {error_mc:.3e} M☉\")\n\nmultidimensional_integration()","type":"content","url":"/scipy#multi-dimensional-integration","position":17},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy","lvl2":"Differential Equations for Dynamics"},"type":"lvl2","url":"/scipy#differential-equations-for-dynamics","position":18},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy","lvl2":"Differential Equations for Dynamics"},"content":"","type":"content","url":"/scipy#differential-equations-for-dynamics","position":19},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy","lvl3":"Solving ODEs: Orbital Mechanics","lvl2":"Differential Equations for Dynamics"},"type":"lvl3","url":"/scipy#solving-odes-orbital-mechanics","position":20},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy","lvl3":"Solving ODEs: Orbital Mechanics","lvl2":"Differential Equations for Dynamics"},"content":"def orbital_dynamics():\n    \"\"\"Solve orbital dynamics with various methods.\"\"\"\n    \n    def three_body_system(t, state, m1, m2, m3):\n        \"\"\"\n        Three-body gravitational system.\n        State = [x1, y1, z1, x2, y2, z2, x3, y3, z3,\n                vx1, vy1, vz1, vx2, vy2, vz2, vx3, vy3, vz3]\n        \"\"\"\n        G = constants.G\n        \n        # Unpack positions and velocities\n        r1 = state[0:3]\n        r2 = state[3:6]\n        r3 = state[6:9]\n        v1 = state[9:12]\n        v2 = state[12:15]\n        v3 = state[15:18]\n        \n        # Calculate distances\n        r12 = np.linalg.norm(r2 - r1)\n        r13 = np.linalg.norm(r3 - r1)\n        r23 = np.linalg.norm(r3 - r2)\n        \n        # Calculate accelerations\n        a1 = G*m2*(r2-r1)/r12**3 + G*m3*(r3-r1)/r13**3\n        a2 = G*m1*(r1-r2)/r12**3 + G*m3*(r3-r2)/r23**3\n        a3 = G*m1*(r1-r3)/r13**3 + G*m2*(r2-r3)/r23**3\n        \n        # Return derivatives\n        return np.concatenate([v1, v2, v3, a1, a2, a3])\n    \n    # Set up figure-8 solution (approximate)\n    x0 = 0.97000436\n    y0 = 0.0\n    vx0 = -0.93240737\n    vy0 = -0.86473146\n    \n    # Initial conditions (scaled units where G=1, m=1)\n    state0 = np.array([\n        x0, -y0, 0,     # Body 1 position\n        -x0, y0, 0,     # Body 2 position\n        0, 0, 0,        # Body 3 position\n        vx0/2, vy0/2, 0,  # Body 1 velocity\n        vx0/2, vy0/2, 0,  # Body 2 velocity\n        -vx0, -vy0, 0     # Body 3 velocity\n    ])\n    \n    # Solve with different methods\n    t_span = (0, 6)\n    t_eval = np.linspace(0, 6, 1000)\n    \n    # RK45 (adaptive step)\n    from scipy.integrate import solve_ivp\n    \n    sol_rk45 = solve_ivp(\n        three_body_system, t_span, state0, \n        method='RK45', t_eval=t_eval, \n        args=(1, 1, 1), rtol=1e-10\n    )\n    \n    # DOP853 (8th order, good for long integration)\n    sol_dop853 = solve_ivp(\n        three_body_system, t_span, state0,\n        method='DOP853', t_eval=t_eval,\n        args=(1, 1, 1), rtol=1e-10\n    )\n    \n    # Plot orbits\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n    \n    # RK45 solution\n    for i, color in enumerate(['red', 'blue', 'green']):\n        axes[0].plot(sol_rk45.y[3*i], sol_rk45.y[3*i+1], \n                    color=color, linewidth=1, alpha=0.7,\n                    label=f'Body {i+1}')\n    axes[0].set_xlabel('X')\n    axes[0].set_ylabel('Y')\n    axes[0].set_title('Three-Body Orbits (RK45)')\n    axes[0].legend()\n    axes[0].axis('equal')\n    axes[0].grid(True, alpha=0.3)\n    \n    # Energy conservation check\n    def total_energy(state, m1, m2, m3):\n        \"\"\"Calculate total energy of system.\"\"\"\n        G = 1  # Scaled units\n        \n        r1, r2, r3 = state[0:3], state[3:6], state[6:9]\n        v1, v2, v3 = state[9:12], state[12:15], state[15:18]\n        \n        # Kinetic energy\n        KE = 0.5 * (m1*np.sum(v1**2) + m2*np.sum(v2**2) + m3*np.sum(v3**2))\n        \n        # Potential energy\n        r12 = np.linalg.norm(r2 - r1)\n        r13 = np.linalg.norm(r3 - r1)\n        r23 = np.linalg.norm(r3 - r2)\n        PE = -G * (m1*m2/r12 + m1*m3/r13 + m2*m3/r23)\n        \n        return KE + PE\n    \n    energies = [total_energy(sol_rk45.y[:, i], 1, 1, 1) \n                for i in range(len(t_eval))]\n    \n    axes[1].plot(t_eval, energies, 'k-', linewidth=1)\n    axes[1].set_xlabel('Time')\n    axes[1].set_ylabel('Total Energy')\n    axes[1].set_title('Energy Conservation')\n    axes[1].grid(True, alpha=0.3)\n    \n    drift = (energies[-1] - energies[0]) / energies[0]\n    axes[1].text(0.5, 0.95, f'Relative drift: {drift:.3e}',\n                transform=axes[1].transAxes, ha='center')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return sol_rk45\n\nsolution = orbital_dynamics()","type":"content","url":"/scipy#solving-odes-orbital-mechanics","position":21},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy","lvl2":"Optimization and Fitting"},"type":"lvl2","url":"/scipy#optimization-and-fitting","position":22},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy","lvl2":"Optimization and Fitting"},"content":"","type":"content","url":"/scipy#optimization-and-fitting","position":23},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy","lvl3":"Least Squares Fitting","lvl2":"Optimization and Fitting"},"type":"lvl3","url":"/scipy#least-squares-fitting","position":24},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy","lvl3":"Least Squares Fitting","lvl2":"Optimization and Fitting"},"content":"def optimization_examples():\n    \"\"\"Optimization methods for astronomical data fitting.\"\"\"\n    \n    # Generate synthetic supernova light curve\n    np.random.seed(42)\n    \n    def supernova_model(t, t0, A, tau_rise, tau_fall):\n        \"\"\"Simplified supernova light curve model.\"\"\"\n        phase = t - t0\n        flux = np.zeros_like(t)\n        \n        # Rising phase\n        mask_rise = (phase >= 0) & (phase < tau_rise)\n        flux[mask_rise] = A * (1 - np.exp(-phase[mask_rise]/tau_rise*3))\n        \n        # Falling phase\n        mask_fall = phase >= tau_rise\n        flux[mask_fall] = A * np.exp(-(phase[mask_fall]-tau_rise)/tau_fall)\n        \n        return flux\n    \n    # True parameters\n    true_params = dict(t0=50, A=100, tau_rise=15, tau_fall=30)\n    \n    # Generate data\n    t_obs = np.linspace(0, 150, 50)\n    flux_true = supernova_model(t_obs, **true_params)\n    flux_err = 5 + 0.05 * flux_true  # Heteroscedastic errors\n    flux_obs = flux_true + np.random.normal(0, flux_err)\n    \n    # 1. Simple least squares\n    from scipy.optimize import curve_fit\n    \n    popt, pcov = curve_fit(\n        supernova_model, t_obs, flux_obs,\n        p0=[45, 90, 10, 25],  # Initial guess\n        sigma=flux_err,\n        absolute_sigma=True\n    )\n    \n    perr = np.sqrt(np.diag(pcov))\n    \n    print(\"Least Squares Fit:\")\n    param_names = ['t0', 'A', 'tau_rise', 'tau_fall']\n    for name, val, err, true in zip(param_names, popt, perr, true_params.values()):\n        print(f\"  {name}: {val:.2f} ± {err:.2f} (true: {true})\")\n    \n    # 2. Robust fitting (handles outliers)\n    from scipy.optimize import least_squares\n    \n    def residuals(params, t, flux, flux_err):\n        model = supernova_model(t, *params)\n        return (flux - model) / flux_err\n    \n    # Add outliers\n    flux_outliers = flux_obs.copy()\n    flux_outliers[10] += 50  # Bad pixel\n    flux_outliers[30] -= 40  # Cosmic ray\n    \n    # Huber loss (robust to outliers)\n    result_robust = least_squares(\n        residuals, [45, 90, 10, 25],\n        args=(t_obs, flux_outliers, flux_err),\n        loss='huber'\n    )\n    \n    print(\"\\nRobust Fit (with outliers):\")\n    for name, val in zip(param_names, result_robust.x):\n        print(f\"  {name}: {val:.2f}\")\n    \n    # 3. Global optimization (for complex χ² surfaces)\n    from scipy.optimize import differential_evolution\n    \n    def chi2(params, t, flux, flux_err):\n        model = supernova_model(t, *params)\n        return np.sum(((flux - model) / flux_err)**2)\n    \n    bounds = [(40, 60), (50, 150), (5, 25), (20, 40)]\n    \n    result_global = differential_evolution(\n        chi2, bounds,\n        args=(t_obs, flux_obs, flux_err),\n        seed=42\n    )\n    \n    print(\"\\nGlobal Optimization:\")\n    for name, val in zip(param_names, result_global.x):\n        print(f\"  {name}: {val:.2f}\")\n    \n    # Plot results\n    fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n    \n    t_model = np.linspace(0, 150, 500)\n    \n    # Original fit\n    axes[0, 0].errorbar(t_obs, flux_obs, yerr=flux_err, \n                       fmt='ko', markersize=4, alpha=0.5, label='Data')\n    axes[0, 0].plot(t_model, supernova_model(t_model, *popt), \n                   'b-', linewidth=2, label='Least Squares')\n    axes[0, 0].plot(t_model, supernova_model(t_model, **true_params), \n                   'r--', linewidth=2, label='True')\n    axes[0, 0].set_xlabel('Time [days]')\n    axes[0, 0].set_ylabel('Flux')\n    axes[0, 0].set_title('Standard Least Squares')\n    axes[0, 0].legend()\n    \n    # With outliers\n    axes[0, 1].errorbar(t_obs, flux_outliers, yerr=flux_err,\n                       fmt='ko', markersize=4, alpha=0.5, label='Data+outliers')\n    axes[0, 1].plot(t_model, supernova_model(t_model, *result_robust.x),\n                   'g-', linewidth=2, label='Robust fit')\n    axes[0, 1].plot(t_model, supernova_model(t_model, *popt),\n                   'b--', linewidth=2, alpha=0.5, label='Standard fit')\n    axes[0, 1].set_xlabel('Time [days]')\n    axes[0, 1].set_ylabel('Flux')\n    axes[0, 1].set_title('Robust Fitting')\n    axes[0, 1].legend()\n    \n    # Residuals\n    residuals_standard = flux_obs - supernova_model(t_obs, *popt)\n    residuals_robust = flux_outliers - supernova_model(t_obs, *result_robust.x)\n    \n    axes[1, 0].scatter(t_obs, residuals_standard/flux_err, alpha=0.5)\n    axes[1, 0].axhline(0, color='r', linestyle='--')\n    axes[1, 0].set_xlabel('Time [days]')\n    axes[1, 0].set_ylabel('Normalized Residuals')\n    axes[1, 0].set_title('Residuals (Standard)')\n    axes[1, 0].set_ylim(-5, 5)\n    \n    axes[1, 1].scatter(t_obs, residuals_robust/flux_err, alpha=0.5)\n    axes[1, 1].axhline(0, color='r', linestyle='--')\n    axes[1, 1].set_xlabel('Time [days]')\n    axes[1, 1].set_ylabel('Normalized Residuals')\n    axes[1, 1].set_title('Residuals (Robust)')\n    axes[1, 1].set_ylim(-5, 5)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return popt, pcov\n\nparams, covariance = optimization_examples()","type":"content","url":"/scipy#least-squares-fitting","position":25},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy","lvl2":"Signal Processing for Time Series"},"type":"lvl2","url":"/scipy#signal-processing-for-time-series","position":26},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy","lvl2":"Signal Processing for Time Series"},"content":"","type":"content","url":"/scipy#signal-processing-for-time-series","position":27},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy","lvl3":"Fourier Analysis and Filtering","lvl2":"Signal Processing for Time Series"},"type":"lvl3","url":"/scipy#fourier-analysis-and-filtering","position":28},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy","lvl3":"Fourier Analysis and Filtering","lvl2":"Signal Processing for Time Series"},"content":"def signal_processing_astronomy():\n    \"\"\"Signal processing for astronomical time series.\"\"\"\n    \n    # Generate synthetic variable star data\n    np.random.seed(42)\n    \n    # Time sampling (uneven, realistic for ground-based)\n    n_nights = 200\n    t_obs = []\n    for night in range(n_nights):\n        # Random number of observations per night\n        n_obs = np.random.poisson(5)\n        if n_obs > 0:\n            # Observations within 6-hour window\n            t_night = night + np.random.uniform(0, 0.25, n_obs)\n            t_obs.extend(t_night)\n    \n    t_obs = np.array(sorted(t_obs))\n    \n    # Multi-periodic signal\n    P1, P2 = 2.3456, 3.7890  # Periods in days\n    A1, A2 = 0.1, 0.05  # Amplitudes\n    \n    signal = (A1 * np.sin(2*np.pi*t_obs/P1) + \n             A2 * np.sin(2*np.pi*t_obs/P2 + 1.5))\n    \n    # Add noise and trends\n    noise = np.random.normal(0, 0.02, len(t_obs))\n    trend = 0.0001 * t_obs  # Linear trend\n    flux = 1.0 + signal + noise + trend\n    \n    fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n    \n    # 1. Raw data\n    axes[0, 0].scatter(t_obs, flux, s=1, alpha=0.5)\n    axes[0, 0].set_xlabel('Time [days]')\n    axes[0, 0].set_ylabel('Flux')\n    axes[0, 0].set_title('Raw Light Curve')\n    \n    # 2. Lomb-Scargle periodogram (for uneven sampling)\n    from scipy.signal import lombscargle\n    \n    frequencies = np.linspace(0.1, 2, 1000)\n    periods = 1 / frequencies\n    \n    # Normalize data\n    flux_norm = flux - np.mean(flux)\n    \n    # Compute periodogram\n    power = lombscargle(t_obs, flux_norm, 2*np.pi*frequencies)\n    \n    # Normalize power\n    power = power / power.max()\n    \n    axes[0, 1].plot(periods, power, 'k-', linewidth=1)\n    axes[0, 1].axvline(P1, color='red', linestyle='--', alpha=0.5, label=f'P1={P1:.3f}')\n    axes[0, 1].axvline(P2, color='blue', linestyle='--', alpha=0.5, label=f'P2={P2:.3f}')\n    axes[0, 1].set_xlabel('Period [days]')\n    axes[0, 1].set_ylabel('Power')\n    axes[0, 1].set_title('Lomb-Scargle Periodogram')\n    axes[0, 1].set_xlim(0, 10)\n    axes[0, 1].legend()\n    \n    # 3. Detrending\n    from scipy.signal import detrend\n    \n    # Polynomial detrending\n    poly_coef = np.polyfit(t_obs, flux, 2)\n    trend_fit = np.polyval(poly_coef, t_obs)\n    flux_detrended = flux - trend_fit + np.mean(flux)\n    \n    axes[0, 2].scatter(t_obs, flux_detrended, s=1, alpha=0.5)\n    axes[0, 2].set_xlabel('Time [days]')\n    axes[0, 2].set_ylabel('Flux')\n    axes[0, 2].set_title('Detrended Light Curve')\n    \n    # 4. Filtering\n    from scipy.signal import savgol_filter\n    \n    # Need regular sampling for filtering\n    t_regular = np.linspace(t_obs.min(), t_obs.max(), 2000)\n    flux_interp = np.interp(t_regular, t_obs, flux_detrended)\n    \n    # Savitzky-Golay filter\n    flux_smooth = savgol_filter(flux_interp, 51, 3)\n    \n    axes[1, 0].plot(t_regular, flux_interp, 'k-', alpha=0.3, linewidth=0.5, label='Interpolated')\n    axes[1, 0].plot(t_regular, flux_smooth, 'r-', linewidth=2, label='Smoothed')\n    axes[1, 0].set_xlabel('Time [days]')\n    axes[1, 0].set_ylabel('Flux')\n    axes[1, 0].set_title('Savitzky-Golay Filter')\n    axes[1, 0].set_xlim(50, 60)  # Zoom in\n    axes[1, 0].legend()\n    \n    # 5. Wavelet analysis\n    from scipy.signal import cwt, ricker\n    \n    # Continuous wavelet transform\n    widths = np.arange(1, 100)\n    cwt_matrix = cwt(flux_interp, ricker, widths)\n    \n    im = axes[1, 1].imshow(np.abs(cwt_matrix), extent=[t_regular.min(), t_regular.max(), \n                                                        widths[-1], widths[0]], \n                          cmap='viridis', aspect='auto')\n    axes[1, 1].set_xlabel('Time [days]')\n    axes[1, 1].set_ylabel('Scale')\n    axes[1, 1].set_title('Wavelet Transform')\n    plt.colorbar(im, ax=axes[1, 1])\n    \n    # 6. Phase folding\n    best_period = P1  # Use known period\n    phase = (t_obs % best_period) / best_period\n    \n    axes[1, 2].scatter(phase, flux_detrended, s=1, alpha=0.3, c='k')\n    axes[1, 2].scatter(phase + 1, flux_detrended, s=1, alpha=0.3, c='k')  # Repeat\n    \n    # Binned curve\n    phase_bins = np.linspace(0, 1, 20)\n    binned_flux = []\n    for i in range(len(phase_bins)-1):\n        mask = (phase > phase_bins[i]) & (phase < phase_bins[i+1])\n        if mask.sum() > 0:\n            binned_flux.append(np.median(flux_detrended[mask]))\n        else:\n            binned_flux.append(np.nan)\n    \n    bin_centers = (phase_bins[:-1] + phase_bins[1:]) / 2\n    axes[1, 2].plot(bin_centers, binned_flux, 'ro-', linewidth=2, markersize=5)\n    axes[1, 2].set_xlabel('Phase')\n    axes[1, 2].set_ylabel('Flux')\n    axes[1, 2].set_title(f'Phase-folded (P={best_period:.4f} days)')\n    axes[1, 2].set_xlim(0, 2)\n    \n    plt.tight_layout()\n    plt.show()\n\nsignal_processing_astronomy()","type":"content","url":"/scipy#fourier-analysis-and-filtering","position":29},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy","lvl2":"Statistical Analysis"},"type":"lvl2","url":"/scipy#statistical-analysis","position":30},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy","lvl2":"Statistical Analysis"},"content":"","type":"content","url":"/scipy#statistical-analysis","position":31},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy","lvl3":"Distribution Fitting and Testing","lvl2":"Statistical Analysis"},"type":"lvl3","url":"/scipy#distribution-fitting-and-testing","position":32},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy","lvl3":"Distribution Fitting and Testing","lvl2":"Statistical Analysis"},"content":"def statistical_analysis():\n    \"\"\"Statistical tests for astronomical data.\"\"\"\n    \n    # Generate galaxy cluster data\n    np.random.seed(42)\n    \n    # Two clusters with different properties\n    n1, n2 = 150, 100\n    \n    # Cluster 1: nearby, rich\n    velocities1 = np.random.normal(5000, 800, n1)  # km/s\n    luminosities1 = np.random.lognormal(10, 0.5, n1)  # Solar luminosities\n    \n    # Cluster 2: distant, poor\n    velocities2 = np.random.normal(15000, 600, n2)\n    luminosities2 = np.random.lognormal(9.5, 0.7, n2)\n    \n    fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n    \n    # 1. Histogram and distribution fitting\n    from scipy.stats import norm, lognorm\n    \n    # Velocity distribution\n    axes[0, 0].hist(velocities1, bins=20, alpha=0.5, density=True, label='Cluster 1')\n    axes[0, 0].hist(velocities2, bins=20, alpha=0.5, density=True, label='Cluster 2')\n    \n    # Fit normal distributions\n    mu1, std1 = norm.fit(velocities1)\n    mu2, std2 = norm.fit(velocities2)\n    \n    v_range = np.linspace(0, 20000, 100)\n    axes[0, 0].plot(v_range, norm.pdf(v_range, mu1, std1), 'b-', linewidth=2)\n    axes[0, 0].plot(v_range, norm.pdf(v_range, mu2, std2), 'r-', linewidth=2)\n    axes[0, 0].set_xlabel('Velocity [km/s]')\n    axes[0, 0].set_ylabel('Probability Density')\n    axes[0, 0].set_title('Velocity Distributions')\n    axes[0, 0].legend()\n    \n    # 2. Q-Q plot for normality test\n    from scipy.stats import probplot\n    \n    probplot(velocities1, dist=\"norm\", plot=axes[0, 1])\n    axes[0, 1].set_title('Q-Q Plot (Cluster 1 Velocities)')\n    \n    # 3. Two-sample tests\n    from scipy.stats import ks_2samp, mannwhitneyu, ttest_ind\n    \n    # Kolmogorov-Smirnov test\n    ks_stat, ks_pvalue = ks_2samp(velocities1, velocities2)\n    \n    # Mann-Whitney U test (non-parametric)\n    mw_stat, mw_pvalue = mannwhitneyu(velocities1, velocities2)\n    \n    # Student's t-test (parametric)\n    t_stat, t_pvalue = ttest_ind(velocities1, velocities2)\n    \n    axes[0, 2].text(0.1, 0.8, 'Two-Sample Tests:', fontsize=12, fontweight='bold',\n                   transform=axes[0, 2].transAxes)\n    axes[0, 2].text(0.1, 0.6, f'K-S test: p = {ks_pvalue:.3e}',\n                   transform=axes[0, 2].transAxes)\n    axes[0, 2].text(0.1, 0.5, f'Mann-Whitney: p = {mw_pvalue:.3e}',\n                   transform=axes[0, 2].transAxes)\n    axes[0, 2].text(0.1, 0.4, f't-test: p = {t_pvalue:.3e}',\n                   transform=axes[0, 2].transAxes)\n    axes[0, 2].text(0.1, 0.2, 'p < 0.05 suggests different distributions',\n                   transform=axes[0, 2].transAxes, fontsize=10, style='italic')\n    axes[0, 2].axis('off')\n    \n    # 4. Correlation analysis\n    from scipy.stats import spearmanr, pearsonr\n    \n    # Add some correlation\n    masses1 = luminosities1 + np.random.normal(0, 1, n1)\n    masses2 = luminosities2 + np.random.normal(0, 1, n2)\n    \n    # Combine data\n    all_lum = np.concatenate([luminosities1, luminosities2])\n    all_mass = np.concatenate([masses1, masses2])\n    \n    axes[1, 0].scatter(np.log10(luminosities1), np.log10(masses1), \n                      alpha=0.5, label='Cluster 1')\n    axes[1, 0].scatter(np.log10(luminosities2), np.log10(masses2), \n                      alpha=0.5, label='Cluster 2')\n    axes[1, 0].set_xlabel('log(Luminosity) [L☉]')\n    axes[1, 0].set_ylabel('log(Mass) [M☉]')\n    axes[1, 0].set_title('Mass-Luminosity Relation')\n    axes[1, 0].legend()\n    \n    # Calculate correlations\n    pearson_r, pearson_p = pearsonr(np.log10(all_lum), np.log10(all_mass))\n    spearman_r, spearman_p = spearmanr(all_lum, all_mass)\n    \n    axes[1, 0].text(0.05, 0.95, f'Pearson r = {pearson_r:.3f}',\n                   transform=axes[1, 0].transAxes)\n    axes[1, 0].text(0.05, 0.90, f'Spearman ρ = {spearman_r:.3f}',\n                   transform=axes[1, 0].transAxes)\n    \n    # 5. Bootstrap confidence intervals\n    from scipy.stats import bootstrap\n    \n    def median_diff(x, y):\n        \"\"\"Difference in medians.\"\"\"\n        return np.median(x) - np.median(y)\n    \n    # Bootstrap\n    rng = np.random.default_rng(42)\n    res = bootstrap((velocities1, velocities2), \n                   lambda x, y: np.median(x) - np.median(y),\n                   n_resamples=10000,\n                   confidence_level=0.95,\n                   random_state=rng,\n                   method='percentile')\n    \n    axes[1, 1].hist(res.bootstrap_distribution, bins=50, alpha=0.7)\n    axes[1, 1].axvline(res.confidence_interval.low, color='red', \n                      linestyle='--', label='95% CI')\n    axes[1, 1].axvline(res.confidence_interval.high, color='red', \n                      linestyle='--')\n    axes[1, 1].set_xlabel('Median Velocity Difference [km/s]')\n    axes[1, 1].set_ylabel('Count')\n    axes[1, 1].set_title('Bootstrap Distribution')\n    axes[1, 1].legend()\n    \n    # 6. Survival analysis (for truncated data)\n    from scipy.stats import kaplan_meier_estimator\n    \n    # Simulate detection limits\n    detection_limit = 11.0\n    detected1 = luminosities1 > detection_limit\n    detected2 = luminosities2 > detection_limit\n    \n    # This is simplified - real survival analysis needs more care\n    axes[1, 2].hist(luminosities1[detected1], bins=20, alpha=0.5, \n                   label=f'Cluster 1 ({detected1.sum()}/{n1} detected)')\n    axes[1, 2].hist(luminosities2[detected2], bins=20, alpha=0.5,\n                   label=f'Cluster 2 ({detected2.sum()}/{n2} detected)')\n    axes[1, 2].axvline(detection_limit, color='red', linestyle='--', \n                      label='Detection limit')\n    axes[1, 2].set_xlabel('Luminosity [L☉]')\n    axes[1, 2].set_ylabel('Count')\n    axes[1, 2].set_title('Truncated Data')\n    axes[1, 2].legend()\n    \n    plt.tight_layout()\n    plt.show()\n\nstatistical_analysis()","type":"content","url":"/scipy#distribution-fitting-and-testing","position":33},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy","lvl2":"Try It Yourself"},"type":"lvl2","url":"/scipy#try-it-yourself","position":34},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy","lvl2":"Try It Yourself"},"content":"","type":"content","url":"/scipy#try-it-yourself","position":35},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy","lvl3":"Exercise 1: Build a Complete Spectral Analysis Pipeline","lvl2":"Try It Yourself"},"type":"lvl3","url":"/scipy#exercise-1-build-a-complete-spectral-analysis-pipeline","position":36},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy","lvl3":"Exercise 1: Build a Complete Spectral Analysis Pipeline","lvl2":"Try It Yourself"},"content":"def spectral_analysis_pipeline(wavelength, flux, error):\n    \"\"\"\n    Complete spectral analysis pipeline.\n    \n    Tasks:\n    1. Interpolate to common wavelength grid\n    2. Identify and mask cosmic rays\n    3. Fit and subtract continuum\n    4. Find emission/absorption lines\n    5. Measure line properties (EW, FWHM, flux)\n    6. Estimate radial velocity\n    7. Calculate S/N ratio\n    \"\"\"\n    # Your code here\n    pass","type":"content","url":"/scipy#exercise-1-build-a-complete-spectral-analysis-pipeline","position":37},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy","lvl3":"Exercise 2: Orbital Fitting with MCMC","lvl2":"Try It Yourself"},"type":"lvl3","url":"/scipy#exercise-2-orbital-fitting-with-mcmc","position":38},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy","lvl3":"Exercise 2: Orbital Fitting with MCMC","lvl2":"Try It Yourself"},"content":"def fit_exoplanet_orbit(times, radial_velocities, errors):\n    \"\"\"\n    Fit Keplerian orbit to radial velocity data.\n    \n    Parameters to fit:\n    - Period\n    - Eccentricity\n    - Semi-amplitude\n    - Argument of periastron\n    - Time of periastron\n    - Systemic velocity\n    \n    Use MCMC for proper uncertainty estimation.\n    \"\"\"\n    # Your code here\n    pass","type":"content","url":"/scipy#exercise-2-orbital-fitting-with-mcmc","position":39},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy","lvl3":"Exercise 3: Image Deconvolution","lvl2":"Try It Yourself"},"type":"lvl3","url":"/scipy#exercise-3-image-deconvolution","position":40},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy","lvl3":"Exercise 3: Image Deconvolution","lvl2":"Try It Yourself"},"content":"def deconvolve_image(image, psf, method='richardson-lucy'):\n    \"\"\"\n    Deconvolve astronomical image.\n    \n    Methods:\n    - Richardson-Lucy\n    - Wiener filter\n    - Blind deconvolution\n    \n    Handle noise and artifacts properly.\n    \"\"\"\n    # Your code here\n    pass","type":"content","url":"/scipy#exercise-3-image-deconvolution","position":41},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy","lvl2":"Key Takeaways"},"type":"lvl2","url":"/scipy#key-takeaways","position":42},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy","lvl2":"Key Takeaways"},"content":"✅ SciPy provides optimized algorithms - Don’t reinvent the wheel✅ Choose interpolation carefully - Cubic can oscillate, consider Akima✅ Never blindly extrapolate - Use physical models instead✅ Integration has many methods - Adaptive quadrature, Monte Carlo for high-D✅ ODEs need appropriate solvers - RK45 for general, DOP853 for long integration✅ Optimization requires good initial guesses - Consider global methods✅ Signal processing handles real data - Uneven sampling, noise, trends✅ Statistical tests have assumptions - Check them before applying","type":"content","url":"/scipy#key-takeaways","position":43},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy","lvl2":"Next Steps"},"type":"lvl2","url":"/scipy#next-steps","position":44},{"hierarchy":{"lvl1":"SciPy: Scientific Algorithms for Astronomy","lvl2":"Next Steps"},"content":"You now have the foundation for scientific computing in Python:\n\nNumPy for array operations and linear algebra\n\nMatplotlib for publication-quality visualizations\n\nSciPy for numerical algorithms and analysis\n\nCombined with your Python fundamentals and optimization techniques, you’re ready to tackle complex astronomical problems. The next section on Pandas will add powerful data manipulation capabilities for working with catalogs and time series data.\n\nRemember: These libraries work best together. Use NumPy for computation, SciPy for algorithms, and Matplotlib for visualization - all integrated in your astronomical workflows.","type":"content","url":"/scipy#next-steps","position":45},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization"},"type":"lvl1","url":"/performance-optimization","position":0},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization"},"content":"","type":"content","url":"/performance-optimization","position":1},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl2":"Learning Objectives"},"type":"lvl2","url":"/performance-optimization#learning-objectives","position":2},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl2":"Learning Objectives"},"content":"By the end of this chapter, you will:\n\nProfile code to identify bottlenecks scientifically\n\nVectorize computations for 10-100x speedups\n\nUse Numba JIT compilation for near-C performance\n\nImplement parallel processing for multi-core systems\n\nOptimize memory usage for large astronomical datasets\n\nKnow when to optimize and when to use existing solutions","type":"content","url":"/performance-optimization#learning-objectives","position":3},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl2":"7.1 Profiling: Measure Before Optimizing"},"type":"lvl2","url":"/performance-optimization#id-7-1-profiling-measure-before-optimizing","position":4},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl2":"7.1 Profiling: Measure Before Optimizing"},"content":"","type":"content","url":"/performance-optimization#id-7-1-profiling-measure-before-optimizing","position":5},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl3":"The Golden Rule of Optimization","lvl2":"7.1 Profiling: Measure Before Optimizing"},"type":"lvl3","url":"/performance-optimization#the-golden-rule-of-optimization","position":6},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl3":"The Golden Rule of Optimization","lvl2":"7.1 Profiling: Measure Before Optimizing"},"content":"import time\nimport numpy as np\nimport cProfile\nimport pstats\nfrom line_profiler import LineProfiler\n\ndef demonstrate_premature_optimization():\n    \"\"\"\n    Knuth's famous quote: \"Premature optimization is the root of all evil\"\n    \n    Profile FIRST, optimize SECOND.\n    \"\"\"\n    \n    # Version 1: Readable but \"inefficient\"?\n    def calculate_distances_readable(coords):\n        n = len(coords)\n        distances = []\n        for i in range(n):\n            for j in range(i+1, n):\n                dist = np.sqrt((coords[i][0] - coords[j][0])**2 + \n                              (coords[i][1] - coords[j][1])**2)\n                distances.append(dist)\n        return distances\n    \n    # Version 2: \"Optimized\" but actually slower!\n    def calculate_distances_clever(coords):\n        n = len(coords)\n        distances = []\n        for i in range(n):\n            for j in range(i+1, n):\n                # \"Optimize\" by avoiding sqrt for comparison\n                dist_sq = (coords[i][0] - coords[j][0])**2 + \\\n                         (coords[i][1] - coords[j][1])**2\n                # But then we need sqrt anyway...\n                distances.append(np.sqrt(dist_sq))\n        return distances\n    \n    # Version 3: Actually optimized\n    def calculate_distances_vectorized(coords):\n        coords = np.array(coords)\n        diff = coords[:, np.newaxis, :] - coords[np.newaxis, :, :]\n        dist_matrix = np.sqrt(np.sum(diff**2, axis=2))\n        return dist_matrix[np.triu_indices_from(dist_matrix, k=1)]\n    \n    # Test with realistic data\n    n_stars = 100\n    coords = [(np.random.uniform(0, 100), np.random.uniform(0, 100)) \n               for _ in range(n_stars)]\n    \n    # Time each version\n    start = time.perf_counter()\n    d1 = calculate_distances_readable(coords)\n    t1 = time.perf_counter() - start\n    \n    start = time.perf_counter()\n    d2 = calculate_distances_clever(coords)\n    t2 = time.perf_counter() - start\n    \n    start = time.perf_counter()\n    d3 = calculate_distances_vectorized(coords)\n    t3 = time.perf_counter() - start\n    \n    print(f\"Readable version:   {t1*1000:.2f} ms\")\n    print(f\"'Clever' version:   {t2*1000:.2f} ms (no improvement!)\")\n    print(f\"Vectorized version: {t3*1000:.2f} ms ({t1/t3:.1f}x faster)\")\n\ndemonstrate_premature_optimization()","type":"content","url":"/performance-optimization#the-golden-rule-of-optimization","position":7},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl3":"Using cProfile for Function-Level Profiling","lvl2":"7.1 Profiling: Measure Before Optimizing"},"type":"lvl3","url":"/performance-optimization#using-cprofile-for-function-level-profiling","position":8},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl3":"Using cProfile for Function-Level Profiling","lvl2":"7.1 Profiling: Measure Before Optimizing"},"content":"def profile_spectrum_analysis():\n    \"\"\"Profile a realistic spectrum analysis pipeline.\"\"\"\n    \n    def load_spectrum(n_points=10000):\n        \"\"\"Simulate loading spectrum data.\"\"\"\n        wavelengths = np.linspace(4000, 7000, n_points)\n        fluxes = np.random.randn(n_points) + 100\n        # Add some spectral lines\n        for line_center in [4861, 6563]:  # H-beta, H-alpha\n            fluxes += 50 * np.exp(-(wavelengths - line_center)**2 / 25)\n        return wavelengths, fluxes\n    \n    def remove_continuum(wavelengths, fluxes, window=100):\n        \"\"\"Remove continuum using median filter.\"\"\"\n        from scipy.ndimage import median_filter\n        continuum = median_filter(fluxes, size=window)\n        return fluxes - continuum\n    \n    def find_lines(wavelengths, fluxes, threshold=3):\n        \"\"\"Find emission lines.\"\"\"\n        from scipy.signal import find_peaks\n        std = np.std(fluxes)\n        peaks, properties = find_peaks(fluxes, height=threshold*std)\n        return wavelengths[peaks], fluxes[peaks]\n    \n    def measure_redshift(line_wavelengths, rest_wavelength=6563):\n        \"\"\"Calculate redshift from lines.\"\"\"\n        if len(line_wavelengths) == 0:\n            return 0\n        # Find closest to expected H-alpha\n        closest_idx = np.argmin(np.abs(line_wavelengths - rest_wavelength))\n        observed = line_wavelengths[closest_idx]\n        return (observed - rest_wavelength) / rest_wavelength\n    \n    # Profile the complete pipeline\n    profiler = cProfile.Profile()\n    profiler.enable()\n    \n    # Run analysis\n    wavelengths, fluxes = load_spectrum(50000)\n    normalized = remove_continuum(wavelengths, fluxes)\n    line_waves, line_fluxes = find_lines(wavelengths, normalized)\n    redshift = measure_redshift(line_waves)\n    \n    profiler.disable()\n    \n    # Print statistics\n    stats = pstats.Stats(profiler)\n    stats.sort_stats('cumulative')\n    print(\"\\nTop 10 functions by cumulative time:\")\n    stats.print_stats(10)\n    \n    return redshift\n\n# z = profile_spectrum_analysis()  # Uncomment to see profiling","type":"content","url":"/performance-optimization#using-cprofile-for-function-level-profiling","position":9},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl3":"Line-by-Line Profiling","lvl2":"7.1 Profiling: Measure Before Optimizing"},"type":"lvl3","url":"/performance-optimization#line-by-line-profiling","position":10},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl3":"Line-by-Line Profiling","lvl2":"7.1 Profiling: Measure Before Optimizing"},"content":"# Use line_profiler for detailed analysis\n# Install: pip install line_profiler\n\ndef detailed_profile_example():\n    \"\"\"Example of line-by-line profiling.\"\"\"\n    \n    # Decorate function with @profile (added by line_profiler)\n    # Run with: kernprof -l -v script.py\n    \n    def process_image(image):  # Add @profile decorator\n        \"\"\"Process CCD image - which lines are slow?\"\"\"\n        # Line 1: Median filter for cosmic ray removal\n        from scipy.ndimage import median_filter\n        cleaned = median_filter(image, size=3)\n        \n        # Line 2: Calculate statistics\n        mean = np.mean(cleaned)\n        std = np.std(cleaned)\n        \n        # Line 3: Find sources above threshold\n        threshold = mean + 5 * std\n        sources = cleaned > threshold\n        \n        # Line 4: Label connected regions\n        from scipy.ndimage import label\n        labeled, num_sources = label(sources)\n        \n        # Line 5: Calculate properties for each source\n        source_properties = []\n        for i in range(1, num_sources + 1):\n            mask = labeled == i\n            flux = np.sum(cleaned[mask])\n            centroid = np.mean(np.argwhere(mask), axis=0)\n            source_properties.append({'flux': flux, 'centroid': centroid})\n        \n        return source_properties\n    \n    # Simulate CCD image\n    image = np.random.randn(1024, 1024) + 100\n    # Add some \"stars\"\n    for _ in range(50):\n        x, y = np.random.randint(10, 1014, 2)\n        image[x-2:x+3, y-2:y+3] += 1000\n    \n    # This would show time spent on each line\n    # sources = process_image(image)\n    \n    print(\"Line profiler example ready - add @profile decorator and run with kernprof\")\n\ndetailed_profile_example()","type":"content","url":"/performance-optimization#line-by-line-profiling","position":11},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl2":"7.2 Vectorization: The NumPy Way"},"type":"lvl2","url":"/performance-optimization#id-7-2-vectorization-the-numpy-way","position":12},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl2":"7.2 Vectorization: The NumPy Way"},"content":"","type":"content","url":"/performance-optimization#id-7-2-vectorization-the-numpy-way","position":13},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl3":"Understanding Why Vectorization is Fast","lvl2":"7.2 Vectorization: The NumPy Way"},"type":"lvl3","url":"/performance-optimization#understanding-why-vectorization-is-fast","position":14},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl3":"Understanding Why Vectorization is Fast","lvl2":"7.2 Vectorization: The NumPy Way"},"content":"def vectorization_explanation():\n    \"\"\"Why is NumPy so much faster than pure Python loops?\"\"\"\n    \n    print(\"Why Vectorization Works:\\n\")\n    print(\"1. Python loops have overhead:\")\n    print(\"   - Type checking each iteration\")\n    print(\"   - Function calls for each operation\")\n    print(\"   - Memory allocation for intermediate results\")\n    \n    print(\"\\n2. NumPy operations:\")\n    print(\"   - Implemented in optimized C\")\n    print(\"   - Use CPU vector instructions (SIMD)\")\n    print(\"   - Better memory access patterns\")\n    print(\"   - No Python overhead in inner loop\")\n    \n    # Demonstration\n    n = 1_000_000\n    \n    # Python list operations\n    python_list = list(range(n))\n    start = time.perf_counter()\n    python_result = [x**2 for x in python_list]\n    python_time = time.perf_counter() - start\n    \n    # NumPy operations\n    numpy_array = np.arange(n)\n    start = time.perf_counter()\n    numpy_result = numpy_array**2\n    numpy_time = time.perf_counter() - start\n    \n    print(f\"\\nSquaring {n:,} numbers:\")\n    print(f\"Python list: {python_time*1000:.1f} ms\")\n    print(f\"NumPy array: {numpy_time*1000:.1f} ms\")\n    print(f\"Speedup: {python_time/numpy_time:.1f}x\")\n\nvectorization_explanation()","type":"content","url":"/performance-optimization#understanding-why-vectorization-is-fast","position":15},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl3":"Vectorization Patterns for Astronomy","lvl2":"7.2 Vectorization: The NumPy Way"},"type":"lvl3","url":"/performance-optimization#vectorization-patterns-for-astronomy","position":16},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl3":"Vectorization Patterns for Astronomy","lvl2":"7.2 Vectorization: The NumPy Way"},"content":"class VectorizedAstronomyCalculations:\n    \"\"\"Common vectorization patterns in astronomy.\"\"\"\n    \n    @staticmethod\n    def angular_distance_matrix(ra, dec):\n        \"\"\"\n        Calculate all pairwise angular distances.\n        Vectorized haversine formula.\n        \"\"\"\n        # Convert to radians\n        ra_rad = np.radians(ra)\n        dec_rad = np.radians(dec)\n        \n        # Use broadcasting to compute all pairs\n        # Shape: (n, 1) - (1, n) = (n, n)\n        dra = ra_rad[:, np.newaxis] - ra_rad[np.newaxis, :]\n        ddec = dec_rad[:, np.newaxis] - dec_rad[np.newaxis, :]\n        \n        # Haversine formula\n        a = np.sin(ddec/2)**2 + \\\n            np.cos(dec_rad[:, np.newaxis]) * np.cos(dec_rad[np.newaxis, :]) * \\\n            np.sin(dra/2)**2\n        \n        c = 2 * np.arcsin(np.sqrt(a))\n        return np.degrees(c)\n    \n    @staticmethod\n    def extinction_correction_vectorized(magnitudes, colors, R_v=3.1):\n        \"\"\"\n        Apply extinction correction to many stars at once.\n        \n        Instead of looping over stars, process all simultaneously.\n        \"\"\"\n        # Ensure arrays\n        magnitudes = np.asarray(magnitudes)\n        colors = np.asarray(colors)\n        \n        # Cardelli extinction law coefficients (simplified)\n        a_v = 0.574 * colors  # Simplified color-extinction relation\n        \n        # Apply to all magnitudes at once\n        corrected = magnitudes - R_v * a_v\n        \n        # Handle edge cases with numpy\n        corrected = np.where(colors < 0, magnitudes, corrected)  # No correction for blue colors\n        \n        return corrected\n    \n    @staticmethod\n    def phase_fold_vectorized(times, fluxes, period):\n        \"\"\"\n        Phase fold a light curve - vectorized version.\n        \"\"\"\n        # Calculate phase for all times at once\n        phases = (times % period) / period\n        \n        # Sort by phase (vectorized)\n        sort_idx = np.argsort(phases)\n        \n        return phases[sort_idx], fluxes[sort_idx]\n    \n    @staticmethod\n    def match_catalogs_vectorized(ra1, dec1, ra2, dec2, max_sep=1.0):\n        \"\"\"\n        Cross-match catalogs using vectorized operations.\n        \n        More memory intensive but much faster than loops.\n        \"\"\"\n        # Convert max separation to radians\n        max_sep_rad = np.radians(max_sep / 3600)  # arcsec to radians\n        \n        # Use KDTree for efficient spatial matching\n        from scipy.spatial import cKDTree\n        \n        # Convert to Cartesian for KDTree\n        def radec_to_xyz(ra, dec):\n            ra_rad = np.radians(ra)\n            dec_rad = np.radians(dec)\n            x = np.cos(dec_rad) * np.cos(ra_rad)\n            y = np.cos(dec_rad) * np.sin(ra_rad)\n            z = np.sin(dec_rad)\n            return np.column_stack([x, y, z])\n        \n        xyz1 = radec_to_xyz(ra1, dec1)\n        xyz2 = radec_to_xyz(ra2, dec2)\n        \n        # Build tree and query\n        tree = cKDTree(xyz2)\n        \n        # Vectorized query for all points at once\n        # Convert angular separation to 3D distance\n        max_3d_dist = 2 * np.sin(max_sep_rad / 2)\n        \n        distances, indices = tree.query(xyz1, distance_upper_bound=max_3d_dist)\n        \n        # Valid matches where distance is within bound\n        valid = distances < max_3d_dist\n        \n        matches = []\n        for i, (is_valid, idx) in enumerate(zip(valid, indices)):\n            if is_valid:\n                matches.append((i, idx))\n        \n        return matches\n\n# Test vectorized calculations\ncalc = VectorizedAstronomyCalculations()\n\n# Generate test data\nn_stars = 1000\nra = np.random.uniform(0, 360, n_stars)\ndec = np.random.uniform(-30, 30, n_stars)\n\n# Time angular distance calculation\nstart = time.perf_counter()\ndist_matrix = calc.angular_distance_matrix(ra, dec)\nprint(f\"Angular distance matrix for {n_stars} stars: {time.perf_counter() - start:.3f}s\")\n\n# Test extinction correction\nmagnitudes = np.random.uniform(10, 15, n_stars)\ncolors = np.random.uniform(-0.5, 2.0, n_stars)\ncorrected = calc.extinction_correction_vectorized(magnitudes, colors)\nprint(f\"Corrected {n_stars} magnitudes (vectorized)\")","type":"content","url":"/performance-optimization#vectorization-patterns-for-astronomy","position":17},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl2":"7.3 Numba: JIT Compilation for Python"},"type":"lvl2","url":"/performance-optimization#id-7-3-numba-jit-compilation-for-python","position":18},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl2":"7.3 Numba: JIT Compilation for Python"},"content":"","type":"content","url":"/performance-optimization#id-7-3-numba-jit-compilation-for-python","position":19},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl3":"Basic Numba Usage","lvl2":"7.3 Numba: JIT Compilation for Python"},"type":"lvl3","url":"/performance-optimization#basic-numba-usage","position":20},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl3":"Basic Numba Usage","lvl2":"7.3 Numba: JIT Compilation for Python"},"content":"from numba import jit, njit, prange, vectorize\nimport numba\n\n@njit  # No Python mode, faster\ndef orbital_integration_numba(x0, v0, mass, dt, n_steps):\n    \"\"\"\n    Orbital integration with Numba acceleration.\n    \n    Compare this to pure Python version!\n    \"\"\"\n    G = 6.67430e-11\n    \n    # Pre-allocate arrays\n    positions = np.zeros((n_steps, 3))\n    velocities = np.zeros((n_steps, 3))\n    \n    positions[0] = x0\n    velocities[0] = v0\n    \n    for i in range(1, n_steps):\n        # Current state\n        r = positions[i-1]\n        v = velocities[i-1]\n        \n        # Calculate acceleration\n        r_mag = np.sqrt(r[0]**2 + r[1]**2 + r[2]**2)\n        a = -G * mass * r / r_mag**3\n        \n        # Leapfrog integration\n        v_half = v + 0.5 * dt * a\n        positions[i] = r + dt * v_half\n        \n        # Update acceleration at new position\n        r_new = positions[i]\n        r_mag_new = np.sqrt(r_new[0]**2 + r_new[1]**2 + r_new[2]**2)\n        a_new = -G * mass * r_new / r_mag_new**3\n        \n        velocities[i] = v_half + 0.5 * dt * a_new\n    \n    return positions, velocities\n\ndef compare_integration_performance():\n    \"\"\"Compare Python vs Numba performance.\"\"\"\n    \n    # Pure Python version (simplified)\n    def orbital_integration_python(x0, v0, mass, dt, n_steps):\n        G = 6.67430e-11\n        positions = []\n        velocities = []\n        \n        r = np.array(x0)\n        v = np.array(v0)\n        \n        for _ in range(n_steps):\n            positions.append(r.copy())\n            velocities.append(v.copy())\n            \n            r_mag = np.linalg.norm(r)\n            a = -G * mass * r / r_mag**3\n            \n            v = v + dt * a\n            r = r + dt * v\n        \n        return np.array(positions), np.array(velocities)\n    \n    # Test parameters\n    x0 = np.array([1.496e11, 0.0, 0.0])  # 1 AU\n    v0 = np.array([0.0, 29780.0, 0.0])   # Earth orbital velocity\n    mass = 1.989e30  # Solar mass\n    dt = 3600.0  # 1 hour\n    n_steps = 10000\n    \n    # Time Python version\n    start = time.perf_counter()\n    pos_py, vel_py = orbital_integration_python(x0, v0, mass, dt, n_steps)\n    python_time = time.perf_counter() - start\n    \n    # Time Numba version (first call includes compilation)\n    start = time.perf_counter()\n    pos_nb, vel_nb = orbital_integration_numba(x0, v0, mass, dt, n_steps)\n    numba_time_with_compile = time.perf_counter() - start\n    \n    # Time Numba version again (already compiled)\n    start = time.perf_counter()\n    pos_nb, vel_nb = orbital_integration_numba(x0, v0, mass, dt, n_steps)\n    numba_time = time.perf_counter() - start\n    \n    print(f\"Integration of {n_steps} steps:\")\n    print(f\"  Pure Python: {python_time:.3f}s\")\n    print(f\"  Numba (with compilation): {numba_time_with_compile:.3f}s\")\n    print(f\"  Numba (compiled): {numba_time:.3f}s\")\n    print(f\"  Speedup: {python_time/numba_time:.1f}x\")\n\ncompare_integration_performance()","type":"content","url":"/performance-optimization#basic-numba-usage","position":21},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl3":"Parallel Computing with Numba","lvl2":"7.3 Numba: JIT Compilation for Python"},"type":"lvl3","url":"/performance-optimization#parallel-computing-with-numba","position":22},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl3":"Parallel Computing with Numba","lvl2":"7.3 Numba: JIT Compilation for Python"},"content":"@njit(parallel=True)\ndef monte_carlo_pi_parallel(n_samples):\n    \"\"\"\n    Parallel Monte Carlo calculation of π.\n    \n    Demonstrates Numba's automatic parallelization.\n    \"\"\"\n    count = 0\n    \n    # prange automatically parallelizes this loop\n    for i in prange(n_samples):\n        x = np.random.random()\n        y = np.random.random()\n        \n        if x*x + y*y <= 1.0:\n            count += 1\n    \n    return 4.0 * count / n_samples\n\n@njit(parallel=True)\ndef process_many_spectra(spectra, noise_threshold=3.0):\n    \"\"\"\n    Process multiple spectra in parallel.\n    \n    Each spectrum is processed independently - perfect for parallelization.\n    \"\"\"\n    n_spectra, n_points = spectra.shape\n    results = np.zeros(n_spectra)\n    \n    # Process each spectrum in parallel\n    for i in prange(n_spectra):\n        spectrum = spectra[i]\n        \n        # Calculate statistics\n        mean = np.mean(spectrum)\n        std = np.std(spectrum)\n        \n        # Count significant peaks\n        threshold = mean + noise_threshold * std\n        n_peaks = 0\n        for j in range(1, n_points - 1):\n            if spectrum[j] > threshold:\n                if spectrum[j] > spectrum[j-1] and spectrum[j] > spectrum[j+1]:\n                    n_peaks += 1\n        \n        results[i] = n_peaks\n    \n    return results\n\n# Test parallel performance\ndef test_parallel_speedup():\n    \"\"\"Demonstrate parallel speedup with Numba.\"\"\"\n    \n    # Monte Carlo test\n    n_samples = 10_000_000\n    \n    start = time.perf_counter()\n    pi_estimate = monte_carlo_pi_parallel(n_samples)\n    parallel_time = time.perf_counter() - start\n    \n    print(f\"π estimate: {pi_estimate:.6f}\")\n    print(f\"Time with {numba.config.NUMBA_NUM_THREADS} threads: {parallel_time:.3f}s\")\n    \n    # Spectra processing test\n    n_spectra = 1000\n    n_points = 2048\n    spectra = np.random.randn(n_spectra, n_points) + 100\n    \n    start = time.perf_counter()\n    peak_counts = process_many_spectra(spectra)\n    spec_time = time.perf_counter() - start\n    \n    print(f\"\\nProcessed {n_spectra} spectra in {spec_time:.3f}s\")\n    print(f\"Average peaks per spectrum: {np.mean(peak_counts):.1f}\")\n\ntest_parallel_speedup()","type":"content","url":"/performance-optimization#parallel-computing-with-numba","position":23},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl3":"Custom NumPy UFuncs with Numba","lvl2":"7.3 Numba: JIT Compilation for Python"},"type":"lvl3","url":"/performance-optimization#custom-numpy-ufuncs-with-numba","position":24},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl3":"Custom NumPy UFuncs with Numba","lvl2":"7.3 Numba: JIT Compilation for Python"},"content":"@vectorize(['float64(float64, float64)'], nopython=True)\ndef magnitude_addition(mag1, mag2):\n    \"\"\"\n    Vectorized function to add astronomical magnitudes.\n    \n    Works element-wise on arrays, just like NumPy functions.\n    \"\"\"\n    flux1 = 10**(-0.4 * mag1)\n    flux2 = 10**(-0.4 * mag2)\n    total_flux = flux1 + flux2\n    return -2.5 * np.log10(total_flux)\n\n@vectorize(['float64(float64, float64, float64)'], nopython=True)\ndef planck_function_fast(wavelength, temperature, scale):\n    \"\"\"\n    Fast Planck function evaluation.\n    \"\"\"\n    h = 6.62607015e-34\n    c = 299792458.0\n    k = 1.380649e-23\n    \n    # Wavelength in meters\n    lam = wavelength * 1e-9\n    \n    # Planck function\n    numerator = 2 * h * c**2 / lam**5\n    x = h * c / (lam * k * temperature)\n    \n    # Avoid overflow\n    if x > 700:\n        return 0.0\n    \n    denominator = np.exp(x) - 1\n    return scale * numerator / denominator\n\n# Test custom ufuncs\ndef test_custom_ufuncs():\n    \"\"\"Test our custom vectorized functions.\"\"\"\n    \n    # Test magnitude addition\n    mag_array1 = np.array([10.0, 11.0, 12.0, 13.0])\n    mag_array2 = np.array([10.5, 10.5, 10.5, 10.5])\n    \n    combined = magnitude_addition(mag_array1, mag_array2)\n    print(\"Combined magnitudes:\")\n    for m1, m2, mc in zip(mag_array1, mag_array2, combined):\n        print(f\"  {m1:.1f} + {m2:.1f} = {mc:.2f}\")\n    \n    # Test Planck function\n    wavelengths = np.linspace(100, 3000, 1000)  # nm\n    temps = np.array([3000, 5778, 10000])  # K\n    \n    print(f\"\\nPlanck function evaluated at {len(wavelengths)} wavelengths\")\n    \n    for temp in temps:\n        start = time.perf_counter()\n        spectrum = planck_function_fast(wavelengths, temp, 1.0)\n        elapsed = time.perf_counter() - start\n        peak_idx = np.argmax(spectrum)\n        print(f\"  T={temp}K: Peak at {wavelengths[peak_idx]:.0f}nm, \"\n              f\"computed in {elapsed*1000:.2f}ms\")\n\ntest_custom_ufuncs()","type":"content","url":"/performance-optimization#custom-numpy-ufuncs-with-numba","position":25},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl2":"7.4 Multiprocessing for CPU-Bound Tasks"},"type":"lvl2","url":"/performance-optimization#id-7-4-multiprocessing-for-cpu-bound-tasks","position":26},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl2":"7.4 Multiprocessing for CPU-Bound Tasks"},"content":"","type":"content","url":"/performance-optimization#id-7-4-multiprocessing-for-cpu-bound-tasks","position":27},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl3":"Process Pool for Parallel Data Processing","lvl2":"7.4 Multiprocessing for CPU-Bound Tasks"},"type":"lvl3","url":"/performance-optimization#process-pool-for-parallel-data-processing","position":28},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl3":"Process Pool for Parallel Data Processing","lvl2":"7.4 Multiprocessing for CPU-Bound Tasks"},"content":"from multiprocessing import Pool, cpu_count\nimport multiprocessing as mp\n\ndef process_single_observation(args):\n    \"\"\"\n    Process a single observation file.\n    \n    This function runs in a separate process.\n    \"\"\"\n    filename, params = args\n    \n    # Simulate loading and processing\n    np.random.seed(hash(filename) % 2**32)  # Reproducible randomness\n    \n    # \"Load\" data\n    data = np.random.randn(1024, 1024) + 1000\n    \n    # Apply calibration\n    dark = np.random.randn(1024, 1024) * 10\n    flat = np.ones((1024, 1024)) + np.random.randn(1024, 1024) * 0.1\n    \n    calibrated = (data - dark) / flat\n    \n    # Extract photometry\n    sources = []\n    threshold = np.mean(calibrated) + 5 * np.std(calibrated)\n    \n    # Find bright pixels (simplified source detection)\n    bright_pixels = np.argwhere(calibrated > threshold)\n    \n    if len(bright_pixels) > 0:\n        # Group into sources (simplified)\n        n_sources = min(10, len(bright_pixels))\n        for i in range(n_sources):\n            y, x = bright_pixels[i]\n            flux = calibrated[y, x]\n            sources.append({\n                'file': filename,\n                'x': x,\n                'y': y,\n                'flux': flux\n            })\n    \n    return sources\n\ndef parallel_pipeline(filenames, n_processes=None):\n    \"\"\"\n    Process multiple observations in parallel.\n    \"\"\"\n    if n_processes is None:\n        n_processes = cpu_count()\n    \n    print(f\"Processing {len(filenames)} files with {n_processes} processes\")\n    \n    # Prepare arguments\n    args = [(f, {'threshold': 5.0}) for f in filenames]\n    \n    # Process in parallel\n    start = time.perf_counter()\n    \n    with Pool(n_processes) as pool:\n        # map applies function to each item in parallel\n        results = pool.map(process_single_observation, args)\n    \n    elapsed = time.perf_counter() - start\n    \n    # Flatten results\n    all_sources = []\n    for source_list in results:\n        all_sources.extend(source_list)\n    \n    print(f\"Found {len(all_sources)} sources in {elapsed:.2f}s\")\n    print(f\"Processing rate: {len(filenames)/elapsed:.1f} files/second\")\n    \n    return all_sources\n\n# Test parallel processing\ndef test_multiprocessing():\n    \"\"\"Compare serial vs parallel processing.\"\"\"\n    \n    # Generate fake filenames\n    n_files = 50\n    filenames = [f\"observation_{i:04d}.fits\" for i in range(n_files)]\n    \n    # Serial processing\n    print(\"Serial processing:\")\n    start = time.perf_counter()\n    serial_results = []\n    for filename in filenames:\n        sources = process_single_observation((filename, {}))\n        serial_results.extend(sources)\n    serial_time = time.perf_counter() - start\n    print(f\"  Time: {serial_time:.2f}s\")\n    \n    # Parallel processing\n    print(\"\\nParallel processing:\")\n    parallel_results = parallel_pipeline(filenames)\n    \n    # Note: Parallel might be slower for small tasks due to overhead!\n    # But scales better for real work\n\ntest_multiprocessing()","type":"content","url":"/performance-optimization#process-pool-for-parallel-data-processing","position":29},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl3":"Shared Memory for Large Arrays","lvl2":"7.4 Multiprocessing for CPU-Bound Tasks"},"type":"lvl3","url":"/performance-optimization#shared-memory-for-large-arrays","position":30},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl3":"Shared Memory for Large Arrays","lvl2":"7.4 Multiprocessing for CPU-Bound Tasks"},"content":"def shared_memory_example():\n    \"\"\"\n    Use shared memory to avoid copying large arrays between processes.\n    \"\"\"\n    from multiprocessing import shared_memory\n    \n    def process_shared_chunk(args):\n        \"\"\"Process a chunk of shared array.\"\"\"\n        shm_name, shape, dtype, start_idx, end_idx = args\n        \n        # Attach to existing shared memory\n        existing_shm = shared_memory.SharedMemory(name=shm_name)\n        \n        # Create numpy array from shared memory\n        array = np.ndarray(shape, dtype=dtype, buffer=existing_shm.buf)\n        \n        # Process the chunk (example: apply median filter)\n        from scipy.ndimage import median_filter\n        chunk = array[start_idx:end_idx]\n        filtered = median_filter(chunk, size=3)\n        \n        # Write back to shared memory\n        array[start_idx:end_idx] = filtered\n        \n        # Clean up\n        existing_shm.close()\n        \n        return f\"Processed rows {start_idx}-{end_idx}\"\n    \n    # Create large array in shared memory\n    size = (4096, 4096)\n    dtype = np.float64\n    \n    # Create shared memory\n    shm = shared_memory.SharedMemory(\n        create=True, \n        size=np.prod(size) * np.dtype(dtype).itemsize\n    )\n    \n    # Create numpy array backed by shared memory\n    shared_array = np.ndarray(size, dtype=dtype, buffer=shm.buf)\n    \n    # Initialize with data\n    shared_array[:] = np.random.randn(*size) + 1000\n    \n    print(f\"Created shared array: {shared_array.shape}, \"\n          f\"size: {shared_array.nbytes / 1e6:.1f} MB\")\n    \n    # Process in parallel without copying\n    n_processes = 4\n    chunk_size = size[0] // n_processes\n    \n    args = []\n    for i in range(n_processes):\n        start = i * chunk_size\n        end = start + chunk_size if i < n_processes - 1 else size[0]\n        args.append((shm.name, size, dtype, start, end))\n    \n    with Pool(n_processes) as pool:\n        results = pool.map(process_shared_chunk, args)\n    \n    print(\"Processing complete:\", results)\n    \n    # Clean up shared memory\n    shm.close()\n    shm.unlink()\n\n# shared_memory_example()  # Uncomment to run\nprint(\"Shared memory example ready\")","type":"content","url":"/performance-optimization#shared-memory-for-large-arrays","position":31},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl2":"7.5 Memory Optimization"},"type":"lvl2","url":"/performance-optimization#id-7-5-memory-optimization","position":32},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl2":"7.5 Memory Optimization"},"content":"","type":"content","url":"/performance-optimization#id-7-5-memory-optimization","position":33},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl3":"Memory Profiling and Optimization","lvl2":"7.5 Memory Optimization"},"type":"lvl3","url":"/performance-optimization#memory-profiling-and-optimization","position":34},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl3":"Memory Profiling and Optimization","lvl2":"7.5 Memory Optimization"},"content":"def memory_optimization_techniques():\n    \"\"\"Demonstrate memory optimization strategies.\"\"\"\n    \n    print(\"Memory Optimization Techniques:\\n\")\n    \n    # 1. Use appropriate dtypes\n    print(\"1. Choose appropriate data types:\")\n    \n    # Bad: Using float64 when not needed\n    large_array_64 = np.random.randn(10000, 10000)  # 800 MB\n    \n    # Good: Use float32 if precision allows\n    large_array_32 = np.random.randn(10000, 10000).astype(np.float32)  # 400 MB\n    \n    # Better: Use int16 for counts\n    count_array = np.random.randint(0, 1000, (10000, 10000), dtype=np.int16)  # 200 MB\n    \n    print(f\"  float64: {large_array_64.nbytes / 1e6:.1f} MB\")\n    print(f\"  float32: {large_array_32.nbytes / 1e6:.1f} MB\")\n    print(f\"  int16:   {count_array.nbytes / 1e6:.1f} MB\")\n    \n    # Clean up\n    del large_array_64, large_array_32, count_array\n    \n    # 2. Use views instead of copies\n    print(\"\\n2. Use views instead of copies:\")\n    \n    spectrum = np.random.randn(100000)\n    \n    # Bad: Creates a copy\n    blue_region_copy = spectrum[10000:20000].copy()\n    \n    # Good: Creates a view (no memory duplication)\n    blue_region_view = spectrum[10000:20000]\n    \n    print(f\"  Original: {spectrum.nbytes / 1e3:.1f} KB\")\n    print(f\"  Copy uses additional: {blue_region_copy.nbytes / 1e3:.1f} KB\")\n    print(f\"  View uses: 0 KB additional\")\n    \n    # 3. Generator expressions for large datasets\n    print(\"\\n3. Use generators for streaming data:\")\n    \n    def load_observations_generator(n_files):\n        \"\"\"Generator: loads one at a time.\"\"\"\n        for i in range(n_files):\n            # Simulate loading\n            yield np.random.randn(1024, 1024)\n    \n    def load_observations_list(n_files):\n        \"\"\"List: loads all into memory.\"\"\"\n        return [np.random.randn(1024, 1024) for i in range(n_files)]\n    \n    # Generator uses constant memory\n    gen = load_observations_generator(100)\n    print(f\"  Generator object size: {sys.getsizeof(gen)} bytes\")\n    \n    # List would use ~800 MB!\n    # observations = load_observations_list(100)  # Don't run this!\n    \n    # 4. Memory mapping for large files\n    print(\"\\n4. Memory-mapped files for large datasets:\")\n    \n    # Create a memory-mapped array\n    filename = 'large_data.dat'\n    shape = (10000, 10000)\n    \n    # Write\n    fp = np.memmap(filename, dtype='float32', mode='w+', shape=shape)\n    fp[:] = np.random.randn(*shape)\n    del fp  # Flush to disk\n    \n    # Read without loading into RAM\n    data = np.memmap(filename, dtype='float32', mode='r', shape=shape)\n    print(f\"  Memory-mapped array: {shape}, accessing without loading all\")\n    \n    # Only accessed parts are loaded\n    small_section = data[0:100, 0:100]\n    print(f\"  Accessed section: {small_section.shape}\")\n    \n    # Clean up\n    del data\n    import os\n    os.remove(filename)\n\nmemory_optimization_techniques()","type":"content","url":"/performance-optimization#memory-profiling-and-optimization","position":35},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl3":"Chunked Processing for Large Datasets","lvl2":"7.5 Memory Optimization"},"type":"lvl3","url":"/performance-optimization#chunked-processing-for-large-datasets","position":36},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl3":"Chunked Processing for Large Datasets","lvl2":"7.5 Memory Optimization"},"content":"class ChunkedProcessor:\n    \"\"\"Process large datasets in manageable chunks.\"\"\"\n    \n    def __init__(self, chunk_size=1000):\n        self.chunk_size = chunk_size\n    \n    def process_large_catalog(self, filename, n_total):\n        \"\"\"\n        Process a large catalog without loading it all.\n        \n        Simulates reading from a huge file.\n        \"\"\"\n        results = []\n        \n        for chunk_start in range(0, n_total, self.chunk_size):\n            chunk_end = min(chunk_start + self.chunk_size, n_total)\n            \n            # \"Load\" just this chunk\n            chunk_data = self._load_chunk(filename, chunk_start, chunk_end)\n            \n            # Process the chunk\n            chunk_results = self._process_chunk(chunk_data)\n            \n            # Store results (or write to disk)\n            results.extend(chunk_results)\n            \n            # Clear chunk from memory\n            del chunk_data\n            \n            if chunk_start % (self.chunk_size * 10) == 0:\n                print(f\"  Processed {chunk_start}/{n_total} objects\")\n        \n        return results\n    \n    def _load_chunk(self, filename, start, end):\n        \"\"\"Simulate loading a chunk of data.\"\"\"\n        n_objects = end - start\n        return {\n            'ra': np.random.uniform(0, 360, n_objects),\n            'dec': np.random.uniform(-90, 90, n_objects),\n            'mag': np.random.uniform(10, 20, n_objects)\n        }\n    \n    def _process_chunk(self, chunk):\n        \"\"\"Process a chunk of objects.\"\"\"\n        # Example: Select bright objects\n        mask = chunk['mag'] < 15\n        \n        bright_objects = []\n        for i in np.where(mask)[0]:\n            bright_objects.append({\n                'ra': chunk['ra'][i],\n                'dec': chunk['dec'][i],\n                'mag': chunk['mag'][i]\n            })\n        \n        return bright_objects\n    \n    def streaming_statistics(self, data_generator):\n        \"\"\"\n        Calculate statistics on streaming data without storing it all.\n        \n        Uses Welford's algorithm for numerical stability.\n        \"\"\"\n        n = 0\n        mean = 0\n        M2 = 0\n        min_val = float('inf')\n        max_val = float('-inf')\n        \n        for chunk in data_generator:\n            for value in chunk:\n                n += 1\n                delta = value - mean\n                mean += delta / n\n                delta2 = value - mean\n                M2 += delta * delta2\n                \n                min_val = min(min_val, value)\n                max_val = max(max_val, value)\n        \n        variance = M2 / (n - 1) if n > 1 else 0\n        std = np.sqrt(variance)\n        \n        return {\n            'count': n,\n            'mean': mean,\n            'std': std,\n            'min': min_val,\n            'max': max_val\n        }\n\n# Test chunked processing\nprocessor = ChunkedProcessor(chunk_size=5000)\n\nprint(\"Processing large catalog in chunks:\")\nresults = processor.process_large_catalog(\"huge_catalog.dat\", n_total=100000)\nprint(f\"Found {len(results)} bright objects\")\n\n# Test streaming statistics\ndef data_stream():\n    \"\"\"Generate data chunks.\"\"\"\n    for _ in range(100):\n        yield np.random.randn(1000) * 10 + 50\n\nprint(\"\\nCalculating statistics on streaming data:\")\nstats = processor.streaming_statistics(data_stream())\nprint(f\"Statistics: mean={stats['mean']:.2f}, std={stats['std']:.2f}\")","type":"content","url":"/performance-optimization#chunked-processing-for-large-datasets","position":37},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl2":"Try It Yourself"},"type":"lvl2","url":"/performance-optimization#try-it-yourself","position":38},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl2":"Try It Yourself"},"content":"","type":"content","url":"/performance-optimization#try-it-yourself","position":39},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl3":"Exercise 7.1: Optimize Light Curve Analysis","lvl2":"Try It Yourself"},"type":"lvl3","url":"/performance-optimization#exercise-7-1-optimize-light-curve-analysis","position":40},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl3":"Exercise 7.1: Optimize Light Curve Analysis","lvl2":"Try It Yourself"},"content":"Profile and optimize this light curve analysis code.def analyze_light_curves_slow(times, fluxes, periods_to_test):\n    \"\"\"\n    Slow light curve period analysis.\n    \n    Your task:\n    1. Profile to find bottlenecks\n    2. Vectorize the period folding\n    3. Add Numba acceleration\n    4. Parallelize across multiple light curves\n    \"\"\"\n    best_periods = []\n    \n    for time, flux in zip(times, fluxes):\n        chi2_values = []\n        \n        for period in periods_to_test:\n            # Phase fold\n            phases = []\n            for t in time:\n                phase = (t % period) / period\n                phases.append(phase)\n            \n            # Sort by phase\n            sorted_indices = sorted(range(len(phases)), key=lambda i: phases[i])\n            sorted_flux = [flux[i] for i in sorted_indices]\n            \n            # Calculate chi-squared (simplified)\n            mean_flux = sum(sorted_flux) / len(sorted_flux)\n            chi2 = 0\n            for f in sorted_flux:\n                chi2 += (f - mean_flux) ** 2\n            \n            chi2_values.append(chi2)\n        \n        # Find best period (minimum chi2)\n        best_idx = chi2_values.index(min(chi2_values))\n        best_periods.append(periods_to_test[best_idx])\n    \n    return best_periods\n\n# Your optimized version:\ndef analyze_light_curves_fast(times, fluxes, periods_to_test):\n    \"\"\"Your optimized implementation.\"\"\"\n    # Your code here\n    pass\n\n# Test data\nn_curves = 100\nn_points = 1000\nn_periods = 100\n\ntimes = [np.sort(np.random.uniform(0, 100, n_points)) for _ in range(n_curves)]\nfluxes = [np.random.randn(n_points) + 100 for _ in range(n_curves)]\nperiods = np.linspace(0.5, 10, n_periods)\n\n# Compare performance\n# slow_results = analyze_light_curves_slow(times[:5], fluxes[:5], periods)\n# fast_results = analyze_light_curves_fast(times, fluxes, periods)","type":"content","url":"/performance-optimization#exercise-7-1-optimize-light-curve-analysis","position":41},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl3":"Exercise 7.2: Memory-Efficient Spectrum Stack","lvl2":"Try It Yourself"},"type":"lvl3","url":"/performance-optimization#exercise-7-2-memory-efficient-spectrum-stack","position":42},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl3":"Exercise 7.2: Memory-Efficient Spectrum Stack","lvl2":"Try It Yourself"},"content":"Implement memory-efficient processing of many spectra.class SpectrumStack:\n    \"\"\"\n    Handle thousands of spectra efficiently.\n    \n    Requirements:\n    - Load spectra on demand, not all at once\n    - Compute median spectrum without loading all data\n    - Find outlier spectra using streaming statistics\n    - Support both in-memory and memory-mapped modes\n    \"\"\"\n    \n    def __init__(self, mode='memory-mapped'):\n        self.mode = mode\n        # Your code here\n        pass\n    \n    def add_spectrum(self, wavelengths, flux):\n        \"\"\"Add a spectrum to the stack.\"\"\"\n        # Your code here\n        pass\n    \n    def compute_median_spectrum(self):\n        \"\"\"\n        Compute median spectrum across all spectra.\n        \n        Challenge: Do this without loading all spectra at once!\n        \"\"\"\n        # Your code here\n        pass\n    \n    def find_outliers(self, threshold=5):\n        \"\"\"\n        Find spectra that deviate significantly from median.\n        \n        Use streaming algorithm to avoid loading all data.\n        \"\"\"\n        # Your code here\n        pass\n    \n    def coadd_spectra(self, weights=None):\n        \"\"\"Coadd all spectra with optional weights.\"\"\"\n        # Your code here\n        pass\n\n# Test your implementation\nstack = SpectrumStack()\n\n# Add many spectra\nfor i in range(1000):\n    wavelengths = np.linspace(4000, 7000, 3000)\n    flux = np.random.randn(3000) + 100\n    if i % 100 == 0:  # Add some outliers\n        flux += 50\n    stack.add_spectrum(wavelengths, flux)\n\n# median = stack.compute_median_spectrum()\n# outliers = stack.find_outliers()\n# print(f\"Found {len(outliers)} outlier spectra\")","type":"content","url":"/performance-optimization#exercise-7-2-memory-efficient-spectrum-stack","position":43},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl3":"Exercise 7.3: Parallel Monte Carlo Simulation","lvl2":"Try It Yourself"},"type":"lvl3","url":"/performance-optimization#exercise-7-3-parallel-monte-carlo-simulation","position":44},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl3":"Exercise 7.3: Parallel Monte Carlo Simulation","lvl2":"Try It Yourself"},"content":"Implement parallel Monte Carlo for stellar population synthesis.def stellar_population_monte_carlo(n_stars, n_realizations, imf_params):\n    \"\"\"\n    Monte Carlo simulation of stellar populations.\n    \n    Your task:\n    1. Implement IMF sampling\n    2. Add stellar evolution tracks\n    3. Parallelize across realizations\n    4. Use Numba for the inner loops\n    5. Optimize memory usage\n    \n    Should return statistics about the population.\n    \"\"\"\n    # Your code here\n    pass\n\n# Target performance:\n# - 10,000 stars per population\n# - 1,000 realizations\n# - Complete in < 10 seconds\n\n# imf_params = {'alpha': -2.35, 'min_mass': 0.08, 'max_mass': 120}\n# results = stellar_population_monte_carlo(10000, 1000, imf_params)","type":"content","url":"/performance-optimization#exercise-7-3-parallel-monte-carlo-simulation","position":45},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl2":"Key Takeaways"},"type":"lvl2","url":"/performance-optimization#key-takeaways","position":46},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl2":"Key Takeaways"},"content":"✅ Profile before optimizing - Use cProfile and line_profiler to find real bottlenecks✅ Vectorization first - NumPy operations are usually fast enough✅ Numba for loops you can’t vectorize - Near C-speed with minimal changes✅ Parallelize embarrassingly parallel problems - Multiple files, independent calculations✅ Choose the right dtype - float32 vs float64 can halve memory usage✅ Stream large datasets - Process in chunks, use generators✅ Memory-map huge files - Access TB-sized files without loading to RAM✅ Know when to stop - “Fast enough” is often better than “fastest possible”","type":"content","url":"/performance-optimization#key-takeaways","position":47},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl2":"Next Chapter Preview"},"type":"lvl2","url":"/performance-optimization#next-chapter-preview","position":48},{"hierarchy":{"lvl1":"Chapter 7: Performance Optimization","lvl2":"Next Chapter Preview"},"content":"Chapter 8 will provide an optional sampler of advanced Python topics including async programming, metaclasses, descriptors, and advanced decorators - choose what’s relevant for your projects!","type":"content","url":"/performance-optimization#next-chapter-preview","position":49},{"hierarchy":{"lvl1":"Pandas: Data Science for Astronomy"},"type":"lvl1","url":"/pandas","position":0},{"hierarchy":{"lvl1":"Pandas: Data Science for Astronomy"},"content":"","type":"content","url":"/pandas","position":1},{"hierarchy":{"lvl1":"Pandas: Data Science for Astronomy","lvl2":"Learning Objectives"},"type":"lvl2","url":"/pandas#learning-objectives","position":2},{"hierarchy":{"lvl1":"Pandas: Data Science for Astronomy","lvl2":"Learning Objectives"},"content":"By the end of this chapter, you will:\n\nMaster DataFrame operations for astronomical catalogs\n\nPerform efficient time series analysis on light curves\n\nExecute complex GroupBy operations for population studies\n\nMerge and join datasets from multiple surveys\n\nPrepare data for machine learning pipelines\n\nHandle missing data and outliers systematically\n\nScale to large astronomical surveys with optimization techniques","type":"content","url":"/pandas#learning-objectives","position":3},{"hierarchy":{"lvl1":"Pandas: Data Science for Astronomy","lvl2":"Introduction: Why Pandas for Astronomy?"},"type":"lvl2","url":"/pandas#introduction-why-pandas-for-astronomy","position":4},{"hierarchy":{"lvl1":"Pandas: Data Science for Astronomy","lvl2":"Introduction: Why Pandas for Astronomy?"},"content":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\nimport seaborn as sns\nsns.set_style('whitegrid')\n\ndef why_pandas_for_astronomy():\n    \"\"\"Demonstrate Pandas' value for astronomical data science.\"\"\"\n    \n    print(\"Pandas bridges astronomy and data science:\")\n    print(\"\\n1. CATALOG MANAGEMENT\")\n    print(\"   - Millions of sources from surveys (SDSS, Gaia, LSST)\")\n    print(\"   - Heterogeneous data types (positions, magnitudes, spectra)\")\n    print(\"   - Missing values and quality flags\")\n    \n    print(\"\\n2. TIME SERIES ANALYSIS\")\n    print(\"   - Irregular sampling from ground-based telescopes\")\n    print(\"   - Multi-band light curves\")\n    print(\"   - Automated classification of variables\")\n    \n    print(\"\\n3. DATA SCIENCE WORKFLOW\")\n    print(\"   - Exploratory data analysis (EDA)\")\n    print(\"   - Feature engineering for ML\")\n    print(\"   - Statistical summaries by groups\")\n    \n    print(\"\\n4. MACHINE LEARNING PREPARATION\")\n    print(\"   - Data cleaning and normalization\")\n    print(\"   - Feature extraction\")\n    print(\"   - Train/test splitting while preserving groups\")\n    \n    # Quick example: Load and explore a galaxy catalog\n    np.random.seed(42)\n    n_galaxies = 10000\n    \n    catalog = pd.DataFrame({\n        'galaxy_id': np.arange(n_galaxies),\n        'ra': np.random.uniform(0, 360, n_galaxies),\n        'dec': np.random.uniform(-90, 90, n_galaxies),\n        'redshift': np.random.gamma(2, 0.5, n_galaxies),\n        'magnitude_r': np.random.normal(20, 2, n_galaxies),\n        'magnitude_g': np.random.normal(21, 2, n_galaxies),\n        'stellar_mass': 10**np.random.normal(10, 0.5, n_galaxies),\n        'morphology': np.random.choice(['E', 'S0', 'Sa', 'Sb', 'Sc', 'Irr'], n_galaxies),\n        'survey': np.random.choice(['SDSS', 'DECALS', 'HSC'], n_galaxies, p=[0.5, 0.3, 0.2])\n    })\n    \n    print(f\"\\n5. EXAMPLE CATALOG OPERATIONS:\")\n    print(f\"   Shape: {catalog.shape}\")\n    print(f\"   Memory usage: {catalog.memory_usage().sum() / 1e6:.1f} MB\")\n    print(f\"   Surveys: {catalog['survey'].value_counts().to_dict()}\")\n    print(f\"   Redshift range: {catalog['redshift'].min():.2f} - {catalog['redshift'].max():.2f}\")\n    \n    return catalog\n\ncatalog = why_pandas_for_astronomy()","type":"content","url":"/pandas#introduction-why-pandas-for-astronomy","position":5},{"hierarchy":{"lvl1":"Pandas: Data Science for Astronomy","lvl2":"DataFrames for Astronomical Catalogs"},"type":"lvl2","url":"/pandas#dataframes-for-astronomical-catalogs","position":6},{"hierarchy":{"lvl1":"Pandas: Data Science for Astronomy","lvl2":"DataFrames for Astronomical Catalogs"},"content":"","type":"content","url":"/pandas#dataframes-for-astronomical-catalogs","position":7},{"hierarchy":{"lvl1":"Pandas: Data Science for Astronomy","lvl3":"Creating and Loading Catalogs","lvl2":"DataFrames for Astronomical Catalogs"},"type":"lvl3","url":"/pandas#creating-and-loading-catalogs","position":8},{"hierarchy":{"lvl1":"Pandas: Data Science for Astronomy","lvl3":"Creating and Loading Catalogs","lvl2":"DataFrames for Astronomical Catalogs"},"content":"def catalog_operations():\n    \"\"\"Essential DataFrame operations for astronomical catalogs.\"\"\"\n    \n    # Create a realistic stellar catalog\n    n_stars = 5000\n    \n    # Generate synthetic Gaia-like data\n    stars = pd.DataFrame({\n        'source_id': np.arange(1000000, 1000000 + n_stars),\n        'ra': np.random.uniform(120, 140, n_stars),  # Limited sky region\n        'dec': np.random.uniform(-30, -10, n_stars),\n        'parallax': np.random.gamma(2, 0.5, n_stars),  # mas\n        'parallax_error': np.random.gamma(1, 0.05, n_stars),\n        'pmra': np.random.normal(0, 5, n_stars),  # mas/yr\n        'pmdec': np.random.normal(0, 5, n_stars),  # mas/yr\n        'phot_g_mean_mag': np.random.normal(15, 2, n_stars),\n        'phot_bp_mean_mag': np.random.normal(15.5, 2, n_stars),\n        'phot_rp_mean_mag': np.random.normal(14.5, 2, n_stars),\n        'radial_velocity': np.random.normal(0, 30, n_stars),\n        'teff_val': np.random.normal(5500, 1000, n_stars),\n        'logg_val': np.random.normal(4.5, 0.5, n_stars),\n        'fe_h': np.random.normal(0, 0.3, n_stars),\n    })\n    \n    # Add some missing values (realistic)\n    rv_missing = np.random.random(n_stars) > 0.3  # 70% have RV\n    stars.loc[rv_missing, 'radial_velocity'] = np.nan\n    \n    print(\"1. BASIC CATALOG INFO:\")\n    print(stars.info())\n    \n    print(\"\\n2. STATISTICAL SUMMARY:\")\n    print(stars[['parallax', 'phot_g_mean_mag', 'teff_val']].describe())\n    \n    # Add derived columns\n    print(\"\\n3. ADDING DERIVED QUANTITIES:\")\n    \n    # Distance from parallax\n    stars['distance_pc'] = 1000 / stars['parallax']\n    \n    # Absolute magnitude\n    stars['abs_mag_g'] = stars['phot_g_mean_mag'] - 5 * np.log10(stars['distance_pc']) + 5\n    \n    # Color indices\n    stars['bp_rp'] = stars['phot_bp_mean_mag'] - stars['phot_rp_mean_mag']\n    stars['g_rp'] = stars['phot_g_mean_mag'] - stars['phot_rp_mean_mag']\n    \n    # Quality flags\n    stars['high_quality'] = (\n        (stars['parallax_error'] / stars['parallax'] < 0.1) &  # Good parallax\n        (stars['parallax'] > 0) &  # Positive parallax\n        (stars['phot_g_mean_mag'] < 18)  # Bright enough\n    )\n    \n    print(f\"High quality stars: {stars['high_quality'].sum()} / {len(stars)}\")\n    \n    # Efficient selection\n    print(\"\\n4. EFFICIENT DATA SELECTION:\")\n    \n    # Method 1: Boolean indexing\n    nearby = stars[stars['distance_pc'] < 100]\n    print(f\"Stars within 100 pc: {len(nearby)}\")\n    \n    # Method 2: Query method (more readable)\n    bright_nearby = stars.query('distance_pc < 100 & phot_g_mean_mag < 10')\n    print(f\"Bright nearby stars: {len(bright_nearby)}\")\n    \n    # Method 3: loc for complex conditions\n    solar_type = stars.loc[\n        (stars['teff_val'].between(5300, 6000)) &\n        (stars['logg_val'].between(4.0, 4.6)) &\n        (stars['fe_h'].between(-0.1, 0.1))\n    ]\n    print(f\"Solar-type stars: {len(solar_type)}\")\n    \n    return stars\n\nstars_df = catalog_operations()","type":"content","url":"/pandas#creating-and-loading-catalogs","position":9},{"hierarchy":{"lvl1":"Pandas: Data Science for Astronomy","lvl3":"Advanced Indexing and MultiIndex","lvl2":"DataFrames for Astronomical Catalogs"},"type":"lvl3","url":"/pandas#advanced-indexing-and-multiindex","position":10},{"hierarchy":{"lvl1":"Pandas: Data Science for Astronomy","lvl3":"Advanced Indexing and MultiIndex","lvl2":"DataFrames for Astronomical Catalogs"},"content":"def advanced_indexing():\n    \"\"\"Advanced indexing techniques for hierarchical astronomical data.\"\"\"\n    \n    # Create multi-band photometry catalog\n    n_objects = 1000\n    n_epochs = 5\n    \n    # Generate hierarchical data\n    data = []\n    for obj_id in range(n_objects):\n        for epoch in range(n_epochs):\n            for band in ['u', 'g', 'r', 'i', 'z']:\n                mjd = 58000 + epoch * 30 + np.random.uniform(-1, 1)\n                mag = np.random.normal(20 + ord(band) * 0.01, 0.1)\n                err = np.random.uniform(0.01, 0.05)\n                \n                data.append({\n                    'object_id': obj_id,\n                    'mjd': mjd,\n                    'band': band,\n                    'magnitude': mag,\n                    'error': err\n                })\n    \n    photometry = pd.DataFrame(data)\n    \n    # Create MultiIndex\n    photometry_indexed = photometry.set_index(['object_id', 'mjd', 'band'])\n    \n    print(\"1. MULTIINDEX STRUCTURE:\")\n    print(photometry_indexed.head(10))\n    \n    # Access specific levels\n    print(\"\\n2. ACCESSING DATA:\")\n    \n    # Single object, all epochs\n    obj_0 = photometry_indexed.loc[0]\n    print(f\"Object 0 measurements: {len(obj_0)}\")\n    \n    # Cross-section: all g-band measurements\n    g_band = photometry_indexed.xs('g', level='band')\n    print(f\"G-band measurements: {len(g_band)}\")\n    \n    # Pivot for analysis\n    print(\"\\n3. PIVOTING FOR ANALYSIS:\")\n    \n    # Wide format for color analysis\n    colors = photometry.pivot_table(\n        values='magnitude',\n        index=['object_id', 'mjd'],\n        columns='band',\n        aggfunc='mean'\n    )\n    \n    # Calculate colors\n    colors['g-r'] = colors['g'] - colors['r']\n    colors['r-i'] = colors['r'] - colors['i']\n    \n    print(colors.head())\n    \n    # Hierarchical grouping\n    print(\"\\n4. HIERARCHICAL GROUPING:\")\n    \n    # Mean magnitude per object per band\n    mean_mags = photometry.groupby(['object_id', 'band'])['magnitude'].agg(['mean', 'std'])\n    print(mean_mags.head(10))\n    \n    return photometry, colors\n\nphotometry_df, colors_df = advanced_indexing()","type":"content","url":"/pandas#advanced-indexing-and-multiindex","position":11},{"hierarchy":{"lvl1":"Pandas: Data Science for Astronomy","lvl2":"Time Series Analysis for Astronomy"},"type":"lvl2","url":"/pandas#time-series-analysis-for-astronomy","position":12},{"hierarchy":{"lvl1":"Pandas: Data Science for Astronomy","lvl2":"Time Series Analysis for Astronomy"},"content":"","type":"content","url":"/pandas#time-series-analysis-for-astronomy","position":13},{"hierarchy":{"lvl1":"Pandas: Data Science for Astronomy","lvl3":"Light Curve Analysis","lvl2":"Time Series Analysis for Astronomy"},"type":"lvl3","url":"/pandas#light-curve-analysis","position":14},{"hierarchy":{"lvl1":"Pandas: Data Science for Astronomy","lvl3":"Light Curve Analysis","lvl2":"Time Series Analysis for Astronomy"},"content":"def time_series_astronomy():\n    \"\"\"Time series analysis for variable stars and transients.\"\"\"\n    \n    # Generate realistic variable star light curve\n    np.random.seed(42)\n    \n    # RR Lyrae-like variable\n    true_period = 0.5427  # days\n    amplitude = 0.8\n    \n    # Irregular sampling (ground-based reality)\n    n_nights = 150\n    observations = []\n    \n    for night in range(n_nights):\n        # Weather: 30% chance of no observation\n        if np.random.random() < 0.3:\n            continue\n            \n        # Multiple observations per night\n        n_obs = np.random.poisson(3)\n        for _ in range(n_obs):\n            mjd = 58000 + night + np.random.uniform(0, 0.3)\n            phase = (mjd % true_period) / true_period\n            \n            # RR Lyrae-like light curve shape\n            mag = 14.5 - amplitude * (0.5 * np.sin(2*np.pi*phase) + \n                                      0.3 * np.sin(4*np.pi*phase) +\n                                      0.1 * np.sin(6*np.pi*phase))\n            \n            # Add noise\n            mag += np.random.normal(0, 0.02)\n            error = np.random.uniform(0.015, 0.025)\n            \n            observations.append({\n                'mjd': mjd,\n                'magnitude': mag,\n                'error': error,\n                'band': 'V'\n            })\n    \n    # Create DataFrame\n    lightcurve = pd.DataFrame(observations)\n    lightcurve['datetime'] = pd.to_datetime(lightcurve['mjd'] - 40587, unit='D', origin='unix')\n    lightcurve = lightcurve.sort_values('mjd')\n    \n    print(\"1. LIGHT CURVE DATA:\")\n    print(lightcurve.info())\n    print(f\"Time span: {lightcurve['mjd'].max() - lightcurve['mjd'].min():.1f} days\")\n    print(f\"Number of observations: {len(lightcurve)}\")\n    \n    # Time series features for ML\n    print(\"\\n2. FEATURE EXTRACTION FOR ML:\")\n    \n    features = {}\n    \n    # Basic statistics\n    features['mean_mag'] = lightcurve['magnitude'].mean()\n    features['std_mag'] = lightcurve['magnitude'].std()\n    features['amplitude'] = lightcurve['magnitude'].max() - lightcurve['magnitude'].min()\n    \n    # Percentiles\n    features['mag_5'] = lightcurve['magnitude'].quantile(0.05)\n    features['mag_95'] = lightcurve['magnitude'].quantile(0.95)\n    \n    # Time series specific\n    features['n_observations'] = len(lightcurve)\n    features['timespan'] = lightcurve['mjd'].max() - lightcurve['mjd'].min()\n    features['mean_sampling'] = features['timespan'] / features['n_observations']\n    \n    # Changes between consecutive observations\n    lightcurve['mag_diff'] = lightcurve['magnitude'].diff()\n    lightcurve['time_diff'] = lightcurve['mjd'].diff()\n    lightcurve['rate_of_change'] = lightcurve['mag_diff'] / lightcurve['time_diff']\n    \n    features['mean_rate_change'] = lightcurve['rate_of_change'].abs().mean()\n    features['max_rate_change'] = lightcurve['rate_of_change'].abs().max()\n    \n    # Skewness and kurtosis (shape indicators)\n    features['skewness'] = lightcurve['magnitude'].skew()\n    features['kurtosis'] = lightcurve['magnitude'].kurtosis()\n    \n    # Beyond features (for period finding)\n    from scipy.stats import skew, kurtosis\n    features['beyond1std'] = ((lightcurve['magnitude'] - features['mean_mag']).abs() > \n                              features['std_mag']).mean()\n    \n    print(\"Extracted features for ML:\")\n    for key, value in features.items():\n        print(f\"  {key}: {value:.3f}\")\n    \n    # Resampling for regular time series\n    print(\"\\n3. RESAMPLING FOR ANALYSIS:\")\n    \n    lightcurve_indexed = lightcurve.set_index('datetime')\n    \n    # Daily binning\n    daily_lc = lightcurve_indexed.resample('1D').agg({\n        'magnitude': ['mean', 'std', 'count'],\n        'error': 'mean'\n    })\n    daily_lc.columns = ['_'.join(col).strip() for col in daily_lc.columns]\n    daily_lc = daily_lc[daily_lc['magnitude_count'] > 0]\n    \n    print(f\"Daily binned: {len(daily_lc)} days with data\")\n    \n    # Rolling statistics (for trend detection)\n    print(\"\\n4. ROLLING WINDOW ANALYSIS:\")\n    \n    window_size = 10  # days\n    lightcurve_indexed['rolling_mean'] = lightcurve_indexed['magnitude'].rolling(\n        window=f'{window_size}D', min_periods=5\n    ).mean()\n    \n    lightcurve_indexed['rolling_std'] = lightcurve_indexed['magnitude'].rolling(\n        window=f'{window_size}D', min_periods=5\n    ).std()\n    \n    # Detect outliers\n    lightcurve_indexed['outlier'] = (\n        np.abs(lightcurve_indexed['magnitude'] - lightcurve_indexed['rolling_mean']) > \n        3 * lightcurve_indexed['rolling_std']\n    )\n    \n    print(f\"Outliers detected: {lightcurve_indexed['outlier'].sum()}\")\n    \n    # Period analysis preparation\n    print(\"\\n5. PERIOD ANALYSIS PREPARATION:\")\n    \n    # Create phase-folded DataFrame for different trial periods\n    def phase_fold(lc_df, period):\n        \"\"\"Phase fold light curve at given period.\"\"\"\n        df = lc_df.copy()\n        df['phase'] = (df['mjd'] % period) / period\n        return df\n    \n    # Try multiple periods\n    trial_periods = np.linspace(0.5, 0.6, 100)\n    chi2_values = []\n    \n    for period in trial_periods:\n        folded = phase_fold(lightcurve, period)\n        \n        # Bin in phase\n        phase_bins = np.linspace(0, 1, 20)\n        binned = folded.groupby(pd.cut(folded['phase'], phase_bins))['magnitude'].agg(['mean', 'std'])\n        \n        # Simple chi-squared\n        chi2 = binned['std'].mean() if not binned['std'].isna().all() else np.inf\n        chi2_values.append(chi2)\n    \n    best_period_idx = np.argmin(chi2_values)\n    best_period = trial_periods[best_period_idx]\n    \n    print(f\"Best period found: {best_period:.4f} days (true: {true_period:.4f})\")\n    \n    return lightcurve, features\n\nlightcurve_df, ml_features = time_series_astronomy()","type":"content","url":"/pandas#light-curve-analysis","position":15},{"hierarchy":{"lvl1":"Pandas: Data Science for Astronomy","lvl2":"GroupBy Operations for Population Studies"},"type":"lvl2","url":"/pandas#groupby-operations-for-population-studies","position":16},{"hierarchy":{"lvl1":"Pandas: Data Science for Astronomy","lvl2":"GroupBy Operations for Population Studies"},"content":"","type":"content","url":"/pandas#groupby-operations-for-population-studies","position":17},{"hierarchy":{"lvl1":"Pandas: Data Science for Astronomy","lvl3":"Analyzing Stellar Populations","lvl2":"GroupBy Operations for Population Studies"},"type":"lvl3","url":"/pandas#analyzing-stellar-populations","position":18},{"hierarchy":{"lvl1":"Pandas: Data Science for Astronomy","lvl3":"Analyzing Stellar Populations","lvl2":"GroupBy Operations for Population Studies"},"content":"def population_analysis():\n    \"\"\"GroupBy operations for stellar population studies.\"\"\"\n    \n    # Create a large stellar survey dataset\n    n_stars = 50000\n    \n    # Multiple stellar populations\n    populations = []\n    \n    # Thin disk\n    n_thin = int(0.7 * n_stars)\n    thin_disk = pd.DataFrame({\n        'population': 'thin_disk',\n        'age_gyr': np.random.gamma(3, 1.5, n_thin),\n        'metallicity': np.random.normal(0, 0.2, n_thin),\n        'velocity_dispersion': np.random.gamma(20, 2, n_thin),\n        'scale_height_pc': np.random.normal(300, 50, n_thin),\n    })\n    \n    # Thick disk\n    n_thick = int(0.2 * n_stars)\n    thick_disk = pd.DataFrame({\n        'population': 'thick_disk',\n        'age_gyr': np.random.gamma(10, 2, n_thick),\n        'metallicity': np.random.normal(-0.5, 0.3, n_thick),\n        'velocity_dispersion': np.random.gamma(40, 5, n_thick),\n        'scale_height_pc': np.random.normal(900, 100, n_thick),\n    })\n    \n    # Halo\n    n_halo = n_stars - n_thin - n_thick\n    halo = pd.DataFrame({\n        'population': 'halo',\n        'age_gyr': np.random.gamma(12, 1, n_halo),\n        'metallicity': np.random.normal(-1.5, 0.5, n_halo),\n        'velocity_dispersion': np.random.gamma(100, 20, n_halo),\n        'scale_height_pc': np.random.exponential(3000, n_halo),\n    })\n    \n    # Combine populations\n    survey = pd.concat([thin_disk, thick_disk, halo], ignore_index=True)\n    \n    # Add observational properties\n    survey['apparent_mag'] = 10 + 5 * np.log10(survey['scale_height_pc']) + np.random.normal(0, 0.5, n_stars)\n    survey['color_index'] = 0.5 + 0.1 * survey['metallicity'] + np.random.normal(0, 0.1, n_stars)\n    \n    print(\"1. POPULATION STATISTICS:\")\n    population_stats = survey.groupby('population').agg({\n        'age_gyr': ['mean', 'std', 'median'],\n        'metallicity': ['mean', 'std'],\n        'velocity_dispersion': ['mean', 'std'],\n        'scale_height_pc': ['mean', 'median']\n    })\n    print(population_stats)\n    \n    print(\"\\n2. CUSTOM AGGREGATIONS:\")\n    \n    def weighted_mean(values, weights):\n        \"\"\"Calculate weighted mean.\"\"\"\n        return np.average(values, weights=weights)\n    \n    # Custom aggregation functions\n    agg_funcs = {\n        'age_gyr': ['mean', 'std', lambda x: np.percentile(x, 90)],\n        'metallicity': ['mean', 'median', lambda x: x.quantile(0.25)],\n        'velocity_dispersion': ['mean', 'max']\n    }\n    \n    custom_stats = survey.groupby('population').agg(agg_funcs)\n    custom_stats.columns = ['_'.join(col).strip() if col[1] else col[0] \n                            for col in custom_stats.columns]\n    print(custom_stats)\n    \n    print(\"\\n3. TRANSFORM OPERATIONS:\")\n    \n    # Normalize within groups\n    survey['age_normalized'] = survey.groupby('population')['age_gyr'].transform(\n        lambda x: (x - x.mean()) / x.std()\n    )\n    \n    # Rank within groups\n    survey['metallicity_rank'] = survey.groupby('population')['metallicity'].rank(\n        method='dense', ascending=False\n    )\n    \n    # Cumulative statistics\n    survey_sorted = survey.sort_values(['population', 'age_gyr'])\n    survey_sorted['cumulative_fraction'] = survey_sorted.groupby('population').cumcount() / \\\n                                           survey_sorted.groupby('population')['population'].transform('count')\n    \n    print(\"Sample of transformed data:\")\n    print(survey[['population', 'age_gyr', 'age_normalized', 'metallicity', 'metallicity_rank']].head(10))\n    \n    print(\"\\n4. GROUPED FILTERING:\")\n    \n    # Keep only populations with enough statistics\n    min_pop_size = 100\n    large_pops = survey.groupby('population').filter(lambda x: len(x) >= min_pop_size)\n    print(f\"Stars in large populations: {len(large_pops)} / {len(survey)}\")\n    \n    # Select extreme objects in each population\n    def select_extremes(group, n=100):\n        \"\"\"Select n most extreme objects by metallicity.\"\"\"\n        return group.nlargest(n, 'metallicity', keep='all')\n    \n    metal_rich = survey.groupby('population').apply(select_extremes, n=50)\n    print(f\"Metal-rich selection: {len(metal_rich)} stars\")\n    \n    print(\"\\n5. BINNING AND CATEGORICAL ANALYSIS:\")\n    \n    # Bin ages for analysis\n    survey['age_bin'] = pd.cut(survey['age_gyr'], \n                               bins=[0, 2, 5, 10, 15],\n                               labels=['young', 'intermediate', 'old', 'ancient'])\n    \n    # Cross-tabulation\n    cross_tab = pd.crosstab(survey['population'], survey['age_bin'], \n                            normalize='index') * 100\n    print(\"Age distribution by population (%):\")\n    print(cross_tab.round(1))\n    \n    return survey\n\nstellar_survey = population_analysis()","type":"content","url":"/pandas#analyzing-stellar-populations","position":19},{"hierarchy":{"lvl1":"Pandas: Data Science for Astronomy","lvl2":"Data Preparation for Machine Learning"},"type":"lvl2","url":"/pandas#data-preparation-for-machine-learning","position":20},{"hierarchy":{"lvl1":"Pandas: Data Science for Astronomy","lvl2":"Data Preparation for Machine Learning"},"content":"","type":"content","url":"/pandas#data-preparation-for-machine-learning","position":21},{"hierarchy":{"lvl1":"Pandas: Data Science for Astronomy","lvl3":"Feature Engineering Pipeline","lvl2":"Data Preparation for Machine Learning"},"type":"lvl3","url":"/pandas#feature-engineering-pipeline","position":22},{"hierarchy":{"lvl1":"Pandas: Data Science for Astronomy","lvl3":"Feature Engineering Pipeline","lvl2":"Data Preparation for Machine Learning"},"content":"def ml_data_preparation():\n    \"\"\"Prepare astronomical data for machine learning.\"\"\"\n    \n    # Create a galaxy classification dataset\n    n_galaxies = 10000\n    \n    # Generate features\n    galaxies = pd.DataFrame({\n        # Photometry\n        'mag_u': np.random.normal(22, 1.5, n_galaxies),\n        'mag_g': np.random.normal(21, 1.5, n_galaxies),\n        'mag_r': np.random.normal(20, 1.5, n_galaxies),\n        'mag_i': np.random.normal(19.5, 1.5, n_galaxies),\n        'mag_z': np.random.normal(19, 1.5, n_galaxies),\n        \n        # Morphology\n        'petrosian_radius': np.random.lognormal(1, 0.5, n_galaxies),\n        'concentration': np.random.uniform(1.5, 5, n_galaxies),\n        'asymmetry': np.random.beta(2, 5, n_galaxies),\n        'smoothness': np.random.beta(2, 8, n_galaxies),\n        \n        # Spectroscopy\n        'redshift': np.random.gamma(2, 0.3, n_galaxies),\n        'velocity_dispersion': np.random.lognormal(5, 0.5, n_galaxies),\n        'h_alpha_flux': np.random.lognormal(-15, 1, n_galaxies),\n        'd4000_break': np.random.normal(1.5, 0.3, n_galaxies),\n        \n        # Environment\n        'n_neighbors_1mpc': np.random.poisson(5, n_galaxies),\n        'distance_to_nearest': np.random.exponential(0.5, n_galaxies),\n    })\n    \n    # Add classification labels (simplified)\n    def classify_galaxy(row):\n        if row['d4000_break'] > 1.6 and row['h_alpha_flux'] < -15:\n            return 'elliptical'\n        elif row['concentration'] > 3.5:\n            return 'spiral_early'\n        elif row['asymmetry'] > 0.3:\n            return 'irregular'\n        else:\n            return 'spiral_late'\n    \n    galaxies['morphology_class'] = galaxies.apply(classify_galaxy, axis=1)\n    \n    print(\"1. INITIAL DATA EXPLORATION:\")\n    print(galaxies.info())\n    print(f\"\\nClass distribution:\\n{galaxies['morphology_class'].value_counts()}\")\n    \n    print(\"\\n2. FEATURE ENGINEERING:\")\n    \n    # Color indices\n    galaxies['u_g'] = galaxies['mag_u'] - galaxies['mag_g']\n    galaxies['g_r'] = galaxies['mag_g'] - galaxies['mag_r']\n    galaxies['r_i'] = galaxies['mag_r'] - galaxies['mag_i']\n    galaxies['i_z'] = galaxies['mag_i'] - galaxies['mag_z']\n    \n    # Composite features\n    galaxies['CAS_score'] = (galaxies['concentration'] + \n                            10 * galaxies['asymmetry'] + \n                            5 * galaxies['smoothness'])\n    \n    # Logarithmic transforms for skewed features\n    galaxies['log_vdisp'] = np.log10(galaxies['velocity_dispersion'])\n    galaxies['log_halpha'] = np.log10(galaxies['h_alpha_flux'] - galaxies['h_alpha_flux'].min() + 1)\n    \n    # Interaction features\n    galaxies['density_indicator'] = galaxies['n_neighbors_1mpc'] / (galaxies['distance_to_nearest'] + 0.1)\n    \n    print(\"Added features:\", [col for col in galaxies.columns if col not in \n                              ['mag_u', 'mag_g', 'mag_r', 'mag_i', 'mag_z', \n                               'morphology_class']])\n    \n    print(\"\\n3. HANDLING MISSING DATA:\")\n    \n    # Introduce missing data (realistic scenario)\n    missing_fraction = 0.1\n    for col in ['velocity_dispersion', 'h_alpha_flux', 'd4000_break']:\n        missing_idx = np.random.choice(galaxies.index, \n                                      size=int(len(galaxies) * missing_fraction),\n                                      replace=False)\n        galaxies.loc[missing_idx, col] = np.nan\n    \n    print(f\"Missing data summary:\\n{galaxies.isnull().sum()}\")\n    \n    # Imputation strategies\n    from sklearn.impute import SimpleImputer, KNNImputer\n    \n    # Simple imputation\n    simple_imputer = SimpleImputer(strategy='median')\n    \n    # KNN imputation (better for correlated features)\n    knn_imputer = KNNImputer(n_neighbors=5)\n    \n    # Apply to numerical features\n    numerical_features = galaxies.select_dtypes(include=[np.number]).columns.drop('morphology_class', errors='ignore')\n    \n    galaxies_simple = galaxies.copy()\n    galaxies_simple[numerical_features] = simple_imputer.fit_transform(galaxies[numerical_features])\n    \n    print(\"\\n4. OUTLIER DETECTION:\")\n    \n    from sklearn.ensemble import IsolationForest\n    \n    # Isolation Forest for anomaly detection\n    iso_forest = IsolationForest(contamination=0.05, random_state=42)\n    outlier_labels = iso_forest.fit_predict(galaxies_simple[numerical_features])\n    \n    galaxies_simple['is_outlier'] = outlier_labels == -1\n    print(f\"Outliers detected: {galaxies_simple['is_outlier'].sum()}\")\n    \n    print(\"\\n5. FEATURE SCALING:\")\n    \n    from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer\n    \n    # Different scaling methods\n    scalers = {\n        'standard': StandardScaler(),\n        'robust': RobustScaler(),  # Better with outliers\n        'quantile': QuantileTransformer(output_distribution='normal')  # Gaussianize\n    }\n    \n    # Apply scaling\n    scaled_features = {}\n    for name, scaler in scalers.items():\n        scaled_features[name] = pd.DataFrame(\n            scaler.fit_transform(galaxies_simple[numerical_features]),\n            columns=numerical_features,\n            index=galaxies_simple.index\n        )\n    \n    print(\"Scaling comparison (first 5 features):\")\n    for name in scalers.keys():\n        print(f\"\\n{name.capitalize()} scaling:\")\n        print(scaled_features[name].iloc[:, :5].describe().loc[['mean', 'std']])\n    \n    print(\"\\n6. TRAIN-TEST SPLIT WITH STRATIFICATION:\")\n    \n    from sklearn.model_selection import train_test_split\n    \n    # Stratified split to maintain class balance\n    X = scaled_features['robust']\n    y = galaxies_simple['morphology_class']\n    \n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, stratify=y, random_state=42\n    )\n    \n    print(f\"Training set: {len(X_train)} samples\")\n    print(f\"Test set: {len(X_test)} samples\")\n    print(f\"\\nClass distribution in train:\\n{y_train.value_counts(normalize=True)}\")\n    print(f\"\\nClass distribution in test:\\n{y_test.value_counts(normalize=True)}\")\n    \n    return galaxies_simple, X_train, X_test, y_train, y_test\n\ngalaxies_ml, X_train, X_test, y_train, y_test = ml_data_preparation()","type":"content","url":"/pandas#feature-engineering-pipeline","position":23},{"hierarchy":{"lvl1":"Pandas: Data Science for Astronomy","lvl2":"Merging and Joining Astronomical Catalogs"},"type":"lvl2","url":"/pandas#merging-and-joining-astronomical-catalogs","position":24},{"hierarchy":{"lvl1":"Pandas: Data Science for Astronomy","lvl2":"Merging and Joining Astronomical Catalogs"},"content":"","type":"content","url":"/pandas#merging-and-joining-astronomical-catalogs","position":25},{"hierarchy":{"lvl1":"Pandas: Data Science for Astronomy","lvl3":"Cross-Matching Surveys","lvl2":"Merging and Joining Astronomical Catalogs"},"type":"lvl3","url":"/pandas#cross-matching-surveys","position":26},{"hierarchy":{"lvl1":"Pandas: Data Science for Astronomy","lvl3":"Cross-Matching Surveys","lvl2":"Merging and Joining Astronomical Catalogs"},"content":"def catalog_merging():\n    \"\"\"Merge and join operations for multi-survey astronomy.\"\"\"\n    \n    # Simulate different surveys\n    n_objects = 5000\n    \n    # Optical survey (like SDSS)\n    optical = pd.DataFrame({\n        'objid': np.arange(1000, 1000 + n_objects),\n        'ra': np.random.uniform(150, 160, n_objects),\n        'dec': np.random.uniform(-5, 5, n_objects),\n        'mag_g': np.random.normal(20, 2, n_objects),\n        'mag_r': np.random.normal(19.5, 2, n_objects),\n        'mag_i': np.random.normal(19, 2, n_objects),\n        'photoz': np.random.gamma(2, 0.3, n_objects),\n        'survey': 'SDSS'\n    })\n    \n    # X-ray survey (like Chandra)\n    n_xray = 500\n    xray_idx = np.random.choice(n_objects, n_xray, replace=False)\n    xray = pd.DataFrame({\n        'source_id': np.arange(2000, 2000 + n_xray),\n        'ra': optical.iloc[xray_idx]['ra'].values + np.random.normal(0, 0.0003, n_xray),  # Small offset\n        'dec': optical.iloc[xray_idx]['dec'].values + np.random.normal(0, 0.0003, n_xray),\n        'flux_soft': np.random.lognormal(-13, 1, n_xray),\n        'flux_hard': np.random.lognormal(-13.5, 1, n_xray),\n        'hardness_ratio': np.random.uniform(-1, 1, n_xray),\n        'survey': 'Chandra'\n    })\n    \n    # Radio survey (like FIRST)\n    n_radio = 300\n    radio_idx = np.random.choice(n_objects, n_radio, replace=False)\n    radio = pd.DataFrame({\n        'name': [f'FIRST_J{i:06d}' for i in range(n_radio)],\n        'ra': optical.iloc[radio_idx]['ra'].values + np.random.normal(0, 0.0005, n_radio),\n        'dec': optical.iloc[radio_idx]['dec'].values + np.random.normal(0, 0.0005, n_radio),\n        'flux_1400mhz': np.random.lognormal(0, 2, n_radio),\n        'spectral_index': np.random.normal(-0.7, 0.3, n_radio),\n        'survey': 'FIRST'\n    })\n    \n    print(\"1. SURVEY SUMMARIES:\")\n    print(f\"Optical: {len(optical)} objects\")\n    print(f\"X-ray: {len(xray)} objects\")\n    print(f\"Radio: {len(radio)} objects\")\n    \n    print(\"\\n2. POSITIONAL CROSS-MATCHING:\")\n    \n    # Function for angular separation\n    def angular_separation(ra1, dec1, ra2, dec2):\n        \"\"\"Calculate angular separation in arcseconds.\"\"\"\n        # Simplified for small angles\n        cos_dec = np.cos(np.radians(dec1))\n        dra = (ra2 - ra1) * cos_dec\n        ddec = dec2 - dec1\n        return 3600 * np.sqrt(dra**2 + ddec**2)\n    \n    # Cross-match optical with X-ray\n    from sklearn.neighbors import BallTree\n    \n    # Convert to radians for BallTree\n    optical_coords = np.radians(optical[['ra', 'dec']].values)\n    xray_coords = np.radians(xray[['ra', 'dec']].values)\n    \n    # Build tree and query\n    tree = BallTree(optical_coords, metric='haversine')\n    max_sep_rad = np.radians(3/3600)  # 3 arcsec\n    \n    indices, distances = tree.query_radius(xray_coords, r=max_sep_rad, return_distance=True)\n    \n    # Create matched catalog\n    matches = []\n    for i, (idx_list, dist_list) in enumerate(zip(indices, distances)):\n        if len(idx_list) > 0:\n            # Take closest match\n            best_idx = idx_list[np.argmin(dist_list)]\n            matches.append({\n                'optical_idx': best_idx,\n                'xray_idx': i,\n                'separation_arcsec': np.degrees(dist_list[np.argmin(dist_list)]) * 3600\n            })\n    \n    match_df = pd.DataFrame(matches)\n    print(f\"Optical-Xray matches: {len(match_df)} / {len(xray)}\")\n    \n    # Merge matched catalogs\n    optical_xray = optical.iloc[match_df['optical_idx'].values].reset_index(drop=True)\n    xray_matched = xray.iloc[match_df['xray_idx'].values].reset_index(drop=True)\n    \n    combined = pd.concat([optical_xray, xray_matched.drop(['ra', 'dec', 'survey'], axis=1)], axis=1)\n    combined['separation'] = match_df['separation_arcsec'].values\n    \n    print(f\"Combined catalog shape: {combined.shape}\")\n    \n    print(\"\\n3. DIFFERENT JOIN TYPES:\")\n    \n    # Create simplified catalogs for demonstration\n    cat1 = optical[['objid', 'ra', 'dec', 'mag_g']].head(100)\n    cat2 = pd.DataFrame({\n        'objid': optical['objid'].iloc[50:150].values,  # Partial overlap\n        'specz': np.random.gamma(2, 0.3, 100),\n        'quality': np.random.choice(['good', 'bad'], 100)\n    })\n    \n    # Inner join - only matched objects\n    inner = pd.merge(cat1, cat2, on='objid', how='inner')\n    print(f\"Inner join: {len(inner)} objects (both catalogs)\")\n    \n    # Left join - all from cat1\n    left = pd.merge(cat1, cat2, on='objid', how='left')\n    print(f\"Left join: {len(left)} objects (all from catalog 1)\")\n    print(f\"  With specz: {left['specz'].notna().sum()}\")\n    \n    # Outer join - all objects\n    outer = pd.merge(cat1, cat2, on='objid', how='outer')\n    print(f\"Outer join: {len(outer)} objects (union)\")\n    \n    print(\"\\n4. CONCATENATING SURVEYS:\")\n    \n    # Standardize column names\n    optical_std = optical[['ra', 'dec', 'survey']].copy()\n    optical_std['flux'] = 10**(-0.4 * optical['mag_r'])\n    \n    xray_std = xray[['ra', 'dec', 'survey']].copy()\n    xray_std['flux'] = xray['flux_soft']\n    \n    radio_std = radio[['ra', 'dec', 'survey']].copy()\n    radio_std['flux'] = radio['flux_1400mhz']\n    \n    # Concatenate all surveys\n    all_surveys = pd.concat([optical_std, xray_std, radio_std], \n                           ignore_index=True, sort=False)\n    \n    print(f\"Combined all surveys: {len(all_surveys)} total detections\")\n    print(all_surveys.groupby('survey')['flux'].describe())\n    \n    return combined, all_surveys\n\nmatched_catalog, all_surveys = catalog_merging()","type":"content","url":"/pandas#cross-matching-surveys","position":27},{"hierarchy":{"lvl1":"Pandas: Data Science for Astronomy","lvl2":"Performance Optimization for Large Catalogs"},"type":"lvl2","url":"/pandas#performance-optimization-for-large-catalogs","position":28},{"hierarchy":{"lvl1":"Pandas: Data Science for Astronomy","lvl2":"Performance Optimization for Large Catalogs"},"content":"","type":"content","url":"/pandas#performance-optimization-for-large-catalogs","position":29},{"hierarchy":{"lvl1":"Pandas: Data Science for Astronomy","lvl3":"Scaling to Big Data","lvl2":"Performance Optimization for Large Catalogs"},"type":"lvl3","url":"/pandas#scaling-to-big-data","position":30},{"hierarchy":{"lvl1":"Pandas: Data Science for Astronomy","lvl3":"Scaling to Big Data","lvl2":"Performance Optimization for Large Catalogs"},"content":"def performance_optimization():\n    \"\"\"Optimize Pandas for large astronomical catalogs.\"\"\"\n    \n    print(\"1. MEMORY OPTIMIZATION:\")\n    \n    # Create large catalog\n    n = 1_000_000\n    \n    # Bad: default dtypes\n    catalog_bad = pd.DataFrame({\n        'id': np.arange(n),  # int64 (8 bytes)\n        'ra': np.random.uniform(0, 360, n),  # float64 (8 bytes)\n        'dec': np.random.uniform(-90, 90, n),  # float64\n        'mag': np.random.uniform(10, 25, n),  # float64\n        'flag': np.random.choice([0, 1], n),  # int64\n        'survey': np.random.choice(['SDSS', 'GAIA', 'WISE'], n)  # object\n    })\n    \n    memory_bad = catalog_bad.memory_usage(deep=True).sum() / 1e6\n    print(f\"Default dtypes: {memory_bad:.1f} MB\")\n    \n    # Good: optimized dtypes\n    catalog_good = pd.DataFrame({\n        'id': pd.array(np.arange(n), dtype='uint32'),  # 4 bytes\n        'ra': pd.array(np.random.uniform(0, 360, n), dtype='float32'),  # 4 bytes\n        'dec': pd.array(np.random.uniform(-90, 90, n), dtype='float32'),\n        'mag': pd.array(np.random.uniform(10, 25, n), dtype='float32'),\n        'flag': pd.array(np.random.choice([0, 1], n), dtype='bool'),  # 1 byte\n        'survey': pd.Categorical(np.random.choice(['SDSS', 'GAIA', 'WISE'], n))  # Categorical\n    })\n    \n    memory_good = catalog_good.memory_usage(deep=True).sum() / 1e6\n    print(f\"Optimized dtypes: {memory_good:.1f} MB\")\n    print(f\"Memory saved: {(1 - memory_good/memory_bad)*100:.1f}%\")\n    \n    print(\"\\n2. CHUNKING FOR LARGE FILES:\")\n    \n    def process_large_catalog(filename, chunksize=10000):\n        \"\"\"Process large catalog in chunks.\"\"\"\n        \n        results = []\n        \n        # Simulate reading chunks\n        for chunk_id in range(3):  # Normally: pd.read_csv(filename, chunksize=chunksize)\n            # Simulate chunk\n            chunk = pd.DataFrame({\n                'ra': np.random.uniform(0, 360, chunksize),\n                'dec': np.random.uniform(-90, 90, chunksize),\n                'mag': np.random.uniform(15, 22, chunksize)\n            })\n            \n            # Process chunk\n            chunk_stats = {\n                'chunk_id': chunk_id,\n                'mean_mag': chunk['mag'].mean(),\n                'bright_count': (chunk['mag'] < 18).sum(),\n                'area_coverage': (chunk['ra'].max() - chunk['ra'].min()) * \n                                (chunk['dec'].max() - chunk['dec'].min())\n            }\n            \n            results.append(chunk_stats)\n        \n        return pd.DataFrame(results)\n    \n    chunk_results = process_large_catalog('large_catalog.csv')\n    print(\"Chunk processing results:\")\n    print(chunk_results)\n    \n    print(\"\\n3. QUERY OPTIMIZATION:\")\n    \n    # Create indexed DataFrame\n    catalog_indexed = catalog_good.set_index('id')\n    \n    # Slow: Python loop\n    import time\n    \n    start = time.time()\n    bright_loop = []\n    for idx, row in catalog_good.iterrows():\n        if row['mag'] < 15 and row['dec'] > 0:\n            bright_loop.append(row['id'])\n        if len(bright_loop) >= 100:\n            break\n    time_loop = time.time() - start\n    \n    # Fast: Vectorized query\n    start = time.time()\n    bright_vector = catalog_good.query('mag < 15 & dec > 0')['id'].head(100).tolist()\n    time_vector = time.time() - start\n    \n    print(f\"Loop time: {time_loop:.4f}s\")\n    print(f\"Vectorized time: {time_vector:.4f}s\")\n    print(f\"Speedup: {time_loop/time_vector:.1f}x\")\n    \n    print(\"\\n4. PARALLEL PROCESSING:\")\n    \n    # For CPU-bound operations\n    import multiprocessing as mp\n    from functools import partial\n    \n    def calculate_color(group):\n        \"\"\"Calculate color for group of objects.\"\"\"\n        return group['mag'].mean() - group['mag'].median()\n    \n    # Split data for parallel processing\n    n_cores = mp.cpu_count()\n    chunks = np.array_split(catalog_good, n_cores)\n    \n    # Parallel apply (conceptual - actual implementation would use Dask)\n    print(f\"Would process on {n_cores} cores\")\n    \n    print(\"\\n5. USING DASK FOR SCALING:\")\n    \n    # Conceptual Dask usage\n    print(\"\"\"\n    import dask.dataframe as dd\n    \n    # Read large catalog\n    ddf = dd.read_csv('huge_catalog.csv', blocksize='100MB')\n    \n    # Operations are lazy\n    result = ddf[ddf.mag < 20].groupby('survey').mag.mean()\n    \n    # Compute when needed\n    result.compute()\n    \"\"\")\n    \n    return catalog_good\n\noptimized_catalog = performance_optimization()","type":"content","url":"/pandas#scaling-to-big-data","position":31},{"hierarchy":{"lvl1":"Pandas: Data Science for Astronomy","lvl2":"Try It Yourself"},"type":"lvl2","url":"/pandas#try-it-yourself","position":32},{"hierarchy":{"lvl1":"Pandas: Data Science for Astronomy","lvl2":"Try It Yourself"},"content":"","type":"content","url":"/pandas#try-it-yourself","position":33},{"hierarchy":{"lvl1":"Pandas: Data Science for Astronomy","lvl3":"Exercise 1: Complete Variable Star Classification Pipeline","lvl2":"Try It Yourself"},"type":"lvl3","url":"/pandas#exercise-1-complete-variable-star-classification-pipeline","position":34},{"hierarchy":{"lvl1":"Pandas: Data Science for Astronomy","lvl3":"Exercise 1: Complete Variable Star Classification Pipeline","lvl2":"Try It Yourself"},"content":"def variable_star_pipeline(lightcurves_df):\n    \"\"\"\n    Build a complete pipeline for variable star classification.\n    \n    Tasks:\n    1. Extract time series features (mean, std, skewness, etc.)\n    2. Find periods using Lomb-Scargle\n    3. Extract phase-folded features\n    4. Handle missing data and outliers\n    5. Prepare features for ML classification\n    6. Split into train/test maintaining class balance\n    \n    Returns feature matrix ready for sklearn classifiers.\n    \"\"\"\n    # Your code here\n    pass","type":"content","url":"/pandas#exercise-1-complete-variable-star-classification-pipeline","position":35},{"hierarchy":{"lvl1":"Pandas: Data Science for Astronomy","lvl3":"Exercise 2: Multi-Survey Catalog Integration","lvl2":"Try It Yourself"},"type":"lvl3","url":"/pandas#exercise-2-multi-survey-catalog-integration","position":36},{"hierarchy":{"lvl1":"Pandas: Data Science for Astronomy","lvl3":"Exercise 2: Multi-Survey Catalog Integration","lvl2":"Try It Yourself"},"content":"def integrate_surveys(optical_df, xray_df, radio_df, ir_df):\n    \"\"\"\n    Integrate multiple astronomical surveys.\n    \n    Requirements:\n    1. Cross-match by position (handle different coordinate precisions)\n    2. Handle different column names and units\n    3. Create unified photometry columns\n    4. Calculate multi-wavelength colors\n    5. Flag objects detected in multiple surveys\n    6. Handle missing data appropriately\n    \n    Return integrated catalog with source classification.\n    \"\"\"\n    # Your code here\n    pass","type":"content","url":"/pandas#exercise-2-multi-survey-catalog-integration","position":37},{"hierarchy":{"lvl1":"Pandas: Data Science for Astronomy","lvl3":"Exercise 3: Galaxy Cluster Analysis","lvl2":"Try It Yourself"},"type":"lvl3","url":"/pandas#exercise-3-galaxy-cluster-analysis","position":38},{"hierarchy":{"lvl1":"Pandas: Data Science for Astronomy","lvl3":"Exercise 3: Galaxy Cluster Analysis","lvl2":"Try It Yourself"},"content":"def analyze_galaxy_cluster(galaxies_df):\n    \"\"\"\n    Comprehensive galaxy cluster analysis.\n    \n    Tasks:\n    1. Identify cluster members using redshift\n    2. Calculate velocity dispersion\n    3. Estimate cluster mass (virial theorem)\n    4. Find red sequence in color-magnitude diagram\n    5. Identify BCG (brightest cluster galaxy)\n    6. Calculate radial profiles\n    7. Prepare data for ML substructure detection\n    \"\"\"\n    # Your code here\n    pass","type":"content","url":"/pandas#exercise-3-galaxy-cluster-analysis","position":39},{"hierarchy":{"lvl1":"Pandas: Data Science for Astronomy","lvl2":"Key Takeaways"},"type":"lvl2","url":"/pandas#key-takeaways","position":40},{"hierarchy":{"lvl1":"Pandas: Data Science for Astronomy","lvl2":"Key Takeaways"},"content":"✅ Pandas bridges astronomy and data science - From catalogs to ML pipelines✅ DataFrames handle heterogeneous data - Mixed types, missing values, metadata✅ GroupBy enables population studies - Statistics by galaxy type, stellar population✅ Time series tools for light curves - Resampling, rolling windows, feature extraction✅ Efficient merging for multi-survey science - Cross-matching, joining, concatenating✅ Feature engineering for ML - From raw observations to ML-ready features✅ Memory optimization crucial - Use appropriate dtypes, chunking, Dask for scaling✅ Vectorization is key - Avoid loops, use query(), apply(), transform()","type":"content","url":"/pandas#key-takeaways","position":41},{"hierarchy":{"lvl1":"Pandas: Data Science for Astronomy","lvl2":"Connecting to Your Course"},"type":"lvl2","url":"/pandas#connecting-to-your-course","position":42},{"hierarchy":{"lvl1":"Pandas: Data Science for Astronomy","lvl2":"Connecting to Your Course"},"content":"This Pandas knowledge directly enables:\n\nProject 3: Statistical analysis of simulation outputs\n\nProject 4: Managing Monte Carlo results\n\nProject 5: Preparing data for neural networks\n\nFinal Project: Professional data science workflows\n\nCombined with NumPy, SciPy, and Matplotlib, you now have the complete toolkit for modern astronomical data science, preparing you for both research and industry applications.","type":"content","url":"/pandas#connecting-to-your-course","position":43},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)"},"type":"lvl1","url":"/advanced","position":0},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)"},"content":"This chapter provides a sampling of advanced Python topics. Choose sections relevant to your projects.","type":"content","url":"/advanced","position":1},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl2":"Learning Objectives"},"type":"lvl2","url":"/advanced#learning-objectives","position":2},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl2":"Learning Objectives"},"content":"This optional chapter introduces:\n\nAsync programming for concurrent I/O operations\n\nAdvanced decorators and descriptors\n\nContext managers for resource management\n\nType hints and static typing\n\nMetaclasses and introspection\n\nPackage development and distribution","type":"content","url":"/advanced#learning-objectives","position":3},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl2":"8.1 Async Programming for Concurrent Operations"},"type":"lvl2","url":"/advanced#id-8-1-async-programming-for-concurrent-operations","position":4},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl2":"8.1 Async Programming for Concurrent Operations"},"content":"","type":"content","url":"/advanced#id-8-1-async-programming-for-concurrent-operations","position":5},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl3":"When Async Makes Sense","lvl2":"8.1 Async Programming for Concurrent Operations"},"type":"lvl3","url":"/advanced#when-async-makes-sense","position":6},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl3":"When Async Makes Sense","lvl2":"8.1 Async Programming for Concurrent Operations"},"content":"import asyncio\nimport aiohttp\nimport time\n\n# Synchronous version - slow for I/O bound tasks\ndef fetch_catalog_sync(catalog_ids):\n    \"\"\"Fetch multiple catalogs synchronously.\"\"\"\n    results = []\n    for cat_id in catalog_ids:\n        # Simulate HTTP request\n        time.sleep(0.5)  # Network delay\n        results.append(f\"Catalog {cat_id} data\")\n    return results\n\n# Asynchronous version - much faster for I/O\nasync def fetch_catalog_async(session, catalog_id):\n    \"\"\"Fetch single catalog asynchronously.\"\"\"\n    # Simulate async HTTP request\n    await asyncio.sleep(0.5)  # Network delay\n    return f\"Catalog {catalog_id} data\"\n\nasync def fetch_all_catalogs(catalog_ids):\n    \"\"\"Fetch multiple catalogs concurrently.\"\"\"\n    # In real code, use aiohttp.ClientSession()\n    tasks = []\n    async with aiohttp.ClientSession() as session:\n        for cat_id in catalog_ids:\n            task = fetch_catalog_async(session, cat_id)\n            tasks.append(task)\n        \n        results = await asyncio.gather(*tasks)\n    return results\n\n# Telescope control example\nclass AsyncTelescopeController:\n    \"\"\"Control telescope with async operations.\"\"\"\n    \n    def __init__(self):\n        self.position = {'ra': 0, 'dec': 0}\n        self.filter = 'V'\n        self.camera_ready = False\n    \n    async def slew_to(self, ra, dec):\n        \"\"\"Slew telescope to position.\"\"\"\n        print(f\"Starting slew to RA={ra}, Dec={dec}\")\n        await asyncio.sleep(3)  # Simulate slew time\n        self.position = {'ra': ra, 'dec': dec}\n        print(\"Slew complete\")\n    \n    async def change_filter(self, filter_name):\n        \"\"\"Change filter wheel.\"\"\"\n        print(f\"Changing filter to {filter_name}\")\n        await asyncio.sleep(1)\n        self.filter = filter_name\n        print(\"Filter changed\")\n    \n    async def prepare_camera(self):\n        \"\"\"Prepare CCD camera.\"\"\"\n        print(\"Preparing camera\")\n        await asyncio.sleep(2)\n        self.camera_ready = True\n        print(\"Camera ready\")\n    \n    async def observe_target(self, ra, dec, filter_name, exposure):\n        \"\"\"Complete observation sequence.\"\"\"\n        # Run preparation tasks concurrently\n        tasks = [\n            self.slew_to(ra, dec),\n            self.change_filter(filter_name),\n            self.prepare_camera()\n        ]\n        \n        await asyncio.gather(*tasks)\n        \n        # Take exposure\n        print(f\"Starting {exposure}s exposure\")\n        await asyncio.sleep(exposure / 10)  # Simulate faster\n        print(\"Exposure complete\")\n        \n        return {'ra': ra, 'dec': dec, 'filter': filter_name, 'data': 'image_data'}\n\n# Example usage\nasync def observation_sequence():\n    \"\"\"Run a sequence of observations.\"\"\"\n    telescope = AsyncTelescopeController()\n    \n    targets = [\n        (10.68, 41.27, 'V', 300),  # M31\n        (5.58, -5.39, 'R', 600),   # M42\n        (13.42, -11.16, 'B', 450)  # M104\n    ]\n    \n    results = []\n    for ra, dec, filt, exp in targets:\n        result = await telescope.observe_target(ra, dec, filt, exp)\n        results.append(result)\n    \n    return results\n\n# Run async code\n# asyncio.run(observation_sequence())","type":"content","url":"/advanced#when-async-makes-sense","position":7},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl3":"Async for Real-Time Data Streams","lvl2":"8.1 Async Programming for Concurrent Operations"},"type":"lvl3","url":"/advanced#async-for-real-time-data-streams","position":8},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl3":"Async for Real-Time Data Streams","lvl2":"8.1 Async Programming for Concurrent Operations"},"content":"class AsyncDataStreamProcessor:\n    \"\"\"Process real-time data streams asynchronously.\"\"\"\n    \n    async def generate_telemetry(self, n_points=100):\n        \"\"\"Simulate telemetry data stream.\"\"\"\n        for i in range(n_points):\n            await asyncio.sleep(0.1)  # Data arrives every 100ms\n            \n            data = {\n                'timestamp': time.time(),\n                'temperature': 20 + np.random.randn(),\n                'pressure': 1013 + np.random.randn() * 10,\n                'seeing': 0.8 + np.random.random() * 0.5\n            }\n            yield data\n    \n    async def process_stream(self):\n        \"\"\"Process incoming data stream.\"\"\"\n        buffer = []\n        \n        async for data in self.generate_telemetry():\n            buffer.append(data)\n            \n            # Process every 10 points\n            if len(buffer) >= 10:\n                await self.analyze_buffer(buffer)\n                buffer = []\n    \n    async def analyze_buffer(self, buffer):\n        \"\"\"Analyze buffered data.\"\"\"\n        temps = [d['temperature'] for d in buffer]\n        seeing = [d['seeing'] for d in buffer]\n        \n        print(f\"Average temp: {np.mean(temps):.1f}°C, \"\n              f\"seeing: {np.mean(seeing):.2f}\\\"\")\n\n# Run: asyncio.run(AsyncDataStreamProcessor().process_stream())","type":"content","url":"/advanced#async-for-real-time-data-streams","position":9},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl2":"8.2 Advanced Decorators"},"type":"lvl2","url":"/advanced#id-8-2-advanced-decorators","position":10},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl2":"8.2 Advanced Decorators"},"content":"","type":"content","url":"/advanced#id-8-2-advanced-decorators","position":11},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl3":"Parametrized Decorators","lvl2":"8.2 Advanced Decorators"},"type":"lvl3","url":"/advanced#parametrized-decorators","position":12},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl3":"Parametrized Decorators","lvl2":"8.2 Advanced Decorators"},"content":"def retry(max_attempts=3, delay=1.0, exceptions=(Exception,)):\n    \"\"\"\n    Decorator to retry failed operations.\n    \n    Useful for network requests, hardware control, etc.\n    \"\"\"\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            for attempt in range(max_attempts):\n                try:\n                    return func(*args, **kwargs)\n                except exceptions as e:\n                    if attempt == max_attempts - 1:\n                        raise\n                    print(f\"Attempt {attempt + 1} failed: {e}\")\n                    time.sleep(delay)\n            return None\n        return wrapper\n    return decorator\n\n@retry(max_attempts=3, delay=0.5, exceptions=(ConnectionError, TimeoutError))\ndef fetch_observation_data(obs_id):\n    \"\"\"Fetch observation with automatic retry.\"\"\"\n    if np.random.random() < 0.7:  # 70% chance of failure\n        raise ConnectionError(\"Network error\")\n    return f\"Data for observation {obs_id}\"\n\n# Test retry decorator\n# try:\n#     data = fetch_observation_data(\"OBS123\")\n#     print(f\"Success: {data}\")\n# except Exception as e:\n#     print(f\"Failed after retries: {e}\")","type":"content","url":"/advanced#parametrized-decorators","position":13},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl3":"Class Decorators","lvl2":"8.2 Advanced Decorators"},"type":"lvl3","url":"/advanced#class-decorators","position":14},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl3":"Class Decorators","lvl2":"8.2 Advanced Decorators"},"content":"def add_validation(cls):\n    \"\"\"Class decorator to add validation to all setters.\"\"\"\n    \n    original_setattr = cls.__setattr__\n    \n    def validated_setattr(self, name, value):\n        # Check if there's a validator method\n        validator_name = f'validate_{name}'\n        if hasattr(self, validator_name):\n            validator = getattr(self, validator_name)\n            value = validator(value)\n        \n        original_setattr(self, name, value)\n    \n    cls.__setattr__ = validated_setattr\n    return cls\n\n@add_validation\nclass ValidatedStar:\n    \"\"\"Star with automatic validation.\"\"\"\n    \n    def __init__(self, mass, temperature):\n        self.mass = mass\n        self.temperature = temperature\n    \n    def validate_mass(self, value):\n        \"\"\"Validate stellar mass.\"\"\"\n        if value <= 0:\n            raise ValueError(\"Mass must be positive\")\n        if value > 150:\n            raise ValueError(\"Mass exceeds stellar limit\")\n        return value\n    \n    def validate_temperature(self, value):\n        \"\"\"Validate temperature.\"\"\"\n        if value < 2000:\n            raise ValueError(\"Temperature too low for star\")\n        if value > 50000:\n            raise ValueError(\"Temperature too high\")\n        return value\n\n# Validation happens automatically\nstar = ValidatedStar(1.0, 5778)\n# star.mass = -1  # Raises ValueError","type":"content","url":"/advanced#class-decorators","position":15},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl2":"8.3 Descriptors and Properties"},"type":"lvl2","url":"/advanced#id-8-3-descriptors-and-properties","position":16},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl2":"8.3 Descriptors and Properties"},"content":"","type":"content","url":"/advanced#id-8-3-descriptors-and-properties","position":17},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl3":"Custom Descriptors","lvl2":"8.3 Descriptors and Properties"},"type":"lvl3","url":"/advanced#custom-descriptors","position":18},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl3":"Custom Descriptors","lvl2":"8.3 Descriptors and Properties"},"content":"class PhysicalQuantity:\n    \"\"\"Descriptor for physical quantities with units.\"\"\"\n    \n    def __init__(self, name, unit, min_value=None, max_value=None):\n        self.name = name\n        self.unit = unit\n        self.min_value = min_value\n        self.max_value = max_value\n        self.data = {}\n    \n    def __get__(self, instance, owner):\n        if instance is None:\n            return self\n        return self.data.get(id(instance), None)\n    \n    def __set__(self, instance, value):\n        if self.min_value is not None and value < self.min_value:\n            raise ValueError(f\"{self.name} must be >= {self.min_value} {self.unit}\")\n        if self.max_value is not None and value > self.max_value:\n            raise ValueError(f\"{self.name} must be <= {self.max_value} {self.unit}\")\n        \n        self.data[id(instance)] = value\n    \n    def __delete__(self, instance):\n        del self.data[id(instance)]\n\nclass Telescope:\n    \"\"\"Telescope with physical quantity descriptors.\"\"\"\n    \n    aperture = PhysicalQuantity('aperture', 'm', min_value=0.1, max_value=100)\n    focal_length = PhysicalQuantity('focal_length', 'm', min_value=0.1)\n    elevation = PhysicalQuantity('elevation', 'degrees', min_value=0, max_value=90)\n    \n    def __init__(self, aperture, focal_length):\n        self.aperture = aperture\n        self.focal_length = focal_length\n        self.elevation = 45  # Default\n    \n    @property\n    def f_ratio(self):\n        \"\"\"Calculate f-ratio.\"\"\"\n        return self.focal_length / self.aperture\n\n# Use descriptors\nscope = Telescope(2.4, 57.6)  # Hubble\nprint(f\"f-ratio: f/{scope.f_ratio:.1f}\")\n# scope.aperture = -1  # Raises ValueError","type":"content","url":"/advanced#custom-descriptors","position":19},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl2":"8.4 Context Managers"},"type":"lvl2","url":"/advanced#id-8-4-context-managers","position":20},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl2":"8.4 Context Managers"},"content":"","type":"content","url":"/advanced#id-8-4-context-managers","position":21},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl3":"Custom Context Managers","lvl2":"8.4 Context Managers"},"type":"lvl3","url":"/advanced#custom-context-managers","position":22},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl3":"Custom Context Managers","lvl2":"8.4 Context Managers"},"content":"class ObservationSession:\n    \"\"\"Context manager for observation sessions.\"\"\"\n    \n    def __init__(self, observer, target):\n        self.observer = observer\n        self.target = target\n        self.start_time = None\n        self.log = []\n    \n    def __enter__(self):\n        \"\"\"Start observation session.\"\"\"\n        self.start_time = time.time()\n        self.log.append(f\"Session started for {self.target}\")\n        print(f\"Beginning observation of {self.target}\")\n        \n        # Initialize equipment\n        self._initialize_equipment()\n        \n        return self\n    \n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"End observation session.\"\"\"\n        elapsed = time.time() - self.start_time\n        \n        if exc_type is not None:\n            self.log.append(f\"Session failed: {exc_val}\")\n            print(f\"Error during observation: {exc_val}\")\n        else:\n            self.log.append(f\"Session completed successfully\")\n        \n        self.log.append(f\"Duration: {elapsed:.1f} seconds\")\n        \n        # Cleanup\n        self._shutdown_equipment()\n        self._save_log()\n        \n        # Return False to propagate exceptions\n        return False\n    \n    def _initialize_equipment(self):\n        \"\"\"Initialize telescope and camera.\"\"\"\n        print(\"  Initializing equipment...\")\n        time.sleep(0.5)\n    \n    def _shutdown_equipment(self):\n        \"\"\"Safely shutdown equipment.\"\"\"\n        print(\"  Shutting down equipment...\")\n        time.sleep(0.5)\n    \n    def _save_log(self):\n        \"\"\"Save observation log.\"\"\"\n        print(f\"  Log saved: {len(self.log)} entries\")\n    \n    def take_exposure(self, duration):\n        \"\"\"Take an exposure.\"\"\"\n        self.log.append(f\"Exposure: {duration}s\")\n        time.sleep(duration / 10)  # Simulate\n        return f\"Image data for {duration}s exposure\"\n\n# Use context manager\nwith ObservationSession(\"Observer1\", \"M31\") as session:\n    image1 = session.take_exposure(30)\n    image2 = session.take_exposure(60)\n    print(f\"  Captured 2 images\")\n\n# Equipment is automatically cleaned up","type":"content","url":"/advanced#custom-context-managers","position":23},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl3":"Contextlib Utilities","lvl2":"8.4 Context Managers"},"type":"lvl3","url":"/advanced#contextlib-utilities","position":24},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl3":"Contextlib Utilities","lvl2":"8.4 Context Managers"},"content":"from contextlib import contextmanager, suppress\n\n@contextmanager\ndef temporary_seed(seed):\n    \"\"\"Temporarily set random seed.\"\"\"\n    state = np.random.get_state()\n    np.random.seed(seed)\n    try:\n        yield\n    finally:\n        np.random.set_state(state)\n\n# Use temporary seed\nprint(\"Random without seed:\", np.random.random())\n\nwith temporary_seed(42):\n    print(\"Random with seed 42:\", np.random.random())\n    print(\"Again with seed 42:\", np.random.random())\n\nprint(\"Random without seed:\", np.random.random())\n\n# Suppress specific exceptions\nwith suppress(FileNotFoundError):\n    os.remove('nonexistent_file.txt')  # Doesn't raise error","type":"content","url":"/advanced#contextlib-utilities","position":25},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl2":"8.5 Type Hints and Static Typing"},"type":"lvl2","url":"/advanced#id-8-5-type-hints-and-static-typing","position":26},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl2":"8.5 Type Hints and Static Typing"},"content":"","type":"content","url":"/advanced#id-8-5-type-hints-and-static-typing","position":27},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl3":"Advanced Type Hints","lvl2":"8.5 Type Hints and Static Typing"},"type":"lvl3","url":"/advanced#advanced-type-hints","position":28},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl3":"Advanced Type Hints","lvl2":"8.5 Type Hints and Static Typing"},"content":"from typing import (\n    Union, Optional, List, Dict, Tuple, \n    TypeVar, Generic, Protocol, Literal,\n    overload, cast\n)\nfrom typing_extensions import TypedDict\n\n# Type variables for generics\nT = TypeVar('T', bound=float)\n\nclass Spectrum(Generic[T]):\n    \"\"\"Generic spectrum class.\"\"\"\n    \n    def __init__(self, wavelengths: List[T], fluxes: List[T]) -> None:\n        self.wavelengths = wavelengths\n        self.fluxes = fluxes\n    \n    def normalize(self) -> 'Spectrum[T]':\n        \"\"\"Normalize spectrum.\"\"\"\n        max_flux = max(self.fluxes)\n        normalized = [f/max_flux for f in self.fluxes]\n        return Spectrum(self.wavelengths, normalized)\n\n# Typed dictionaries for structured data\nclass ObservationData(TypedDict):\n    \"\"\"Type hints for observation dictionary.\"\"\"\n    target: str\n    ra: float\n    dec: float\n    filter: Literal['U', 'B', 'V', 'R', 'I']\n    exposure: float\n    airmass: Optional[float]\n\ndef process_observation(data: ObservationData) -> Dict[str, float]:\n    \"\"\"Process observation with type checking.\"\"\"\n    # Type checker knows the structure\n    magnitude = -2.5 * np.log10(data['exposure'])\n    \n    if data['airmass'] is not None:\n        magnitude += 0.2 * data['airmass']  # Extinction\n    \n    return {'magnitude': magnitude}\n\n# Protocol for duck typing with types\nclass Plottable(Protocol):\n    \"\"\"Protocol for objects that can be plotted.\"\"\"\n    \n    def get_x_data(self) -> List[float]: ...\n    def get_y_data(self) -> List[float]: ...\n    def get_label(self) -> str: ...\n\ndef plot_data(obj: Plottable) -> None:\n    \"\"\"Plot any object following Plottable protocol.\"\"\"\n    x = obj.get_x_data()\n    y = obj.get_y_data()\n    label = obj.get_label()\n    # plt.plot(x, y, label=label)\n\n# Function overloading\n@overload\ndef load_data(filename: str) -> np.ndarray: ...\n\n@overload\ndef load_data(filename: str, return_header: Literal[True]) -> Tuple[np.ndarray, Dict]: ...\n\ndef load_data(filename, return_header=False):\n    \"\"\"Load data with optional header.\"\"\"\n    data = np.random.randn(100, 100)\n    header = {'OBJECT': 'M31', 'EXPTIME': 300}\n    \n    if return_header:\n        return data, header\n    return data","type":"content","url":"/advanced#advanced-type-hints","position":29},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl2":"8.6 Metaclasses and Introspection"},"type":"lvl2","url":"/advanced#id-8-6-metaclasses-and-introspection","position":30},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl2":"8.6 Metaclasses and Introspection"},"content":"","type":"content","url":"/advanced#id-8-6-metaclasses-and-introspection","position":31},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl3":"Understanding Metaclasses","lvl2":"8.6 Metaclasses and Introspection"},"type":"lvl3","url":"/advanced#understanding-metaclasses","position":32},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl3":"Understanding Metaclasses","lvl2":"8.6 Metaclasses and Introspection"},"content":"class SingletonMeta(type):\n    \"\"\"Metaclass for creating singleton classes.\"\"\"\n    \n    _instances = {}\n    \n    def __call__(cls, *args, **kwargs):\n        if cls not in cls._instances:\n            cls._instances[cls] = super().__call__(*args, **kwargs)\n        return cls._instances[cls]\n\nclass ObservatoryConfig(metaclass=SingletonMeta):\n    \"\"\"Singleton configuration class.\"\"\"\n    \n    def __init__(self):\n        self.location = \"Palomar\"\n        self.latitude = 33.36\n        self.longitude = -116.86\n        self.elevation = 1706\n        print(\"Creating config instance\")\n\n# Only one instance ever created\nconfig1 = ObservatoryConfig()\nconfig2 = ObservatoryConfig()\nprint(f\"Same instance? {config1 is config2}\")\n\n# Automatic registration metaclass\nclass RegisteredMeta(type):\n    \"\"\"Metaclass that registers all subclasses.\"\"\"\n    \n    registry = {}\n    \n    def __new__(mcs, name, bases, namespace):\n        cls = super().__new__(mcs, name, bases, namespace)\n        \n        # Register non-base classes\n        if bases:\n            mcs.registry[name] = cls\n        \n        return cls\n\nclass Instrument(metaclass=RegisteredMeta):\n    \"\"\"Base instrument class.\"\"\"\n    pass\n\nclass CCD(Instrument):\n    \"\"\"CCD camera.\"\"\"\n    pass\n\nclass Spectrograph(Instrument):\n    \"\"\"Spectrograph.\"\"\"\n    pass\n\nprint(f\"Registered instruments: {list(Instrument.registry.keys())}\")","type":"content","url":"/advanced#understanding-metaclasses","position":33},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl3":"Introspection and Reflection","lvl2":"8.6 Metaclasses and Introspection"},"type":"lvl3","url":"/advanced#introspection-and-reflection","position":34},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl3":"Introspection and Reflection","lvl2":"8.6 Metaclasses and Introspection"},"content":"import inspect\n\nclass IntrospectableObject:\n    \"\"\"Object with introspection capabilities.\"\"\"\n    \n    def __init__(self, **kwargs):\n        for key, value in kwargs.items():\n            setattr(self, key, value)\n    \n    def introspect(self):\n        \"\"\"Examine object's attributes and methods.\"\"\"\n        print(f\"Class: {self.__class__.__name__}\")\n        print(f\"Module: {self.__class__.__module__}\")\n        \n        print(\"\\nAttributes:\")\n        for name, value in inspect.getmembers(self):\n            if not name.startswith('_') and not inspect.ismethod(value):\n                print(f\"  {name}: {value}\")\n        \n        print(\"\\nMethods:\")\n        for name, method in inspect.getmembers(self, predicate=inspect.ismethod):\n            if not name.startswith('_'):\n                sig = inspect.signature(method)\n                print(f\"  {name}{sig}\")\n    \n    def describe_method(self, method_name):\n        \"\"\"Get detailed info about a method.\"\"\"\n        method = getattr(self, method_name)\n        sig = inspect.signature(method)\n        \n        print(f\"Method: {method_name}\")\n        print(f\"Signature: {sig}\")\n        print(f\"Parameters:\")\n        \n        for param_name, param in sig.parameters.items():\n            print(f\"  {param_name}: {param.annotation if param.annotation != param.empty else 'Any'}\")\n        \n        if method.__doc__:\n            print(f\"Docstring: {method.__doc__.strip()}\")\n\n# Dynamic class creation\ndef create_filter_class(filter_name, wavelength):\n    \"\"\"Dynamically create filter classes.\"\"\"\n    \n    def __init__(self):\n        self.name = filter_name\n        self.wavelength = wavelength\n    \n    def info(self):\n        return f\"{self.name} filter at {self.wavelength}nm\"\n    \n    # Create class dynamically\n    FilterClass = type(\n        f'{filter_name}Filter',\n        (object,),\n        {\n            '__init__': __init__,\n            'info': info,\n            'filter_type': filter_name\n        }\n    )\n    \n    return FilterClass\n\n# Create filter classes dynamically\nVFilter = create_filter_class('V', 550)\nRFilter = create_filter_class('R', 700)\n\nv_filter = VFilter()\nprint(f\"Dynamic class: {v_filter.info()}\")\n\n## 8.7 Packaging and Distribution\n\n### Creating a Python Package\n\n```python\n# Example package structure for an astronomy library\n\"\"\"\nastrotools/\n├── setup.py\n├── setup.cfg\n├── pyproject.toml\n├── README.md\n├── LICENSE\n├── requirements.txt\n├── astrotools/\n│   ├── __init__.py\n│   ├── photometry/\n│   │   ├── __init__.py\n│   │   ├── aperture.py\n│   │   └── psf.py\n│   ├── spectroscopy/\n│   │   ├── __init__.py\n│   │   ├── extraction.py\n│   │   └── calibration.py\n│   └── utils/\n│       ├── __init__.py\n│       └── coordinates.py\n└── tests/\n    ├── test_photometry.py\n    └── test_spectroscopy.py\n\"\"\"\n\n# setup.py example\nSETUP_PY = \"\"\"\nfrom setuptools import setup, find_packages\n\nwith open(\"README.md\", \"r\") as fh:\n    long_description = fh.read()\n\nsetup(\n    name=\"astrotools\",\n    version=\"0.1.0\",\n    author=\"Your Name\",\n    author_email=\"your.email@example.com\",\n    description=\"Astronomical data processing tools\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/yourusername/astrotools\",\n    packages=find_packages(),\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n        \"Intended Audience :: Science/Research\",\n        \"Topic :: Scientific/Engineering :: Astronomy\",\n    ],\n    python_requires=\">=3.8\",\n    install_requires=[\n        \"numpy>=1.20\",\n        \"scipy>=1.7\",\n        \"astropy>=5.0\",\n    ],\n    extras_require={\n        \"dev\": [\"pytest\", \"black\", \"flake8\"],\n        \"docs\": [\"sphinx\", \"sphinx-rtd-theme\"],\n    },\n    entry_points={\n        \"console_scripts\": [\n            \"process-spectrum=astrotools.scripts.process:main\",\n        ],\n    },\n)\n\"\"\"\n\n# pyproject.toml for modern packaging\nPYPROJECT_TOML = \"\"\"\n[build-system]\nrequires = [\"setuptools>=45\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"astrotools\"\nversion = \"0.1.0\"\ndescription = \"Astronomical data processing tools\"\nauthors = [{name = \"Your Name\", email = \"your.email@example.com\"}]\ndependencies = [\n    \"numpy>=1.20\",\n    \"scipy>=1.7\",\n    \"astropy>=5.0\",\n]\n\n[project.optional-dependencies]\ndev = [\"pytest\", \"black\", \"mypy\"]\ndocs = [\"sphinx\", \"sphinx-rtd-theme\"]\n\n[tool.black]\nline-length = 88\ntarget-version = ['py38']\n\n[tool.mypy]\npython_version = \"3.8\"\nwarn_return_any = true\nwarn_unused_configs = true\n\"\"\"\n\nprint(\"Package structure examples saved to variables SETUP_PY and PYPROJECT_TOML\")","type":"content","url":"/advanced#introspection-and-reflection","position":35},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl2":"8.8 Functional Programming Patterns"},"type":"lvl2","url":"/advanced#id-8-8-functional-programming-patterns","position":36},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl2":"8.8 Functional Programming Patterns"},"content":"","type":"content","url":"/advanced#id-8-8-functional-programming-patterns","position":37},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl3":"Functional Approaches in Scientific Computing","lvl2":"8.8 Functional Programming Patterns"},"type":"lvl3","url":"/advanced#functional-approaches-in-scientific-computing","position":38},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl3":"Functional Approaches in Scientific Computing","lvl2":"8.8 Functional Programming Patterns"},"content":"from functools import reduce, partial\nfrom operator import add, mul\nimport itertools\n\nclass FunctionalAstronomy:\n    \"\"\"Functional programming patterns for astronomy.\"\"\"\n    \n    @staticmethod\n    def compose(*functions):\n        \"\"\"Compose multiple functions.\"\"\"\n        def inner(x):\n            return reduce(lambda v, f: f(v), functions, x)\n        return inner\n    \n    @staticmethod\n    def curry(func):\n        \"\"\"Curry a function for partial application.\"\"\"\n        def curried(*args, **kwargs):\n            if len(args) + len(kwargs) >= func.__code__.co_argcount:\n                return func(*args, **kwargs)\n            return partial(func, *args, **kwargs)\n        return curried\n    \n    @staticmethod\n    def pipeline_example():\n        \"\"\"Example: Processing pipeline with function composition.\"\"\"\n        \n        # Define processing steps\n        def load_spectrum(filename):\n            \"\"\"Load spectrum from file.\"\"\"\n            return np.random.randn(1000) + 100\n        \n        def remove_cosmic_rays(spectrum):\n            \"\"\"Remove cosmic ray hits.\"\"\"\n            cleaned = spectrum.copy()\n            cleaned[cleaned > np.percentile(cleaned, 99)] = np.median(cleaned)\n            return cleaned\n        \n        def normalize(spectrum):\n            \"\"\"Normalize to unit maximum.\"\"\"\n            return spectrum / np.max(spectrum)\n        \n        def smooth(window=5):\n            \"\"\"Return smoothing function.\"\"\"\n            def smoother(spectrum):\n                from scipy.ndimage import uniform_filter1d\n                return uniform_filter1d(spectrum, window)\n            return smoother\n        \n        # Compose pipeline\n        process = FunctionalAstronomy.compose(\n            load_spectrum,\n            remove_cosmic_rays,\n            normalize,\n            smooth(window=10)\n        )\n        \n        # Process data\n        result = process(\"spectrum.fits\")\n        return result\n    \n    @staticmethod\n    def lazy_evaluation_example():\n        \"\"\"Demonstrate lazy evaluation with generators.\"\"\"\n        \n        def fibonacci_orbit_periods():\n            \"\"\"Generate orbital periods following Fibonacci sequence.\"\"\"\n            a, b = 1, 1\n            while True:\n                yield a\n                a, b = b, a + b\n        \n        # Take only what we need\n        periods = itertools.islice(fibonacci_orbit_periods(), 10)\n        return list(periods)\n    \n    @staticmethod\n    @curry\n    def redshift_wavelength(z, rest_wavelength):\n        \"\"\"Curried function for redshift calculation.\"\"\"\n        return rest_wavelength * (1 + z)\n\n# Examples\nfa = FunctionalAstronomy()\n\n# Function composition\nresult = fa.pipeline_example()\nprint(f\"Pipeline result shape: {result.shape}\")\n\n# Currying\nredshift_z2 = fa.redshift_wavelength(2.0)  # Partial application\nh_alpha_z2 = redshift_z2(656.3)\nprint(f\"H-alpha at z=2: {h_alpha_z2:.1f} nm\")\n\n# Lazy evaluation\nfib_periods = fa.lazy_evaluation_example()\nprint(f\"Fibonacci periods: {fib_periods}\")","type":"content","url":"/advanced#functional-approaches-in-scientific-computing","position":39},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl2":"8.9 Working with Binary Data"},"type":"lvl2","url":"/advanced#id-8-9-working-with-binary-data","position":40},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl2":"8.9 Working with Binary Data"},"content":"","type":"content","url":"/advanced#id-8-9-working-with-binary-data","position":41},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl3":"Binary File Formats","lvl2":"8.9 Working with Binary Data"},"type":"lvl3","url":"/advanced#binary-file-formats","position":42},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl3":"Binary File Formats","lvl2":"8.9 Working with Binary Data"},"content":"import struct\n\nclass BinaryDataHandler:\n    \"\"\"Handle binary astronomical data formats.\"\"\"\n    \n    @staticmethod\n    def write_binary_catalog(filename, catalog):\n        \"\"\"Write catalog to binary format.\"\"\"\n        # Format: int32(n_objects), then for each object:\n        # float64(ra), float64(dec), float32(mag), int32(id)\n        \n        with open(filename, 'wb') as f:\n            # Write header\n            n_objects = len(catalog)\n            f.write(struct.pack('i', n_objects))\n            \n            # Write each object\n            for obj in catalog:\n                data = struct.pack(\n                    'ddfI',  # double, double, float, unsigned int\n                    obj['ra'],\n                    obj['dec'],\n                    obj['mag'],\n                    obj['id']\n                )\n                f.write(data)\n    \n    @staticmethod\n    def read_binary_catalog(filename):\n        \"\"\"Read catalog from binary format.\"\"\"\n        catalog = []\n        \n        with open(filename, 'rb') as f:\n            # Read header\n            n_objects = struct.unpack('i', f.read(4))[0]\n            \n            # Read each object\n            for _ in range(n_objects):\n                data = f.read(struct.calcsize('ddfI'))\n                ra, dec, mag, obj_id = struct.unpack('ddfI', data)\n                \n                catalog.append({\n                    'ra': ra,\n                    'dec': dec,\n                    'mag': mag,\n                    'id': obj_id\n                })\n        \n        return catalog\n    \n    @staticmethod\n    def memory_mapped_array_example():\n        \"\"\"Work with memory-mapped arrays.\"\"\"\n        \n        # Create a large memory-mapped array\n        filename = 'large_image.dat'\n        shape = (4096, 4096)\n        dtype = np.float32\n        \n        # Create and write\n        fp = np.memmap(filename, dtype=dtype, mode='w+', shape=shape)\n        fp[:] = np.random.randn(*shape)\n        del fp\n        \n        # Read specific sections without loading all\n        fp = np.memmap(filename, dtype=dtype, mode='r', shape=shape)\n        \n        # Only this section is loaded into memory\n        cutout = fp[1000:1100, 2000:2100]\n        print(f\"Cutout shape: {cutout.shape}, mean: {np.mean(cutout):.3f}\")\n        \n        del fp\n        os.remove(filename)\n\n# Test binary handling\nhandler = BinaryDataHandler()\n\n# Create test catalog\ntest_catalog = [\n    {'ra': 150.0 + i*0.1, 'dec': 30.0 + i*0.05, 'mag': 12.0 + i*0.2, 'id': i}\n    for i in range(10)\n]\n\n# Write and read\nhandler.write_binary_catalog('test.cat', test_catalog)\nloaded = handler.read_binary_catalog('test.cat')\nprint(f\"Loaded {len(loaded)} objects from binary catalog\")\nos.remove('test.cat')\n\nhandler.memory_mapped_array_example()","type":"content","url":"/advanced#binary-file-formats","position":43},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl2":"Try It Yourself"},"type":"lvl2","url":"/advanced#try-it-yourself","position":44},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl2":"Try It Yourself"},"content":"","type":"content","url":"/advanced#try-it-yourself","position":45},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl3":"Exercise 8.1: Async Observatory Controller","lvl2":"Try It Yourself"},"type":"lvl3","url":"/advanced#exercise-8-1-async-observatory-controller","position":46},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl3":"Exercise 8.1: Async Observatory Controller","lvl2":"Try It Yourself"},"content":"Build an async system for controlling multiple telescopes.class AsyncObservatory:\n    \"\"\"\n    Control multiple telescopes asynchronously.\n    \n    Requirements:\n    - Coordinate multiple telescopes\n    - Handle concurrent observations\n    - Manage shared resources (e.g., weather station)\n    - Implement error recovery\n    \"\"\"\n    \n    def __init__(self, n_telescopes):\n        self.telescopes = [f\"T{i+1}\" for i in range(n_telescopes)]\n        # Your code here\n    \n    async def observe_target_list(self, targets):\n        \"\"\"\n        Observe list of targets optimally using all telescopes.\n        \n        Should:\n        - Distribute targets among telescopes\n        - Handle failures gracefully\n        - Optimize for minimal total time\n        \"\"\"\n        # Your code here\n        pass\n    \n    async def emergency_stop(self):\n        \"\"\"Emergency stop all telescopes.\"\"\"\n        # Your code here\n        pass\n\n# Test your implementation\n# observatory = AsyncObservatory(3)\n# targets = [(\"M31\", 300), (\"M42\", 600), (\"M51\", 450)]\n# asyncio.run(observatory.observe_target_list(targets))","type":"content","url":"/advanced#exercise-8-1-async-observatory-controller","position":47},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl3":"Exercise 8.2: Custom Descriptor System","lvl2":"Try It Yourself"},"type":"lvl3","url":"/advanced#exercise-8-2-custom-descriptor-system","position":48},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl3":"Exercise 8.2: Custom Descriptor System","lvl2":"Try It Yourself"},"content":"Create a descriptor system for validated scientific data.class ScientificProperty:\n    \"\"\"\n    Descriptor for scientific properties with:\n    - Units and unit conversion\n    - Validation ranges\n    - Uncertainty tracking\n    - Automatic documentation\n    \"\"\"\n    \n    def __init__(self, name, unit, uncertainty=None):\n        # Your code here\n        pass\n    \n    def __get__(self, instance, owner):\n        # Your code here\n        pass\n    \n    def __set__(self, instance, value):\n        # Your code here\n        pass\n\nclass Measurement:\n    \"\"\"Use your ScientificProperty descriptor.\"\"\"\n    \n    # Your properties here\n    # temperature = ScientificProperty('temperature', 'K', uncertainty=0.1)\n    # pressure = ScientificProperty('pressure', 'Pa', uncertainty=10)\n    \n    pass\n\n# Test your descriptor\n# m = Measurement()\n# m.temperature = (273.15, 0.05)  # Value with uncertainty\n# print(f\"T = {m.temperature}\")","type":"content","url":"/advanced#exercise-8-2-custom-descriptor-system","position":49},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl3":"Exercise 8.3: Metaclass for Data Validation","lvl2":"Try It Yourself"},"type":"lvl3","url":"/advanced#exercise-8-3-metaclass-for-data-validation","position":50},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl3":"Exercise 8.3: Metaclass for Data Validation","lvl2":"Try It Yourself"},"content":"Create a metaclass that automatically validates data classes.class ValidatedMeta(type):\n    \"\"\"\n    Metaclass that:\n    - Automatically creates validators from type hints\n    - Adds logging to all methods\n    - Implements singleton pattern for config classes\n    - Registers all subclasses\n    \"\"\"\n    \n    def __new__(mcs, name, bases, namespace):\n        # Your code here\n        pass\n\nclass ObservationData(metaclass=ValidatedMeta):\n    \"\"\"Your data class using the metaclass.\"\"\"\n    \n    # Type hints that become validators\n    ra: float  # Should validate 0 <= ra <= 360\n    dec: float  # Should validate -90 <= dec <= 90\n    magnitude: float  # Should validate reasonable range\n    \n    # Your implementation\n    pass\n\n# Test your metaclass\n# obs = ObservationData()\n# obs.ra = 361  # Should raise ValueError","type":"content","url":"/advanced#exercise-8-3-metaclass-for-data-validation","position":51},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl2":"Key Takeaways"},"type":"lvl2","url":"/advanced#key-takeaways","position":52},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl2":"Key Takeaways"},"content":"✅ Async for I/O - Use asyncio for telescope control, data streaming✅ Decorators add functionality - Retry logic, validation, caching✅ Descriptors for properties - Custom validation and unit handling✅ Context managers for resources - Automatic setup/cleanup✅ Type hints improve code - Catch errors early, better documentation✅ Metaclasses are powerful but rare - Use for frameworks, not applications✅ Functional patterns have their place - Composition, currying for data pipelines✅ Binary data needs care - Struct module, memory mapping for large files","type":"content","url":"/advanced#key-takeaways","position":53},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl2":"Moving Forward"},"type":"lvl2","url":"/advanced#moving-forward","position":54},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl2":"Moving Forward"},"content":"This sampler introduced advanced Python concepts. Remember:\n\nUse advanced features when they solve real problems - Not just to be clever\n\nSimple code is often better - Readability counts\n\nProfile before optimizing - Measure, don’t guess\n\nTest complex code thoroughly - Advanced features can hide bugs\n\nDocument why, not just what - Especially for metaclasses and descriptors","type":"content","url":"/advanced#moving-forward","position":55},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl2":"Next Steps"},"type":"lvl2","url":"/advanced#next-steps","position":56},{"hierarchy":{"lvl1":"Chapter 8: Advanced Python Topics (Optional Sampler)","lvl2":"Next Steps"},"content":"You’re now ready for the Scientific Python Libraries section:\n\nNumPy - The foundation of scientific computing\n\nMatplotlib - Publication-quality visualization\n\nSciPy - Scientific algorithms and tools\n\nPandas - Data analysis and manipulation\n\nChoose which advanced topics to explore based on your project needs!","type":"content","url":"/advanced#next-steps","position":57},{"hierarchy":{"lvl1":"JAX Fundamentals: NumPy on Steroids"},"type":"lvl1","url":"/jax-fundamentals-chapter","position":0},{"hierarchy":{"lvl1":"JAX Fundamentals: NumPy on Steroids"},"content":"","type":"content","url":"/jax-fundamentals-chapter","position":1},{"hierarchy":{"lvl1":"JAX Fundamentals: NumPy on Steroids","lvl2":"Learning Objectives"},"type":"lvl2","url":"/jax-fundamentals-chapter#learning-objectives","position":2},{"hierarchy":{"lvl1":"JAX Fundamentals: NumPy on Steroids","lvl2":"Learning Objectives"},"content":"By the end of this chapter, you will:\n\nUnderstand JAX’s core philosophy and why it matters for scientific computing\n\nMaster functional programming patterns required by JAX\n\nUse automatic differentiation for physics problems\n\nCompile functions with JIT for massive speedups\n\nVectorize computations with vmap\n\nGenerate reproducible random numbers with JAX’s PRNG","type":"content","url":"/jax-fundamentals-chapter#learning-objectives","position":3},{"hierarchy":{"lvl1":"JAX Fundamentals: NumPy on Steroids","lvl2":"Why JAX? The Revolution in Scientific Computing"},"type":"lvl2","url":"/jax-fundamentals-chapter#why-jax-the-revolution-in-scientific-computing","position":4},{"hierarchy":{"lvl1":"JAX Fundamentals: NumPy on Steroids","lvl2":"Why JAX? The Revolution in Scientific Computing"},"content":"import jax\nimport jax.numpy as jnp\nfrom jax import grad, jit, vmap, pmap\nfrom jax import random\nimport numpy as np\nimport time\nimport matplotlib.pyplot as plt\n\ndef why_jax_matters():\n    \"\"\"Demonstrate JAX's game-changing features for astronomy.\"\"\"\n    \n    print(\"JAX: Three Transformations That Change Everything\")\n    print(\"=\" * 50)\n    \n    # 1. NumPy API but faster\n    print(\"\\n1. FAMILIAR BUT FASTER:\")\n    \n    # NumPy computation\n    np_array = np.random.randn(1000, 1000)\n    start = time.perf_counter()\n    np_result = np.sin(np_array) ** 2 + np.cos(np_array) ** 2\n    numpy_time = time.perf_counter() - start\n    \n    # JAX computation\n    jax_array = jnp.array(np_array)\n    start = time.perf_counter()\n    jax_result = jnp.sin(jax_array) ** 2 + jnp.cos(jax_array) ** 2\n    jax_time = time.perf_counter() - start\n    \n    print(f\"  NumPy time: {numpy_time*1000:.2f} ms\")\n    print(f\"  JAX time: {jax_time*1000:.2f} ms\")\n    print(f\"  Results match: {np.allclose(np_result, jax_result)}\")\n    \n    # 2. Automatic differentiation\n    print(\"\\n2. AUTOMATIC DIFFERENTIATION:\")\n    \n    def gravitational_potential(r, mass=1.0):\n        \"\"\"Gravitational potential energy.\"\"\"\n        return -mass / jnp.linalg.norm(r)\n    \n    # Get gradient automatically!\n    grad_potential = grad(gravitational_potential)\n    \n    position = jnp.array([1.0, 0.0, 0.0])\n    force = -grad_potential(position)\n    print(f\"  Position: {position}\")\n    print(f\"  Force: {force}\")\n    print(f\"  |F| = {jnp.linalg.norm(force):.4f} (expected: 1.0)\")\n    \n    # 3. Compilation with JIT\n    print(\"\\n3. JUST-IN-TIME COMPILATION:\")\n    \n    def orbital_step(state, dt):\n        \"\"\"Single step of orbital integration.\"\"\"\n        r, v = state[:3], state[3:]\n        a = -r / jnp.linalg.norm(r)**3\n        v_new = v + a * dt\n        r_new = r + v_new * dt\n        return jnp.concatenate([r_new, v_new])\n    \n    orbital_step_jit = jit(orbital_step)\n    \n    state = jnp.array([1.0, 0.0, 0.0, 0.0, 1.0, 0.0])\n    \n    # First call includes compilation\n    start = time.perf_counter()\n    _ = orbital_step_jit(state, 0.01)\n    first_time = time.perf_counter() - start\n    \n    # Subsequent calls are fast\n    start = time.perf_counter()\n    for _ in range(1000):\n        state = orbital_step_jit(state, 0.01)\n    compiled_time = time.perf_counter() - start\n    \n    # Compare with non-compiled\n    state = jnp.array([1.0, 0.0, 0.0, 0.0, 1.0, 0.0])\n    start = time.perf_counter()\n    for _ in range(1000):\n        state = orbital_step(state, 0.01)\n    python_time = time.perf_counter() - start\n    \n    print(f\"  First call (with compilation): {first_time*1000:.2f} ms\")\n    print(f\"  1000 steps compiled: {compiled_time*1000:.2f} ms\")\n    print(f\"  1000 steps python: {python_time*1000:.2f} ms\")\n    print(f\"  Speedup: {python_time/compiled_time:.1f}x\")\n    \n    # 4. Vectorization with vmap\n    print(\"\\n4. AUTOMATIC VECTORIZATION:\")\n    \n    def distance(r1, r2):\n        \"\"\"Distance between two points.\"\"\"\n        return jnp.linalg.norm(r1 - r2)\n    \n    # Vectorize over first argument\n    distances_from_origin = vmap(distance, in_axes=(0, None))\n    \n    positions = jnp.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 1, 1]])\n    origin = jnp.array([0, 0, 0])\n    \n    dists = distances_from_origin(positions, origin)\n    print(f\"  Distances from origin: {dists}\")\n    \n    return True\n\nwhy_jax_matters()","type":"content","url":"/jax-fundamentals-chapter#why-jax-the-revolution-in-scientific-computing","position":5},{"hierarchy":{"lvl1":"JAX Fundamentals: NumPy on Steroids","lvl2":"The Functional Programming Paradigm"},"type":"lvl2","url":"/jax-fundamentals-chapter#the-functional-programming-paradigm","position":6},{"hierarchy":{"lvl1":"JAX Fundamentals: NumPy on Steroids","lvl2":"The Functional Programming Paradigm"},"content":"","type":"content","url":"/jax-fundamentals-chapter#the-functional-programming-paradigm","position":7},{"hierarchy":{"lvl1":"JAX Fundamentals: NumPy on Steroids","lvl3":"Pure Functions: The Heart of JAX","lvl2":"The Functional Programming Paradigm"},"type":"lvl3","url":"/jax-fundamentals-chapter#pure-functions-the-heart-of-jax","position":8},{"hierarchy":{"lvl1":"JAX Fundamentals: NumPy on Steroids","lvl3":"Pure Functions: The Heart of JAX","lvl2":"The Functional Programming Paradigm"},"content":"def functional_programming_in_jax():\n    \"\"\"JAX requires pure functional programming - here's why and how.\"\"\"\n    \n    print(\"PURE FUNCTIONS IN JAX\")\n    print(\"=\" * 50)\n    \n    # ❌ BAD: Impure function with side effects\n    global_counter = 0\n    \n    def impure_function(x):\n        global global_counter\n        global_counter += 1  # Side effect!\n        return x * global_counter\n    \n    # This won't work properly with JAX transformations\n    # result = jit(impure_function)(5)  # Would give unexpected results\n    \n    # ✅ GOOD: Pure function\n    def pure_function(x, counter):\n        \"\"\"Pure function - output depends only on inputs.\"\"\"\n        return x * counter, counter + 1\n    \n    # This works perfectly with JAX\n    pure_jit = jit(pure_function)\n    result, new_counter = pure_jit(5.0, 1.0)\n    print(f\"Pure function result: {result}, new counter: {new_counter}\")\n    \n    # Example: Stellar evolution step\n    print(\"\\n STELLAR EVOLUTION EXAMPLE:\")\n    \n    # ❌ BAD: Using mutation\n    class Star:\n        def __init__(self, mass, luminosity):\n            self.mass = mass\n            self.luminosity = luminosity\n        \n        def evolve(self, dt):\n            self.luminosity *= 1.01  # Mutating state!\n            self.mass *= 0.999\n    \n    # ✅ GOOD: Functional approach\n    def evolve_star(state, dt):\n        \"\"\"\n        Evolve star state functionally.\n        \n        Parameters\n        ----------\n        state : dict\n            Star properties {mass, luminosity, age}\n        dt : float\n            Time step\n        \n        Returns\n        -------\n        dict\n            New star state\n        \"\"\"\n        mass_loss_rate = 1e-7 * state['luminosity']\n        \n        new_state = {\n            'mass': state['mass'] - mass_loss_rate * dt,\n            'luminosity': state['luminosity'] * (1 + 0.01 * dt),\n            'age': state['age'] + dt\n        }\n        \n        return new_state\n    \n    # JAX-friendly star evolution\n    @jit\n    def evolve_star_jax(mass, luminosity, age, dt):\n        \"\"\"Evolve star with JAX.\"\"\"\n        mass_loss_rate = 1e-7 * luminosity\n        \n        new_mass = mass - mass_loss_rate * dt\n        new_luminosity = luminosity * (1 + 0.01 * dt)\n        new_age = age + dt\n        \n        return new_mass, new_luminosity, new_age\n    \n    # Run evolution\n    mass, lum, age = 1.0, 1.0, 0.0\n    for _ in range(100):\n        mass, lum, age = evolve_star_jax(mass, lum, age, 0.01)\n    \n    print(f\"After evolution: M={mass:.4f}, L={lum:.4f}, Age={age:.2f}\")\n    \n    # Carrying state through computations\n    print(\"\\nCARRYING STATE FUNCTIONALLY:\")\n    \n    from functools import partial\n    \n    @jit\n    def integrate_orbit(carry, dt):\n        \"\"\"Single integration step.\"\"\"\n        position, velocity = carry\n        acceleration = -position / jnp.linalg.norm(position)**3\n        \n        new_velocity = velocity + acceleration * dt\n        new_position = position + new_velocity * dt\n        \n        return (new_position, new_velocity)\n    \n    # Use scan for sequential computations\n    from jax.lax import scan\n    \n    def simulate_orbit(initial_state, dt, n_steps):\n        \"\"\"Simulate orbit for n_steps.\"\"\"\n        \n        def step(carry, _):\n            new_carry = integrate_orbit(carry, dt)\n            return new_carry, new_carry  # Return carry and output\n        \n        final_state, trajectory = scan(step, initial_state, None, length=n_steps)\n        return trajectory\n    \n    initial = (jnp.array([1.0, 0.0, 0.0]), jnp.array([0.0, 1.0, 0.0]))\n    trajectory = simulate_orbit(initial, 0.01, 1000)\n    \n    positions = trajectory[0]\n    print(f\"Simulated {len(positions)} orbital positions\")\n    print(f\"Final position: {positions[-1]}\")\n\nfunctional_programming_in_jax()","type":"content","url":"/jax-fundamentals-chapter#pure-functions-the-heart-of-jax","position":9},{"hierarchy":{"lvl1":"JAX Fundamentals: NumPy on Steroids","lvl2":"Automatic Differentiation: The Killer Feature"},"type":"lvl2","url":"/jax-fundamentals-chapter#automatic-differentiation-the-killer-feature","position":10},{"hierarchy":{"lvl1":"JAX Fundamentals: NumPy on Steroids","lvl2":"Automatic Differentiation: The Killer Feature"},"content":"","type":"content","url":"/jax-fundamentals-chapter#automatic-differentiation-the-killer-feature","position":11},{"hierarchy":{"lvl1":"JAX Fundamentals: NumPy on Steroids","lvl3":"Gradients for Physics","lvl2":"Automatic Differentiation: The Killer Feature"},"type":"lvl3","url":"/jax-fundamentals-chapter#gradients-for-physics","position":12},{"hierarchy":{"lvl1":"JAX Fundamentals: NumPy on Steroids","lvl3":"Gradients for Physics","lvl2":"Automatic Differentiation: The Killer Feature"},"content":"def automatic_differentiation_astronomy():\n    \"\"\"Automatic differentiation for astronomical applications.\"\"\"\n    \n    print(\"AUTOMATIC DIFFERENTIATION IN ASTRONOMY\")\n    print(\"=\" * 50)\n    \n    # 1. Simple derivatives\n    print(\"\\n1. BASIC DERIVATIVES:\")\n    \n    def planck_law(wavelength, temperature):\n        \"\"\"Planck's law for blackbody radiation.\"\"\"\n        h = 6.626e-34\n        c = 3e8\n        k = 1.38e-23\n        \n        wavelength = wavelength * 1e-9  # nm to m\n        \n        numerator = 2 * h * c**2 / wavelength**5\n        denominator = jnp.exp(h * c / (wavelength * k * temperature)) - 1\n        \n        return numerator / denominator\n    \n    # Derivative with respect to temperature\n    dplanck_dT = grad(planck_law, argnums=1)\n    \n    wavelength = 500.0  # nm\n    temperature = 5778.0  # K\n    \n    intensity = planck_law(wavelength, temperature)\n    gradient = dplanck_dT(wavelength, temperature)\n    \n    print(f\"  B(λ={wavelength}nm, T={temperature}K) = {intensity:.3e}\")\n    print(f\"  ∂B/∂T = {gradient:.3e}\")\n    \n    # 2. Gradient of gravitational N-body potential\n    print(\"\\n2. N-BODY FORCES FROM POTENTIAL:\")\n    \n    def nbody_potential(positions, masses):\n        \"\"\"\n        Total gravitational potential energy.\n        \n        Parameters\n        ----------\n        positions : array shape (n, 3)\n            Positions of n bodies\n        masses : array shape (n,)\n            Masses of bodies\n        \"\"\"\n        n = len(masses)\n        potential = 0.0\n        \n        for i in range(n):\n            for j in range(i+1, n):\n                r_ij = jnp.linalg.norm(positions[i] - positions[j])\n                potential -= masses[i] * masses[j] / r_ij\n        \n        return potential\n    \n    # Get forces from potential gradient\n    def get_forces(positions, masses):\n        \"\"\"Calculate forces as negative gradient of potential.\"\"\"\n        return -grad(nbody_potential)(positions, masses)\n    \n    # Three-body system\n    positions = jnp.array([\n        [1.0, 0.0, 0.0],\n        [-0.5, 0.866, 0.0],\n        [-0.5, -0.866, 0.0]\n    ])\n    masses = jnp.array([1.0, 1.0, 1.0])\n    \n    forces = get_forces(positions, masses)\n    print(f\"  Forces on 3 bodies:\")\n    for i, f in enumerate(forces):\n        print(f\"    Body {i}: {f}\")\n    print(f\"  Total force: {jnp.sum(forces, axis=0)} (should be ~0)\")\n    \n    # 3. Hessian for optimization\n    print(\"\\n3. HESSIAN FOR FINDING MINIMA:\")\n    \n    def chi_squared(params, x_data, y_data):\n        \"\"\"Chi-squared for linear fit.\"\"\"\n        a, b = params\n        y_model = a * x_data + b\n        return jnp.sum((y_data - y_model)**2)\n    \n    # Get gradient and Hessian\n    from jax import hessian\n    \n    grad_chi2 = grad(chi_squared)\n    hess_chi2 = hessian(chi_squared)\n    \n    # Sample data\n    x_data = jnp.linspace(0, 10, 20)\n    y_true = 2.5 * x_data + 1.0\n    y_data = y_true + 0.5 * random.normal(random.PRNGKey(0), shape=x_data.shape)\n    \n    params = jnp.array([2.0, 0.5])  # Initial guess\n    \n    gradient = grad_chi2(params, x_data, y_data)\n    hessian_matrix = hess_chi2(params, x_data, y_data)\n    \n    print(f\"  Gradient at {params}: {gradient}\")\n    print(f\"  Hessian:\\n{hessian_matrix}\")\n    \n    # Use for Newton's method\n    params_new = params - jnp.linalg.inv(hessian_matrix) @ gradient\n    print(f\"  Newton step: {params} -> {params_new}\")\n    \n    # 4. Jacobian for coordinate transformations\n    print(\"\\n4. JACOBIAN FOR COORDINATE TRANSFORMS:\")\n    \n    from jax import jacfwd, jacrev\n    \n    def spherical_to_cartesian(spherical):\n        \"\"\"Convert spherical to Cartesian coordinates.\"\"\"\n        r, theta, phi = spherical\n        x = r * jnp.sin(theta) * jnp.cos(phi)\n        y = r * jnp.sin(theta) * jnp.sin(phi)\n        z = r * jnp.cos(theta)\n        return jnp.array([x, y, z])\n    \n    # Jacobian matrix\n    jacobian = jacfwd(spherical_to_cartesian)\n    \n    spherical = jnp.array([1.0, jnp.pi/4, jnp.pi/3])\n    J = jacobian(spherical)\n    \n    print(f\"  Spherical: r={spherical[0]}, θ={spherical[1]:.3f}, φ={spherical[2]:.3f}\")\n    print(f\"  Jacobian matrix:\")\n    print(f\"{J}\")\n    print(f\"  Determinant: {jnp.linalg.det(J):.3f}\")\n\nautomatic_differentiation_astronomy()","type":"content","url":"/jax-fundamentals-chapter#gradients-for-physics","position":13},{"hierarchy":{"lvl1":"JAX Fundamentals: NumPy on Steroids","lvl2":"JIT Compilation: Making Python Fast"},"type":"lvl2","url":"/jax-fundamentals-chapter#jit-compilation-making-python-fast","position":14},{"hierarchy":{"lvl1":"JAX Fundamentals: NumPy on Steroids","lvl2":"JIT Compilation: Making Python Fast"},"content":"","type":"content","url":"/jax-fundamentals-chapter#jit-compilation-making-python-fast","position":15},{"hierarchy":{"lvl1":"JAX Fundamentals: NumPy on Steroids","lvl3":"Understanding JIT","lvl2":"JIT Compilation: Making Python Fast"},"type":"lvl3","url":"/jax-fundamentals-chapter#understanding-jit","position":16},{"hierarchy":{"lvl1":"JAX Fundamentals: NumPy on Steroids","lvl3":"Understanding JIT","lvl2":"JIT Compilation: Making Python Fast"},"content":"def jit_compilation_deep_dive():\n    \"\"\"Deep dive into JIT compilation for scientific computing.\"\"\"\n    \n    print(\"JIT COMPILATION IN DETAIL\")\n    print(\"=\" * 50)\n    \n    # 1. Basic JIT compilation\n    print(\"\\n1. BASIC JIT:\")\n    \n    def slow_function(x):\n        \"\"\"Computationally intensive function.\"\"\"\n        result = x\n        for _ in range(100):\n            result = jnp.sin(result) + jnp.cos(result) * jnp.exp(-result**2)\n        return result\n    \n    fast_function = jit(slow_function)\n    \n    x = jnp.linspace(-2, 2, 1000)\n    \n    # Time comparison\n    start = time.perf_counter()\n    _ = slow_function(x)\n    slow_time = time.perf_counter() - start\n    \n    # First call (includes compilation)\n    start = time.perf_counter()\n    _ = fast_function(x)\n    first_time = time.perf_counter() - start\n    \n    # Second call (already compiled)\n    start = time.perf_counter()\n    _ = fast_function(x)\n    fast_time = time.perf_counter() - start\n    \n    print(f\"  Slow: {slow_time*1000:.2f} ms\")\n    print(f\"  First JIT call: {first_time*1000:.2f} ms\")\n    print(f\"  Subsequent JIT: {fast_time*1000:.2f} ms\")\n    print(f\"  Speedup: {slow_time/fast_time:.1f}x\")\n    \n    # 2. JIT with static arguments\n    print(\"\\n2. STATIC ARGUMENTS:\")\n    \n    @partial(jit, static_argnums=(1, 2))\n    def simulate_galaxy(positions, n_steps, dt):\n        \"\"\"\n        Simulate galaxy dynamics.\n        n_steps and dt are static (known at compile time).\n        \"\"\"\n        def step(pos):\n            # Simplified dynamics\n            center_of_mass = jnp.mean(pos, axis=0)\n            forces = -(pos - center_of_mass) / 100\n            return pos + forces * dt\n        \n        for _ in range(n_steps):\n            positions = step(positions)\n        \n        return positions\n    \n    # Different compiled versions for different static args\n    pos = random.normal(random.PRNGKey(0), (100, 3))\n    \n    # These create different compiled functions\n    result1 = simulate_galaxy(pos, 10, 0.1)   # Compilation 1\n    result2 = simulate_galaxy(pos, 10, 0.1)   # Uses compilation 1\n    result3 = simulate_galaxy(pos, 20, 0.1)   # New compilation!\n    \n    print(f\"  Static arguments create specialized compiled versions\")\n    \n    # 3. JIT pitfalls to avoid\n    print(\"\\n3. JIT PITFALLS:\")\n    \n    # ❌ BAD: Python control flow depending on values\n    def bad_function(x):\n        if x > 0:  # This depends on the VALUE of x\n            return jnp.sin(x)\n        else:\n            return jnp.cos(x)\n    \n    # This won't work with JIT for dynamic x\n    # jitted_bad = jit(bad_function)\n    # jitted_bad(jnp.array(0.5))  # Would fail!\n    \n    # ✅ GOOD: Use JAX control flow\n    def good_function(x):\n        return jax.lax.cond(\n            x > 0,\n            lambda x: jnp.sin(x),\n            lambda x: jnp.cos(x),\n            x\n        )\n    \n    jitted_good = jit(good_function)\n    result = jitted_good(0.5)\n    print(f\"  Conditional result: {result:.3f}\")\n    \n    # 4. Debugging JIT compilation\n    print(\"\\n4. DEBUGGING JIT:\")\n    \n    # Use jax.debug.print inside JIT\n    @jit\n    def debug_function(x):\n        x = x * 2\n        jax.debug.print(\"x after doubling: {x}\", x=x)\n        x = jnp.sin(x)\n        jax.debug.print(\"x after sin: {x}\", x=x)\n        return x\n    \n    result = debug_function(1.0)\n    \n    # Check what XLA sees\n    from jax import make_jaxpr\n    \n    def simple_function(x, y):\n        return jnp.dot(x, y) + 1\n    \n    x = jnp.ones((3, 3))\n    y = jnp.ones((3, 3))\n    \n    print(\"\\n  JAX expression tree:\")\n    print(make_jaxpr(simple_function)(x, y))\n\njit_compilation_deep_dive()","type":"content","url":"/jax-fundamentals-chapter#understanding-jit","position":17},{"hierarchy":{"lvl1":"JAX Fundamentals: NumPy on Steroids","lvl2":"Vectorization with vmap"},"type":"lvl2","url":"/jax-fundamentals-chapter#vectorization-with-vmap","position":18},{"hierarchy":{"lvl1":"JAX Fundamentals: NumPy on Steroids","lvl2":"Vectorization with vmap"},"content":"","type":"content","url":"/jax-fundamentals-chapter#vectorization-with-vmap","position":19},{"hierarchy":{"lvl1":"JAX Fundamentals: NumPy on Steroids","lvl3":"Parallel Operations Made Simple","lvl2":"Vectorization with vmap"},"type":"lvl3","url":"/jax-fundamentals-chapter#parallel-operations-made-simple","position":20},{"hierarchy":{"lvl1":"JAX Fundamentals: NumPy on Steroids","lvl3":"Parallel Operations Made Simple","lvl2":"Vectorization with vmap"},"content":"def vmap_for_astronomy():\n    \"\"\"Vectorization patterns for astronomical computations.\"\"\"\n    \n    print(\"VMAP: AUTOMATIC VECTORIZATION\")\n    print(\"=\" * 50)\n    \n    # 1. Basic vectorization\n    print(\"\\n1. VECTORIZING DISTANCE CALCULATIONS:\")\n    \n    def angular_distance(ra1, dec1, ra2, dec2):\n        \"\"\"Angular distance between two points (haversine).\"\"\"\n        dra = ra2 - ra1\n        ddec = dec2 - dec1\n        \n        a = jnp.sin(ddec/2)**2 + jnp.cos(dec1) * jnp.cos(dec2) * jnp.sin(dra/2)**2\n        c = 2 * jnp.arcsin(jnp.sqrt(a))\n        \n        return c\n    \n    # Vectorize over first source (compare one to many)\n    vmap_one_to_many = vmap(angular_distance, in_axes=(None, None, 0, 0))\n    \n    # Vectorize over both (pairwise)\n    vmap_pairwise = vmap(angular_distance, in_axes=(0, 0, 0, 0))\n    \n    # Test data\n    ra_catalog = random.uniform(random.PRNGKey(0), (1000,), minval=0, maxval=2*jnp.pi)\n    dec_catalog = random.uniform(random.PRNGKey(1), (1000,), minval=-jnp.pi/2, maxval=jnp.pi/2)\n    \n    # Distance from single source to catalog\n    ra_source, dec_source = jnp.pi, 0.0\n    distances = vmap_one_to_many(ra_source, dec_source, ra_catalog, dec_catalog)\n    print(f\"  Distances from source: shape {distances.shape}\")\n    print(f\"  Nearest neighbor: {jnp.min(distances):.4f} rad\")\n    \n    # 2. Nested vmap for all-pairs\n    print(\"\\n2. ALL-PAIRS DISTANCES:\")\n    \n    # Nested vmap for N×N distance matrix\n    vmap_all_pairs = vmap(\n        vmap(angular_distance, in_axes=(None, None, 0, 0)),\n        in_axes=(0, 0, None, None)\n    )\n    \n    # Small sample for all-pairs\n    ra_sample = ra_catalog[:10]\n    dec_sample = dec_catalog[:10]\n    \n    distance_matrix = vmap_all_pairs(ra_sample, dec_sample, ra_sample, dec_sample)\n    print(f\"  Distance matrix shape: {distance_matrix.shape}\")\n    print(f\"  Symmetric: {jnp.allclose(distance_matrix, distance_matrix.T)}\")\n    \n    # 3. Vectorizing complex functions\n    print(\"\\n3. VECTORIZING ORBIT INTEGRATION:\")\n    \n    @jit\n    def integrate_single_orbit(initial_conditions, n_steps):\n        \"\"\"Integrate single orbit.\"\"\"\n        r0, v0 = initial_conditions[:3], initial_conditions[3:]\n        \n        def step(carry, _):\n            r, v = carry\n            a = -r / jnp.linalg.norm(r)**3\n            v_new = v + a * 0.01\n            r_new = r + v_new * 0.01\n            return (r_new, v_new), r_new\n        \n        _, trajectory = scan(step, (r0, v0), None, length=n_steps)\n        return trajectory\n    \n    # Vectorize over different initial conditions\n    vmap_orbits = vmap(integrate_single_orbit, in_axes=(0, None))\n    \n    # Multiple initial conditions (different eccentricities)\n    initial_conditions = jnp.array([\n        [1.0, 0.0, 0.0, 0.0, 1.0, 0.0],   # Circular\n        [1.0, 0.0, 0.0, 0.0, 1.2, 0.0],   # Elliptical\n        [1.0, 0.0, 0.0, 0.0, 0.8, 0.0],   # Elliptical\n    ])\n    \n    trajectories = vmap_orbits(initial_conditions, 100)\n    print(f\"  Integrated {len(trajectories)} orbits in parallel\")\n    print(f\"  Trajectories shape: {trajectories.shape}\")\n    \n    # 4. Combining vmap with grad\n    print(\"\\n4. VECTORIZED GRADIENTS:\")\n    \n    def potential(position, mass_distribution):\n        \"\"\"Gravitational potential at position.\"\"\"\n        positions, masses = mass_distribution\n        distances = vmap(lambda p: jnp.linalg.norm(position - p))(positions)\n        return -jnp.sum(masses / distances)\n    \n    # Gradient of potential\n    grad_potential = grad(potential)\n    \n    # Vectorize gradient calculation over multiple positions\n    vmap_gradient = vmap(grad_potential, in_axes=(0, None))\n    \n    # Mass distribution (galaxy model)\n    n_masses = 100\n    mass_positions = random.normal(random.PRNGKey(2), (n_masses, 3)) * 10\n    masses = random.uniform(random.PRNGKey(3), (n_masses,), minval=0.1, maxval=1.0)\n    mass_distribution = (mass_positions, masses)\n    \n    # Calculate gradient at multiple points\n    test_positions = jnp.array([\n        [0.0, 0.0, 0.0],\n        [5.0, 0.0, 0.0],\n        [0.0, 5.0, 0.0],\n        [5.0, 5.0, 5.0]\n    ])\n    \n    gradients = vmap_gradient(test_positions, mass_distribution)\n    print(f\"  Gradients at {len(test_positions)} positions:\")\n    for i, (pos, grad) in enumerate(zip(test_positions, gradients)):\n        print(f\"    Position {pos}: Force = {-grad}\")\n\nvmap_for_astronomy()","type":"content","url":"/jax-fundamentals-chapter#parallel-operations-made-simple","position":21},{"hierarchy":{"lvl1":"JAX Fundamentals: NumPy on Steroids","lvl2":"Random Numbers in JAX"},"type":"lvl2","url":"/jax-fundamentals-chapter#random-numbers-in-jax","position":22},{"hierarchy":{"lvl1":"JAX Fundamentals: NumPy on Steroids","lvl2":"Random Numbers in JAX"},"content":"","type":"content","url":"/jax-fundamentals-chapter#random-numbers-in-jax","position":23},{"hierarchy":{"lvl1":"JAX Fundamentals: NumPy on Steroids","lvl3":"Reproducible Randomness","lvl2":"Random Numbers in JAX"},"type":"lvl3","url":"/jax-fundamentals-chapter#reproducible-randomness","position":24},{"hierarchy":{"lvl1":"JAX Fundamentals: NumPy on Steroids","lvl3":"Reproducible Randomness","lvl2":"Random Numbers in JAX"},"content":"def jax_random_numbers():\n    \"\"\"JAX's approach to random numbers for reproducible science.\"\"\"\n    \n    print(\"RANDOM NUMBERS IN JAX\")\n    print(\"=\" * 50)\n    \n    # 1. JAX PRNG basics\n    print(\"\\n1. PRNG BASICS:\")\n    \n    # Create a random key\n    key = random.PRNGKey(42)\n    print(f\"  Initial key: {key}\")\n    \n    # Split key for independent streams\n    key, subkey = random.split(key)\n    print(f\"  Split keys: {key}, {subkey}\")\n    \n    # Generate random numbers\n    uniform_samples = random.uniform(subkey, shape=(5,))\n    print(f\"  Uniform samples: {uniform_samples}\")\n    \n    # 2. Why explicit keys matter\n    print(\"\\n2. REPRODUCIBILITY:\")\n    \n    def monte_carlo_integration(f, n_samples, key):\n        \"\"\"Monte Carlo integration of function f over [0,1]³.\"\"\"\n        # Split key for different random numbers\n        key1, key2, key3 = random.split(key, 3)\n        \n        x = random.uniform(key1, (n_samples,))\n        y = random.uniform(key2, (n_samples,))\n        z = random.uniform(key3, (n_samples,))\n        \n        samples = vmap(f)(x, y, z)\n        return jnp.mean(samples)\n    \n    def test_function(x, y, z):\n        return x**2 + y**2 + z**2\n    \n    # Same key → same result (reproducible!)\n    key1 = random.PRNGKey(123)\n    result1 = monte_carlo_integration(test_function, 10000, key1)\n    \n    key2 = random.PRNGKey(123)  # Same seed\n    result2 = monte_carlo_integration(test_function, 10000, key2)\n    \n    print(f\"  Result 1: {result1:.6f}\")\n    print(f\"  Result 2: {result2:.6f}\")\n    print(f\"  Identical: {result1 == result2}\")\n    \n    # 3. Random numbers in parallel computations\n    print(\"\\n3. PARALLEL RANDOM STREAMS:\")\n    \n    @jit\n    def parallel_monte_carlo(keys, n_samples_per_thread):\n        \"\"\"Run MC in parallel with independent random streams.\"\"\"\n        \n        def single_mc(key):\n            samples = random.normal(key, (n_samples_per_thread,))\n            return jnp.mean(samples**2)  # Estimate <x²>\n        \n        # vmap over different keys\n        results = vmap(single_mc)(keys)\n        return results\n    \n    # Create independent keys for parallel execution\n    main_key = random.PRNGKey(0)\n    n_threads = 4\n    keys = random.split(main_key, n_threads)\n    \n    estimates = parallel_monte_carlo(keys, 10000)\n    print(f\"  Parallel estimates of <x²>: {estimates}\")\n    print(f\"  Mean: {jnp.mean(estimates):.4f} (expected: 1.0)\")\n    \n    # 4. Sampling from distributions\n    print(\"\\n4. ASTRONOMICAL DISTRIBUTIONS:\")\n    \n    key = random.PRNGKey(42)\n    \n    # Initial Mass Function (power law)\n    def sample_imf(key, n_stars, alpha=-2.35):\n        \"\"\"Sample from Salpeter IMF.\"\"\"\n        key1, key2 = random.split(key)\n        \n        # Inverse transform sampling\n        u = random.uniform(key1, (n_stars,))\n        m_min, m_max = 0.1, 100.0\n        \n        if alpha == -1:\n            masses = m_min * (m_max/m_min)**u\n        else:\n            masses = ((m_max**(alpha+1) - m_min**(alpha+1)) * u + \n                     m_min**(alpha+1))**(1/(alpha+1))\n        \n        return masses\n    \n    masses = sample_imf(key, 1000)\n    print(f\"  Sampled {len(masses)} stellar masses\")\n    print(f\"  Mass range: {jnp.min(masses):.2f} - {jnp.max(masses):.2f} M☉\")\n    print(f\"  Mean mass: {jnp.mean(masses):.2f} M☉\")\n\njax_random_numbers()","type":"content","url":"/jax-fundamentals-chapter#reproducible-randomness","position":25},{"hierarchy":{"lvl1":"JAX Fundamentals: NumPy on Steroids","lvl2":"Putting It All Together: N-Body Simulation"},"type":"lvl2","url":"/jax-fundamentals-chapter#putting-it-all-together-n-body-simulation","position":26},{"hierarchy":{"lvl1":"JAX Fundamentals: NumPy on Steroids","lvl2":"Putting It All Together: N-Body Simulation"},"content":"","type":"content","url":"/jax-fundamentals-chapter#putting-it-all-together-n-body-simulation","position":27},{"hierarchy":{"lvl1":"JAX Fundamentals: NumPy on Steroids","lvl3":"Complete Example with All JAX Features","lvl2":"Putting It All Together: N-Body Simulation"},"type":"lvl3","url":"/jax-fundamentals-chapter#complete-example-with-all-jax-features","position":28},{"hierarchy":{"lvl1":"JAX Fundamentals: NumPy on Steroids","lvl3":"Complete Example with All JAX Features","lvl2":"Putting It All Together: N-Body Simulation"},"content":"def nbody_simulation_complete():\n    \"\"\"Complete N-body simulation showcasing all JAX features.\"\"\"\n    \n    print(\"COMPLETE N-BODY SIMULATION WITH JAX\")\n    print(\"=\" * 50)\n    \n    # Define the physics\n    @jit\n    def compute_forces(positions, masses):\n        \"\"\"Compute gravitational forces between all bodies.\"\"\"\n        n = len(masses)\n        forces = jnp.zeros_like(positions)\n        \n        for i in range(n):\n            # Vectorize force calculation from body i to all others\n            def force_from_j(j):\n                r_ij = positions[j] - positions[i]\n                dist = jnp.linalg.norm(r_ij)\n                # Softening to avoid singularities\n                dist_soft = jnp.maximum(dist, 0.01)\n                return jax.lax.cond(\n                    i == j,\n                    lambda _: jnp.zeros(3),\n                    lambda _: masses[j] * r_ij / dist_soft**3,\n                    None\n                )\n            \n            total_force = jnp.sum(vmap(force_from_j)(jnp.arange(n)), axis=0)\n            forces = forces.at[i].set(masses[i] * total_force)\n        \n        return forces\n    \n    # Integration step\n    @jit\n    def leapfrog_step(state, dt):\n        \"\"\"Single leapfrog integration step.\"\"\"\n        positions, velocities, masses = state\n        \n        # Compute forces\n        forces = compute_forces(positions, masses)\n        accelerations = forces / masses[:, None]\n        \n        # Leapfrog update\n        velocities_half = velocities + 0.5 * dt * accelerations\n        positions_new = positions + dt * velocities_half\n        \n        forces_new = compute_forces(positions_new, masses)\n        accelerations_new = forces_new / masses[:, None]\n        \n        velocities_new = velocities_half + 0.5 * dt * accelerations_new\n        \n        return (positions_new, velocities_new, masses)\n    \n    # Energy calculations for verification\n    @jit\n    def total_energy(state):\n        \"\"\"Calculate total energy of the system.\"\"\"\n        positions, velocities, masses = state\n        \n        # Kinetic energy\n        kinetic = 0.5 * jnp.sum(masses[:, None] * velocities**2)\n        \n        # Potential energy\n        potential = 0.0\n        n = len(masses)\n        for i in range(n):\n            for j in range(i+1, n):\n                r_ij = jnp.linalg.norm(positions[i] - positions[j])\n                r_soft = jnp.maximum(r_ij, 0.01)\n                potential -= masses[i] * masses[j] / r_soft\n        \n        return kinetic + potential\n    \n    # Simulate function\n    @jit\n    def simulate(initial_state, dt, n_steps):\n        \"\"\"Run full simulation.\"\"\"\n        \n        def step(carry, _):\n            state = carry\n            new_state = leapfrog_step(state, dt)\n            energy = total_energy(new_state)\n            return new_state, (new_state[0], energy)  # Return positions and energy\n        \n        final_state, (trajectory, energies) = scan(\n            step, initial_state, None, length=n_steps\n        )\n        \n        return final_state, trajectory, energies\n    \n    # Set up initial conditions (Pythagorean 3-body)\n    positions = jnp.array([\n        [1.0, 3.0, 0.0],\n        [-2.0, -1.0, 0.0],\n        [1.0, -1.0, 0.0]\n    ])\n    \n    velocities = jnp.array([\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0]\n    ])\n    \n    masses = jnp.array([3.0, 4.0, 5.0])\n    \n    initial_state = (positions, velocities, masses)\n    \n    # Run simulation\n    print(\"\\nRunning simulation...\")\n    start_time = time.perf_counter()\n    \n    final_state, trajectory, energies = simulate(\n        initial_state, dt=0.001, n_steps=10000\n    )\n    \n    elapsed = time.perf_counter() - start_time\n    print(f\"  Simulated 10,000 steps in {elapsed:.3f} seconds\")\n    \n    # Check energy conservation\n    initial_energy = total_energy(initial_state)\n    final_energy = total_energy(final_state)\n    \n    print(f\"\\n  Initial energy: {initial_energy:.6f}\")\n    print(f\"  Final energy: {final_energy:.6f}\")\n    print(f\"  Relative error: {abs(final_energy - initial_energy) / abs(initial_energy):.2e}\")\n    \n    # Plot results\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n    \n    # Trajectories\n    for i in range(3):\n        ax1.plot(trajectory[:, i, 0], trajectory[:, i, 1], \n                alpha=0.7, linewidth=1, label=f'Mass {masses[i]:.1f}')\n    ax1.set_xlabel('X')\n    ax1.set_ylabel('Y')\n    ax1.set_title('Three-Body Trajectories')\n    ax1.legend()\n    ax1.axis('equal')\n    ax1.grid(True, alpha=0.3)\n    \n    # Energy conservation\n    ax2.plot((energies - initial_energy) / abs(initial_energy))\n    ax2.set_xlabel('Time step')\n    ax2.set_ylabel('Relative energy error')\n    ax2.set_title('Energy Conservation')\n    ax2.grid(True, alpha=0.3)\n    ax2.set_yscale('log')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return trajectory\n\ntrajectory = nbody_simulation_complete()","type":"content","url":"/jax-fundamentals-chapter#complete-example-with-all-jax-features","position":29},{"hierarchy":{"lvl1":"JAX Fundamentals: NumPy on Steroids","lvl2":"Try It Yourself"},"type":"lvl2","url":"/jax-fundamentals-chapter#try-it-yourself","position":30},{"hierarchy":{"lvl1":"JAX Fundamentals: NumPy on Steroids","lvl2":"Try It Yourself"},"content":"","type":"content","url":"/jax-fundamentals-chapter#try-it-yourself","position":31},{"hierarchy":{"lvl1":"JAX Fundamentals: NumPy on Steroids","lvl3":"Exercise 1: Differentiable Cosmology","lvl2":"Try It Yourself"},"type":"lvl3","url":"/jax-fundamentals-chapter#exercise-1-differentiable-cosmology","position":32},{"hierarchy":{"lvl1":"JAX Fundamentals: NumPy on Steroids","lvl3":"Exercise 1: Differentiable Cosmology","lvl2":"Try It Yourself"},"content":"def differentiable_cosmology():\n    \"\"\"\n    Build a differentiable cosmological distance calculator.\n    \n    Tasks:\n    1. Implement luminosity distance as function of z, H0, Omega_m, Omega_Lambda\n    2. Use grad to get derivatives with respect to all parameters\n    3. JIT compile for speed\n    4. Fit to supernova data using gradient descent\n    \"\"\"\n    # Your code here\n    pass","type":"content","url":"/jax-fundamentals-chapter#exercise-1-differentiable-cosmology","position":33},{"hierarchy":{"lvl1":"JAX Fundamentals: NumPy on Steroids","lvl3":"Exercise 2: Vectorized Light Curve Analysis","lvl2":"Try It Yourself"},"type":"lvl3","url":"/jax-fundamentals-chapter#exercise-2-vectorized-light-curve-analysis","position":34},{"hierarchy":{"lvl1":"JAX Fundamentals: NumPy on Steroids","lvl3":"Exercise 2: Vectorized Light Curve Analysis","lvl2":"Try It Yourself"},"content":"def analyze_light_curves_jax(times, fluxes, periods_to_test):\n    \"\"\"\n    Analyze multiple light curves using JAX.\n    \n    Requirements:\n    1. Use vmap to process multiple light curves in parallel\n    2. JIT compile the period-finding algorithm\n    3. Implement Lomb-Scargle using JAX operations\n    4. Return best periods and their uncertainties\n    \"\"\"\n    # Your code here\n    pass","type":"content","url":"/jax-fundamentals-chapter#exercise-2-vectorized-light-curve-analysis","position":35},{"hierarchy":{"lvl1":"JAX Fundamentals: NumPy on Steroids","lvl3":"Exercise 3: Differentiable Ray Tracing","lvl2":"Try It Yourself"},"type":"lvl3","url":"/jax-fundamentals-chapter#exercise-3-differentiable-ray-tracing","position":36},{"hierarchy":{"lvl1":"JAX Fundamentals: NumPy on Steroids","lvl3":"Exercise 3: Differentiable Ray Tracing","lvl2":"Try It Yourself"},"content":"def ray_tracing_jax(rays, lens_params):\n    \"\"\"\n    Differentiable ray tracing through gravitational lens.\n    \n    Tasks:\n    1. Trace rays through gravitational potential\n    2. Use grad to optimize lens parameters\n    3. Implement critical curves and caustics\n    4. JIT compile for real-time visualization\n    \"\"\"\n    # Your code here\n    pass","type":"content","url":"/jax-fundamentals-chapter#exercise-3-differentiable-ray-tracing","position":37},{"hierarchy":{"lvl1":"JAX Fundamentals: NumPy on Steroids","lvl2":"Key Takeaways"},"type":"lvl2","url":"/jax-fundamentals-chapter#key-takeaways","position":38},{"hierarchy":{"lvl1":"JAX Fundamentals: NumPy on Steroids","lvl2":"Key Takeaways"},"content":"✅ JAX = NumPy + autodiff + JIT + vmap - Composable transformations✅ Functional programming required - No mutations, pure functions only✅ Automatic differentiation - Get gradients of any function for free✅ JIT compilation - Near C++ speeds from Python code✅ vmap for parallelization - Vectorize any function automatically✅ Explicit random keys - Reproducible randomness in parallel✅ Composable transformations - Combine JIT + grad + vmap freely✅ GPU/TPU ready - Same code runs on accelerators","type":"content","url":"/jax-fundamentals-chapter#key-takeaways","position":39},{"hierarchy":{"lvl1":"JAX Fundamentals: NumPy on Steroids","lvl2":"Next Chapter Preview"},"type":"lvl2","url":"/jax-fundamentals-chapter#next-chapter-preview","position":40},{"hierarchy":{"lvl1":"JAX Fundamentals: NumPy on Steroids","lvl2":"Next Chapter Preview"},"content":"JAX Advanced Patterns: Control flow, custom derivatives, and performance optimization for large-scale astronomical simulations.","type":"content","url":"/jax-fundamentals-chapter#next-chapter-preview","position":41},{"hierarchy":{"lvl1":"JAX Scientific Computing Stack: Equinox, Diffrax, and Optimistix"},"type":"lvl1","url":"/jax-scientific-stack","position":0},{"hierarchy":{"lvl1":"JAX Scientific Computing Stack: Equinox, Diffrax, and Optimistix"},"content":"","type":"content","url":"/jax-scientific-stack","position":1},{"hierarchy":{"lvl1":"JAX Scientific Computing Stack: Equinox, Diffrax, and Optimistix","lvl2":"Learning Objectives"},"type":"lvl2","url":"/jax-scientific-stack#learning-objectives","position":2},{"hierarchy":{"lvl1":"JAX Scientific Computing Stack: Equinox, Diffrax, and Optimistix","lvl2":"Learning Objectives"},"content":"By the end of this chapter, you will:\n\nBuild neural networks and scientific models with Equinox\n\nSolve ODEs/SDEs with Diffrax’s advanced solvers\n\nImplement root-finding and optimization with Optimistix\n\nUse jaxtyping for runtime type checking\n\nCombine these libraries for complex astronomical simulations","type":"content","url":"/jax-scientific-stack#learning-objectives","position":3},{"hierarchy":{"lvl1":"JAX Scientific Computing Stack: Equinox, Diffrax, and Optimistix","lvl2":"Equinox: Neural Networks as PyTrees"},"type":"lvl2","url":"/jax-scientific-stack#equinox-neural-networks-as-pytrees","position":4},{"hierarchy":{"lvl1":"JAX Scientific Computing Stack: Equinox, Diffrax, and Optimistix","lvl2":"Equinox: Neural Networks as PyTrees"},"content":"","type":"content","url":"/jax-scientific-stack#equinox-neural-networks-as-pytrees","position":5},{"hierarchy":{"lvl1":"JAX Scientific Computing Stack: Equinox, Diffrax, and Optimistix","lvl3":"Introduction to Equinox","lvl2":"Equinox: Neural Networks as PyTrees"},"type":"lvl3","url":"/jax-scientific-stack#introduction-to-equinox","position":6},{"hierarchy":{"lvl1":"JAX Scientific Computing Stack: Equinox, Diffrax, and Optimistix","lvl3":"Introduction to Equinox","lvl2":"Equinox: Neural Networks as PyTrees"},"content":"import jax\nimport jax.numpy as jnp\nfrom jax import grad, jit, vmap, random\nimport equinox as eqx\nfrom jaxtyping import Array, Float, Int, PyTree, jaxtyped\nfrom typing import Optional\nimport matplotlib.pyplot as plt\nimport time\n\ndef equinox_fundamentals():\n    \"\"\"Learn Equinox's approach to neural networks and models.\"\"\"\n    \n    print(\"EQUINOX: NEURAL NETWORKS AS PYTREES\")\n    print(\"=\" * 50)\n    \n    # 1. Basic neural network\n    print(\"\\n1. SIMPLE NEURAL NETWORK:\")\n    \n    class StellarClassifier(eqx.Module):\n        \"\"\"Classify stellar types from spectra.\"\"\"\n        layers: list\n        dropout: eqx.nn.Dropout\n        \n        def __init__(self, input_dim: int, hidden_dim: int, output_dim: int, key):\n            keys = random.split(key, 4)\n            \n            self.layers = [\n                eqx.nn.Linear(input_dim, hidden_dim, key=keys[0]),\n                eqx.nn.Linear(hidden_dim, hidden_dim, key=keys[1]),\n                eqx.nn.Linear(hidden_dim, output_dim, key=keys[2])\n            ]\n            self.dropout = eqx.nn.Dropout(p=0.1)\n        \n        def __call__(self, x: Float[Array, \"wavelengths\"], *, key: Optional[random.PRNGKey] = None) -> Float[Array, \"classes\"]:\n            # First hidden layer\n            x = self.layers[0](x)\n            x = jax.nn.relu(x)\n            \n            # Dropout during training\n            if key is not None:\n                x = self.dropout(x, key=key)\n            \n            # Second hidden layer\n            x = self.layers[1](x)\n            x = jax.nn.relu(x)\n            \n            # Output layer\n            x = self.layers[2](x)\n            return jax.nn.log_softmax(x)\n    \n    # Initialize model\n    key = random.PRNGKey(0)\n    model = StellarClassifier(input_dim=1000, hidden_dim=128, output_dim=7, key=key)\n    \n    # Test forward pass\n    spectrum = random.normal(key, (1000,))\n    logits = model(spectrum)\n    print(f\"  Input shape: {spectrum.shape}\")\n    print(f\"  Output shape: {logits.shape}\")\n    print(f\"  Predicted class: {jnp.argmax(logits)}\")\n    \n    # 2. Model manipulation as PyTrees\n    print(\"\\n2. PYTREE OPERATIONS:\")\n    \n    # Get model parameters\n    params, static = eqx.partition(model, eqx.is_array)\n    \n    # Count parameters\n    num_params = sum(x.size for x in jax.tree_util.tree_leaves(params))\n    print(f\"  Total parameters: {num_params:,}\")\n    \n    # Modify parameters\n    def add_noise(params, key, noise_scale=0.01):\n        \"\"\"Add noise to parameters.\"\"\"\n        leaves, treedef = jax.tree_util.tree_flatten(params)\n        keys = random.split(key, len(leaves))\n        \n        noisy_leaves = [\n            leaf + noise_scale * random.normal(k, leaf.shape)\n            for leaf, k in zip(leaves, keys)\n        ]\n        \n        return jax.tree_util.tree_unflatten(treedef, noisy_leaves)\n    \n    noisy_params = add_noise(params, key)\n    noisy_model = eqx.combine(noisy_params, static)\n    \n    # 3. Advanced model with custom layers\n    print(\"\\n3. CUSTOM LAYERS AND MODULES:\")\n    \n    class SpectralConvolution(eqx.Module):\n        \"\"\"1D convolution for spectral features.\"\"\"\n        weight: Float[Array, \"out_channels in_channels kernel_size\"]\n        bias: Optional[Float[Array, \"out_channels\"]]\n        \n        def __init__(self, in_channels, out_channels, kernel_size, *, key, use_bias=True):\n            wkey, bkey = random.split(key)\n            \n            # He initialization\n            lim = jnp.sqrt(2.0 / (in_channels * kernel_size))\n            self.weight = random.uniform(\n                wkey, \n                (out_channels, in_channels, kernel_size),\n                minval=-lim, maxval=lim\n            )\n            \n            if use_bias:\n                self.bias = jnp.zeros((out_channels,))\n            else:\n                self.bias = None\n        \n        def __call__(self, x: Float[Array, \"batch channels length\"]) -> Float[Array, \"batch out_channels new_length\"]:\n            # Apply convolution\n            out = jax.lax.conv_general_dilated(\n                x, self.weight, \n                window_strides=(1,),\n                padding='SAME'\n            )\n            \n            if self.bias is not None:\n                out = out + self.bias[None, :, None]\n            \n            return out\n    \n    # Use custom layer\n    conv_layer = SpectralConvolution(1, 16, kernel_size=5, key=key)\n    test_input = random.normal(key, (32, 1, 100))  # batch, channels, length\n    output = conv_layer(test_input)\n    print(f\"  Conv input: {test_input.shape}\")\n    print(f\"  Conv output: {output.shape}\")\n    \n    # 4. Filtering and freezing\n    print(\"\\n4. FILTERING AND FREEZING PARAMETERS:\")\n    \n    # Filter specific layers\n    def get_linear_params(model):\n        \"\"\"Extract only Linear layer parameters.\"\"\"\n        return eqx.filter(model, lambda x: isinstance(x, eqx.nn.Linear))\n    \n    linear_params = get_linear_params(model)\n    \n    # Freeze layers\n    @eqx.filter_jit\n    def forward_with_frozen_first_layer(model, x):\n        \"\"\"Forward pass with first layer frozen.\"\"\"\n        # Partition into first layer and rest\n        first_layer = model.layers[0]\n        other_layers = model.layers[1:]\n        \n        # Stop gradient on first layer\n        x = jax.lax.stop_gradient(first_layer(x))\n        x = jax.nn.relu(x)\n        \n        # Continue with gradients\n        for layer in other_layers[:-1]:\n            x = layer(x)\n            x = jax.nn.relu(x)\n        \n        x = other_layers[-1](x)\n        return x\n    \n    print(\"  Filtered and frozen layer operations configured\")\n\nequinox_fundamentals()","type":"content","url":"/jax-scientific-stack#introduction-to-equinox","position":7},{"hierarchy":{"lvl1":"JAX Scientific Computing Stack: Equinox, Diffrax, and Optimistix","lvl3":"Training with Equinox","lvl2":"Equinox: Neural Networks as PyTrees"},"type":"lvl3","url":"/jax-scientific-stack#training-with-equinox","position":8},{"hierarchy":{"lvl1":"JAX Scientific Computing Stack: Equinox, Diffrax, and Optimistix","lvl3":"Training with Equinox","lvl2":"Equinox: Neural Networks as PyTrees"},"content":"def equinox_training():\n    \"\"\"Training loops and optimization with Equinox.\"\"\"\n    \n    print(\"\\nTRAINING WITH EQUINOX\")\n    print(\"=\" * 50)\n    \n    import optax\n    \n    # 1. Define model for photometric redshift\n    print(\"\\n1. PHOTOMETRIC REDSHIFT MODEL:\")\n    \n    class PhotoZNet(eqx.Module):\n        \"\"\"Estimate redshift from photometry.\"\"\"\n        encoder: eqx.nn.Sequential\n        decoder: eqx.nn.Sequential\n        \n        def __init__(self, n_bands: int, key):\n            key1, key2 = random.split(key)\n            \n            self.encoder = eqx.nn.Sequential([\n                eqx.nn.Linear(n_bands, 64, key=key1),\n                eqx.nn.Lambda(jax.nn.relu),\n                eqx.nn.Linear(64, 32, key=key2),\n                eqx.nn.Lambda(jax.nn.relu),\n            ])\n            \n            self.decoder = eqx.nn.Sequential([\n                eqx.nn.Linear(32, 16, key=key1),\n                eqx.nn.Lambda(jax.nn.relu),\n                eqx.nn.Linear(16, 1, key=key2),  # Single redshift value\n            ])\n        \n        def __call__(self, x: Float[Array, \"n_bands\"]) -> Float[Array, \"1\"]:\n            features = self.encoder(x)\n            return self.decoder(features).squeeze()\n    \n    # Initialize\n    key = random.PRNGKey(42)\n    model = PhotoZNet(n_bands=5, key=key)\n    \n    # 2. Loss functions\n    print(\"\\n2. LOSS FUNCTIONS:\")\n    \n    @eqx.filter_jit\n    def loss_fn(model, x, y_true):\n        \"\"\"MSE loss with outlier robustness.\"\"\"\n        y_pred = vmap(model)(x)\n        \n        # Huber loss for robustness\n        delta = 0.1\n        residual = jnp.abs(y_pred - y_true)\n        \n        loss = jnp.where(\n            residual < delta,\n            0.5 * residual ** 2,\n            delta * residual - 0.5 * delta ** 2\n        )\n        \n        return jnp.mean(loss)\n    \n    # Generate synthetic data\n    key, subkey = random.split(key)\n    n_samples = 1000\n    X_train = random.normal(subkey, (n_samples, 5))\n    z_true = random.uniform(subkey, (n_samples,), minval=0, maxval=3)\n    \n    # 3. Training loop\n    print(\"\\n3. TRAINING LOOP:\")\n    \n    # Optimizer\n    optimizer = optax.adam(learning_rate=1e-3)\n    opt_state = optimizer.init(eqx.filter(model, eqx.is_array))\n    \n    @eqx.filter_jit\n    def train_step(model, opt_state, x_batch, y_batch):\n        \"\"\"Single training step.\"\"\"\n        # Compute loss and gradients\n        loss, grads = eqx.filter_value_and_grad(loss_fn)(model, x_batch, y_batch)\n        \n        # Update parameters\n        updates, opt_state = optimizer.update(grads, opt_state)\n        model = eqx.apply_updates(model, updates)\n        \n        return model, opt_state, loss\n    \n    # Training\n    batch_size = 32\n    n_epochs = 10\n    \n    for epoch in range(n_epochs):\n        # Shuffle data\n        key, subkey = random.split(key)\n        perm = random.permutation(subkey, n_samples)\n        X_shuffled = X_train[perm]\n        z_shuffled = z_true[perm]\n        \n        # Mini-batches\n        epoch_loss = 0.0\n        n_batches = n_samples // batch_size\n        \n        for i in range(n_batches):\n            start = i * batch_size\n            end = start + batch_size\n            \n            x_batch = X_shuffled[start:end]\n            y_batch = z_shuffled[start:end]\n            \n            model, opt_state, loss = train_step(model, opt_state, x_batch, y_batch)\n            epoch_loss += loss\n        \n        if epoch % 2 == 0:\n            print(f\"  Epoch {epoch}: Loss = {epoch_loss/n_batches:.4f}\")\n    \n    # 4. Model evaluation\n    print(\"\\n4. MODEL EVALUATION:\")\n    \n    @eqx.filter_jit\n    def evaluate(model, x, y_true):\n        \"\"\"Evaluate model performance.\"\"\"\n        y_pred = vmap(model)(x)\n        \n        # Metrics\n        mse = jnp.mean((y_pred - y_true) ** 2)\n        mae = jnp.mean(jnp.abs(y_pred - y_true))\n        \n        # Outlier fraction (|Δz| > 0.15)\n        outliers = jnp.sum(jnp.abs(y_pred - y_true) > 0.15) / len(y_true)\n        \n        return {'mse': mse, 'mae': mae, 'outlier_frac': outliers}\n    \n    # Test set\n    X_test = random.normal(random.PRNGKey(123), (200, 5))\n    z_test = random.uniform(random.PRNGKey(124), (200,), minval=0, maxval=3)\n    \n    metrics = evaluate(model, X_test, z_test)\n    print(f\"  Test MSE: {metrics['mse']:.4f}\")\n    print(f\"  Test MAE: {metrics['mae']:.4f}\")\n    print(f\"  Outlier fraction: {metrics['outlier_frac']:.2%}\")\n\nequinox_training()","type":"content","url":"/jax-scientific-stack#training-with-equinox","position":9},{"hierarchy":{"lvl1":"JAX Scientific Computing Stack: Equinox, Diffrax, and Optimistix","lvl2":"Diffrax: Advanced ODE/SDE Solvers"},"type":"lvl2","url":"/jax-scientific-stack#diffrax-advanced-ode-sde-solvers","position":10},{"hierarchy":{"lvl1":"JAX Scientific Computing Stack: Equinox, Diffrax, and Optimistix","lvl2":"Diffrax: Advanced ODE/SDE Solvers"},"content":"","type":"content","url":"/jax-scientific-stack#diffrax-advanced-ode-sde-solvers","position":11},{"hierarchy":{"lvl1":"JAX Scientific Computing Stack: Equinox, Diffrax, and Optimistix","lvl3":"Solving ODEs with Diffrax","lvl2":"Diffrax: Advanced ODE/SDE Solvers"},"type":"lvl3","url":"/jax-scientific-stack#solving-odes-with-diffrax","position":12},{"hierarchy":{"lvl1":"JAX Scientific Computing Stack: Equinox, Diffrax, and Optimistix","lvl3":"Solving ODEs with Diffrax","lvl2":"Diffrax: Advanced ODE/SDE Solvers"},"content":"import diffrax\n\ndef diffrax_ode_solvers():\n    \"\"\"Advanced ODE solving for astronomical systems.\"\"\"\n    \n    print(\"\\nDIFFRAX: ADVANCED ODE SOLVERS\")\n    print(\"=\" * 50)\n    \n    # 1. Basic ODE: Binary star system\n    print(\"\\n1. BINARY STAR SYSTEM:\")\n    \n    def binary_system(t, y, args):\n        \"\"\"Binary star dynamics with tidal effects.\"\"\"\n        # y = [r1, v1, r2, v2] (6D each)\n        r1, v1, r2, v2 = y[0:3], y[3:6], y[6:9], y[9:12]\n        m1, m2 = args['m1'], args['m2']\n        \n        # Gravitational forces\n        r = r2 - r1\n        r_norm = jnp.linalg.norm(r)\n        f_grav = r / (r_norm**3 + 1e-10)\n        \n        # Tidal forces (simplified)\n        a1 = m2 * f_grav\n        a2 = -m1 * f_grav\n        \n        return jnp.concatenate([v1, a1, v2, a2])\n    \n    # Initial conditions\n    y0 = jnp.array([\n        1.0, 0.0, 0.0,    # r1\n        0.0, 0.3, 0.0,    # v1\n        -1.0, 0.0, 0.0,   # r2\n        0.0, -0.3, 0.0    # v2\n    ])\n    \n    # Solve with different methods\n    args = {'m1': 1.0, 'm2': 0.8}\n    t0, t1 = 0.0, 100.0\n    dt0 = 0.01\n    \n    # Dopri5 (adaptive RK4/5)\n    solver = diffrax.Dopri5()\n    term = diffrax.ODETerm(binary_system)\n    saveat = diffrax.SaveAt(ts=jnp.linspace(t0, t1, 1000))\n    \n    sol = diffrax.diffeqsolve(\n        term, solver, t0, t1, dt0, y0,\n        args=args, saveat=saveat,\n        stepsize_controller=diffrax.PIDController(rtol=1e-8, atol=1e-10)\n    )\n    \n    print(f\"  Solved with {sol.stats['num_steps']} steps\")\n    print(f\"  Final positions: r1={sol.ys[-1, 0:3]}, r2={sol.ys[-1, 6:9]}\")\n    \n    # 2. Stiff ODEs: Chemical evolution\n    print(\"\\n2. STIFF SYSTEM - CHEMICAL NETWORK:\")\n    \n    def chemical_network(t, y, args):\n        \"\"\"Simplified CNO cycle in stellar interior.\"\"\"\n        # y = [C12, N14, O16]\n        C12, N14, O16 = y\n        T = args['temperature']  # in 10^7 K\n        \n        # Reaction rates (simplified)\n        k1 = 1e-2 * jnp.exp(-15.0 / T)  # C12 + p -> N14\n        k2 = 1e-3 * jnp.exp(-20.0 / T)  # N14 + p -> O16\n        k3 = 1e-4 * jnp.exp(-25.0 / T)  # O16 + p -> C12\n        \n        dC12_dt = -k1 * C12 + k3 * O16\n        dN14_dt = k1 * C12 - k2 * N14\n        dO16_dt = k2 * N14 - k3 * O16\n        \n        return jnp.array([dC12_dt, dN14_dt, dO16_dt])\n    \n    # Use implicit solver for stiff system\n    y0_chem = jnp.array([1.0, 0.0, 0.0])  # Start with pure C12\n    \n    solver_stiff = diffrax.Kvaerno5()  # Implicit solver\n    term_stiff = diffrax.ODETerm(chemical_network)\n    \n    sol_stiff = diffrax.diffeqsolve(\n        term_stiff, solver_stiff, 0.0, 1e10, 1e6, y0_chem,\n        args={'temperature': 2.0},  # 20 million K\n        max_steps=10000\n    )\n    \n    print(f\"  Final abundances: C12={sol_stiff.ys[-1, 0]:.3f}, \" +\n          f\"N14={sol_stiff.ys[-1, 1]:.3f}, O16={sol_stiff.ys[-1, 2]:.3f}\")\n    \n    # 3. Event detection\n    print(\"\\n3. EVENT DETECTION - PERICENTER PASSAGE:\")\n    \n    def detect_pericenter(state, **kwargs):\n        \"\"\"Detect when binary reaches pericenter.\"\"\"\n        r1, v1, r2, v2 = state[0:3], state[3:6], state[6:9], state[9:12]\n        separation = jnp.linalg.norm(r2 - r1)\n        return separation - 0.5  # Trigger when separation < 0.5\n    \n    event = diffrax.DiscreteTerminatingEvent(detect_pericenter)\n    \n    sol_event = diffrax.diffeqsolve(\n        term, solver, t0, t1, dt0, y0,\n        args=args,\n        discrete_terminating_event=event\n    )\n    \n    if sol_event.event_mask:\n        print(f\"  Pericenter reached at t={sol_event.ts[-1]:.2f}\")\n    \n    # 4. Sensitivity analysis\n    print(\"\\n4. SENSITIVITY ANALYSIS:\")\n    \n    def binary_with_sensitivity(t, y, args):\n        \"\"\"Binary system with parameter sensitivity.\"\"\"\n        return binary_system(t, y, args)\n    \n    # Gradient with respect to initial conditions\n    @jit\n    def trajectory_loss(y0, args):\n        \"\"\"Loss based on final state.\"\"\"\n        sol = diffrax.diffeqsolve(\n            term, solver, t0, 10.0, dt0, y0,\n            args=args\n        )\n        return jnp.sum(sol.ys[-1] ** 2)\n    \n    grad_fn = grad(trajectory_loss)\n    sensitivity = grad_fn(y0, args)\n    print(f\"  Sensitivity of final state to initial conditions:\")\n    print(f\"  Max gradient: {jnp.max(jnp.abs(sensitivity)):.3e}\")\n\ndiffrax_ode_solvers()","type":"content","url":"/jax-scientific-stack#solving-odes-with-diffrax","position":13},{"hierarchy":{"lvl1":"JAX Scientific Computing Stack: Equinox, Diffrax, and Optimistix","lvl3":"Stochastic Differential Equations","lvl2":"Diffrax: Advanced ODE/SDE Solvers"},"type":"lvl3","url":"/jax-scientific-stack#stochastic-differential-equations","position":14},{"hierarchy":{"lvl1":"JAX Scientific Computing Stack: Equinox, Diffrax, and Optimistix","lvl3":"Stochastic Differential Equations","lvl2":"Diffrax: Advanced ODE/SDE Solvers"},"content":"def diffrax_sde_solvers():\n    \"\"\"Solve SDEs for stochastic astronomical processes.\"\"\"\n    \n    print(\"\\nSTOCHASTIC DIFFERENTIAL EQUATIONS\")\n    print(\"=\" * 50)\n    \n    # 1. Brownian motion in globular cluster\n    print(\"\\n1. STELLAR BROWNIAN MOTION:\")\n    \n    def drift(t, y, args):\n        \"\"\"Drift term: gravitational force.\"\"\"\n        # Simplified central potential\n        r = jnp.linalg.norm(y[:3])\n        force = -y[:3] / (r**3 + 0.1)\n        return jnp.concatenate([y[3:6], force])\n    \n    def diffusion(t, y, args):\n        \"\"\"Diffusion term: random kicks from encounters.\"\"\"\n        sigma = args['sigma']\n        # Only velocity gets random kicks\n        return jnp.concatenate([\n            jnp.zeros(3),\n            jnp.eye(3) * sigma\n        ])\n    \n    # Setup SDE\n    key = random.PRNGKey(0)\n    \n    brownian_motion = diffrax.VirtualBrownianTree(\n        t0=0.0, t1=100.0, tol=1e-3, shape=(3,), key=key\n    )\n    \n    drift_term = diffrax.ODETerm(drift)\n    diffusion_term = diffrax.ControlTerm(diffusion, brownian_motion)\n    terms = diffrax.MultiTerm(drift_term, diffusion_term)\n    \n    solver_sde = diffrax.Euler()  # Euler-Maruyama for SDEs\n    \n    y0_sde = jnp.array([1.0, 0.0, 0.0, 0.0, 1.0, 0.0])  # position, velocity\n    \n    # Solve SDE\n    sol_sde = diffrax.diffeqsolve(\n        terms, solver_sde, 0.0, 100.0, 0.01, y0_sde,\n        args={'sigma': 0.01},\n        saveat=diffrax.SaveAt(ts=jnp.linspace(0, 100, 1000))\n    )\n    \n    positions = sol_sde.ys[:, :3]\n    print(f\"  Final position: {positions[-1]}\")\n    print(f\"  RMS displacement: {jnp.sqrt(jnp.mean(jnp.sum(positions**2, axis=1))):.3f}\")\n    \n    # 2. Stochastic accretion\n    print(\"\\n2. STOCHASTIC ACCRETION DISK:\")\n    \n    def accretion_drift(t, y, args):\n        \"\"\"Mean accretion rate.\"\"\"\n        M, mdot = y\n        alpha = args['alpha']  # Viscosity parameter\n        \n        # Simplified accretion model\n        dM_dt = mdot\n        dmdot_dt = -alpha * mdot  # Decay of accretion\n        \n        return jnp.array([dM_dt, dmdot_dt])\n    \n    def accretion_noise(t, y, args):\n        \"\"\"Turbulent fluctuations.\"\"\"\n        M, mdot = y\n        beta = args['beta']\n        \n        # Noise proportional to accretion rate\n        return jnp.array([[0.0], [beta * jnp.sqrt(jnp.abs(mdot))]])\n    \n    # Setup\n    key, subkey = random.split(key)\n    brownian_1d = diffrax.VirtualBrownianTree(\n        t0=0.0, t1=10.0, tol=1e-3, shape=(1,), key=subkey\n    )\n    \n    drift_acc = diffrax.ODETerm(accretion_drift)\n    diffusion_acc = diffrax.ControlTerm(accretion_noise, brownian_1d)\n    terms_acc = diffrax.MultiTerm(drift_acc, diffusion_acc)\n    \n    y0_acc = jnp.array([1.0, 0.1])  # Initial mass and accretion rate\n    \n    sol_acc = diffrax.diffeqsolve(\n        terms_acc, solver_sde, 0.0, 10.0, 0.001, y0_acc,\n        args={'alpha': 0.1, 'beta': 0.05}\n    )\n    \n    print(f\"  Final mass: {sol_acc.ys[-1, 0]:.3f}\")\n    print(f\"  Final accretion rate: {sol_acc.ys[-1, 1]:.4f}\")\n    \n    # 3. Jump diffusion for flares\n    print(\"\\n3. JUMP DIFFUSION - STELLAR FLARES:\")\n    \n    class FlareProcess(eqx.Module):\n        \"\"\"Jump process for stellar flares.\"\"\"\n        rate: float = 0.1  # Flare rate\n        \n        def __call__(self, t, y, args, key):\n            \"\"\"Generate jump if flare occurs.\"\"\"\n            subkey1, subkey2 = random.split(key)\n            \n            # Poisson process for flare occurrence\n            occurs = random.uniform(subkey1) < self.rate * 0.01  # dt = 0.01\n            \n            # Flare amplitude (log-normal distribution)\n            amplitude = jnp.where(\n                occurs,\n                jnp.exp(random.normal(subkey2) * 0.5),\n                0.0\n            )\n            \n            return jnp.array([amplitude])\n    \n    # Combined drift-diffusion-jump process\n    def luminosity_drift(t, y, args):\n        \"\"\"Decay of luminosity.\"\"\"\n        return -0.1 * y  # Exponential decay\n    \n    # Solve with jumps (simplified - Diffrax doesn't have built-in jump diffusion)\n    # This would require custom implementation\n    \n    print(\"  Jump diffusion setup complete (requires custom implementation)\")\n\ndiffrax_sde_solvers()","type":"content","url":"/jax-scientific-stack#stochastic-differential-equations","position":15},{"hierarchy":{"lvl1":"JAX Scientific Computing Stack: Equinox, Diffrax, and Optimistix","lvl2":"Optimistix: Root Finding and Optimization"},"type":"lvl2","url":"/jax-scientific-stack#optimistix-root-finding-and-optimization","position":16},{"hierarchy":{"lvl1":"JAX Scientific Computing Stack: Equinox, Diffrax, and Optimistix","lvl2":"Optimistix: Root Finding and Optimization"},"content":"","type":"content","url":"/jax-scientific-stack#optimistix-root-finding-and-optimization","position":17},{"hierarchy":{"lvl1":"JAX Scientific Computing Stack: Equinox, Diffrax, and Optimistix","lvl3":"Nonlinear Solvers","lvl2":"Optimistix: Root Finding and Optimization"},"type":"lvl3","url":"/jax-scientific-stack#nonlinear-solvers","position":18},{"hierarchy":{"lvl1":"JAX Scientific Computing Stack: Equinox, Diffrax, and Optimistix","lvl3":"Nonlinear Solvers","lvl2":"Optimistix: Root Finding and Optimization"},"content":"import optimistix as optx\n\ndef optimistix_solvers():\n    \"\"\"Root finding and optimization for astronomical problems.\"\"\"\n    \n    print(\"\\nOPTIMISTIX: ROOT FINDING & OPTIMIZATION\")\n    print(\"=\" * 50)\n    \n    # 1. Root finding: Kepler's equation\n    print(\"\\n1. KEPLER'S EQUATION:\")\n    \n    @jit\n    def kepler_equation(E, M_and_e):\n        \"\"\"Kepler's equation: M = E - e*sin(E)\"\"\"\n        M, e = M_and_e\n        return E - e * jnp.sin(E) - M\n    \n    # Solve for different mean anomalies\n    M_values = jnp.linspace(0, 2*jnp.pi, 10)\n    e = 0.5  # Eccentricity\n    \n    E_solutions = []\n    for M in M_values:\n        solver = optx.Newton(rtol=1e-8, atol=1e-10)\n        sol = optx.root_find(\n            lambda E: kepler_equation(E, (M, e)),\n            solver,\n            M,  # Initial guess\n            max_steps=100\n        )\n        E_solutions.append(sol.value)\n    \n    E_solutions = jnp.array(E_solutions)\n    print(f\"  Solved {len(E_solutions)} values\")\n    print(f\"  Max residual: {max(abs(kepler_equation(E, (M, e))) for E, M in zip(E_solutions, M_values)):.2e}\")\n    \n    # 2. Least squares: Orbit fitting\n    print(\"\\n2. ORBIT FITTING WITH LEAST SQUARES:\")\n    \n    def orbit_residuals(params, data):\n        \"\"\"Residuals for orbit fitting.\"\"\"\n        a, e, i, omega, Omega, T0 = params\n        times, ra, dec = data\n        \n        # Compute predicted positions (simplified)\n        n = 2 * jnp.pi / (a ** 1.5)  # Mean motion\n        M = n * (times - T0)\n        \n        # Would solve Kepler's equation here for each M\n        # For simplicity, use small eccentricity approximation\n        E = M + e * jnp.sin(M)\n        \n        # True anomaly (simplified)\n        f = E + 2 * e * jnp.sin(E)\n        \n        # Predicted RA/Dec (very simplified!)\n        ra_pred = a * jnp.cos(f + omega)\n        dec_pred = a * jnp.sin(f + omega) * jnp.sin(i)\n        \n        return jnp.concatenate([ra - ra_pred, dec - dec_pred])\n    \n    # Synthetic observations\n    key = random.PRNGKey(42)\n    n_obs = 20\n    times = jnp.linspace(0, 10, n_obs)\n    true_params = jnp.array([1.0, 0.3, 0.5, 0.2, 0.1, 0.0])\n    \n    # Generate noisy observations\n    noise_key = random.split(key, 2)\n    ra_obs = true_params[0] * jnp.cos(2*jnp.pi/true_params[0]**1.5 * times) + \\\n              0.01 * random.normal(noise_key[0], (n_obs,))\n    dec_obs = true_params[0] * jnp.sin(2*jnp.pi/true_params[0]**1.5 * times) * 0.5 + \\\n               0.01 * random.normal(noise_key[1], (n_obs,))\n    \n    data = (times, ra_obs, dec_obs)\n    \n    # Least squares solver\n    solver_ls = optx.LevenbergMarquardt(rtol=1e-6, atol=1e-8)\n    initial_guess = jnp.array([1.1, 0.2, 0.4, 0.3, 0.2, 0.1])\n    \n    sol_ls = optx.least_squares(\n        lambda p: orbit_residuals(p, data),\n        solver_ls,\n        initial_guess,\n        max_steps=100\n    )\n    \n    print(f\"  Converged in {sol_ls.stats['num_steps']} steps\")\n    print(f\"  True params: {true_params}\")\n    print(f\"  Fitted params: {sol_ls.value}\")\n    \n    # 3. Minimization: Maximum likelihood\n    print(\"\\n3. MAXIMUM LIKELIHOOD OPTIMIZATION:\")\n    \n    def negative_log_likelihood(params, data):\n        \"\"\"Negative log likelihood for power law fit.\"\"\"\n        alpha, x_min = params\n        x_data = data\n        \n        # Power law likelihood\n        if alpha <= 1.0 or x_min <= 0:\n            return jnp.inf\n        \n        norm = (alpha - 1) / x_min * (x_min / jnp.maximum(x_data, x_min)) ** alpha\n        log_like = jnp.sum(jnp.log(norm))\n        \n        return -log_like\n    \n    # Generate power law distributed data (e.g., mass function)\n    key, subkey = random.split(key)\n    alpha_true = 2.35  # Salpeter IMF\n    x_min_true = 0.1\n    n_stars = 1000\n    \n    # Inverse transform sampling\n    u = random.uniform(subkey, (n_stars,))\n    masses = x_min_true * (1 - u) ** (-1/(alpha_true - 1))\n    \n    # Optimize\n    solver_opt = optx.BFGS(rtol=1e-6, atol=1e-8)\n    initial = jnp.array([2.0, 0.15])\n    \n    sol_opt = optx.minimise(\n        lambda p: negative_log_likelihood(p, masses),\n        solver_opt,\n        initial,\n        max_steps=100\n    )\n    \n    print(f\"  True parameters: α={alpha_true}, x_min={x_min_true}\")\n    print(f\"  MLE estimates: α={sol_opt.value[0]:.3f}, x_min={sol_opt.value[1]:.3f}\")\n    \n    # 4. Fixed point iteration\n    print(\"\\n4. FIXED POINT - STELLAR STRUCTURE:\")\n    \n    def stellar_structure_iteration(y, args):\n        \"\"\"\n        Fixed point iteration for stellar structure.\n        Simplified Lane-Emden equation.\n        \"\"\"\n        rho, T = y\n        gamma = args['gamma']\n        \n        # Hydrostatic equilibrium + energy transport\n        rho_new = T ** (1/(gamma - 1))\n        T_new = rho_new ** (gamma - 1)\n        \n        # Add some nonlinearity\n        rho_new = 0.9 * rho_new + 0.1 * rho\n        T_new = 0.9 * T_new + 0.1 * T\n        \n        return jnp.array([rho_new, T_new])\n    \n    solver_fp = optx.FixedPointIteration(rtol=1e-6, atol=1e-8)\n    initial_structure = jnp.array([1.0, 1.0])\n    \n    sol_fp = optx.fixed_point(\n        lambda y: stellar_structure_iteration(y, {'gamma': 5/3}),\n        solver_fp,\n        initial_structure,\n        max_steps=100\n    )\n    \n    print(f\"  Converged to: ρ={sol_fp.value[0]:.3f}, T={sol_fp.value[1]:.3f}\")\n\noptimistix_solvers()","type":"content","url":"/jax-scientific-stack#nonlinear-solvers","position":19},{"hierarchy":{"lvl1":"JAX Scientific Computing Stack: Equinox, Diffrax, and Optimistix","lvl2":"Lineax: Linear Algebra"},"type":"lvl2","url":"/jax-scientific-stack#lineax-linear-algebra","position":20},{"hierarchy":{"lvl1":"JAX Scientific Computing Stack: Equinox, Diffrax, and Optimistix","lvl2":"Lineax: Linear Algebra"},"content":"","type":"content","url":"/jax-scientific-stack#lineax-linear-algebra","position":21},{"hierarchy":{"lvl1":"JAX Scientific Computing Stack: Equinox, Diffrax, and Optimistix","lvl3":"Linear System Solvers","lvl2":"Lineax: Linear Algebra"},"type":"lvl3","url":"/jax-scientific-stack#linear-system-solvers","position":22},{"hierarchy":{"lvl1":"JAX Scientific Computing Stack: Equinox, Diffrax, and Optimistix","lvl3":"Linear System Solvers","lvl2":"Lineax: Linear Algebra"},"content":"import lineax as lx\n\ndef lineax_solvers():\n    \"\"\"Advanced linear algebra for astronomical applications.\"\"\"\n    \n    print(\"\\nLINEAX: LINEAR ALGEBRA SOLVERS\")\n    print(\"=\" * 50)\n    \n    # 1. Basic linear solve\n    print(\"\\n1. BASIC LINEAR SYSTEM:\")\n    \n    # Poisson equation for gravitational potential\n    def create_poisson_system(n):\n        \"\"\"Create discrete Poisson equation.\"\"\"\n        # -∇²φ = 4πGρ\n        # Finite difference discretization\n        h = 1.0 / n\n        \n        # Tridiagonal matrix (1D simplification)\n        A = (\n            -2 * jnp.eye(n) +\n            jnp.eye(n, k=1) +\n            jnp.eye(n, k=-1)\n        ) / h**2\n        \n        # Random density distribution\n        key = random.PRNGKey(0)\n        rho = random.uniform(key, (n,))\n        b = 4 * jnp.pi * rho\n        \n        return A, b\n    \n    A, b = create_poisson_system(100)\n    \n    # Direct solve\n    solver = lx.LU()\n    sol = lx.linear_solve(A, b, solver)\n    \n    print(f\"  Solved {A.shape[0]}x{A.shape[0]} system\")\n    print(f\"  Residual norm: {jnp.linalg.norm(A @ sol.value - b):.2e}\")\n    \n    # 2. Iterative solvers for large systems\n    print(\"\\n2. ITERATIVE SOLVERS:\")\n    \n    # Large sparse system (PSF deconvolution)\n    def psf_convolution_operator(x, psf_kernel):\n        \"\"\"Apply PSF convolution.\"\"\"\n        # Simplified: just blur with kernel\n        return jax.scipy.signal.convolve(x, psf_kernel, mode='same')\n    \n    # Create blurred image problem\n    n_pixels = 1000\n    key = random.PRNGKey(1)\n    \n    # PSF kernel (Gaussian)\n    x_kernel = jnp.arange(-5, 6)\n    psf_kernel = jnp.exp(-x_kernel**2 / 2) / jnp.sqrt(2 * jnp.pi)\n    \n    # True image (point sources)\n    true_image = jnp.zeros(n_pixels)\n    source_positions = random.choice(key, n_pixels, (10,), replace=False)\n    true_image = true_image.at[source_positions].set(1.0)\n    \n    # Blurred observation\n    blurred = psf_convolution_operator(true_image, psf_kernel)\n    noise = 0.01 * random.normal(key, (n_pixels,))\n    observed = blurred + noise\n    \n    # Define linear operator\n    psf_op = lx.FunctionLinearOperator(\n        lambda x: psf_convolution_operator(x, psf_kernel),\n        observed.shape\n    )\n    \n    # Conjugate gradient solver\n    solver_cg = lx.CG(rtol=1e-5, atol=1e-7)\n    sol_cg = lx.linear_solve(psf_op, observed, solver_cg)\n    \n    print(f\"  Deconvolved image with CG\")\n    print(f\"  Iterations: {sol_cg.stats['num_steps']}\")\n    \n    # 3. Matrix decompositions\n    print(\"\\n3. MATRIX DECOMPOSITIONS:\")\n    \n    # Covariance matrix for galaxy clustering\n    def create_covariance_matrix(n_galaxies, correlation_length=0.1):\n        \"\"\"Create spatial covariance matrix.\"\"\"\n        # Positions\n        key = random.PRNGKey(2)\n        positions = random.uniform(key, (n_galaxies, 2))\n        \n        # Distance matrix\n        dist = jnp.sqrt(\n            ((positions[:, None, :] - positions[None, :, :]) ** 2).sum(axis=2)\n        )\n        \n        # Exponential covariance\n        C = jnp.exp(-dist / correlation_length)\n        \n        return C, positions\n    \n    C, positions = create_covariance_matrix(50)\n    \n    # Eigendecomposition for PCA\n    eigenvalues, eigenvectors = jnp.linalg.eigh(C)\n    \n    print(f\"  Covariance matrix: {C.shape}\")\n    print(f\"  Top 5 eigenvalues: {eigenvalues[-5:]}\")\n    print(f\"  Explained variance (top 10): {jnp.sum(eigenvalues[-10:]) / jnp.sum(eigenvalues):.1%}\")\n    \n    # 4. Regularized solutions\n    print(\"\\n4. REGULARIZED INVERSE PROBLEMS:\")\n    \n    def tikhonov_solve(A, b, alpha=0.01):\n        \"\"\"Tikhonov regularization.\"\"\"\n        # Solve (A^T A + alpha I) x = A^T b\n        n = A.shape[1]\n        A_reg = A.T @ A + alpha * jnp.eye(n)\n        b_reg = A.T @ b\n        \n        solver = lx.LU()\n        sol = lx.linear_solve(A_reg, b_reg, solver)\n        \n        return sol.value\n    \n    # Ill-conditioned problem\n    A_ill = random.normal(random.PRNGKey(3), (100, 50))\n    # Make it ill-conditioned\n    U, S, Vt = jnp.linalg.svd(A_ill, full_matrices=False)\n    S = S.at[:10].set(S[:10] * 1e-6)  # Small singular values\n    A_ill = U @ jnp.diag(S) @ Vt\n    \n    x_true = random.normal(random.PRNGKey(4), (50,))\n    b_ill = A_ill @ x_true + 0.01 * random.normal(random.PRNGKey(5), (100,))\n    \n    # Compare regularized vs non-regularized\n    x_reg = tikhonov_solve(A_ill, b_ill, alpha=0.1)\n    \n    print(f\"  Condition number: {jnp.linalg.cond(A_ill):.2e}\")\n    print(f\"  Regularized solution error: {jnp.linalg.norm(x_reg - x_true) / jnp.linalg.norm(x_true):.3f}\")\n\nlineax_solvers()","type":"content","url":"/jax-scientific-stack#linear-system-solvers","position":23},{"hierarchy":{"lvl1":"JAX Scientific Computing Stack: Equinox, Diffrax, and Optimistix","lvl2":"jaxtyping: Runtime Type Checking"},"type":"lvl2","url":"/jax-scientific-stack#jaxtyping-runtime-type-checking","position":24},{"hierarchy":{"lvl1":"JAX Scientific Computing Stack: Equinox, Diffrax, and Optimistix","lvl2":"jaxtyping: Runtime Type Checking"},"content":"","type":"content","url":"/jax-scientific-stack#jaxtyping-runtime-type-checking","position":25},{"hierarchy":{"lvl1":"JAX Scientific Computing Stack: Equinox, Diffrax, and Optimistix","lvl3":"Type-Safe Scientific Computing","lvl2":"jaxtyping: Runtime Type Checking"},"type":"lvl3","url":"/jax-scientific-stack#type-safe-scientific-computing","position":26},{"hierarchy":{"lvl1":"JAX Scientific Computing Stack: Equinox, Diffrax, and Optimistix","lvl3":"Type-Safe Scientific Computing","lvl2":"jaxtyping: Runtime Type Checking"},"content":"from jaxtyping import Float, Int, Bool, Complex, PyTree, Shaped, jaxtyped\nfrom typeguard import typechecked as typechecker\n\ndef jaxtyping_examples():\n    \"\"\"Type checking for safer scientific code.\"\"\"\n    \n    print(\"\\nJAXTYPING: TYPE-SAFE SCIENTIFIC COMPUTING\")\n    print(\"=\" * 50)\n    \n    # 1. Basic type annotations\n    print(\"\\n1. BASIC TYPE ANNOTATIONS:\")\n    \n    @jaxtyped(typechecker=typechecker)\n    def process_spectrum(\n        wavelengths: Float[Array, \"n_wavelengths\"],\n        flux: Float[Array, \"n_wavelengths\"],\n        errors: Optional[Float[Array, \"n_wavelengths\"]] = None\n    ) -> tuple[Float[Array, \"\"], Float[Array, \"\"]]:\n        \"\"\"Process spectrum with type checking.\"\"\"\n        \n        # Compute signal-to-noise\n        if errors is not None:\n            snr = flux / errors\n            median_snr = jnp.median(snr)\n        else:\n            median_snr = jnp.nan\n        \n        # Compute equivalent width\n        ew = jnp.trapz(1 - flux, wavelengths)\n        \n        return median_snr, ew\n    \n    # Test with correct types\n    wl = jnp.linspace(4000, 7000, 1000)\n    flux = 1.0 - 0.1 * jnp.exp(-(wl - 5500)**2 / 100**2)\n    errors = 0.01 * jnp.ones_like(flux)\n    \n    snr, ew = process_spectrum(wl, flux, errors)\n    print(f\"  SNR: {snr:.1f}, EW: {ew:.1f} Å\")\n    \n    # 2. Complex shape relationships\n    print(\"\\n2. COMPLEX SHAPE ANNOTATIONS:\")\n    \n    @jaxtyped(typechecker=typechecker)\n    def n_body_step(\n        positions: Float[Array, \"n_bodies 3\"],\n        velocities: Float[Array, \"n_bodies 3\"],\n        masses: Float[Array, \"n_bodies\"],\n        dt: float\n    ) -> tuple[Float[Array, \"n_bodies 3\"], Float[Array, \"n_bodies 3\"]]:\n        \"\"\"N-body integration step with shape checking.\"\"\"\n        \n        n = positions.shape[0]\n        forces = jnp.zeros_like(positions)\n        \n        # Compute forces\n        for i in range(n):\n            for j in range(n):\n                if i != j:\n                    r_ij = positions[j] - positions[i]\n                    r3 = jnp.linalg.norm(r_ij) ** 3\n                    forces = forces.at[i].add(masses[j] * r_ij / r3)\n        \n        # Update\n        accelerations = forces\n        new_velocities = velocities + accelerations * dt\n        new_positions = positions + new_velocities * dt\n        \n        return new_positions, new_velocities\n    \n    # Test\n    n = 3\n    pos = random.normal(random.PRNGKey(0), (n, 3))\n    vel = random.normal(random.PRNGKey(1), (n, 3)) * 0.1\n    m = jnp.ones(n)\n    \n    new_pos, new_vel = n_body_step(pos, vel, m, 0.01)\n    print(f\"  Updated {n} bodies\")\n    \n    # 3. PyTree annotations\n    print(\"\\n3. PYTREE TYPE ANNOTATIONS:\")\n    \n    @jaxtyped(typechecker=typechecker)\n    class GalaxyModel(eqx.Module):\n        \"\"\"Type-annotated galaxy model.\"\"\"\n        \n        disk_params: dict[str, Float[Array, \"...\"]]\n        bulge_params: dict[str, Float[Array, \"...\"]]\n        n_stars: Int[Array, \"\"]\n        \n        def __init__(self, n_stars: int):\n            self.disk_params = {\n                'scale_radius': jnp.array(5.0),\n                'scale_height': jnp.array(0.5),\n                'mass': jnp.array(1e10)\n            }\n            self.bulge_params = {\n                'effective_radius': jnp.array(1.0),\n                'sersic_index': jnp.array(4.0),\n                'mass': jnp.array(1e9)\n            }\n            self.n_stars = jnp.array(n_stars)\n        \n        @jaxtyped(typechecker=typechecker)\n        def density_profile(\n            self, \n            r: Float[Array, \"n_points\"],\n            component: str = 'disk'\n        ) -> Float[Array, \"n_points\"]:\n            \"\"\"Compute density profile.\"\"\"\n            \n            if component == 'disk':\n                r_d = self.disk_params['scale_radius']\n                return jnp.exp(-r / r_d) / r_d\n            else:\n                r_e = self.bulge_params['effective_radius']\n                n = self.bulge_params['sersic_index']\n                return jnp.exp(-7.67 * ((r / r_e) ** (1/n) - 1))\n    \n    galaxy = GalaxyModel(n_stars=10000)\n    radii = jnp.linspace(0.1, 20, 100)\n    density = galaxy.density_profile(radii, 'disk')\n    print(f\"  Disk density computed at {len(radii)} points\")\n    \n    # 4. Dimension variables\n    print(\"\\n4. DIMENSION VARIABLES:\")\n    \n    from jaxtyping import Float32, Float64\n    \n    @jaxtyped(typechecker=typechecker)\n    def mixed_precision_computation(\n        high_precision: Float64[Array, \"n m\"],\n        low_precision: Float32[Array, \"n m\"]\n    ) -> Float64[Array, \"n n\"]:\n        \"\"\"Mixed precision matrix computation.\"\"\"\n        \n        # Upcast low precision\n        low_as_high = low_precision.astype(jnp.float64)\n        \n        # Compute in high precision\n        result = high_precision @ low_as_high.T\n        \n        return result\n    \n    # Test mixed precision\n    hp = jnp.array(random.normal(random.PRNGKey(6), (10, 5)), dtype=jnp.float64)\n    lp = jnp.array(random.normal(random.PRNGKey(7), (10, 5)), dtype=jnp.float32)\n    \n    result = mixed_precision_computation(hp, lp)\n    print(f\"  Mixed precision result: {result.dtype}, shape {result.shape}\")\n\njaxtyping_examples()","type":"content","url":"/jax-scientific-stack#type-safe-scientific-computing","position":27},{"hierarchy":{"lvl1":"JAX Scientific Computing Stack: Equinox, Diffrax, and Optimistix","lvl2":"Integration Example: Complete Scientific Workflow"},"type":"lvl2","url":"/jax-scientific-stack#integration-example-complete-scientific-workflow","position":28},{"hierarchy":{"lvl1":"JAX Scientific Computing Stack: Equinox, Diffrax, and Optimistix","lvl2":"Integration Example: Complete Scientific Workflow"},"content":"","type":"content","url":"/jax-scientific-stack#integration-example-complete-scientific-workflow","position":29},{"hierarchy":{"lvl1":"JAX Scientific Computing Stack: Equinox, Diffrax, and Optimistix","lvl3":"Combining All Libraries","lvl2":"Integration Example: Complete Scientific Workflow"},"type":"lvl3","url":"/jax-scientific-stack#combining-all-libraries","position":30},{"hierarchy":{"lvl1":"JAX Scientific Computing Stack: Equinox, Diffrax, and Optimistix","lvl3":"Combining All Libraries","lvl2":"Integration Example: Complete Scientific Workflow"},"content":"def complete_scientific_workflow():\n    \"\"\"\n    Complete workflow combining Equinox, Diffrax, Optimistix, and Lineax.\n    Example: Fitting a dynamical model to observations.\n    \"\"\"\n    \n    print(\"\\nCOMPLETE SCIENTIFIC WORKFLOW\")\n    print(\"=\" * 50)\n    \n    # 1. Define the dynamical model with Equinox\n    class DynamicalModel(eqx.Module):\n        \"\"\"Neural ODE for galaxy dynamics.\"\"\"\n        \n        potential_net: eqx.nn.MLP\n        \n        def __init__(self, key):\n            self.potential_net = eqx.nn.MLP(\n                in_size=3,  # x, y, z\n                out_size=1,  # potential\n                width_size=64,\n                depth=3,\n                activation=jax.nn.tanh,\n                key=key\n            )\n        \n        def potential(self, position: Float[Array, \"3\"]) -> Float[Array, \"\"]:\n            \"\"\"Gravitational potential.\"\"\"\n            return self.potential_net(position).squeeze()\n        \n        def dynamics(self, t, state, args):\n            \"\"\"Hamiltonian dynamics.\"\"\"\n            q, p = state[:3], state[3:]\n            \n            # Gradient of potential\n            dV_dq = grad(self.potential)(q)\n            \n            dq_dt = p  # dq/dt = p\n            dp_dt = -dV_dq  # dp/dt = -∇V\n            \n            return jnp.concatenate([dq_dt, dp_dt])\n    \n    # 2. Generate synthetic observations\n    key = random.PRNGKey(42)\n    key, model_key, data_key = random.split(key, 3)\n    \n    true_model = DynamicalModel(model_key)\n    \n    # Integrate orbits with Diffrax\n    def integrate_orbit(model, initial_state, t_obs):\n        \"\"\"Integrate orbit and return positions at observation times.\"\"\"\n        \n        term = diffrax.ODETerm(model.dynamics)\n        solver = diffrax.Dopri5()\n        saveat = diffrax.SaveAt(ts=t_obs)\n        \n        sol = diffrax.diffeqsolve(\n            term, solver, t_obs[0], t_obs[-1], 0.01, initial_state,\n            saveat=saveat\n        )\n        \n        return sol.ys[:, :3]  # Return positions only\n    \n    # Generate observations\n    n_orbits = 5\n    n_obs_per_orbit = 20\n    \n    observations = []\n    initial_states = []\n    \n    for i in range(n_orbits):\n        key, subkey = random.split(key)\n        \n        # Random initial condition\n        q0 = random.normal(subkey, (3,)) * 2\n        p0 = random.normal(subkey, (3,)) * 0.5\n        initial = jnp.concatenate([q0, p0])\n        initial_states.append(initial)\n        \n        # Observation times\n        t_obs = jnp.linspace(0, 10, n_obs_per_orbit)\n        \n        # Integrate and add noise\n        true_positions = integrate_orbit(true_model, initial, t_obs)\n        noise = 0.05 * random.normal(subkey, true_positions.shape)\n        observed = true_positions + noise\n        \n        observations.append(observed)\n    \n    print(f\"  Generated {n_orbits} orbits with {n_obs_per_orbit} observations each\")\n    \n    # 3. Define loss function\n    @eqx.filter_jit\n    def loss_function(model, initial_states, observations, t_obs):\n        \"\"\"Loss for orbit fitting.\"\"\"\n        \n        total_loss = 0.0\n        \n        for initial, observed in zip(initial_states, observations):\n            predicted = integrate_orbit(model, initial, t_obs)\n            residuals = predicted - observed\n            total_loss += jnp.sum(residuals ** 2)\n        \n        return total_loss / len(observations)\n    \n    # 4. Optimize with Optimistix\n    print(\"\\n  Fitting model to observations...\")\n    \n    # Initialize model to fit\n    key, fit_key = random.split(key)\n    fitted_model = DynamicalModel(fit_key)\n    \n    t_obs = jnp.linspace(0, 10, n_obs_per_orbit)\n    \n    # Use gradient descent\n    import optax\n    \n    optimizer = optax.adam(learning_rate=1e-3)\n    opt_state = optimizer.init(eqx.filter(fitted_model, eqx.is_array))\n    \n    # Training loop\n    n_epochs = 50\n    \n    for epoch in range(n_epochs):\n        loss, grads = eqx.filter_value_and_grad(loss_function)(\n            fitted_model, initial_states, observations, t_obs\n        )\n        \n        updates, opt_state = optimizer.update(grads, opt_state)\n        fitted_model = eqx.apply_updates(fitted_model, updates)\n        \n        if epoch % 10 == 0:\n            print(f\"    Epoch {epoch}: Loss = {loss:.4f}\")\n    \n    # 5. Analyze results with Lineax\n    print(\"\\n  Analyzing fitted model...\")\n    \n    # Compute Hessian at minimum for uncertainty estimation\n    def potential_at_points(model, points):\n        \"\"\"Evaluate potential at multiple points.\"\"\"\n        return vmap(model.potential)(points)\n    \n    # Sample points\n    test_points = random.normal(random.PRNGKey(99), (100, 3))\n    \n    # Compare potentials\n    true_pot = potential_at_points(true_model, test_points)\n    fitted_pot = potential_at_points(fitted_model, test_points)\n    \n    error = jnp.sqrt(jnp.mean((true_pot - fitted_pot) ** 2))\n    print(f\"  RMS potential error: {error:.4f}\")\n    \n    # Compute correlation\n    correlation = jnp.corrcoef(true_pot, fitted_pot)[0, 1]\n    print(f\"  Potential correlation: {correlation:.3f}\")\n    \n    # Visualize one orbit\n    initial_test = initial_states[0]\n    t_plot = jnp.linspace(0, 20, 200)\n    \n    true_orbit = integrate_orbit(true_model, initial_test, t_plot)\n    fitted_orbit = integrate_orbit(fitted_model, initial_test, t_plot)\n    \n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n    \n    # True model\n    axes[0].plot(true_orbit[:, 0], true_orbit[:, 1], 'b-', label='True')\n    axes[0].plot(fitted_orbit[:, 0], fitted_orbit[:, 1], 'r--', label='Fitted')\n    axes[0].set_xlabel('x')\n    axes[0].set_ylabel('y')\n    axes[0].set_title('Orbit Comparison')\n    axes[0].legend()\n    axes[0].set_aspect('equal')\n    \n    # Potential contours\n    x = y = jnp.linspace(-3, 3, 50)\n    X, Y = jnp.meshgrid(x, y)\n    Z = jnp.zeros_like(X)\n    \n    points_grid = jnp.stack([X.ravel(), Y.ravel(), Z.ravel()], axis=1)\n    true_pot_grid = potential_at_points(true_model, points_grid).reshape(X.shape)\n    fitted_pot_grid = potential_at_points(fitted_model, points_grid).reshape(X.shape)\n    \n    axes[1].contour(X, Y, true_pot_grid, levels=10, colors='blue', alpha=0.5, label='True')\n    axes[1].contour(X, Y, fitted_pot_grid, levels=10, colors='red', alpha=0.5, linestyles='--', label='Fitted')\n    axes[1].set_xlabel('x')\n    axes[1].set_ylabel('y')\n    axes[1].set_title('Potential Contours (z=0)')\n    axes[1].set_aspect('equal')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return fitted_model\n\n# Run complete workflow\nfitted_model = complete_scientific_workflow()","type":"content","url":"/jax-scientific-stack#combining-all-libraries","position":31},{"hierarchy":{"lvl1":"JAX Scientific Computing Stack: Equinox, Diffrax, and Optimistix","lvl2":"Key Takeaways"},"type":"lvl2","url":"/jax-scientific-stack#key-takeaways","position":32},{"hierarchy":{"lvl1":"JAX Scientific Computing Stack: Equinox, Diffrax, and Optimistix","lvl2":"Key Takeaways"},"content":"✅ Equinox - Neural networks and models as PyTrees, perfect for scientific ML✅ Diffrax - State-of-the-art ODE/SDE solvers with automatic differentiation✅ Optimistix - Root finding, optimization, and fixed-point solvers✅ Lineax - Linear algebra solvers with multiple backends✅ jaxtyping - Runtime type checking for safer scientific code✅ Integration - These libraries work seamlessly together for complex workflows","type":"content","url":"/jax-scientific-stack#key-takeaways","position":33},{"hierarchy":{"lvl1":"JAX Scientific Computing Stack: Equinox, Diffrax, and Optimistix","lvl2":"Next Chapter Preview"},"type":"lvl2","url":"/jax-scientific-stack#next-chapter-preview","position":34},{"hierarchy":{"lvl1":"JAX Scientific Computing Stack: Equinox, Diffrax, and Optimistix","lvl2":"Next Chapter Preview"},"content":"Deep Learning Stack: Flax for large-scale neural networks, Optax for optimization, and Orbax for checkpointing.","type":"content","url":"/jax-scientific-stack#next-chapter-preview","position":35},{"hierarchy":{"lvl1":"JAX Deep Learning Stack: Flax, Optax, and Orbax"},"type":"lvl1","url":"/jax-deep-learning-stack","position":0},{"hierarchy":{"lvl1":"JAX Deep Learning Stack: Flax, Optax, and Orbax"},"content":"","type":"content","url":"/jax-deep-learning-stack","position":1},{"hierarchy":{"lvl1":"JAX Deep Learning Stack: Flax, Optax, and Orbax","lvl2":"Learning Objectives"},"type":"lvl2","url":"/jax-deep-learning-stack#learning-objectives","position":2},{"hierarchy":{"lvl1":"JAX Deep Learning Stack: Flax, Optax, and Orbax","lvl2":"Learning Objectives"},"content":"By the end of this chapter, you will:\n\nBuild large-scale neural networks with Flax\n\nImplement advanced optimization strategies with Optax\n\nManage checkpoints and experiment tracking with Orbax\n\nTrain transformer models for astronomical applications\n\nImplement distributed training across multiple GPUs\n\nBuild production ML pipelines for astronomy","type":"content","url":"/jax-deep-learning-stack#learning-objectives","position":3},{"hierarchy":{"lvl1":"JAX Deep Learning Stack: Flax, Optax, and Orbax","lvl2":"Flax: Scalable Neural Networks"},"type":"lvl2","url":"/jax-deep-learning-stack#flax-scalable-neural-networks","position":4},{"hierarchy":{"lvl1":"JAX Deep Learning Stack: Flax, Optax, and Orbax","lvl2":"Flax: Scalable Neural Networks"},"content":"","type":"content","url":"/jax-deep-learning-stack#flax-scalable-neural-networks","position":5},{"hierarchy":{"lvl1":"JAX Deep Learning Stack: Flax, Optax, and Orbax","lvl3":"Introduction to Flax","lvl2":"Flax: Scalable Neural Networks"},"type":"lvl3","url":"/jax-deep-learning-stack#introduction-to-flax","position":6},{"hierarchy":{"lvl1":"JAX Deep Learning Stack: Flax, Optax, and Orbax","lvl3":"Introduction to Flax","lvl2":"Flax: Scalable Neural Networks"},"content":"import jax\nimport jax.numpy as jnp\nfrom jax import random, grad, jit, vmap\nimport flax.linen as nn\nfrom flax.training import train_state\nfrom flax.core import freeze, unfreeze\nimport optax\nimport orbax.checkpoint as ocp\nfrom typing import Any, Callable, Sequence, Optional\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef flax_fundamentals():\n    \"\"\"Learn Flax's approach to neural network design.\"\"\"\n    \n    print(\"FLAX: SCALABLE NEURAL NETWORKS\")\n    print(\"=\" * 50)\n    \n    # 1. Basic Flax module\n    print(\"\\n1. BASIC FLAX MODULE:\")\n    \n    class SpectralClassifier(nn.Module):\n        \"\"\"Classify astronomical spectra.\"\"\"\n        \n        features: Sequence[int]\n        dropout_rate: float = 0.1\n        \n        @nn.compact\n        def __call__(self, x, training: bool = False):\n            # Input: (batch, wavelengths)\n            \n            for i, feat in enumerate(self.features):\n                x = nn.Dense(feat)(x)\n                \n                # Batch normalization\n                x = nn.BatchNorm(use_running_average=not training)(x)\n                \n                # Activation\n                x = nn.relu(x)\n                \n                # Dropout\n                if training:\n                    x = nn.Dropout(rate=self.dropout_rate)(x, deterministic=False)\n            \n            # Output layer (7 stellar classes)\n            x = nn.Dense(7)(x)\n            return x\n    \n    # Initialize model\n    model = SpectralClassifier(features=[128, 64, 32])\n    \n    # Create dummy input\n    key = random.PRNGKey(0)\n    dummy_input = jnp.ones((1, 1000))  # (batch, wavelengths)\n    \n    # Initialize parameters\n    params = model.init(key, dummy_input)\n    \n    # Forward pass\n    output = model.apply(params, dummy_input, training=False)\n    \n    print(f\"  Model initialized\")\n    print(f\"  Parameter tree structure: {jax.tree_map(lambda x: x.shape, params)}\")\n    print(f\"  Output shape: {output.shape}\")\n    \n    # 2. Advanced architectures\n    print(\"\\n2. CONVOLUTIONAL NETWORK FOR IMAGES:\")\n    \n    class GalaxyMorphologyNet(nn.Module):\n        \"\"\"Classify galaxy morphology from images.\"\"\"\n        \n        @nn.compact\n        def __call__(self, x, training: bool = False):\n            # Input: (batch, height, width, channels)\n            \n            # Convolutional blocks\n            x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n            x = nn.BatchNorm(use_running_average=not training)(x)\n            x = nn.relu(x)\n            x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n            \n            x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n            x = nn.BatchNorm(use_running_average=not training)(x)\n            x = nn.relu(x)\n            x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n            \n            x = nn.Conv(features=128, kernel_size=(3, 3))(x)\n            x = nn.BatchNorm(use_running_average=not training)(x)\n            x = nn.relu(x)\n            \n            # Global average pooling\n            x = jnp.mean(x, axis=(1, 2))\n            \n            # Dense layers\n            x = nn.Dense(64)(x)\n            x = nn.relu(x)\n            \n            if training:\n                x = nn.Dropout(rate=0.3)(x, deterministic=False)\n            \n            # Output: Hubble types (E, S0, Sa, Sb, Sc, Irr)\n            x = nn.Dense(6)(x)\n            \n            return x\n    \n    # Initialize CNN\n    cnn_model = GalaxyMorphologyNet()\n    dummy_image = jnp.ones((1, 128, 128, 3))\n    cnn_params = cnn_model.init(key, dummy_image)\n    \n    print(f\"  CNN initialized for 128x128 RGB images\")\n    \n    # 3. Attention mechanisms\n    print(\"\\n3. ATTENTION FOR TIME SERIES:\")\n    \n    class LightCurveTransformer(nn.Module):\n        \"\"\"Transformer for variable star classification.\"\"\"\n        \n        embed_dim: int = 256\n        num_heads: int = 8\n        num_layers: int = 4\n        mlp_dim: int = 512\n        dropout: float = 0.1\n        \n        @nn.compact\n        def __call__(self, times, fluxes, training: bool = False):\n            # Input: (batch, sequence_length)\n            batch_size, seq_len = fluxes.shape\n            \n            # Positional encoding using observation times\n            time_embed = nn.Dense(self.embed_dim)(times[:, :, None])\n            flux_embed = nn.Dense(self.embed_dim)(fluxes[:, :, None])\n            \n            x = time_embed + flux_embed\n            \n            # Transformer blocks\n            for _ in range(self.num_layers):\n                # Multi-head attention\n                attn = nn.MultiHeadDotProductAttention(\n                    num_heads=self.num_heads,\n                    dropout_rate=self.dropout if training else 0.0\n                )\n                \n                x_norm = nn.LayerNorm()(x)\n                attn_out = attn(x_norm, x_norm)\n                x = x + attn_out\n                \n                # MLP block\n                x_norm = nn.LayerNorm()(x)\n                mlp_out = nn.Sequential([\n                    nn.Dense(self.mlp_dim),\n                    nn.relu,\n                    nn.Dropout(rate=self.dropout, deterministic=not training),\n                    nn.Dense(self.embed_dim)\n                ])(x_norm)\n                x = x + mlp_out\n            \n            # Global pooling\n            x = jnp.mean(x, axis=1)\n            \n            # Classification head\n            x = nn.Dense(10)(x)  # 10 variable star types\n            \n            return x\n    \n    # Initialize transformer\n    transformer = LightCurveTransformer()\n    dummy_times = jnp.linspace(0, 100, 200)[None, :]  # (1, 200)\n    dummy_fluxes = jnp.ones((1, 200))\n    transformer_params = transformer.init(key, dummy_times, dummy_fluxes)\n    \n    print(f\"  Transformer initialized for light curves\")\n    \n    # 4. Custom layers\n    print(\"\\n4. CUSTOM LAYERS:\")\n    \n    class SpectralConvolution(nn.Module):\n        \"\"\"1D convolution with physical constraints.\"\"\"\n        \n        features: int\n        kernel_size: int\n        use_wavelength_weighting: bool = True\n        \n        @nn.compact\n        def __call__(self, x, wavelengths=None):\n            # Standard convolution\n            conv_out = nn.Conv(\n                features=self.features,\n                kernel_size=(self.kernel_size,),\n                padding='SAME'\n            )(x)\n            \n            # Wavelength-dependent weighting\n            if self.use_wavelength_weighting and wavelengths is not None:\n                # Weight by inverse wavelength (blue more important)\n                weights = 1.0 / wavelengths\n                weights = weights / jnp.mean(weights)\n                conv_out = conv_out * weights[None, :, None]\n            \n            return conv_out\n    \n    print(\"  Custom spectral convolution layer defined\")\n\nflax_fundamentals()","type":"content","url":"/jax-deep-learning-stack#introduction-to-flax","position":7},{"hierarchy":{"lvl1":"JAX Deep Learning Stack: Flax, Optax, and Orbax","lvl3":"Training with Flax","lvl2":"Flax: Scalable Neural Networks"},"type":"lvl3","url":"/jax-deep-learning-stack#training-with-flax","position":8},{"hierarchy":{"lvl1":"JAX Deep Learning Stack: Flax, Optax, and Orbax","lvl3":"Training with Flax","lvl2":"Flax: Scalable Neural Networks"},"content":"def flax_training():\n    \"\"\"Complete training pipeline with Flax.\"\"\"\n    \n    print(\"\\nFLAX TRAINING PIPELINE\")\n    print(\"=\" * 50)\n    \n    # 1. Create training state\n    print(\"\\n1. TRAINING STATE MANAGEMENT:\")\n    \n    class PhotometricRedshiftNet(nn.Module):\n        \"\"\"Estimate redshift from photometry.\"\"\"\n        \n        @nn.compact\n        def __call__(self, x, training: bool = False):\n            x = nn.Dense(128)(x)\n            x = nn.relu(x)\n            x = nn.Dropout(0.2, deterministic=not training)(x)\n            \n            x = nn.Dense(64)(x)\n            x = nn.relu(x)\n            x = nn.Dropout(0.2, deterministic=not training)(x)\n            \n            x = nn.Dense(32)(x)\n            x = nn.relu(x)\n            \n            # Output: redshift and uncertainty\n            mean = nn.Dense(1)(x)\n            log_std = nn.Dense(1)(x)\n            \n            return mean, log_std\n    \n    # Initialize\n    model = PhotometricRedshiftNet()\n    key = random.PRNGKey(42)\n    \n    # Create optimizer\n    learning_rate_schedule = optax.warmup_cosine_decay_schedule(\n        init_value=0.0,\n        peak_value=1e-3,\n        warmup_steps=100,\n        decay_steps=1000,\n        end_value=1e-5\n    )\n    \n    optimizer = optax.chain(\n        optax.clip_by_global_norm(1.0),\n        optax.adam(learning_rate_schedule)\n    )\n    \n    # Initialize training state\n    def create_train_state(rng, model, optimizer, input_shape):\n        \"\"\"Create initial training state.\"\"\"\n        dummy_input = jnp.ones(input_shape)\n        params = model.init(rng, dummy_input)\n        \n        return train_state.TrainState.create(\n            apply_fn=model.apply,\n            params=params,\n            tx=optimizer\n        )\n    \n    state = create_train_state(key, model, optimizer, (1, 5))  # 5 photometric bands\n    \n    print(f\"  Training state created with {optimizer}\")\n    \n    # 2. Loss functions\n    print(\"\\n2. LOSS FUNCTIONS:\")\n    \n    def gaussian_nll_loss(params, batch, training=True):\n        \"\"\"Gaussian negative log likelihood.\"\"\"\n        inputs, targets = batch\n        mean, log_std = state.apply_fn(params, inputs, training=training)\n        \n        # Negative log likelihood\n        std = jnp.exp(log_std)\n        nll = 0.5 * jnp.log(2 * jnp.pi) + log_std + \\\n              0.5 * ((targets - mean) / std) ** 2\n        \n        return jnp.mean(nll)\n    \n    def robust_loss(params, batch, training=True):\n        \"\"\"Robust loss using Huber.\"\"\"\n        inputs, targets = batch\n        mean, _ = state.apply_fn(params, inputs, training=training)\n        \n        delta = 0.1  # Huber delta\n        residuals = jnp.abs(targets - mean)\n        \n        loss = jnp.where(\n            residuals < delta,\n            0.5 * residuals ** 2,\n            delta * (residuals - 0.5 * delta)\n        )\n        \n        return jnp.mean(loss)\n    \n    # 3. Training step\n    print(\"\\n3. TRAINING STEP:\")\n    \n    @jit\n    def train_step(state, batch, rng):\n        \"\"\"Single training step.\"\"\"\n        dropout_rng = rng\n        \n        def loss_fn(params):\n            return gaussian_nll_loss(params, batch, training=True)\n        \n        # Compute loss and gradients\n        loss, grads = jax.value_and_grad(loss_fn)(state.params)\n        \n        # Update parameters\n        state = state.apply_gradients(grads=grads)\n        \n        return state, loss\n    \n    @jit\n    def eval_step(state, batch):\n        \"\"\"Evaluation step.\"\"\"\n        loss = gaussian_nll_loss(state.params, batch, training=False)\n        \n        inputs, targets = batch\n        mean, log_std = state.apply_fn(state.params, inputs, training=False)\n        \n        # Compute metrics\n        mse = jnp.mean((mean - targets) ** 2)\n        mae = jnp.mean(jnp.abs(mean - targets))\n        \n        # Calibration: fraction within 1-sigma\n        std = jnp.exp(log_std)\n        within_1sigma = jnp.mean(jnp.abs(mean - targets) < std)\n        \n        return {\n            'loss': loss,\n            'mse': mse,\n            'mae': mae,\n            'calibration': within_1sigma\n        }\n    \n    # 4. Data loading\n    print(\"\\n4. DATA LOADING:\")\n    \n    def create_dataset(key, n_samples=10000):\n        \"\"\"Create synthetic photometric redshift dataset.\"\"\"\n        keys = random.split(key, 6)\n        \n        # Generate redshifts\n        z_true = random.uniform(keys[0], (n_samples, 1), minval=0, maxval=3)\n        \n        # Generate photometry (simplified SED model)\n        wavelengths = jnp.array([3500, 4500, 5500, 6500, 7500])  # ugriz\n        \n        # Rest-frame SED (simplified)\n        rest_wavelengths = wavelengths[None, :] / (1 + z_true)\n        \n        # Blackbody approximation\n        T_eff = 5000  # K\n        h, c, k = 6.626e-34, 3e8, 1.38e-23\n        \n        planck = lambda lam, T: (\n            2 * h * c**2 / lam**5 / \n            (jnp.exp(h * c / (lam * k * T)) - 1)\n        )\n        \n        # Add noise\n        photometry = jnp.log10(planck(rest_wavelengths * 1e-10, T_eff))\n        photometry += 0.1 * random.normal(keys[1], photometry.shape)\n        \n        return photometry, z_true\n    \n    # Create datasets\n    train_data = create_dataset(random.PRNGKey(0), n_samples=5000)\n    val_data = create_dataset(random.PRNGKey(1), n_samples=1000)\n    \n    print(f\"  Created training set: {train_data[0].shape}\")\n    print(f\"  Created validation set: {val_data[0].shape}\")\n    \n    # 5. Training loop\n    print(\"\\n5. TRAINING LOOP:\")\n    \n    def train_epoch(state, train_data, batch_size, rng):\n        \"\"\"Train for one epoch.\"\"\"\n        X_train, y_train = train_data\n        n_samples = len(X_train)\n        n_batches = n_samples // batch_size\n        \n        # Shuffle data\n        rng, shuffle_rng = random.split(rng)\n        perm = random.permutation(shuffle_rng, n_samples)\n        X_train = X_train[perm]\n        y_train = y_train[perm]\n        \n        epoch_loss = 0.0\n        \n        for i in range(n_batches):\n            rng, step_rng = random.split(rng)\n            \n            start = i * batch_size\n            end = start + batch_size\n            batch = (X_train[start:end], y_train[start:end])\n            \n            state, loss = train_step(state, batch, step_rng)\n            epoch_loss += loss\n        \n        return state, epoch_loss / n_batches\n    \n    # Train\n    n_epochs = 10\n    batch_size = 32\n    \n    train_losses = []\n    val_metrics = []\n    \n    for epoch in range(n_epochs):\n        key, epoch_rng = random.split(key)\n        \n        # Training\n        state, train_loss = train_epoch(state, train_data, batch_size, epoch_rng)\n        train_losses.append(train_loss)\n        \n        # Validation\n        val_batch = (val_data[0][:100], val_data[1][:100])  # Sample\n        metrics = eval_step(state, val_batch)\n        val_metrics.append(metrics)\n        \n        if epoch % 2 == 0:\n            print(f\"  Epoch {epoch}: Train Loss = {train_loss:.4f}, \" +\n                  f\"Val MAE = {metrics['mae']:.4f}, \" +\n                  f\"Calibration = {metrics['calibration']:.2%}\")\n\nflax_training()","type":"content","url":"/jax-deep-learning-stack#training-with-flax","position":9},{"hierarchy":{"lvl1":"JAX Deep Learning Stack: Flax, Optax, and Orbax","lvl2":"Optax: Advanced Optimization"},"type":"lvl2","url":"/jax-deep-learning-stack#optax-advanced-optimization","position":10},{"hierarchy":{"lvl1":"JAX Deep Learning Stack: Flax, Optax, and Orbax","lvl2":"Optax: Advanced Optimization"},"content":"","type":"content","url":"/jax-deep-learning-stack#optax-advanced-optimization","position":11},{"hierarchy":{"lvl1":"JAX Deep Learning Stack: Flax, Optax, and Orbax","lvl3":"Optimization Algorithms","lvl2":"Optax: Advanced Optimization"},"type":"lvl3","url":"/jax-deep-learning-stack#optimization-algorithms","position":12},{"hierarchy":{"lvl1":"JAX Deep Learning Stack: Flax, Optax, and Orbax","lvl3":"Optimization Algorithms","lvl2":"Optax: Advanced Optimization"},"content":"def optax_optimizers():\n    \"\"\"Advanced optimization strategies with Optax.\"\"\"\n    \n    print(\"\\nOPTAX: ADVANCED OPTIMIZATION\")\n    print(\"=\" * 50)\n    \n    # 1. Basic optimizers\n    print(\"\\n1. OPTIMIZER COMPARISON:\")\n    \n    def create_loss_landscape():\n        \"\"\"Create a challenging loss landscape.\"\"\"\n        def loss(params):\n            x, y = params\n            # Rosenbrock function (challenging optimization)\n            return (1 - x)**2 + 100 * (y - x**2)**2\n        \n        return loss\n    \n    loss_fn = create_loss_landscape()\n    \n    # Compare optimizers\n    optimizers = {\n        'SGD': optax.sgd(learning_rate=1e-3),\n        'Adam': optax.adam(learning_rate=1e-3),\n        'RMSprop': optax.rmsprop(learning_rate=1e-3),\n        'AdamW': optax.adamw(learning_rate=1e-3, weight_decay=1e-4),\n        'LAMB': optax.lamb(learning_rate=1e-3),\n    }\n    \n    initial_params = jnp.array([-1.0, 1.0])\n    \n    for name, optimizer in optimizers.items():\n        params = initial_params.copy()\n        opt_state = optimizer.init(params)\n        \n        for step in range(100):\n            grads = grad(loss_fn)(params)\n            updates, opt_state = optimizer.update(grads, opt_state)\n            params = optax.apply_updates(params, updates)\n        \n        final_loss = loss_fn(params)\n        print(f\"  {name}: Final loss = {final_loss:.6f}, Final params = {params}\")\n    \n    # 2. Learning rate schedules\n    print(\"\\n2. LEARNING RATE SCHEDULES:\")\n    \n    # Different schedules\n    schedules = {\n        'Constant': optax.constant_schedule(1e-3),\n        'Exponential': optax.exponential_decay(\n            init_value=1e-3,\n            transition_steps=100,\n            decay_rate=0.9\n        ),\n        'Cosine': optax.cosine_decay_schedule(\n            init_value=1e-3,\n            decay_steps=1000\n        ),\n        'Warmup-Cosine': optax.warmup_cosine_decay_schedule(\n            init_value=0.0,\n            peak_value=1e-3,\n            warmup_steps=100,\n            decay_steps=1000\n        ),\n        'Piecewise': optax.piecewise_constant_schedule(\n            init_value=1e-3,\n            boundaries_and_scales={200: 0.1, 400: 0.1}\n        ),\n    }\n    \n    # Visualize schedules\n    steps = jnp.arange(1000)\n    \n    for name, schedule in schedules.items():\n        lrs = [schedule(step) for step in steps]\n        print(f\"  {name}: LR range [{min(lrs):.6f}, {max(lrs):.6f}]\")\n    \n    # 3. Gradient transformations\n    print(\"\\n3. GRADIENT TRANSFORMATIONS:\")\n    \n    # Chain multiple transformations\n    optimizer_chain = optax.chain(\n        optax.clip_by_global_norm(1.0),  # Gradient clipping\n        optax.scale_by_adam(),            # Adam scaling\n        optax.add_decayed_weights(1e-4),  # Weight decay\n        optax.scale(-1e-3)                # Learning rate\n    )\n    \n    print(\"  Chained optimizer created with:\")\n    print(\"    - Global norm clipping (1.0)\")\n    print(\"    - Adam scaling\")\n    print(\"    - Weight decay (1e-4)\")\n    print(\"    - Learning rate scaling\")\n    \n    # 4. Advanced techniques\n    print(\"\\n4. ADVANCED OPTIMIZATION TECHNIQUES:\")\n    \n    # Lookahead optimizer\n    base_optimizer = optax.adam(1e-3)\n    lookahead_optimizer = optax.lookahead(base_optimizer, slow_step_size=0.5, period=5)\n    \n    print(\"  Lookahead optimizer configured\")\n    \n    # Gradient accumulation\n    def gradient_accumulation_optimizer(base_opt, accumulation_steps=4):\n        \"\"\"Accumulate gradients over multiple steps.\"\"\"\n        \n        def init_fn(params):\n            return {\n                'base_state': base_opt.init(params),\n                'accumulated_grads': jax.tree_map(jnp.zeros_like, params),\n                'step': 0\n            }\n        \n        def update_fn(grads, state, params=None):\n            accumulated_grads = jax.tree_map(\n                lambda a, g: a + g / accumulation_steps,\n                state['accumulated_grads'], grads\n            )\n            \n            step = state['step'] + 1\n            \n            if step % accumulation_steps == 0:\n                # Apply accumulated gradients\n                updates, base_state = base_opt.update(\n                    accumulated_grads, state['base_state'], params\n                )\n                # Reset accumulation\n                accumulated_grads = jax.tree_map(jnp.zeros_like, accumulated_grads)\n            else:\n                # Just accumulate\n                updates = jax.tree_map(jnp.zeros_like, grads)\n                base_state = state['base_state']\n            \n            new_state = {\n                'base_state': base_state,\n                'accumulated_grads': accumulated_grads,\n                'step': step\n            }\n            \n            return updates, new_state\n        \n        return optax.GradientTransformation(init_fn, update_fn)\n    \n    acc_optimizer = gradient_accumulation_optimizer(optax.adam(1e-3))\n    print(\"  Gradient accumulation optimizer created\")\n    \n    # 5. Per-parameter learning rates\n    print(\"\\n5. PER-PARAMETER LEARNING RATES:\")\n    \n    def create_model_with_different_lrs():\n        \"\"\"Different learning rates for different layers.\"\"\"\n        \n        class Model(nn.Module):\n            @nn.compact\n            def __call__(self, x):\n                # Feature extractor (lower LR)\n                x = nn.Dense(128, name='feature_extractor')(x)\n                x = nn.relu(x)\n                \n                # Classifier (higher LR)\n                x = nn.Dense(10, name='classifier')(x)\n                return x\n        \n        model = Model()\n        dummy_input = jnp.ones((1, 100))\n        params = model.init(random.PRNGKey(0), dummy_input)\n        \n        # Create optimizer with different LRs\n        def partition_params(params):\n            \"\"\"Partition parameters by layer.\"\"\"\n            feature_params = {'feature_extractor': params['params']['feature_extractor']}\n            classifier_params = {'classifier': params['params']['classifier']}\n            return feature_params, classifier_params\n        \n        # Different optimizers for different parts\n        feature_opt = optax.adam(1e-4)  # Lower LR\n        classifier_opt = optax.adam(1e-2)  # Higher LR\n        \n        return model, params, (feature_opt, classifier_opt)\n    \n    model, params, opts = create_model_with_different_lrs()\n    print(\"  Model with per-layer learning rates created\")\n\noptax_optimizers()","type":"content","url":"/jax-deep-learning-stack#optimization-algorithms","position":13},{"hierarchy":{"lvl1":"JAX Deep Learning Stack: Flax, Optax, and Orbax","lvl3":"Advanced Training Strategies","lvl2":"Optax: Advanced Optimization"},"type":"lvl3","url":"/jax-deep-learning-stack#advanced-training-strategies","position":14},{"hierarchy":{"lvl1":"JAX Deep Learning Stack: Flax, Optax, and Orbax","lvl3":"Advanced Training Strategies","lvl2":"Optax: Advanced Optimization"},"content":"def advanced_training_strategies():\n    \"\"\"Advanced training techniques with Optax.\"\"\"\n    \n    print(\"\\nADVANCED TRAINING STRATEGIES\")\n    print(\"=\" * 50)\n    \n    # 1. Mixed precision training\n    print(\"\\n1. MIXED PRECISION TRAINING:\")\n    \n    class MixedPrecisionModel(nn.Module):\n        \"\"\"Model with mixed precision computation.\"\"\"\n        \n        use_mixed_precision: bool = True\n        \n        @nn.compact\n        def __call__(self, x):\n            dtype = jnp.float16 if self.use_mixed_precision else jnp.float32\n            \n            # Cast to lower precision\n            x = x.astype(dtype)\n            \n            # Compute in lower precision\n            x = nn.Dense(128, dtype=dtype)(x)\n            x = nn.relu(x)\n            x = nn.Dense(64, dtype=dtype)(x)\n            x = nn.relu(x)\n            \n            # Cast back to float32 for loss\n            x = nn.Dense(10, dtype=jnp.float32)(x.astype(jnp.float32))\n            \n            return x\n    \n    # Loss scaling for mixed precision\n    def create_loss_scaled_optimizer(optimizer, loss_scale=1024):\n        \"\"\"Add loss scaling for mixed precision.\"\"\"\n        return optax.chain(\n            optax.scale(1 / loss_scale),  # Unscale gradients\n            optimizer\n        )\n    \n    base_opt = optax.adam(1e-3)\n    scaled_opt = create_loss_scaled_optimizer(base_opt)\n    \n    print(\"  Mixed precision optimizer with loss scaling created\")\n    \n    # 2. Differential learning rates\n    print(\"\\n2. DIFFERENTIAL LEARNING RATES:\")\n    \n    def layer_wise_lr_decay(base_lr, decay_factor, num_layers):\n        \"\"\"Exponentially decay LR for earlier layers.\"\"\"\n        lrs = []\n        for i in range(num_layers):\n            lr = base_lr * (decay_factor ** (num_layers - i - 1))\n            lrs.append(lr)\n        return lrs\n    \n    layer_lrs = layer_wise_lr_decay(1e-3, 0.5, 4)\n    print(f\"  Layer-wise LRs: {layer_lrs}\")\n    \n    # 3. Gradient penalty\n    print(\"\\n3. GRADIENT PENALTIES:\")\n    \n    def add_gradient_penalty(loss_fn, penalty_weight=0.1):\n        \"\"\"Add gradient penalty for regularization.\"\"\"\n        \n        def penalized_loss(params, inputs):\n            # Original loss\n            loss = loss_fn(params, inputs)\n            \n            # Gradient penalty\n            grads = grad(loss_fn)(params, inputs)\n            grad_norm = jnp.sqrt(\n                sum(jnp.sum(g**2) for g in jax.tree_leaves(grads))\n            )\n            \n            penalty = penalty_weight * grad_norm\n            \n            return loss + penalty\n        \n        return penalized_loss\n    \n    print(\"  Gradient penalty wrapper created\")\n    \n    # 4. EMA (Exponential Moving Average)\n    print(\"\\n4. EXPONENTIAL MOVING AVERAGE:\")\n    \n    def create_ema():\n        \"\"\"Create EMA of model parameters.\"\"\"\n        \n        def init_fn(params):\n            return params  # Initialize with current params\n        \n        def update_fn(params, ema_params, decay=0.999):\n            \"\"\"Update EMA parameters.\"\"\"\n            return jax.tree_map(\n                lambda e, p: decay * e + (1 - decay) * p,\n                ema_params, params\n            )\n        \n        return init_fn, update_fn\n    \n    ema_init, ema_update = create_ema()\n    \n    print(\"  EMA functions created for model averaging\")\n    \n    # 5. Stochastic Weight Averaging\n    print(\"\\n5. STOCHASTIC WEIGHT AVERAGING (SWA):\")\n    \n    class SWAState:\n        \"\"\"State for SWA training.\"\"\"\n        \n        def __init__(self, params):\n            self.params = params\n            self.swa_params = jax.tree_map(jnp.zeros_like, params)\n            self.n_averaged = 0\n        \n        def update(self, new_params, start_averaging_step, current_step):\n            \"\"\"Update SWA parameters.\"\"\"\n            if current_step >= start_averaging_step:\n                # Update running average\n                self.n_averaged += 1\n                self.swa_params = jax.tree_map(\n                    lambda swa, p: swa + (p - swa) / self.n_averaged,\n                    self.swa_params, new_params\n                )\n            \n            self.params = new_params\n            return self\n    \n    print(\"  SWA state management created\")\n\nadvanced_training_strategies()","type":"content","url":"/jax-deep-learning-stack#advanced-training-strategies","position":15},{"hierarchy":{"lvl1":"JAX Deep Learning Stack: Flax, Optax, and Orbax","lvl2":"Orbax: Checkpointing and Experiment Management"},"type":"lvl2","url":"/jax-deep-learning-stack#orbax-checkpointing-and-experiment-management","position":16},{"hierarchy":{"lvl1":"JAX Deep Learning Stack: Flax, Optax, and Orbax","lvl2":"Orbax: Checkpointing and Experiment Management"},"content":"","type":"content","url":"/jax-deep-learning-stack#orbax-checkpointing-and-experiment-management","position":17},{"hierarchy":{"lvl1":"JAX Deep Learning Stack: Flax, Optax, and Orbax","lvl3":"Checkpoint Management","lvl2":"Orbax: Checkpointing and Experiment Management"},"type":"lvl3","url":"/jax-deep-learning-stack#checkpoint-management","position":18},{"hierarchy":{"lvl1":"JAX Deep Learning Stack: Flax, Optax, and Orbax","lvl3":"Checkpoint Management","lvl2":"Orbax: Checkpointing and Experiment Management"},"content":"def orbax_checkpointing():\n    \"\"\"Checkpoint management with Orbax.\"\"\"\n    \n    print(\"\\nORBAX: CHECKPOINT MANAGEMENT\")\n    print(\"=\" * 50)\n    \n    import orbax.checkpoint as ocp\n    import tempfile\n    import os\n    \n    # 1. Basic checkpointing\n    print(\"\\n1. BASIC CHECKPOINTING:\")\n    \n    # Create temporary directory for checkpoints\n    checkpoint_dir = tempfile.mkdtemp()\n    \n    # Create a simple model and optimizer\n    class SimpleModel(nn.Module):\n        @nn.compact\n        def __call__(self, x):\n            x = nn.Dense(128)(x)\n            x = nn.relu(x)\n            x = nn.Dense(10)(x)\n            return x\n    \n    model = SimpleModel()\n    key = random.PRNGKey(0)\n    params = model.init(key, jnp.ones((1, 100)))\n    \n    optimizer = optax.adam(1e-3)\n    opt_state = optimizer.init(params)\n    \n    # Create checkpoint manager\n    options = ocp.CheckpointManagerOptions(\n        max_to_keep=3,\n        keep_period=5,\n        create=True\n    )\n    \n    checkpoint_manager = ocp.CheckpointManager(\n        checkpoint_dir,\n        options=options,\n        item_names=('params', 'opt_state', 'metadata')\n    )\n    \n    # Save checkpoint\n    step = 0\n    metadata = {'epoch': 0, 'loss': 0.5, 'accuracy': 0.85}\n    \n    checkpoint_manager.save(\n        step,\n        args=ocp.args.Composite(\n            params=ocp.args.StandardSave(params),\n            opt_state=ocp.args.StandardSave(opt_state),\n            metadata=ocp.args.JsonSave(metadata)\n        )\n    )\n    \n    print(f\"  Checkpoint saved at step {step}\")\n    \n    # 2. Restoring checkpoints\n    print(\"\\n2. RESTORING CHECKPOINTS:\")\n    \n    # Restore latest checkpoint\n    restored = checkpoint_manager.restore(\n        checkpoint_manager.latest_step(),\n        args=ocp.args.Composite(\n            params=ocp.args.StandardRestore(params),\n            opt_state=ocp.args.StandardRestore(opt_state),\n            metadata=ocp.args.JsonRestore()\n        )\n    )\n    \n    print(f\"  Restored from step {checkpoint_manager.latest_step()}\")\n    print(f\"  Metadata: {restored['metadata']}\")\n    \n    # 3. Async checkpointing\n    print(\"\\n3. ASYNCHRONOUS CHECKPOINTING:\")\n    \n    async_checkpoint_manager = ocp.CheckpointManager(\n        os.path.join(checkpoint_dir, 'async'),\n        options=ocp.CheckpointManagerOptions(\n            max_to_keep=2,\n            save_interval_steps=10,\n            enable_async_checkpointing=True\n        )\n    )\n    \n    print(\"  Async checkpoint manager created\")\n    \n    # 4. Best model tracking\n    print(\"\\n4. BEST MODEL TRACKING:\")\n    \n    class BestModelTracker:\n        \"\"\"Track and save best model based on metric.\"\"\"\n        \n        def __init__(self, checkpoint_dir, metric='loss', mode='min'):\n            self.checkpoint_dir = checkpoint_dir\n            self.metric = metric\n            self.mode = mode\n            self.best_value = float('inf') if mode == 'min' else float('-inf')\n            self.checkpointer = ocp.Checkpointer(\n                ocp.StandardCheckpointHandler()\n            )\n        \n        def update(self, params, metrics, step):\n            \"\"\"Update best model if improved.\"\"\"\n            current_value = metrics[self.metric]\n            \n            is_better = (\n                (self.mode == 'min' and current_value < self.best_value) or\n                (self.mode == 'max' and current_value > self.best_value)\n            )\n            \n            if is_better:\n                self.best_value = current_value\n                \n                # Save best model\n                best_path = os.path.join(self.checkpoint_dir, 'best_model')\n                self.checkpointer.save(\n                    best_path,\n                    args=ocp.args.Composite(\n                        params=ocp.args.StandardSave(params),\n                        metrics=ocp.args.JsonSave(metrics),\n                        step=ocp.args.JsonSave({'step': step})\n                    )\n                )\n                \n                print(f\"    New best model saved: {self.metric}={current_value:.4f}\")\n                return True\n            \n            return False\n    \n    tracker = BestModelTracker(checkpoint_dir, metric='loss', mode='min')\n    \n    # Simulate training with metric tracking\n    for step in range(5):\n        fake_loss = 1.0 / (step + 1)  # Decreasing loss\n        metrics = {'loss': fake_loss, 'accuracy': 1 - fake_loss}\n        tracker.update(params, metrics, step)\n    \n    # 5. Multi-host checkpointing\n    print(\"\\n5. DISTRIBUTED CHECKPOINTING:\")\n    \n    # For multi-host training (conceptual)\n    def create_distributed_checkpoint_manager():\n        \"\"\"Create checkpoint manager for distributed training.\"\"\"\n        \n        # Would use in multi-host setting\n        # checkpoint_manager = ocp.CheckpointManager(\n        #     directory,\n        #     options=ocp.CheckpointManagerOptions(\n        #         save_interval_steps=100,\n        #         max_to_keep=3\n        #     ),\n        #     metadata={'host_count': jax.process_count()}\n        # )\n        \n        print(\"  Distributed checkpointing configured (requires multi-host setup)\")\n    \n    create_distributed_checkpoint_manager()\n    \n    # Cleanup\n    import shutil\n    shutil.rmtree(checkpoint_dir)\n\norbax_checkpointing()","type":"content","url":"/jax-deep-learning-stack#checkpoint-management","position":19},{"hierarchy":{"lvl1":"JAX Deep Learning Stack: Flax, Optax, and Orbax","lvl2":"Complete Training Pipeline"},"type":"lvl2","url":"/jax-deep-learning-stack#complete-training-pipeline","position":20},{"hierarchy":{"lvl1":"JAX Deep Learning Stack: Flax, Optax, and Orbax","lvl2":"Complete Training Pipeline"},"content":"","type":"content","url":"/jax-deep-learning-stack#complete-training-pipeline","position":21},{"hierarchy":{"lvl1":"JAX Deep Learning Stack: Flax, Optax, and Orbax","lvl3":"Production ML Pipeline","lvl2":"Complete Training Pipeline"},"type":"lvl3","url":"/jax-deep-learning-stack#production-ml-pipeline","position":22},{"hierarchy":{"lvl1":"JAX Deep Learning Stack: Flax, Optax, and Orbax","lvl3":"Production ML Pipeline","lvl2":"Complete Training Pipeline"},"content":"def complete_ml_pipeline():\n    \"\"\"Complete ML pipeline for astronomical applications.\"\"\"\n    \n    print(\"\\nCOMPLETE ML PIPELINE\")\n    print(\"=\" * 50)\n    \n    # 1. Model definition\n    class TransientClassifier(nn.Module):\n        \"\"\"Classify astronomical transients from light curves.\"\"\"\n        \n        num_classes: int = 5  # SN Ia, SN II, SN Ibc, SLSN, Kilonova\n        hidden_dim: int = 256\n        num_heads: int = 8\n        num_layers: int = 4\n        dropout: float = 0.1\n        \n        @nn.compact\n        def __call__(self, times, fluxes, errors, filters, training: bool = False):\n            batch_size, seq_len = fluxes.shape\n            \n            # Embed observations\n            obs_features = jnp.stack([\n                fluxes,\n                errors,\n                times,\n                filters\n            ], axis=-1)  # (batch, seq, 4)\n            \n            # Initial projection\n            x = nn.Dense(self.hidden_dim)(obs_features)\n            \n            # Positional encoding from times\n            pos_encoding = self.param(\n                'pos_encoding',\n                nn.initializers.normal(stddev=0.02),\n                (1, seq_len, self.hidden_dim)\n            )\n            x = x + pos_encoding\n            \n            # Transformer layers\n            for i in range(self.num_layers):\n                # Self-attention\n                attn_out = nn.MultiHeadDotProductAttention(\n                    num_heads=self.num_heads,\n                    dropout_rate=self.dropout if training else 0.0,\n                    deterministic=not training\n                )(x, x)\n                x = nn.LayerNorm()(x + attn_out)\n                \n                # FFN\n                ffn_out = nn.Sequential([\n                    nn.Dense(self.hidden_dim * 4),\n                    nn.gelu,\n                    nn.Dropout(self.dropout, deterministic=not training),\n                    nn.Dense(self.hidden_dim)\n                ])(x)\n                x = nn.LayerNorm()(x + ffn_out)\n            \n            # Global pooling with attention\n            attention_weights = nn.Dense(1)(x)\n            attention_weights = nn.softmax(attention_weights, axis=1)\n            x = jnp.sum(x * attention_weights, axis=1)\n            \n            # Classification head\n            x = nn.Dense(self.hidden_dim // 2)(x)\n            x = nn.relu(x)\n            x = nn.Dropout(self.dropout, deterministic=not training)(x)\n            logits = nn.Dense(self.num_classes)(x)\n            \n            return logits\n    \n    # 2. Data generation\n    def generate_transient_data(key, n_samples=1000):\n        \"\"\"Generate synthetic transient light curves.\"\"\"\n        keys = random.split(key, 5)\n        \n        all_times = []\n        all_fluxes = []\n        all_errors = []\n        all_filters = []\n        all_labels = []\n        \n        for i in range(n_samples):\n            # Random transient type\n            label = random.choice(keys[0], 5)\n            \n            # Generate light curve based on type\n            n_obs = random.choice(keys[1], 1, minval=20, maxval=100)[0]\n            times = jnp.sort(random.uniform(keys[2], (n_obs,), minval=0, maxval=100))\n            \n            # Different templates for different types\n            if label == 0:  # SN Ia\n                peak_time = 20.0\n                rise_time = 15.0\n                decay_time = 30.0\n            elif label == 1:  # SN II\n                peak_time = 30.0\n                rise_time = 20.0\n                decay_time = 60.0\n            else:\n                peak_time = 25.0\n                rise_time = 10.0\n                decay_time = 40.0\n            \n            # Generate flux (simplified)\n            fluxes = jnp.where(\n                times < peak_time,\n                jnp.exp(-(times - peak_time)**2 / (2 * rise_time**2)),\n                jnp.exp(-(times - peak_time)**2 / (2 * decay_time**2))\n            )\n            \n            # Add noise\n            errors = 0.05 + 0.05 * random.uniform(keys[3], (n_obs,))\n            fluxes += errors * random.normal(keys[4], (n_obs,))\n            \n            # Random filters (ugriz)\n            filters = random.choice(keys[0], 5, shape=(n_obs,))\n            \n            # Pad to fixed length\n            max_len = 100\n            padded_times = jnp.pad(times, (0, max_len - len(times)))\n            padded_fluxes = jnp.pad(fluxes, (0, max_len - len(fluxes)))\n            padded_errors = jnp.pad(errors, (0, max_len - len(errors)))\n            padded_filters = jnp.pad(filters, (0, max_len - len(filters)))\n            \n            all_times.append(padded_times)\n            all_fluxes.append(padded_fluxes)\n            all_errors.append(padded_errors)\n            all_filters.append(padded_filters)\n            all_labels.append(label)\n        \n        return (\n            jnp.stack(all_times),\n            jnp.stack(all_fluxes),\n            jnp.stack(all_errors),\n            jnp.stack(all_filters),\n            jnp.array(all_labels)\n        )\n    \n    # Generate data\n    key = random.PRNGKey(42)\n    train_data = generate_transient_data(key, n_samples=500)\n    val_data = generate_transient_data(random.PRNGKey(43), n_samples=100)\n    \n    print(f\"  Generated {len(train_data[0])} training samples\")\n    \n    # 3. Training setup\n    model = TransientClassifier()\n    \n    # Initialize\n    dummy_batch = tuple(x[:1] for x in train_data[:-1])\n    params = model.init(key, *dummy_batch)\n    \n    # Optimizer with schedule\n    schedule = optax.warmup_cosine_decay_schedule(\n        init_value=0.0,\n        peak_value=1e-3,\n        warmup_steps=50,\n        decay_steps=500\n    )\n    \n    optimizer = optax.chain(\n        optax.clip_by_global_norm(1.0),\n        optax.adamw(learning_rate=schedule, weight_decay=1e-4)\n    )\n    \n    state = train_state.TrainState.create(\n        apply_fn=model.apply,\n        params=params,\n        tx=optimizer\n    )\n    \n    # 4. Training functions\n    @jit\n    def train_step(state, batch, dropout_key):\n        \"\"\"Training step.\"\"\"\n        times, fluxes, errors, filters, labels = batch\n        \n        def loss_fn(params):\n            logits = state.apply_fn(\n                params, times, fluxes, errors, filters,\n                training=True, rngs={'dropout': dropout_key}\n            )\n            \n            # Cross-entropy loss\n            loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels)\n            loss = jnp.mean(loss)\n            \n            # L2 regularization (handled by adamw)\n            \n            return loss, logits\n        \n        (loss, logits), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n        \n        # Update\n        state = state.apply_gradients(grads=grads)\n        \n        # Metrics\n        accuracy = jnp.mean(jnp.argmax(logits, axis=-1) == labels)\n        \n        return state, {'loss': loss, 'accuracy': accuracy}\n    \n    @jit\n    def eval_step(state, batch):\n        \"\"\"Evaluation step.\"\"\"\n        times, fluxes, errors, filters, labels = batch\n        \n        logits = state.apply_fn(\n            state.params, times, fluxes, errors, filters,\n            training=False\n        )\n        \n        loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels)\n        loss = jnp.mean(loss)\n        \n        predictions = jnp.argmax(logits, axis=-1)\n        accuracy = jnp.mean(predictions == labels)\n        \n        # Per-class accuracy\n        per_class_acc = []\n        for c in range(5):\n            mask = labels == c\n            if jnp.sum(mask) > 0:\n                class_acc = jnp.mean(predictions[mask] == c)\n                per_class_acc.append(class_acc)\n        \n        return {\n            'loss': loss,\n            'accuracy': accuracy,\n            'per_class_accuracy': per_class_acc\n        }\n    \n    # 5. Training loop\n    print(\"\\n  Training transient classifier...\")\n    \n    n_epochs = 5\n    batch_size = 32\n    \n    for epoch in range(n_epochs):\n        # Training\n        key, dropout_key = random.split(key)\n        \n        n_batches = len(train_data[0]) // batch_size\n        epoch_metrics = {'loss': 0.0, 'accuracy': 0.0}\n        \n        for i in range(n_batches):\n            start = i * batch_size\n            end = start + batch_size\n            \n            batch = tuple(x[start:end] for x in train_data)\n            \n            dropout_key, step_key = random.split(dropout_key)\n            state, metrics = train_step(state, batch, step_key)\n            \n            epoch_metrics['loss'] += metrics['loss']\n            epoch_metrics['accuracy'] += metrics['accuracy']\n        \n        epoch_metrics['loss'] /= n_batches\n        epoch_metrics['accuracy'] /= n_batches\n        \n        # Validation\n        val_batch = tuple(x[:batch_size] for x in val_data)\n        val_metrics = eval_step(state, val_batch)\n        \n        print(f\"  Epoch {epoch}: \"\n              f\"Train Loss={epoch_metrics['loss']:.4f}, \"\n              f\"Train Acc={epoch_metrics['accuracy']:.2%}, \"\n              f\"Val Loss={val_metrics['loss']:.4f}, \"\n              f\"Val Acc={val_metrics['accuracy']:.2%}\")\n    \n    print(\"\\n  Training complete!\")\n    \n    return state\n\n# Run pipeline\nfinal_state = complete_ml_pipeline()","type":"content","url":"/jax-deep-learning-stack#production-ml-pipeline","position":23},{"hierarchy":{"lvl1":"JAX Deep Learning Stack: Flax, Optax, and Orbax","lvl2":"Key Takeaways"},"type":"lvl2","url":"/jax-deep-learning-stack#key-takeaways","position":24},{"hierarchy":{"lvl1":"JAX Deep Learning Stack: Flax, Optax, and Orbax","lvl2":"Key Takeaways"},"content":"✅ Flax - Production-ready neural networks with clean module system✅ Optax - Composable optimization with advanced schedules and techniques✅ Orbax - Robust checkpointing and experiment management✅ Integration - Seamless workflow from model definition to production✅ Scalability - Ready for multi-GPU and large-scale training✅ Best Practices - Type safety, mixed precision, and monitoring built-in","type":"content","url":"/jax-deep-learning-stack#key-takeaways","position":25},{"hierarchy":{"lvl1":"JAX Deep Learning Stack: Flax, Optax, and Orbax","lvl2":"Next Chapter Preview"},"type":"lvl2","url":"/jax-deep-learning-stack#next-chapter-preview","position":26},{"hierarchy":{"lvl1":"JAX Deep Learning Stack: Flax, Optax, and Orbax","lvl2":"Next Chapter Preview"},"content":"Specialized Libraries: BlackJAX for MCMC, NetKet for quantum systems, and more domain-specific JAX tools.","type":"content","url":"/jax-deep-learning-stack#next-chapter-preview","position":27},{"hierarchy":{"lvl1":"JAX Specialized Libraries: BlackJAX, Numpyro, and Domain-Specific Tools"},"type":"lvl1","url":"/jax-specialized-libraries","position":0},{"hierarchy":{"lvl1":"JAX Specialized Libraries: BlackJAX, Numpyro, and Domain-Specific Tools"},"content":"","type":"content","url":"/jax-specialized-libraries","position":1},{"hierarchy":{"lvl1":"JAX Specialized Libraries: BlackJAX, Numpyro, and Domain-Specific Tools","lvl2":"Learning Objectives"},"type":"lvl2","url":"/jax-specialized-libraries#learning-objectives","position":2},{"hierarchy":{"lvl1":"JAX Specialized Libraries: BlackJAX, Numpyro, and Domain-Specific Tools","lvl2":"Learning Objectives"},"content":"By the end of this chapter, you will:\n\nImplement MCMC samplers with BlackJAX\n\nBuild probabilistic models with Numpyro\n\nUse JAXopt for constrained optimization\n\nApply ChemJAX for molecular dynamics\n\nLeverage domain-specific JAX libraries for astronomy\n\nBuild custom JAX-based tools for your research","type":"content","url":"/jax-specialized-libraries#learning-objectives","position":3},{"hierarchy":{"lvl1":"JAX Specialized Libraries: BlackJAX, Numpyro, and Domain-Specific Tools","lvl2":"BlackJAX: Modern MCMC Sampling"},"type":"lvl2","url":"/jax-specialized-libraries#blackjax-modern-mcmc-sampling","position":4},{"hierarchy":{"lvl1":"JAX Specialized Libraries: BlackJAX, Numpyro, and Domain-Specific Tools","lvl2":"BlackJAX: Modern MCMC Sampling"},"content":"","type":"content","url":"/jax-specialized-libraries#blackjax-modern-mcmc-sampling","position":5},{"hierarchy":{"lvl1":"JAX Specialized Libraries: BlackJAX, Numpyro, and Domain-Specific Tools","lvl3":"Introduction to BlackJAX","lvl2":"BlackJAX: Modern MCMC Sampling"},"type":"lvl3","url":"/jax-specialized-libraries#introduction-to-blackjax","position":6},{"hierarchy":{"lvl1":"JAX Specialized Libraries: BlackJAX, Numpyro, and Domain-Specific Tools","lvl3":"Introduction to BlackJAX","lvl2":"BlackJAX: Modern MCMC Sampling"},"content":"import jax\nimport jax.numpy as jnp\nfrom jax import random, grad, jit, vmap\nimport blackjax\nimport blackjax.smc as smc\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom typing import NamedTuple, Callable\n\ndef blackjax_fundamentals():\n    \"\"\"Learn BlackJAX for MCMC sampling in astronomy.\"\"\"\n    \n    print(\"BLACKJAX: MODERN MCMC SAMPLING\")\n    print(\"=\" * 50)\n    \n    # 1. Basic HMC sampling\n    print(\"\\n1. HAMILTONIAN MONTE CARLO:\")\n    \n    # Define a cosmological likelihood\n    def log_likelihood(theta):\n        \"\"\"Log likelihood for cosmological parameters.\"\"\"\n        omega_m, h0 = theta\n        \n        # Mock SNe Ia data\n        z_data = jnp.array([0.01, 0.05, 0.1, 0.5, 1.0])\n        mu_obs = jnp.array([33.0, 36.0, 38.0, 42.0, 44.0])\n        mu_err = jnp.array([0.1, 0.15, 0.2, 0.25, 0.3])\n        \n        # Theoretical distance modulus (simplified)\n        def luminosity_distance(z, om, h):\n            # Simplified for flat universe\n            c = 3e5  # km/s\n            dL = c * z * (1 + z/2 * (1 - om))  # Taylor expansion\n            return dL / h\n        \n        dL = vmap(lambda z: luminosity_distance(z, omega_m, h0))(z_data)\n        mu_theory = 5 * jnp.log10(dL) + 25\n        \n        # Chi-squared\n        chi2 = jnp.sum(((mu_obs - mu_theory) / mu_err) ** 2)\n        return -0.5 * chi2\n    \n    def log_prior(theta):\n        \"\"\"Log prior for cosmological parameters.\"\"\"\n        omega_m, h0 = theta\n        \n        # Uniform priors\n        if 0 < omega_m < 1 and 50 < h0 < 100:\n            return 0.0\n        return -jnp.inf\n    \n    def log_prob(theta):\n        \"\"\"Log posterior probability.\"\"\"\n        lp = log_prior(theta)\n        if jnp.isfinite(lp):\n            return lp + log_likelihood(theta)\n        return lp\n    \n    # Initialize HMC\n    key = random.PRNGKey(0)\n    initial_position = jnp.array([0.3, 70.0])\n    \n    # Build HMC kernel\n    inv_mass_matrix = jnp.array([0.01, 1.0])  # Diagonal mass matrix\n    num_integration_steps = 10\n    step_size = 0.01\n    \n    hmc = blackjax.hmc(\n        log_prob,\n        step_size=step_size,\n        inverse_mass_matrix=inv_mass_matrix,\n        num_integration_steps=num_integration_steps\n    )\n    \n    # Initialize state\n    state = hmc.init(initial_position)\n    \n    # Run sampling\n    def inference_loop(rng_key, kernel, initial_state, num_samples):\n        \"\"\"MCMC sampling loop.\"\"\"\n        \n        @jit\n        def one_step(state, rng_key):\n            state, info = kernel(rng_key, state)\n            return state, (state, info)\n        \n        keys = random.split(rng_key, num_samples)\n        _, (states, infos) = jax.lax.scan(one_step, initial_state, keys)\n        \n        return states, infos\n    \n    key, sample_key = random.split(key)\n    states, infos = inference_loop(sample_key, hmc.step, state, 1000)\n    \n    samples = states.position\n    print(f\"  Sampled {len(samples)} points\")\n    print(f\"  Mean Ωₘ = {jnp.mean(samples[:, 0]):.3f} ± {jnp.std(samples[:, 0]):.3f}\")\n    print(f\"  Mean H₀ = {jnp.mean(samples[:, 1]):.1f} ± {jnp.std(samples[:, 1]):.1f}\")\n    print(f\"  Acceptance rate: {jnp.mean(infos.is_accepted):.2%}\")\n    \n    # 2. NUTS (No U-Turn Sampler)\n    print(\"\\n2. NUTS SAMPLER:\")\n    \n    nuts = blackjax.nuts(log_prob, step_size=step_size)\n    state = nuts.init(initial_position)\n    \n    key, sample_key = random.split(key)\n    states, infos = inference_loop(sample_key, nuts.step, state, 500)\n    \n    print(f\"  NUTS: {len(states.position)} samples\")\n    print(f\"  Mean tree depth: {jnp.mean(infos.num_trajectory_expansions):.1f}\")\n    \n    # 3. Adaptive sampling\n    print(\"\\n3. ADAPTIVE SAMPLING:\")\n    \n    # Window adaptation for step size and mass matrix\n    def run_adaptive_sampling(kernel_factory, num_chains=4):\n        \"\"\"Run parallel chains with adaptation.\"\"\"\n        \n        key = random.PRNGKey(42)\n        keys = random.split(key, num_chains + 1)\n        \n        # Initialize multiple chains\n        initial_positions = jnp.array([\n            [0.25 + 0.1 * random.normal(keys[i], ()), \n             65.0 + 5.0 * random.normal(keys[i+1], ())]\n            for i in range(num_chains)\n        ])\n        \n        # Warmup with window adaptation\n        warmup = blackjax.window_adaptation(\n            blackjax.nuts,\n            log_prob,\n            num_steps=500,\n            initial_step_size=0.01,\n            target_acceptance_rate=0.8\n        )\n        \n        # Run warmup\n        (last_states, parameters), _ = warmup.run(keys[-1], initial_positions[0])\n        \n        print(f\"  Adapted step size: {parameters['step_size']:.4f}\")\n        \n        return last_states, parameters\n    \n    adapted_state, adapted_params = run_adaptive_sampling(blackjax.nuts)\n    \n    # 4. Sequential Monte Carlo\n    print(\"\\n4. SEQUENTIAL MONTE CARLO:\")\n    \n    # Temperature schedule for tempering\n    def tempered_log_prob(theta, beta):\n        \"\"\"Tempered posterior for SMC.\"\"\"\n        return beta * log_prob(theta)\n    \n    # SMC sampler\n    def run_smc(num_particles=100):\n        \"\"\"Run SMC sampler.\"\"\"\n        key = random.PRNGKey(123)\n        \n        # Initial particles from prior\n        init_key, smc_key = random.split(key)\n        initial_particles = jnp.array([\n            [random.uniform(init_key, (), minval=0.1, maxval=0.5),\n             random.uniform(init_key, (), minval=60, maxval=80)]\n            for _ in range(num_particles)\n        ])\n        \n        # Temperature schedule\n        betas = jnp.linspace(0, 1, 10)\n        \n        print(f\"  SMC with {num_particles} particles, {len(betas)} temperatures\")\n        \n        # Would run full SMC here\n        return initial_particles\n    \n    smc_particles = run_smc()\n\nblackjax_fundamentals()","type":"content","url":"/jax-specialized-libraries#introduction-to-blackjax","position":7},{"hierarchy":{"lvl1":"JAX Specialized Libraries: BlackJAX, Numpyro, and Domain-Specific Tools","lvl3":"Advanced Sampling Techniques","lvl2":"BlackJAX: Modern MCMC Sampling"},"type":"lvl3","url":"/jax-specialized-libraries#advanced-sampling-techniques","position":8},{"hierarchy":{"lvl1":"JAX Specialized Libraries: BlackJAX, Numpyro, and Domain-Specific Tools","lvl3":"Advanced Sampling Techniques","lvl2":"BlackJAX: Modern MCMC Sampling"},"content":"def advanced_blackjax():\n    \"\"\"Advanced sampling techniques with BlackJAX.\"\"\"\n    \n    print(\"\\nADVANCED BLACKJAX TECHNIQUES\")\n    print(\"=\" * 50)\n    \n    # 1. Riemannian HMC\n    print(\"\\n1. RIEMANNIAN HMC:\")\n    \n    def log_prob_with_metric(theta):\n        \"\"\"Log probability with position-dependent metric.\"\"\"\n        # Stellar population synthesis parameters\n        age, metallicity = theta\n        \n        # Mock observables\n        color_obs = 0.8\n        magnitude_obs = -2.0\n        \n        # Model predictions (simplified)\n        color_model = 0.5 + 0.1 * age - 0.2 * metallicity\n        mag_model = -3.0 + 0.2 * age + 0.3 * metallicity\n        \n        # Position-dependent uncertainties\n        sigma_color = 0.1 * (1 + 0.1 * jnp.abs(age))\n        sigma_mag = 0.2 * (1 + 0.05 * jnp.abs(metallicity))\n        \n        log_like = -0.5 * (\n            ((color_obs - color_model) / sigma_color) ** 2 +\n            ((magnitude_obs - mag_model) / sigma_mag) ** 2\n        )\n        \n        # Priors\n        log_prior = -0.5 * (age ** 2 / 100 + metallicity ** 2 / 4)\n        \n        return log_like + log_prior\n    \n    # Metric tensor (Fisher information)\n    def metric_fn(theta):\n        \"\"\"Compute metric tensor at position.\"\"\"\n        hess = jax.hessian(log_prob_with_metric)(theta)\n        return -hess  # Fisher information\n    \n    # Would implement full Riemannian HMC here\n    print(\"  Riemannian HMC configured for curved parameter space\")\n    \n    # 2. Parallel tempering\n    print(\"\\n2. PARALLEL TEMPERING:\")\n    \n    def parallel_tempering(log_prob, num_chains=4, num_samples=1000):\n        \"\"\"Parallel tempering MCMC.\"\"\"\n        \n        # Temperature ladder\n        betas = jnp.array([1.0, 0.5, 0.25, 0.1])\n        \n        def tempered_log_prob(theta, beta):\n            return beta * log_prob(theta)\n        \n        # Initialize chains at different temperatures\n        key = random.PRNGKey(42)\n        keys = random.split(key, num_chains + 1)\n        \n        initial_positions = jnp.array([\n            [0.3 + 0.05 * i, 70.0 + 2.0 * i] \n            for i in range(num_chains)\n        ])\n        \n        # Build kernels for each temperature\n        kernels = []\n        states = []\n        for i, beta in enumerate(betas):\n            kernel = blackjax.hmc(\n                lambda theta: tempered_log_prob(theta, beta),\n                step_size=0.01 / jnp.sqrt(beta),  # Adjust step size\n                inverse_mass_matrix=jnp.ones(2),\n                num_integration_steps=10\n            )\n            kernels.append(kernel)\n            states.append(kernel.init(initial_positions[i]))\n        \n        # Sampling with swaps\n        @jit\n        def swap_step(states, key):\n            \"\"\"Propose and accept/reject swaps.\"\"\"\n            swap_key, accept_key = random.split(key)\n            \n            # Random pair to swap\n            pair = random.choice(swap_key, num_chains - 1)\n            \n            # Compute swap acceptance probability\n            theta_i = states[pair].position\n            theta_j = states[pair + 1].position\n            \n            log_prob_i = tempered_log_prob(theta_i, betas[pair])\n            log_prob_j = tempered_log_prob(theta_j, betas[pair + 1])\n            \n            log_prob_i_swap = tempered_log_prob(theta_j, betas[pair])\n            log_prob_j_swap = tempered_log_prob(theta_i, betas[pair + 1])\n            \n            log_alpha = (log_prob_i_swap + log_prob_j_swap - \n                        log_prob_i - log_prob_j)\n            \n            # Accept/reject\n            accept = random.uniform(accept_key) < jnp.exp(log_alpha)\n            \n            # Swap if accepted (simplified - would use lax.cond)\n            return states, accept\n        \n        print(f\"  Parallel tempering with {num_chains} chains\")\n        print(f\"  Temperatures: {1/betas}\")\n        \n        return states\n    \n    # Example cosmology log prob\n    def cosmo_log_prob(theta):\n        omega_m, h0 = theta\n        if 0 < omega_m < 1 and 50 < h0 < 100:\n            return -0.5 * ((omega_m - 0.3)**2 / 0.01 + (h0 - 70)**2 / 25)\n        return -jnp.inf\n    \n    pt_states = parallel_tempering(cosmo_log_prob)\n    \n    # 3. Ensemble samplers\n    print(\"\\n3. ENSEMBLE SAMPLERS:\")\n    \n    def affine_invariant_ensemble_sampler(log_prob, num_walkers=32):\n        \"\"\"Affine invariant ensemble sampler (like emcee).\"\"\"\n        \n        key = random.PRNGKey(99)\n        ndim = 2\n        \n        # Initialize walkers\n        initial_positions = random.normal(key, (num_walkers, ndim))\n        \n        @jit\n        def stretch_move(positions, key):\n            \"\"\"Stretch move for ensemble sampler.\"\"\"\n            n_walkers = len(positions)\n            keys = random.split(key, n_walkers + 2)\n            \n            # Random pairs\n            pairs = random.choice(keys[0], n_walkers, (n_walkers,))\n            \n            # Stretch factors\n            a = 2.0  # Stretch scale\n            z = ((a - 1) * random.uniform(keys[1], (n_walkers,)) + 1) ** 2 / a\n            \n            # Propose new positions\n            proposals = positions[pairs] + z[:, None] * (positions - positions[pairs])\n            \n            # Compute acceptance\n            log_probs_old = vmap(log_prob)(positions)\n            log_probs_new = vmap(log_prob)(proposals)\n            \n            log_alpha = (ndim - 1) * jnp.log(z) + log_probs_new - log_probs_old\n            \n            # Accept/reject\n            accept = random.uniform(keys[2], (n_walkers,)) < jnp.exp(log_alpha)\n            \n            new_positions = jnp.where(accept[:, None], proposals, positions)\n            \n            return new_positions, accept\n        \n        # Run ensemble\n        positions = initial_positions\n        for i in range(100):\n            key, step_key = random.split(key)\n            positions, accept = stretch_move(positions, step_key)\n        \n        print(f\"  Ensemble sampler with {num_walkers} walkers\")\n        print(f\"  Mean position: {jnp.mean(positions, axis=0)}\")\n        \n        return positions\n    \n    ensemble_samples = affine_invariant_ensemble_sampler(cosmo_log_prob)\n\nadvanced_blackjax()","type":"content","url":"/jax-specialized-libraries#advanced-sampling-techniques","position":9},{"hierarchy":{"lvl1":"JAX Specialized Libraries: BlackJAX, Numpyro, and Domain-Specific Tools","lvl2":"Numpyro: Probabilistic Programming"},"type":"lvl2","url":"/jax-specialized-libraries#numpyro-probabilistic-programming","position":10},{"hierarchy":{"lvl1":"JAX Specialized Libraries: BlackJAX, Numpyro, and Domain-Specific Tools","lvl2":"Numpyro: Probabilistic Programming"},"content":"","type":"content","url":"/jax-specialized-libraries#numpyro-probabilistic-programming","position":11},{"hierarchy":{"lvl1":"JAX Specialized Libraries: BlackJAX, Numpyro, and Domain-Specific Tools","lvl3":"Building Probabilistic Models","lvl2":"Numpyro: Probabilistic Programming"},"type":"lvl3","url":"/jax-specialized-libraries#building-probabilistic-models","position":12},{"hierarchy":{"lvl1":"JAX Specialized Libraries: BlackJAX, Numpyro, and Domain-Specific Tools","lvl3":"Building Probabilistic Models","lvl2":"Numpyro: Probabilistic Programming"},"content":"import numpyro\nimport numpyro.distributions as dist\nfrom numpyro.infer import MCMC, NUTS, SVI, Trace_ELBO\nfrom numpyro.infer.autoguide import AutoNormal\n\ndef numpyro_fundamentals():\n    \"\"\"Probabilistic programming with Numpyro.\"\"\"\n    \n    print(\"\\nNUMPYRO: PROBABILISTIC PROGRAMMING\")\n    print(\"=\" * 50)\n    \n    # 1. Basic hierarchical model\n    print(\"\\n1. HIERARCHICAL MODEL - CEPHEID CALIBRATION:\")\n    \n    def cepheid_model(periods, magnitudes=None):\n        \"\"\"\n        Hierarchical model for Cepheid period-luminosity relation.\n        Different galaxies have different zero points.\n        \"\"\"\n        n_obs = len(periods)\n        \n        # Hyperpriors for P-L relation\n        alpha = numpyro.sample('alpha', dist.Normal(-2.5, 0.5))\n        beta = numpyro.sample('beta', dist.Normal(-3.0, 0.5))\n        \n        # Intrinsic scatter\n        sigma_int = numpyro.sample('sigma_int', dist.HalfNormal(0.1))\n        \n        # Galaxy-specific zero points (distance moduli)\n        # Assume we have galaxy IDs\n        n_galaxies = 5\n        galaxy_ids = jnp.array([i % n_galaxies for i in range(n_obs)])\n        \n        with numpyro.plate('galaxies', n_galaxies):\n            mu_gal = numpyro.sample('mu_gal', dist.Normal(30.0, 2.0))\n        \n        # Expected magnitude\n        log_P = jnp.log10(periods)\n        M_expected = alpha * log_P + beta  # Absolute magnitude\n        m_expected = M_expected + mu_gal[galaxy_ids]  # Apparent magnitude\n        \n        # Observational uncertainty\n        sigma_obs = 0.05\n        sigma_total = jnp.sqrt(sigma_int**2 + sigma_obs**2)\n        \n        # Likelihood\n        with numpyro.plate('observations', n_obs):\n            numpyro.sample('magnitudes', \n                          dist.Normal(m_expected, sigma_total),\n                          obs=magnitudes)\n    \n    # Generate synthetic data\n    key = random.PRNGKey(0)\n    n_cepheids = 100\n    true_alpha = -2.43\n    true_beta = -3.05\n    \n    periods = jnp.exp(random.uniform(key, (n_cepheids,), minval=0, maxval=3))\n    galaxy_ids = jnp.array([i % 5 for i in range(n_cepheids)])\n    true_mu = jnp.array([29.5, 30.0, 30.5, 31.0, 31.5])\n    \n    log_P = jnp.log10(periods)\n    M_true = true_alpha * log_P + true_beta\n    m_true = M_true + true_mu[galaxy_ids]\n    \n    key, noise_key = random.split(key)\n    observed_mags = m_true + 0.05 * random.normal(noise_key, (n_cepheids,))\n    \n    # Run MCMC\n    nuts_kernel = NUTS(cepheid_model)\n    mcmc = MCMC(nuts_kernel, num_warmup=500, num_samples=1000)\n    \n    key, run_key = random.split(key)\n    mcmc.run(run_key, periods, magnitudes=observed_mags)\n    \n    samples = mcmc.get_samples()\n    print(f\"  α = {jnp.mean(samples['alpha']):.3f} ± {jnp.std(samples['alpha']):.3f}\")\n    print(f\"  β = {jnp.mean(samples['beta']):.3f} ± {jnp.std(samples['beta']):.3f}\")\n    \n    # 2. Mixture models\n    print(\"\\n2. MIXTURE MODEL - STELLAR POPULATIONS:\")\n    \n    def stellar_population_mixture(colors=None, magnitudes=None):\n        \"\"\"\n        Mixture model for stellar populations.\n        Main sequence, giants, and white dwarfs.\n        \"\"\"\n        n_obs = len(colors) if colors is not None else 100\n        \n        # Mixture weights\n        weights = numpyro.sample('weights', dist.Dirichlet(jnp.ones(3)))\n        \n        # Component parameters\n        with numpyro.plate('components', 3):\n            mu_color = numpyro.sample('mu_color', dist.Normal(0.0, 2.0))\n            mu_mag = numpyro.sample('mu_mag', dist.Normal(0.0, 5.0))\n            sigma_color = numpyro.sample('sigma_color', dist.HalfNormal(0.5))\n            sigma_mag = numpyro.sample('sigma_mag', dist.HalfNormal(1.0))\n        \n        # Mixture assignment\n        with numpyro.plate('stars', n_obs):\n            assignment = numpyro.sample('assignment', dist.Categorical(weights))\n            \n            # Observed color and magnitude\n            numpyro.sample('colors',\n                          dist.Normal(mu_color[assignment], sigma_color[assignment]),\n                          obs=colors)\n            numpyro.sample('magnitudes',\n                          dist.Normal(mu_mag[assignment], sigma_mag[assignment]),\n                          obs=magnitudes)\n    \n    # Generate synthetic CMD data\n    key, data_key = random.split(key)\n    n_stars = 200\n    \n    # True mixture\n    true_weights = jnp.array([0.7, 0.2, 0.1])  # MS, Giants, WDs\n    true_mu_color = jnp.array([0.5, 1.5, -0.5])\n    true_mu_mag = jnp.array([5.0, 0.0, 10.0])\n    \n    assignments = random.categorical(data_key, jnp.log(true_weights), shape=(n_stars,))\n    \n    keys = random.split(data_key, 3)\n    obs_colors = true_mu_color[assignments] + 0.2 * random.normal(keys[0], (n_stars,))\n    obs_mags = true_mu_mag[assignments] + 0.5 * random.normal(keys[1], (n_stars,))\n    \n    print(\"  Mixture model for CMD analysis configured\")\n    \n    # 3. Time series model\n    print(\"\\n3. TIME SERIES - QUASAR VARIABILITY:\")\n    \n    def quasar_drw_model(times, fluxes=None):\n        \"\"\"\n        Damped Random Walk model for quasar variability.\n        \"\"\"\n        n_obs = len(times)\n        dt = jnp.diff(times)\n        \n        # DRW parameters\n        tau = numpyro.sample('tau', dist.LogNormal(jnp.log(100), 0.5))\n        sigma = numpyro.sample('sigma', dist.HalfNormal(0.1))\n        \n        # Mean flux\n        mean_flux = numpyro.sample('mean_flux', dist.Normal(0, 1))\n        \n        # Initial state\n        flux_0 = numpyro.sample('flux_0', dist.Normal(mean_flux, sigma))\n        \n        # Evolution\n        def transition(carry, dt):\n            flux_prev = carry\n            \n            # DRW transition\n            decay = jnp.exp(-dt / tau)\n            noise_var = sigma**2 * (1 - decay**2)\n            \n            flux_next = numpyro.sample('flux_next',\n                                       dist.Normal(mean_flux + decay * (flux_prev - mean_flux),\n                                                  jnp.sqrt(noise_var)))\n            return flux_next, flux_next\n        \n        # Scan through time series\n        _, flux_trajectory = jax.lax.scan(transition, flux_0, dt)\n        flux_trajectory = jnp.concatenate([jnp.array([flux_0]), flux_trajectory])\n        \n        # Observational uncertainty\n        obs_error = 0.01\n        \n        with numpyro.plate('observations', n_obs):\n            numpyro.sample('fluxes',\n                          dist.Normal(flux_trajectory, obs_error),\n                          obs=fluxes)\n    \n    print(\"  DRW model for quasar variability created\")\n\nnumpyro_fundamentals()","type":"content","url":"/jax-specialized-libraries#building-probabilistic-models","position":13},{"hierarchy":{"lvl1":"JAX Specialized Libraries: BlackJAX, Numpyro, and Domain-Specific Tools","lvl3":"Variational Inference with Numpyro","lvl2":"Numpyro: Probabilistic Programming"},"type":"lvl3","url":"/jax-specialized-libraries#variational-inference-with-numpyro","position":14},{"hierarchy":{"lvl1":"JAX Specialized Libraries: BlackJAX, Numpyro, and Domain-Specific Tools","lvl3":"Variational Inference with Numpyro","lvl2":"Numpyro: Probabilistic Programming"},"content":"def numpyro_variational_inference():\n    \"\"\"Variational inference for large-scale problems.\"\"\"\n    \n    print(\"\\nVARIATIONAL INFERENCE WITH NUMPYRO\")\n    print(\"=\" * 50)\n    \n    # 1. SVI for galaxy clustering\n    print(\"\\n1. SVI FOR GALAXY CLUSTERING:\")\n    \n    def galaxy_clustering_model(positions, redshifts=None):\n        \"\"\"\n        Hierarchical model for galaxy clustering.\n        \"\"\"\n        n_galaxies = len(positions)\n        \n        # Cosmological parameters\n        omega_m = numpyro.sample('omega_m', dist.Uniform(0.1, 0.5))\n        sigma_8 = numpyro.sample('sigma_8', dist.Uniform(0.5, 1.0))\n        \n        # Bias parameters for different galaxy types\n        n_types = 3\n        with numpyro.plate('galaxy_types', n_types):\n            bias = numpyro.sample('bias', dist.LogNormal(0, 0.5))\n        \n        # Galaxy type assignments (latent)\n        with numpyro.plate('galaxies', n_galaxies):\n            galaxy_type = numpyro.sample('galaxy_type',\n                                         dist.Categorical(jnp.ones(n_types) / n_types))\n            \n            # Simplified clustering likelihood\n            # In reality, would use correlation function\n            clustering_strength = bias[galaxy_type] * sigma_8\n            \n            # Observed redshift (with peculiar velocities)\n            if redshifts is not None:\n                z_cosmo = jnp.linalg.norm(positions, axis=1) / 3000  # Hubble flow\n                z_pec = clustering_strength * random.normal(random.PRNGKey(0), (n_galaxies,)) * 0.001\n                \n                numpyro.sample('redshifts',\n                              dist.Normal(z_cosmo + z_pec, 0.0001),\n                              obs=redshifts)\n    \n    # Generate mock data\n    key = random.PRNGKey(42)\n    n_gal = 1000\n    positions = random.normal(key, (n_gal, 3)) * 100\n    redshifts = jnp.linalg.norm(positions, axis=1) / 3000 + \\\n                0.001 * random.normal(key, (n_gal,))\n    \n    # Setup SVI\n    guide = AutoNormal(galaxy_clustering_model)\n    optimizer = numpyro.optim.Adam(step_size=0.01)\n    svi = SVI(galaxy_clustering_model, guide, optimizer, loss=Trace_ELBO())\n    \n    # Run optimization\n    svi_result = svi.run(random.PRNGKey(0), 1000, positions, redshifts)\n    \n    params = svi_result.params\n    print(f\"  SVI converged with loss: {svi_result.losses[-1]:.2f}\")\n    \n    # 2. Custom guides\n    print(\"\\n2. CUSTOM VARIATIONAL GUIDES:\")\n    \n    def custom_guide(positions, redshifts=None):\n        \"\"\"Custom guide for more control.\"\"\"\n        n_galaxies = len(positions)\n        \n        # Cosmological parameters with custom distributions\n        omega_m_loc = numpyro.param('omega_m_loc', 0.3)\n        omega_m_scale = numpyro.param('omega_m_scale', 0.01,\n                                      constraint=dist.constraints.positive)\n        omega_m = numpyro.sample('omega_m', dist.Normal(omega_m_loc, omega_m_scale))\n        \n        sigma_8_loc = numpyro.param('sigma_8_loc', 0.8)\n        sigma_8_scale = numpyro.param('sigma_8_scale', 0.01,\n                                      constraint=dist.constraints.positive)\n        sigma_8 = numpyro.sample('sigma_8', dist.Normal(sigma_8_loc, sigma_8_scale))\n        \n        # Galaxy bias parameters\n        with numpyro.plate('galaxy_types', 3):\n            bias_loc = numpyro.param('bias_loc', jnp.ones(3))\n            bias_scale = numpyro.param('bias_scale', 0.1 * jnp.ones(3),\n                                       constraint=dist.constraints.positive)\n            numpyro.sample('bias', dist.LogNormal(bias_loc, bias_scale))\n        \n        # Galaxy type assignments (discrete - use Gumbel-softmax)\n        with numpyro.plate('galaxies', n_galaxies):\n            type_logits = numpyro.param('type_logits', jnp.zeros((n_galaxies, 3)))\n            numpyro.sample('galaxy_type', dist.CategoricalLogits(type_logits))\n    \n    print(\"  Custom variational guide configured\")\n    \n    # 3. Normalizing flows\n    print(\"\\n3. NORMALIZING FLOWS FOR COMPLEX POSTERIORS:\")\n    \n    from numpyro.infer.autoguide import AutoIAFNormal\n    \n    def complex_posterior_model(data=None):\n        \"\"\"Model with complex, multimodal posterior.\"\"\"\n        # Bimodal distribution\n        mode = numpyro.sample('mode', dist.Bernoulli(0.5))\n        \n        # Different parameters for each mode\n        with numpyro.handlers.mask(mask=mode):\n            theta1 = numpyro.sample('theta1', dist.Normal(2.0, 0.5))\n        \n        with numpyro.handlers.mask(mask=~mode):\n            theta2 = numpyro.sample('theta2', dist.Normal(-2.0, 0.5))\n        \n        # Combine\n        theta = jnp.where(mode, theta1, theta2)\n        \n        # Likelihood\n        numpyro.sample('data', dist.Normal(theta, 0.1), obs=data)\n    \n    # Use IAF (Inverse Autoregressive Flow)\n    flow_guide = AutoIAFNormal(complex_posterior_model, num_flows=3)\n    \n    print(\"  Normalizing flow guide created for complex posteriors\")\n\nnumpyro_variational_inference()","type":"content","url":"/jax-specialized-libraries#variational-inference-with-numpyro","position":15},{"hierarchy":{"lvl1":"JAX Specialized Libraries: BlackJAX, Numpyro, and Domain-Specific Tools","lvl2":"JAXopt: Constrained Optimization"},"type":"lvl2","url":"/jax-specialized-libraries#jaxopt-constrained-optimization","position":16},{"hierarchy":{"lvl1":"JAX Specialized Libraries: BlackJAX, Numpyro, and Domain-Specific Tools","lvl2":"JAXopt: Constrained Optimization"},"content":"","type":"content","url":"/jax-specialized-libraries#jaxopt-constrained-optimization","position":17},{"hierarchy":{"lvl1":"JAX Specialized Libraries: BlackJAX, Numpyro, and Domain-Specific Tools","lvl3":"Optimization with Constraints","lvl2":"JAXopt: Constrained Optimization"},"type":"lvl3","url":"/jax-specialized-libraries#optimization-with-constraints","position":18},{"hierarchy":{"lvl1":"JAX Specialized Libraries: BlackJAX, Numpyro, and Domain-Specific Tools","lvl3":"Optimization with Constraints","lvl2":"JAXopt: Constrained Optimization"},"content":"import jaxopt\n\ndef jaxopt_optimization():\n    \"\"\"Constrained optimization for astronomical problems.\"\"\"\n    \n    print(\"\\nJAXOPT: CONSTRAINED OPTIMIZATION\")\n    print(\"=\" * 50)\n    \n    # 1. Constrained least squares\n    print(\"\\n1. CONSTRAINED ORBIT FITTING:\")\n    \n    def orbit_objective(params, observations):\n        \"\"\"Objective for orbit fitting.\"\"\"\n        a, e, i, omega = params\n        times, ra_obs, dec_obs = observations\n        \n        # Kepler orbit (simplified)\n        M = 2 * jnp.pi * times / (a ** 1.5)\n        E = M  # Small eccentricity approximation\n        \n        # Projected positions\n        x = a * (jnp.cos(E) - e)\n        y = a * jnp.sqrt(1 - e**2) * jnp.sin(E)\n        \n        # Rotate by inclination and argument\n        ra_model = x * jnp.cos(omega) - y * jnp.sin(omega) * jnp.cos(i)\n        dec_model = x * jnp.sin(omega) + y * jnp.cos(omega) * jnp.cos(i)\n        \n        # Residuals\n        residuals = jnp.sum((ra_obs - ra_model)**2 + (dec_obs - dec_model)**2)\n        return residuals\n    \n    # Constraints: physical orbital elements\n    def orbit_constraints(params):\n        a, e, i, omega = params\n        return jnp.array([\n            a - 0.1,      # a > 0.1 AU\n            10.0 - a,     # a < 10 AU\n            e,            # e >= 0\n            1.0 - e,      # e < 1\n            i,            # i >= 0\n            jnp.pi - i,   # i <= pi\n        ])\n    \n    # Generate mock observations\n    key = random.PRNGKey(123)\n    times = jnp.linspace(0, 10, 50)\n    true_params = jnp.array([2.0, 0.3, 0.5, 1.0])\n    \n    # Mock data (using true model)\n    M_true = 2 * jnp.pi * times / (true_params[0] ** 1.5)\n    x_true = true_params[0] * jnp.cos(M_true)\n    y_true = true_params[0] * jnp.sin(M_true) * jnp.sqrt(1 - true_params[1]**2)\n    \n    ra_obs = x_true + 0.01 * random.normal(key, x_true.shape)\n    dec_obs = y_true + 0.01 * random.normal(key, y_true.shape)\n    observations = (times, ra_obs, dec_obs)\n    \n    # Solve with constraints\n    initial_params = jnp.array([1.5, 0.2, 0.4, 0.8])\n    \n    solver = jaxopt.ProjectedGradient(\n        fun=lambda p: orbit_objective(p, observations),\n        projection=jaxopt.projection.projection_box,\n        maxiter=100\n    )\n    \n    # Box constraints\n    lower_bounds = jnp.array([0.1, 0.0, 0.0, 0.0])\n    upper_bounds = jnp.array([10.0, 0.99, jnp.pi, 2*jnp.pi])\n    \n    result = solver.run(initial_params, \n                        hyperparams_proj=(lower_bounds, upper_bounds))\n    \n    print(f\"  Optimization converged: {result.state.error < 1e-3}\")\n    print(f\"  True params: {true_params}\")\n    print(f\"  Fitted params: {result.params}\")\n    \n    # 2. Quadratic programming\n    print(\"\\n2. QUADRATIC PROGRAMMING - TELESCOPE SCHEDULING:\")\n    \n    def telescope_scheduling():\n        \"\"\"Optimize telescope observation schedule.\"\"\"\n        \n        # Problem: maximize scientific value subject to constraints\n        # Variables: time allocated to each target\n        n_targets = 10\n        \n        # Scientific value (priority) of each target\n        values = jnp.array([9, 8, 7, 6, 5, 5, 4, 3, 2, 1])\n        \n        # Observability windows (hours)\n        max_observable = jnp.array([3, 4, 2, 5, 3, 2, 4, 3, 2, 1])\n        \n        # Quadratic objective (negative for maximization)\n        # Include diversity bonus for observing multiple targets\n        Q = -jnp.eye(n_targets) * 0.1  # Diversity penalty\n        c = -values  # Linear term (negative for maximization)\n        \n        # Constraints: Ax <= b\n        # Total time constraint\n        A_time = jnp.ones((1, n_targets))\n        b_time = jnp.array([8.0])  # 8 hours total\n        \n        # Combine with observability constraints\n        A_obs = jnp.eye(n_targets)\n        b_obs = max_observable\n        \n        A = jnp.vstack([A_time, A_obs])\n        b = jnp.concatenate([b_time, b_obs])\n        \n        # Also need non-negativity: x >= 0\n        \n        # Solve QP\n        qp_solver = jaxopt.QuadraticProgramming()\n        \n        # Initial guess\n        x_init = jnp.ones(n_targets) * 0.5\n        \n        solution = qp_solver.run(x_init, params_obj=(Q, c),\n                                 params_ineq=(A, b))\n        \n        schedule = solution.params\n        \n        print(f\"  Telescope schedule optimized\")\n        print(f\"  Total time used: {jnp.sum(schedule):.1f} hours\")\n        print(f\"  Targets observed: {jnp.sum(schedule > 0.01)}\")\n        print(f\"  Scientific value: {jnp.dot(values, schedule):.1f}\")\n        \n        return schedule\n    \n    schedule = telescope_scheduling()\n    \n    # 3. Proximal gradient methods\n    print(\"\\n3. PROXIMAL METHODS - SPARSE DECONVOLUTION:\")\n    \n    def sparse_deconvolution():\n        \"\"\"Deconvolve image with sparsity constraint.\"\"\"\n        \n        # Create blurred image\n        key = random.PRNGKey(456)\n        true_image = jnp.zeros((50, 50))\n        \n        # Add point sources\n        n_sources = 10\n        positions = random.choice(key, 50*50, (n_sources,), replace=False)\n        true_image = true_image.flatten()\n        true_image = true_image.at[positions].set(\n            random.uniform(key, (n_sources,), minval=0.5, maxval=2.0)\n        )\n        true_image = true_image.reshape(50, 50)\n        \n        # Blur with PSF\n        def create_psf(size=5):\n            x = jnp.arange(size) - size // 2\n            X, Y = jnp.meshgrid(x, x)\n            psf = jnp.exp(-(X**2 + Y**2) / 2)\n            return psf / jnp.sum(psf)\n        \n        psf = create_psf()\n        \n        # Convolve (simplified - using scipy in practice)\n        blurred = jax.scipy.signal.convolve2d(true_image, psf, mode='same')\n        \n        # Add noise\n        observed = blurred + 0.01 * random.normal(key, blurred.shape)\n        \n        # Deconvolution with L1 regularization for sparsity\n        def objective(x):\n            # Data fidelity\n            convolved = jax.scipy.signal.convolve2d(x.reshape(50, 50), psf, mode='same')\n            fidelity = 0.5 * jnp.sum((convolved - observed) ** 2)\n            \n            # L1 regularization\n            sparsity = 0.01 * jnp.sum(jnp.abs(x))\n            \n            return fidelity + sparsity\n        \n        # Use proximal gradient\n        prox_solver = jaxopt.ProximalGradient(\n            fun=objective,\n            prox=jaxopt.prox.prox_lasso,\n            maxiter=100\n        )\n        \n        x_init = observed.flatten()\n        result = prox_solver.run(x_init)\n        \n        deconvolved = result.params.reshape(50, 50)\n        \n        print(f\"  Sparse deconvolution completed\")\n        print(f\"  Sparsity: {jnp.sum(jnp.abs(deconvolved) > 0.01)} non-zero pixels\")\n        \n        return deconvolved\n    \n    deconvolved_image = sparse_deconvolution()\n\njaxopt_optimization()","type":"content","url":"/jax-specialized-libraries#optimization-with-constraints","position":19},{"hierarchy":{"lvl1":"JAX Specialized Libraries: BlackJAX, Numpyro, and Domain-Specific Tools","lvl2":"Domain-Specific Libraries"},"type":"lvl2","url":"/jax-specialized-libraries#domain-specific-libraries","position":20},{"hierarchy":{"lvl1":"JAX Specialized Libraries: BlackJAX, Numpyro, and Domain-Specific Tools","lvl2":"Domain-Specific Libraries"},"content":"","type":"content","url":"/jax-specialized-libraries#domain-specific-libraries","position":21},{"hierarchy":{"lvl1":"JAX Specialized Libraries: BlackJAX, Numpyro, and Domain-Specific Tools","lvl3":"Astronomical JAX Tools","lvl2":"Domain-Specific Libraries"},"type":"lvl3","url":"/jax-specialized-libraries#astronomical-jax-tools","position":22},{"hierarchy":{"lvl1":"JAX Specialized Libraries: BlackJAX, Numpyro, and Domain-Specific Tools","lvl3":"Astronomical JAX Tools","lvl2":"Domain-Specific Libraries"},"content":"def astronomical_jax_libraries():\n    \"\"\"Specialized JAX libraries for astronomy.\"\"\"\n    \n    print(\"\\nASTRONOMICAL JAX LIBRARIES\")\n    print(\"=\" * 50)\n    \n    # 1. s2fft - Spherical harmonic transforms\n    print(\"\\n1. S2FFT - SPHERICAL HARMONICS FOR CMB:\")\n    \n    # Note: This is conceptual - s2fft would need to be installed\n    def cmb_analysis():\n        \"\"\"Analyze CMB maps using spherical harmonics.\"\"\"\n        \n        # Generate mock CMB map\n        nside = 64  # HEALPix resolution\n        npix = 12 * nside**2\n        \n        key = random.PRNGKey(789)\n        \n        # Generate random alm coefficients (simplified)\n        lmax = 100\n        n_alm = (lmax + 1) * (lmax + 2) // 2\n        \n        # Power spectrum (simplified)\n        ell = jnp.arange(lmax + 1)\n        Cl = 100 / (ell + 10) ** 2  # Simplified CMB spectrum\n        \n        # Random realization\n        alm_real = jnp.sqrt(Cl[None, :]) * random.normal(key, (1, lmax + 1))\n        \n        print(f\"  CMB analysis with lmax={lmax}\")\n        print(f\"  Power spectrum computed\")\n        \n        # Would use s2fft for actual transforms\n        # import s2fft\n        # map_data = s2fft.inverse_transform(alm_real, L=lmax)\n        \n        return alm_real\n    \n    cmb_alm = cmb_analysis()\n    \n    # 2. jax-cosmo - Cosmological calculations\n    print(\"\\n2. JAX-COSMO - COSMOLOGICAL CALCULATIONS:\")\n    \n    def cosmological_calculations():\n        \"\"\"Differentiable cosmology calculations.\"\"\"\n        \n        # Cosmological parameters\n        cosmo_params = {\n            'omega_m': 0.3,\n            'omega_de': 0.7,\n            'h': 0.7,\n            'omega_b': 0.05,\n            'n_s': 0.96,\n            'sigma_8': 0.8\n        }\n        \n        # Redshifts\n        z = jnp.linspace(0, 2, 100)\n        \n        # Comoving distance (simplified Friedmann equation)\n        def comoving_distance(z, omega_m, omega_de):\n            def integrand(z):\n                return 1 / jnp.sqrt(omega_m * (1 + z)**3 + omega_de)\n            \n            # Integrate (simplified - use quadrature in practice)\n            dz = z[1] - z[0]\n            integral = jnp.cumsum(vmap(integrand)(z)) * dz\n            \n            c_over_H0 = 3000 / 0.7  # Mpc\n            return c_over_H0 * integral\n        \n        d_c = comoving_distance(z, cosmo_params['omega_m'], cosmo_params['omega_de'])\n        \n        # Angular diameter distance\n        d_a = d_c / (1 + z)\n        \n        # Luminosity distance\n        d_l = d_c * (1 + z)\n        \n        print(f\"  Computed distances for {len(z)} redshifts\")\n        print(f\"  Max comoving distance: {d_c[-1]:.0f} Mpc\")\n        \n        # Growth function (simplified)\n        def growth_factor(z, omega_m):\n            a = 1 / (1 + z)\n            # Approximate growth factor\n            omega_m_z = omega_m / (omega_m + (1 - omega_m) * a**3)\n            return a * omega_m_z**0.55\n        \n        D_growth = vmap(lambda zi: growth_factor(zi, cosmo_params['omega_m']))(z)\n        \n        print(f\"  Growth factor at z=0: {D_growth[0]:.3f}\")\n        \n        return d_c, d_a, d_l, D_growth\n    \n    distances = cosmological_calculations()\n    \n    # 3. Neural density estimation\n    print(\"\\n3. NEURAL DENSITY ESTIMATION FOR INFERENCE:\")\n    \n    def neural_posterior_estimation():\n        \"\"\"Use neural networks for likelihood-free inference.\"\"\"\n        \n        import flax.linen as nn\n        \n        class PosteriorNetwork(nn.Module):\n            \"\"\"Neural network to approximate posterior.\"\"\"\n            \n            @nn.compact\n            def __call__(self, data, params):\n                # Concatenate data and parameters\n                x = jnp.concatenate([data, params])\n                \n                # Deep network\n                x = nn.Dense(128)(x)\n                x = nn.relu(x)\n                x = nn.Dense(128)(x)\n                x = nn.relu(x)\n                x = nn.Dense(64)(x)\n                x = nn.relu(x)\n                \n                # Output: parameters of posterior distribution\n                mean = nn.Dense(len(params))(x)\n                log_std = nn.Dense(len(params))(x)\n                \n                return mean, log_std\n        \n        print(\"  Neural posterior estimator configured\")\n        \n        # Would train on simulated data\n        # This enables likelihood-free inference for complex simulators\n        \n        return PosteriorNetwork()\n    \n    npe_model = neural_posterior_estimation()\n\nastronomical_jax_libraries()","type":"content","url":"/jax-specialized-libraries#astronomical-jax-tools","position":23},{"hierarchy":{"lvl1":"JAX Specialized Libraries: BlackJAX, Numpyro, and Domain-Specific Tools","lvl2":"Complete Example: Gravitational Wave Analysis"},"type":"lvl2","url":"/jax-specialized-libraries#complete-example-gravitational-wave-analysis","position":24},{"hierarchy":{"lvl1":"JAX Specialized Libraries: BlackJAX, Numpyro, and Domain-Specific Tools","lvl2":"Complete Example: Gravitational Wave Analysis"},"content":"","type":"content","url":"/jax-specialized-libraries#complete-example-gravitational-wave-analysis","position":25},{"hierarchy":{"lvl1":"JAX Specialized Libraries: BlackJAX, Numpyro, and Domain-Specific Tools","lvl3":"End-to-End GW Pipeline","lvl2":"Complete Example: Gravitational Wave Analysis"},"type":"lvl3","url":"/jax-specialized-libraries#end-to-end-gw-pipeline","position":26},{"hierarchy":{"lvl1":"JAX Specialized Libraries: BlackJAX, Numpyro, and Domain-Specific Tools","lvl3":"End-to-End GW Pipeline","lvl2":"Complete Example: Gravitational Wave Analysis"},"content":"def gravitational_wave_pipeline():\n    \"\"\"\n    Complete gravitational wave analysis pipeline.\n    Combines multiple JAX libraries for production analysis.\n    \"\"\"\n    \n    print(\"\\nGRAVITATIONAL WAVE ANALYSIS PIPELINE\")\n    print(\"=\" * 50)\n    \n    # 1. Generate GW signal\n    def generate_gw_signal(params, times):\n        \"\"\"Generate gravitational wave signal.\"\"\"\n        m1, m2, d_l, t_c, phi_c = params\n        \n        # Chirp mass\n        M_chirp = (m1 * m2) ** (3/5) / (m1 + m2) ** (1/5)\n        \n        # Frequency evolution (simplified)\n        t_to_merger = t_c - times\n        f_gw = jnp.where(\n            t_to_merger > 0,\n            (M_chirp / t_to_merger) ** (3/8),\n            0.0\n        )\n        \n        # Amplitude\n        amplitude = (M_chirp ** (5/3) * f_gw ** (2/3)) / d_l\n        \n        # Phase\n        phase = 2 * jnp.pi * jnp.cumsum(f_gw) * 0.001 + phi_c\n        \n        # Strain\n        h_plus = amplitude * jnp.cos(phase)\n        h_cross = amplitude * jnp.sin(phase)\n        \n        return h_plus, h_cross\n    \n    # 2. Detector response\n    def detector_response(h_plus, h_cross, detector_params):\n        \"\"\"Compute detector response to GW signal.\"\"\"\n        F_plus, F_cross, psi = detector_params\n        \n        strain = F_plus * h_plus + F_cross * h_cross\n        return strain\n    \n    # 3. Likelihood function\n    def log_likelihood(params, data, times, noise_psd):\n        \"\"\"Log likelihood for GW parameters.\"\"\"\n        h_plus, h_cross = generate_gw_signal(params, times)\n        \n        # Detector response (simplified - single detector)\n        detector_params = (0.5, 0.5, 0.0)  # Antenna patterns\n        h_model = detector_response(h_plus, h_cross, detector_params)\n        \n        # Matched filter in frequency domain\n        h_data_fft = jnp.fft.rfft(data)\n        h_model_fft = jnp.fft.rfft(h_model)\n        \n        # Inner product weighted by noise\n        inner_product = jnp.sum(\n            jnp.conj(h_data_fft) * h_model_fft / noise_psd\n        ).real\n        \n        # Normalization\n        norm = jnp.sum(jnp.abs(h_model_fft) ** 2 / noise_psd)\n        \n        return inner_product - 0.5 * norm\n    \n    # 4. Generate mock data\n    key = random.PRNGKey(2024)\n    n_samples = 4096\n    times = jnp.linspace(0, 1, n_samples)\n    \n    # True parameters\n    true_params = jnp.array([\n        30.0,   # m1 (solar masses)\n        25.0,   # m2\n        100.0,  # luminosity distance (Mpc)\n        0.5,    # coalescence time\n        0.0     # phase\n    ])\n    \n    # Generate signal\n    h_plus_true, h_cross_true = generate_gw_signal(true_params, times)\n    signal = detector_response(h_plus_true, h_cross_true, (0.5, 0.5, 0.0))\n    \n    # Add noise\n    noise_psd = jnp.ones(n_samples // 2 + 1)  # White noise (simplified)\n    noise = 1e-21 * random.normal(key, (n_samples,))\n    data = signal + noise\n    \n    print(f\"  Generated GW signal with SNR ≈ {jnp.max(jnp.abs(signal)) / jnp.std(noise):.1f}\")\n    \n    # 5. Parameter estimation with Numpyro\n    def gw_model(data, times):\n        \"\"\"Numpyro model for GW parameter estimation.\"\"\"\n        \n        # Priors\n        m1 = numpyro.sample('m1', dist.Uniform(5, 50))\n        m2 = numpyro.sample('m2', dist.Uniform(5, 50))\n        d_l = numpyro.sample('d_l', dist.Uniform(10, 500))\n        t_c = numpyro.sample('t_c', dist.Uniform(0.4, 0.6))\n        phi_c = numpyro.sample('phi_c', dist.Uniform(0, 2 * jnp.pi))\n        \n        params = jnp.array([m1, m2, d_l, t_c, phi_c])\n        \n        # Likelihood (simplified)\n        h_plus, h_cross = generate_gw_signal(params, times)\n        h_model = detector_response(h_plus, h_cross, (0.5, 0.5, 0.0))\n        \n        # Gaussian likelihood\n        sigma = 1e-21\n        numpyro.sample('data', dist.Normal(h_model, sigma), obs=data)\n    \n    # Run MCMC\n    print(\"\\n  Running parameter estimation...\")\n    \n    nuts_kernel = NUTS(gw_model)\n    mcmc = MCMC(nuts_kernel, num_warmup=500, num_samples=1000)\n    \n    key, run_key = random.split(key)\n    mcmc.run(run_key, data, times)\n    \n    samples = mcmc.get_samples()\n    \n    print(f\"\\n  Parameter estimates:\")\n    for param in ['m1', 'm2', 'd_l', 't_c']:\n        mean = jnp.mean(samples[param])\n        std = jnp.std(samples[param])\n        true = true_params[['m1', 'm2', 'd_l', 't_c'].index(param)]\n        print(f\"    {param}: {mean:.2f} ± {std:.2f} (true: {true:.2f})\")\n    \n    # 6. Visualization\n    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n    \n    # Signal\n    axes[0, 0].plot(times, data, 'gray', alpha=0.5, label='Data')\n    axes[0, 0].plot(times, signal, 'b', label='True signal')\n    axes[0, 0].set_xlabel('Time (s)')\n    axes[0, 0].set_ylabel('Strain')\n    axes[0, 0].set_title('Gravitational Wave Signal')\n    axes[0, 0].legend()\n    axes[0, 0].set_xlim(0.45, 0.55)\n    \n    # Mass posterior\n    axes[0, 1].scatter(samples['m1'], samples['m2'], alpha=0.1, s=1)\n    axes[0, 1].scatter(true_params[0], true_params[1], c='r', s=100, marker='*')\n    axes[0, 1].set_xlabel('m₁ (M☉)')\n    axes[0, 1].set_ylabel('m₂ (M☉)')\n    axes[0, 1].set_title('Mass Posterior')\n    \n    # Distance posterior\n    axes[1, 0].hist(samples['d_l'], bins=30, alpha=0.7)\n    axes[1, 0].axvline(true_params[2], c='r', ls='--', label='True')\n    axes[1, 0].set_xlabel('Luminosity Distance (Mpc)')\n    axes[1, 0].set_ylabel('Samples')\n    axes[1, 0].set_title('Distance Posterior')\n    axes[1, 0].legend()\n    \n    # Coalescence time\n    axes[1, 1].hist(samples['t_c'], bins=30, alpha=0.7)\n    axes[1, 1].axvline(true_params[3], c='r', ls='--', label='True')\n    axes[1, 1].set_xlabel('Coalescence Time (s)')\n    axes[1, 1].set_ylabel('Samples')\n    axes[1, 1].set_title('Merger Time Posterior')\n    axes[1, 1].legend()\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return samples\n\n# Run complete pipeline\ngw_samples = gravitational_wave_pipeline()","type":"content","url":"/jax-specialized-libraries#end-to-end-gw-pipeline","position":27},{"hierarchy":{"lvl1":"JAX Specialized Libraries: BlackJAX, Numpyro, and Domain-Specific Tools","lvl2":"Key Takeaways"},"type":"lvl2","url":"/jax-specialized-libraries#key-takeaways","position":28},{"hierarchy":{"lvl1":"JAX Specialized Libraries: BlackJAX, Numpyro, and Domain-Specific Tools","lvl2":"Key Takeaways"},"content":"✅ BlackJAX - State-of-the-art MCMC samplers with HMC, NUTS, and more✅ Numpyro - Probabilistic programming with automatic inference✅ JAXopt - Constrained optimization and proximal methods✅ Domain-specific - Growing ecosystem of astronomical JAX tools✅ Integration - Libraries work together for complete pipelines✅ Performance - Orders of magnitude faster than traditional tools✅ Differentiable - Gradients everywhere enable new methods","type":"content","url":"/jax-specialized-libraries#key-takeaways","position":29},{"hierarchy":{"lvl1":"JAX Specialized Libraries: BlackJAX, Numpyro, and Domain-Specific Tools","lvl2":"Next Steps"},"type":"lvl2","url":"/jax-specialized-libraries#next-steps","position":30},{"hierarchy":{"lvl1":"JAX Specialized Libraries: BlackJAX, Numpyro, and Domain-Specific Tools","lvl2":"Next Steps"},"content":"With these specialized libraries, you can:\n\nReplace traditional MCMC codes with faster JAX versions\n\nBuild differentiable simulators for likelihood-free inference\n\nImplement neural posterior estimation\n\nCreate custom domain-specific tools\n\nScale to massive datasets with GPU acceleration\n\nThe JAX ecosystem continues to grow rapidly, with new libraries appearing regularly for specialized scientific applications!","type":"content","url":"/jax-specialized-libraries#next-steps","position":31},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization"},"type":"lvl1","url":"/jax-advanced-chapter","position":0},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization"},"content":"","type":"content","url":"/jax-advanced-chapter","position":1},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization","lvl2":"Learning Objectives"},"type":"lvl2","url":"/jax-advanced-chapter#learning-objectives","position":2},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization","lvl2":"Learning Objectives"},"content":"By the end of this chapter, you will:\n\nMaster JAX control flow primitives (cond, scan, while_loop)\n\nImplement custom derivatives and VJP rules\n\nOptimize memory and performance for large-scale problems\n\nUse sharding for multi-GPU computations\n\nDebug and profile JAX programs effectively\n\nBuild production-ready astronomical simulations","type":"content","url":"/jax-advanced-chapter#learning-objectives","position":3},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization","lvl2":"Control Flow in JAX"},"type":"lvl2","url":"/jax-advanced-chapter#control-flow-in-jax","position":4},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization","lvl2":"Control Flow in JAX"},"content":"","type":"content","url":"/jax-advanced-chapter#control-flow-in-jax","position":5},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization","lvl3":"Conditional Execution with lax.cond","lvl2":"Control Flow in JAX"},"type":"lvl3","url":"/jax-advanced-chapter#conditional-execution-with-lax-cond","position":6},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization","lvl3":"Conditional Execution with lax.cond","lvl2":"Control Flow in JAX"},"content":"import jax\nimport jax.numpy as jnp\nfrom jax import grad, jit, vmap, pmap\nfrom jax.lax import cond, scan, while_loop, fori_loop, switch\nfrom jax import random, custom_vjp, custom_jvp\nimport time\nimport matplotlib.pyplot as plt\n\ndef jax_control_flow():\n    \"\"\"Master JAX's functional control flow primitives.\"\"\"\n    \n    print(\"JAX CONTROL FLOW PRIMITIVES\")\n    print(\"=\" * 50)\n    \n    # 1. Conditional execution with cond\n    print(\"\\n1. CONDITIONAL EXECUTION:\")\n    \n    @jit\n    def stellar_evolution_step(age, mass):\n        \"\"\"\n        Evolve star based on evolutionary phase.\n        Different physics for different phases.\n        \"\"\"\n        \n        def main_sequence(args):\n            age, mass = args\n            # Main sequence evolution\n            luminosity = mass ** 3.5\n            radius = mass ** 0.8\n            return luminosity, radius\n        \n        def giant_branch(args):\n            age, mass = args\n            # Red giant evolution\n            luminosity = mass ** 2.5 * (age / 10.0)\n            radius = mass ** 0.5 * (age / 10.0) ** 0.3\n            return luminosity, radius\n        \n        def white_dwarf(args):\n            age, mass = args\n            # White dwarf cooling\n            luminosity = 0.001 * jnp.exp(-(age - 12.0) / 2.0)\n            radius = 0.01\n            return luminosity, radius\n        \n        # Multi-way conditional\n        ms_turnoff = 10.0 / mass ** 2.5  # Simplified\n        \n        # Nested conditions\n        return cond(\n            age < ms_turnoff,\n            main_sequence,\n            lambda args: cond(\n                age < ms_turnoff * 1.2,\n                giant_branch,\n                white_dwarf,\n                args\n            ),\n            (age, mass)\n        )\n    \n    # Test different evolutionary phases\n    ages = jnp.array([0.1, 5.0, 11.0, 15.0])\n    mass = 1.0\n    \n    for age in ages:\n        L, R = stellar_evolution_step(age, mass)\n        print(f\"  Age {age:.1f} Gyr: L={L:.3f} L☉, R={R:.3f} R☉\")\n    \n    # 2. Switch for multiple branches\n    print(\"\\n2. SWITCH STATEMENT:\")\n    \n    @jit\n    def process_observation(obs_type, data):\n        \"\"\"Process different observation types.\"\"\"\n        \n        def process_photometry(data):\n            # Magnitude calculation\n            return -2.5 * jnp.log10(data) + 25.0\n        \n        def process_spectroscopy(data):\n            # Continuum normalization\n            continuum = jnp.median(data)\n            return data / continuum\n        \n        def process_imaging(data):\n            # Background subtraction\n            background = jnp.percentile(data, 10)\n            return data - background\n        \n        branches = [process_photometry, process_spectroscopy, process_imaging]\n        \n        return switch(obs_type, branches, data)\n    \n    # Different observation types\n    data = jnp.array([100.0, 150.0, 200.0])\n    \n    for obs_type in range(3):\n        result = process_observation(obs_type, data)\n        print(f\"  Type {obs_type}: {result[0]:.3f}\")\n    \n    # 3. Gradient through conditionals\n    print(\"\\n3. GRADIENTS THROUGH CONDITIONALS:\")\n    \n    @jit\n    def piecewise_potential(r):\n        \"\"\"Piecewise gravitational potential.\"\"\"\n        \n        def inner_region(r):\n            # Constant density sphere\n            return -1.5 + 0.5 * r**2\n        \n        def outer_region(r):\n            # Point mass\n            return -1.0 / r\n        \n        return cond(r < 1.0, inner_region, outer_region, r)\n    \n    # Gradient (force) is continuous at boundary!\n    grad_potential = grad(piecewise_potential)\n    \n    radii = jnp.array([0.5, 0.99, 1.0, 1.01, 2.0])\n    for r in radii:\n        force = -grad_potential(r)\n        print(f\"  r={r:.2f}: F={force:.4f}\")\n\njax_control_flow()","type":"content","url":"/jax-advanced-chapter#conditional-execution-with-lax-cond","position":7},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization","lvl3":"Loops with scan and fori_loop","lvl2":"Control Flow in JAX"},"type":"lvl3","url":"/jax-advanced-chapter#loops-with-scan-and-fori-loop","position":8},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization","lvl3":"Loops with scan and fori_loop","lvl2":"Control Flow in JAX"},"content":"def jax_loops():\n    \"\"\"Efficient loops in JAX using scan and fori_loop.\"\"\"\n    \n    print(\"\\nLOOPS IN JAX\")\n    print(\"=\" * 50)\n    \n    # 1. scan for carrying state\n    print(\"\\n1. SCAN FOR SEQUENTIAL COMPUTATIONS:\")\n    \n    @jit\n    def runge_kutta_4(dynamics, initial_state, t_span, dt):\n        \"\"\"\n        RK4 integration using scan.\n        \n        Parameters\n        ----------\n        dynamics : callable\n            dy/dt = dynamics(y, t)\n        initial_state : array\n            Initial conditions\n        t_span : tuple\n            (t_start, t_end)\n        dt : float\n            Time step\n        \"\"\"\n        \n        def rk4_step(carry, t):\n            y = carry\n            \n            k1 = dynamics(y, t)\n            k2 = dynamics(y + 0.5 * dt * k1, t + 0.5 * dt)\n            k3 = dynamics(y + 0.5 * dt * k2, t + 0.5 * dt)\n            k4 = dynamics(y + dt * k3, t + dt)\n            \n            y_new = y + (dt / 6.0) * (k1 + 2*k2 + 2*k3 + k4)\n            \n            return y_new, y_new  # carry and output\n        \n        t_start, t_end = t_span\n        n_steps = int((t_end - t_start) / dt)\n        times = jnp.linspace(t_start, t_end, n_steps)\n        \n        _, trajectory = scan(rk4_step, initial_state, times)\n        \n        return times, trajectory\n    \n    # Kepler problem\n    def kepler_dynamics(state, t):\n        \"\"\"Kepler two-body dynamics.\"\"\"\n        r, v = state[:2], state[2:]\n        r_norm = jnp.linalg.norm(r)\n        a = -r / r_norm**3\n        return jnp.concatenate([v, a])\n    \n    initial = jnp.array([1.0, 0.0, 0.0, 1.0])\n    times, trajectory = runge_kutta_4(kepler_dynamics, initial, (0, 10), 0.01)\n    \n    print(f\"  Integrated {len(times)} steps\")\n    print(f\"  Final position: {trajectory[-1, :2]}\")\n    \n    # Check energy conservation\n    def energy(state):\n        r, v = state[:2], state[2:]\n        return 0.5 * jnp.sum(v**2) - 1.0 / jnp.linalg.norm(r)\n    \n    initial_energy = energy(trajectory[0])\n    final_energy = energy(trajectory[-1])\n    print(f\"  Energy drift: {abs(final_energy - initial_energy):.2e}\")\n    \n    # 2. fori_loop for fixed iterations\n    print(\"\\n2. FORI_LOOP FOR FIXED ITERATIONS:\")\n    \n    @jit\n    def jacobi_iteration(A, b, x0, n_iterations):\n        \"\"\"Solve Ax = b using Jacobi iteration.\"\"\"\n        \n        D = jnp.diag(jnp.diag(A))  # Diagonal part\n        R = A - D  # Off-diagonal part\n        \n        def iteration(i, x):\n            return jnp.linalg.solve(D, b - R @ x)\n        \n        return fori_loop(0, n_iterations, iteration, x0)\n    \n    # Test system\n    A = jnp.array([[4.0, -1.0, 0.0],\n                   [-1.0, 4.0, -1.0],\n                   [0.0, -1.0, 3.0]])\n    b = jnp.array([15.0, 10.0, 10.0])\n    x0 = jnp.zeros(3)\n    \n    solution = jacobi_iteration(A, b, x0, 50)\n    print(f\"  Solution: {solution}\")\n    print(f\"  Residual: {jnp.linalg.norm(A @ solution - b):.2e}\")\n    \n    # 3. while_loop for adaptive algorithms\n    print(\"\\n3. WHILE_LOOP FOR ADAPTIVE ALGORITHMS:\")\n    \n    @jit\n    def adaptive_integration(f, x0, x1, tol=1e-6, max_depth=10):\n        \"\"\"Adaptive Simpson's integration.\"\"\"\n        \n        def simpson(a, b):\n            \"\"\"Simpson's rule on [a, b].\"\"\"\n            mid = (a + b) / 2\n            return (b - a) / 6 * (f(a) + 4*f(mid) + f(b))\n        \n        def should_refine(carry):\n            a, b, depth, _ = carry\n            return (depth < max_depth)\n        \n        def refine_step(carry):\n            a, b, depth, integral = carry\n            mid = (a + b) / 2\n            \n            whole = simpson(a, b)\n            left = simpson(a, mid)\n            right = simpson(mid, b)\n            \n            error = abs(whole - (left + right))\n            \n            # Simplified: just refine if error is large\n            # In practice, would accumulate integral properly\n            return (a, b, depth + 1, left + right)\n        \n        initial_carry = (x0, x1, 0, simpson(x0, x1))\n        final_carry = while_loop(should_refine, refine_step, initial_carry)\n        \n        return final_carry[3]\n    \n    # Test function\n    def test_func(x):\n        return jnp.sin(x) ** 2\n    \n    result = adaptive_integration(test_func, 0.0, jnp.pi)\n    print(f\"  ∫sin²(x)dx from 0 to π = {result:.6f}\")\n    print(f\"  Expected: {jnp.pi/2:.6f}\")\n    \n    # 4. Nested loops\n    print(\"\\n4. NESTED LOOPS WITH SCAN:\")\n    \n    @jit\n    def double_pendulum_poincare(initial_conditions, n_periods, points_per_period):\n        \"\"\"\n        Compute Poincaré section of double pendulum.\n        Nested loop: outer for periods, inner for integration.\n        \"\"\"\n        \n        def dynamics(state, t):\n            # Simplified double pendulum dynamics\n            theta1, theta2, p1, p2 = state\n            \n            # Just for demonstration (not accurate physics)\n            dtheta1 = p1\n            dtheta2 = p2\n            dp1 = -jnp.sin(theta1) - 0.1 * jnp.sin(theta1 - theta2)\n            dp2 = -jnp.sin(theta2) + 0.1 * jnp.sin(theta1 - theta2)\n            \n            return jnp.array([dtheta1, dtheta2, dp1, dp2])\n        \n        def integrate_period(carry, _):\n            state = carry\n            \n            # Inner loop: integrate for one period\n            def step(s, _):\n                new_s = s + 0.01 * dynamics(s, 0)\n                return new_s, new_s\n            \n            final_state, trajectory = scan(\n                step, state, None, length=points_per_period\n            )\n            \n            # Return final state and Poincaré point\n            return final_state, final_state[:2]  # Only positions\n        \n        # Outer loop: collect Poincaré points\n        _, poincare_points = scan(\n            integrate_period, initial_conditions, None, length=n_periods\n        )\n        \n        return poincare_points\n    \n    initial = jnp.array([0.1, 0.1, 0.0, 0.0])\n    poincare = double_pendulum_poincare(initial, 100, 100)\n    \n    print(f\"  Generated {len(poincare)} Poincaré points\")\n    print(f\"  Phase space bounds: θ₁∈[{poincare[:, 0].min():.2f}, {poincare[:, 0].max():.2f}]\")\n\njax_loops()","type":"content","url":"/jax-advanced-chapter#loops-with-scan-and-fori-loop","position":9},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization","lvl2":"Custom Derivatives"},"type":"lvl2","url":"/jax-advanced-chapter#custom-derivatives","position":10},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization","lvl2":"Custom Derivatives"},"content":"","type":"content","url":"/jax-advanced-chapter#custom-derivatives","position":11},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization","lvl3":"Defining Custom VJP Rules","lvl2":"Custom Derivatives"},"type":"lvl3","url":"/jax-advanced-chapter#defining-custom-vjp-rules","position":12},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization","lvl3":"Defining Custom VJP Rules","lvl2":"Custom Derivatives"},"content":"def custom_derivatives():\n    \"\"\"Define custom derivatives for specialized functions.\"\"\"\n    \n    print(\"\\nCUSTOM DERIVATIVES IN JAX\")\n    print(\"=\" * 50)\n    \n    # 1. Custom VJP for numerical stability\n    print(\"\\n1. STABLE SOFTPLUS WITH CUSTOM VJP:\")\n    \n    @custom_vjp\n    def stable_softplus(x):\n        \"\"\"Softplus with numerical stability.\"\"\"\n        return jnp.log1p(jnp.exp(-jnp.abs(x))) + jnp.maximum(x, 0)\n    \n    def stable_softplus_fwd(x):\n        \"\"\"Forward pass: compute value and save residuals.\"\"\"\n        y = stable_softplus(x)\n        return y, (x,)  # Save x for backward pass\n    \n    def stable_softplus_bwd(res, g):\n        \"\"\"Backward pass: compute VJP.\"\"\"\n        x, = res\n        # Derivative of softplus is sigmoid\n        sigmoid_x = 1 / (1 + jnp.exp(-x))\n        return (g * sigmoid_x,)\n    \n    stable_softplus.defvjp(stable_softplus_fwd, stable_softplus_bwd)\n    \n    # Test gradient\n    x_test = jnp.array([-100.0, -1.0, 0.0, 1.0, 100.0])\n    grad_fn = grad(lambda x: jnp.sum(stable_softplus(x)))\n    grads = grad_fn(x_test)\n    \n    print(f\"  x: {x_test}\")\n    print(f\"  softplus(x): {stable_softplus(x_test)}\")\n    print(f\"  gradients: {grads}\")\n    \n    # 2. Custom derivative for interpolation\n    print(\"\\n2. DIFFERENTIABLE INTERPOLATION:\")\n    \n    @custom_vjp\n    def interp1d(x, xp, fp):\n        \"\"\"Linear interpolation with custom gradient.\"\"\"\n        return jnp.interp(x, xp, fp)\n    \n    def interp1d_fwd(x, xp, fp):\n        y = jnp.interp(x, xp, fp)\n        return y, (x, xp, fp)\n    \n    def interp1d_bwd(res, g):\n        x, xp, fp = res\n        \n        # Find surrounding points\n        idx = jnp.searchsorted(xp, x)\n        idx = jnp.clip(idx, 1, len(xp) - 1)\n        \n        # Linear interpolation gradient\n        x0, x1 = xp[idx - 1], xp[idx]\n        f0, f1 = fp[idx - 1], fp[idx]\n        \n        # Gradient w.r.t. x\n        dfdx = (f1 - f0) / (x1 - x0)\n        \n        # Gradients w.r.t. xp and fp (simplified)\n        # In practice, these would be more complex\n        dxp = jnp.zeros_like(xp)\n        dfp = jnp.zeros_like(fp)\n        \n        # Weight for linear interpolation\n        alpha = (x - x0) / (x1 - x0)\n        dfp = dfp.at[idx - 1].add(g * (1 - alpha))\n        dfp = dfp.at[idx].add(g * alpha)\n        \n        return g * dfdx, dxp, dfp\n    \n    interp1d.defvjp(interp1d_fwd, interp1d_bwd)\n    \n    # Test interpolation gradient\n    xp = jnp.array([0.0, 1.0, 2.0, 3.0])\n    fp = jnp.array([0.0, 1.0, 0.5, 2.0])\n    \n    def loss(x):\n        return interp1d(x, xp, fp) ** 2\n    \n    x_test = 1.5\n    value = interp1d(x_test, xp, fp)\n    gradient = grad(loss)(x_test)\n    \n    print(f\"  Interpolated value at x={x_test}: {value:.3f}\")\n    print(f\"  Gradient: {gradient:.3f}\")\n    \n    # 3. Custom JVP for forward-mode AD\n    print(\"\\n3. CUSTOM JVP FOR SPECIAL FUNCTIONS:\")\n    \n    @custom_jvp\n    def safe_log(x):\n        \"\"\"Logarithm with safe gradient at x=0.\"\"\"\n        return jnp.log(jnp.maximum(x, 1e-10))\n    \n    @safe_log.defjvp\n    def safe_log_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        \n        # Primal computation\n        y = safe_log(x)\n        \n        # Tangent computation (forward-mode derivative)\n        # Use safe derivative\n        y_dot = x_dot / jnp.maximum(x, 1e-10)\n        \n        return y, y_dot\n    \n    # Test near zero\n    x_vals = jnp.array([1e-15, 1e-10, 0.1, 1.0])\n    grad_safe_log = grad(lambda x: jnp.sum(safe_log(x)))\n    grads = grad_safe_log(x_vals)\n    \n    print(f\"  x: {x_vals}\")\n    print(f\"  safe_log(x): {safe_log(x_vals)}\")\n    print(f\"  gradients: {grads}\")\n    \n    # 4. Physics-informed custom derivatives\n    print(\"\\n4. PHYSICS-INFORMED DERIVATIVES:\")\n    \n    @custom_vjp\n    def gravitational_lensing_deflection(source_pos, lens_mass, lens_pos):\n        \"\"\"\n        Gravitational lensing deflection angle.\n        Custom derivative ensures correct physics.\n        \"\"\"\n        # Vector from lens to source\n        r = source_pos - lens_pos\n        r_norm = jnp.linalg.norm(r)\n        \n        # Einstein radius effect (simplified)\n        deflection = lens_mass * r / (r_norm**2 + 0.01)  # Softening\n        \n        return deflection\n    \n    def lensing_fwd(source_pos, lens_mass, lens_pos):\n        deflection = gravitational_lensing_deflection(source_pos, lens_mass, lens_pos)\n        return deflection, (source_pos, lens_mass, lens_pos)\n    \n    def lensing_bwd(res, g):\n        source_pos, lens_mass, lens_pos = res\n        r = source_pos - lens_pos\n        r_norm = jnp.linalg.norm(r)\n        \n        # Gradients from physics\n        # ∂α/∂source_pos\n        d_source = g * lens_mass * (\n            jnp.eye(2) / (r_norm**2 + 0.01) - \n            2 * jnp.outer(r, r) / (r_norm**2 + 0.01)**2\n        )\n        \n        # ∂α/∂lens_mass\n        d_mass = jnp.dot(g, r / (r_norm**2 + 0.01))\n        \n        # ∂α/∂lens_pos\n        d_lens = -d_source  # Newton's third law!\n        \n        return d_source, d_mass, d_lens\n    \n    gravitational_lensing_deflection.defvjp(lensing_fwd, lensing_bwd)\n    \n    # Test lensing gradients\n    source = jnp.array([1.0, 0.5])\n    mass = 1.0\n    lens = jnp.array([0.0, 0.0])\n    \n    def lensing_potential(s):\n        deflection = gravitational_lensing_deflection(s, mass, lens)\n        return jnp.sum(deflection**2)\n    \n    grad_lensing = grad(lensing_potential)\n    gradient = grad_lensing(source)\n    \n    print(f\"  Source position: {source}\")\n    print(f\"  Deflection gradient: {gradient}\")\n\ncustom_derivatives()","type":"content","url":"/jax-advanced-chapter#defining-custom-vjp-rules","position":13},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization","lvl2":"Memory and Performance Optimization"},"type":"lvl2","url":"/jax-advanced-chapter#memory-and-performance-optimization","position":14},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization","lvl2":"Memory and Performance Optimization"},"content":"","type":"content","url":"/jax-advanced-chapter#memory-and-performance-optimization","position":15},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization","lvl3":"Checkpointing and Memory Management","lvl2":"Memory and Performance Optimization"},"type":"lvl3","url":"/jax-advanced-chapter#checkpointing-and-memory-management","position":16},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization","lvl3":"Checkpointing and Memory Management","lvl2":"Memory and Performance Optimization"},"content":"def memory_optimization():\n    \"\"\"Optimize memory usage in JAX computations.\"\"\"\n    \n    print(\"\\nMEMORY OPTIMIZATION IN JAX\")\n    print(\"=\" * 50)\n    \n    # 1. Gradient checkpointing\n    print(\"\\n1. GRADIENT CHECKPOINTING:\")\n    \n    from jax.experimental import checkpoint\n    \n    def deep_network(x, n_layers=50):\n        \"\"\"Deep network that would use lots of memory.\"\"\"\n        \n        @checkpoint  # Don't store","type":"content","url":"/jax-advanced-chapter#checkpointing-and-memory-management","position":17},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization (Continued)"},"type":"lvl1","url":"/jax-advanced-continued","position":0},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization (Continued)"},"content":"","type":"content","url":"/jax-advanced-continued","position":1},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization (Continued)","lvl2":"Memory and Performance Optimization (Continued)"},"type":"lvl2","url":"/jax-advanced-continued#memory-and-performance-optimization-continued","position":2},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization (Continued)","lvl2":"Memory and Performance Optimization (Continued)"},"content":"","type":"content","url":"/jax-advanced-continued#memory-and-performance-optimization-continued","position":3},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization (Continued)","lvl3":"Checkpointing and Memory Management","lvl2":"Memory and Performance Optimization (Continued)"},"type":"lvl3","url":"/jax-advanced-continued#checkpointing-and-memory-management","position":4},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization (Continued)","lvl3":"Checkpointing and Memory Management","lvl2":"Memory and Performance Optimization (Continued)"},"content":"def memory_optimization():\n    \"\"\"Optimize memory usage in JAX computations.\"\"\"\n    \n    print(\"\\nMEMORY OPTIMIZATION IN JAX\")\n    print(\"=\" * 50)\n    \n    # 1. Gradient checkpointing\n    print(\"\\n1. GRADIENT CHECKPOINTING:\")\n    \n    from jax.experimental import checkpoint\n    \n    def deep_network(x, n_layers=50):\n        \"\"\"Deep network that would use lots of memory.\"\"\"\n        \n        @checkpoint  # Don't store intermediate activations\n        def layer(x):\n            # Expensive computation\n            x = jnp.tanh(x @ jnp.ones((100, 100)))\n            x = jnp.sin(x) + jnp.cos(x)\n            return x\n        \n        for _ in range(n_layers):\n            x = layer(x)\n        return jnp.sum(x)\n    \n    # Without checkpointing: stores all intermediate values\n    def deep_network_no_checkpoint(x, n_layers=50):\n        for _ in range(n_layers):\n            x = jnp.tanh(x @ jnp.ones((100, 100)))\n            x = jnp.sin(x) + jnp.cos(x)\n        return jnp.sum(x)\n    \n    x = random.normal(random.PRNGKey(0), (100,))\n    \n    # Compare memory usage (conceptual - actual measurement requires profiling)\n    print(\"  With checkpointing: recomputes forward pass during backprop\")\n    print(\"  Without checkpointing: stores all intermediate activations\")\n    \n    # Gradient computation\n    grad_checkpoint = grad(deep_network)\n    grad_no_checkpoint = grad(deep_network_no_checkpoint)\n    \n    g1 = grad_checkpoint(x)\n    g2 = grad_no_checkpoint(x)\n    \n    print(f\"  Gradients match: {jnp.allclose(g1, g2)}\")\n    \n    # 2. Donation for in-place updates\n    print(\"\\n2. BUFFER DONATION:\")\n    \n    @jit\n    def evolve_without_donation(state, dt):\n        \"\"\"Standard evolution - creates new arrays.\"\"\"\n        positions, velocities = state\n        new_velocities = velocities - positions * dt\n        new_positions = positions + new_velocities * dt\n        return (new_positions, new_velocities)\n    \n    @jit\n    def evolve_with_donation(state, dt):\n        \"\"\"Evolution with buffer donation - reuses memory.\"\"\"\n        positions, velocities = state\n        # JAX can reuse input buffers when safe\n        velocities = velocities - positions * dt\n        positions = positions + velocities * dt\n        return (positions, velocities)\n    \n    # Using donate_argnums for explicit donation\n    @partial(jit, donate_argnums=(0,))\n    def evolve_explicit_donation(state, dt):\n        \"\"\"Explicitly donate input buffer.\"\"\"\n        positions, velocities = state\n        velocities = velocities - positions * dt\n        positions = positions + velocities * dt\n        return (positions, velocities)\n    \n    state = (random.normal(random.PRNGKey(0), (1000, 3)),\n             random.normal(random.PRNGKey(1), (1000, 3)))\n    \n    # All produce same result, but memory usage differs\n    result1 = evolve_without_donation(state, 0.01)\n    result2 = evolve_with_donation(state, 0.01)\n    # Note: after donation, original 'state' should not be used!\n    result3 = evolve_explicit_donation(state, 0.01)\n    \n    print(f\"  Results match: {jnp.allclose(result1[0], result2[0])}\")\n    \n    # 3. Chunking large computations\n    print(\"\\n3. CHUNKING WITH SCAN:\")\n    \n    @jit\n    def chunked_matrix_multiply(A, B, chunk_size=100):\n        \"\"\"\n        Compute A @ B in chunks to control memory.\n        Useful when A or B is very large.\n        \"\"\"\n        n, k = A.shape\n        k2, m = B.shape\n        assert k == k2\n        \n        # Process in chunks along the k dimension\n        def process_chunk(carry, chunk_idx):\n            result = carry\n            start = chunk_idx * chunk_size\n            end = jnp.minimum(start + chunk_size, k)\n            \n            A_chunk = A[:, start:end]\n            B_chunk = B[start:end, :]\n            \n            result = result + A_chunk @ B_chunk\n            return result, None\n        \n        n_chunks = (k + chunk_size - 1) // chunk_size\n        initial = jnp.zeros((n, m))\n        \n        result, _ = scan(process_chunk, initial, jnp.arange(n_chunks))\n        return result\n    \n    # Test chunked multiplication\n    A = random.normal(random.PRNGKey(2), (100, 1000))\n    B = random.normal(random.PRNGKey(3), (1000, 50))\n    \n    result_direct = A @ B\n    result_chunked = chunked_matrix_multiply(A, B)\n    \n    print(f\"  Chunked multiplication error: {jnp.linalg.norm(result_direct - result_chunked):.2e}\")\n    \n    # 4. Selective computation with stop_gradient\n    print(\"\\n4. STOP_GRADIENT FOR PARTIAL DERIVATIVES:\")\n    \n    def loss_with_regularization(params, data, lambda_reg=0.1):\n        \"\"\"Loss function with optional gradient stopping.\"\"\"\n        weights, biases = params\n        x, y = data\n        \n        # Forward pass\n        predictions = x @ weights + biases\n        \n        # Main loss\n        main_loss = jnp.mean((predictions - y) ** 2)\n        \n        # Regularization (can stop gradient if needed)\n        reg_loss = lambda_reg * (jnp.sum(weights**2) + jnp.sum(biases**2))\n        \n        # Option to not backprop through regularization\n        # reg_loss = jax.lax.stop_gradient(reg_loss)\n        \n        return main_loss + reg_loss\n    \n    # Example with and without gradient stopping\n    weights = random.normal(random.PRNGKey(4), (10, 5))\n    biases = random.normal(random.PRNGKey(5), (5,))\n    params = (weights, biases)\n    \n    x = random.normal(random.PRNGKey(6), (100, 10))\n    y = random.normal(random.PRNGKey(7), (100, 5))\n    data = (x, y)\n    \n    grad_fn = grad(loss_with_regularization)\n    grads = grad_fn(params, data)\n    \n    print(f\"  Weight gradient norm: {jnp.linalg.norm(grads[0]):.3f}\")\n    print(f\"  Bias gradient norm: {jnp.linalg.norm(grads[1]):.3f}\")\n\nmemory_optimization()","type":"content","url":"/jax-advanced-continued#checkpointing-and-memory-management","position":5},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization (Continued)","lvl2":"Multi-GPU and Distributed Computing"},"type":"lvl2","url":"/jax-advanced-continued#multi-gpu-and-distributed-computing","position":6},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization (Continued)","lvl2":"Multi-GPU and Distributed Computing"},"content":"","type":"content","url":"/jax-advanced-continued#multi-gpu-and-distributed-computing","position":7},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization (Continued)","lvl3":"Data and Model Parallelism with pmap","lvl2":"Multi-GPU and Distributed Computing"},"type":"lvl3","url":"/jax-advanced-continued#data-and-model-parallelism-with-pmap","position":8},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization (Continued)","lvl3":"Data and Model Parallelism with pmap","lvl2":"Multi-GPU and Distributed Computing"},"content":"def distributed_computing():\n    \"\"\"Distributed and parallel computing patterns in JAX.\"\"\"\n    \n    print(\"\\nDISTRIBUTED COMPUTING WITH JAX\")\n    print(\"=\" * 50)\n    \n    # Note: These examples are conceptual - actual multi-GPU requires hardware\n    \n    # 1. Data parallelism with pmap\n    print(\"\\n1. DATA PARALLELISM WITH PMAP:\")\n    \n    # Get device count (will be 1 on single GPU/CPU)\n    n_devices = jax.device_count()\n    print(f\"  Available devices: {n_devices}\")\n    \n    @jit\n    def single_device_nbody_step(positions, velocities, masses, dt):\n        \"\"\"N-body step on single device.\"\"\"\n        n = len(masses)\n        forces = jnp.zeros_like(positions)\n        \n        for i in range(n):\n            for j in range(n):\n                if i != j:\n                    r_ij = positions[j] - positions[i]\n                    r_norm = jnp.linalg.norm(r_ij)\n                    forces = forces.at[i].add(\n                        masses[j] * r_ij / (r_norm**3 + 0.01)\n                    )\n        \n        accelerations = forces\n        velocities_new = velocities + accelerations * dt\n        positions_new = positions + velocities_new * dt\n        \n        return positions_new, velocities_new\n    \n    # Parallel version (conceptual - needs multiple devices)\n    def parallel_nbody_step(positions, velocities, masses, dt):\n        \"\"\"\n        N-body with particles distributed across devices.\n        Each device computes forces for its particles.\n        \"\"\"\n        # This would use pmap in practice\n        # positions shape: [n_devices, particles_per_device, 3]\n        \n        # All-gather pattern to share positions\n        all_positions = jax.lax.all_gather(positions, axis_name='devices')\n        \n        # Each device computes forces for its particles\n        # ... (computation here)\n        \n        # Return updated local positions and velocities\n        pass\n    \n    # 2. Sharding specifications\n    print(\"\\n2. SHARDING FOR LARGE ARRAYS:\")\n    \n    from jax.sharding import PartitionSpec as P\n    from jax.experimental import mesh_utils\n    from jax.sharding import Mesh, NamedSharding\n    \n    # Create a mesh (conceptual - requires actual devices)\n    # devices = mesh_utils.create_device_mesh((2, 2))\n    # mesh = Mesh(devices, axis_names=('x', 'y'))\n    \n    def create_sharding_example():\n        \"\"\"Example of array sharding across devices.\"\"\"\n        \n        # Define how to shard a large array\n        # P('x', 'y') means shard first dim along 'x', second along 'y'\n        # P('x', None) means shard first dim, replicate second\n        # P(None, 'y') means replicate first, shard second\n        \n        spec_2d = P('x', 'y')  # Shard both dimensions\n        spec_rows = P('x', None)  # Shard rows only\n        spec_cols = P(None, 'y')  # Shard columns only\n        spec_replicated = P(None, None)  # Replicate everywhere\n        \n        return spec_2d, spec_rows, spec_cols, spec_replicated\n    \n    specs = create_sharding_example()\n    print(\"  Sharding patterns created (requires multi-device setup)\")\n    \n    # 3. Collective operations\n    print(\"\\n3. COLLECTIVE OPERATIONS:\")\n    \n    def collective_operations_example():\n        \"\"\"\n        Examples of collective communication patterns.\n        These are used inside pmap'd functions.\n        \"\"\"\n        \n        # Inside a pmap'd function:\n        # local_sum = jnp.sum(local_data)\n        \n        # All-reduce: sum across all devices\n        # global_sum = jax.lax.psum(local_sum, axis_name='devices')\n        \n        # All-gather: collect from all devices\n        # all_data = jax.lax.all_gather(local_data, axis_name='devices')\n        \n        # Scatter: distribute data\n        # scattered = jax.lax.pscatter(data, axis_name='devices')\n        \n        print(\"  Collective ops: psum, all_gather, pscatter\")\n        print(\"  Used for communication in parallel computations\")\n    \n    collective_operations_example()\n    \n    # 4. Pipeline parallelism pattern\n    print(\"\\n4. PIPELINE PARALLELISM PATTERN:\")\n    \n    @jit\n    def pipeline_stage_1(x):\n        \"\"\"First stage of pipeline.\"\"\"\n        return jnp.tanh(x @ jnp.ones((100, 100)))\n    \n    @jit\n    def pipeline_stage_2(x):\n        \"\"\"Second stage of pipeline.\"\"\"\n        return jnp.sin(x) + jnp.cos(x)\n    \n    @jit\n    def pipeline_stage_3(x):\n        \"\"\"Third stage of pipeline.\"\"\"\n        return x @ jnp.ones((100, 50))\n    \n    def pipelined_computation(batches):\n        \"\"\"\n        Process batches through pipeline.\n        In practice, stages would run on different devices.\n        \"\"\"\n        results = []\n        \n        # Simplified pipeline (no overlap)\n        for batch in batches:\n            x = pipeline_stage_1(batch)\n            x = pipeline_stage_2(x)\n            x = pipeline_stage_3(x)\n            results.append(x)\n        \n        return jnp.stack(results)\n    \n    # Test pipeline\n    batches = [random.normal(random.PRNGKey(i), (100,)) for i in range(4)]\n    results = pipelined_computation(batches)\n    print(f\"  Pipeline processed {len(batches)} batches\")\n    print(f\"  Output shape: {results.shape}\")\n\ndistributed_computing()","type":"content","url":"/jax-advanced-continued#data-and-model-parallelism-with-pmap","position":9},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization (Continued)","lvl2":"Profiling and Debugging"},"type":"lvl2","url":"/jax-advanced-continued#profiling-and-debugging","position":10},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization (Continued)","lvl2":"Profiling and Debugging"},"content":"","type":"content","url":"/jax-advanced-continued#profiling-and-debugging","position":11},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization (Continued)","lvl3":"Performance Analysis Tools","lvl2":"Profiling and Debugging"},"type":"lvl3","url":"/jax-advanced-continued#performance-analysis-tools","position":12},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization (Continued)","lvl3":"Performance Analysis Tools","lvl2":"Profiling and Debugging"},"content":"def profiling_and_debugging():\n    \"\"\"Tools and techniques for profiling JAX code.\"\"\"\n    \n    print(\"\\nPROFILING AND DEBUGGING JAX\")\n    print(\"=\" * 50)\n    \n    # 1. Basic timing\n    print(\"\\n1. BASIC PERFORMANCE TIMING:\")\n    \n    def time_function(f, *args, n_runs=100):\n        \"\"\"Time a JIT-compiled function.\"\"\"\n        # Compile\n        f_jit = jit(f)\n        _ = f_jit(*args)  # Trigger compilation\n        \n        # Time\n        start = time.perf_counter()\n        for _ in range(n_runs):\n            _ = f_jit(*args)\n        elapsed = time.perf_counter() - start\n        \n        return elapsed / n_runs\n    \n    def test_function(x):\n        for _ in range(10):\n            x = jnp.sin(x) @ jnp.cos(x.T)\n        return x\n    \n    x = random.normal(random.PRNGKey(0), (100, 100))\n    avg_time = time_function(test_function, x)\n    print(f\"  Average time: {avg_time*1000:.3f} ms\")\n    \n    # 2. Block until result is ready\n    print(\"\\n2. BLOCKING FOR ACCURATE TIMING:\")\n    \n    @jit\n    def async_computation(x):\n        \"\"\"JAX computations are asynchronous.\"\"\"\n        return jnp.sum(x @ x.T)\n    \n    x = random.normal(random.PRNGKey(1), (1000, 1000))\n    \n    # Wrong way (doesn't wait for completion)\n    start = time.perf_counter()\n    result = async_computation(x)  # Returns immediately!\n    wrong_time = time.perf_counter() - start\n    \n    # Right way (blocks until ready)\n    start = time.perf_counter()\n    result = async_computation(x)\n    result.block_until_ready()  # Wait for computation\n    correct_time = time.perf_counter() - start\n    \n    print(f\"  Without blocking: {wrong_time*1000:.3f} ms (incorrect!)\")\n    print(f\"  With blocking: {correct_time*1000:.3f} ms (correct)\")\n    \n    # 3. Compilation inspection\n    print(\"\\n3. INSPECTING COMPILATION:\")\n    \n    from jax import make_jaxpr\n    from jax._src.lib import xla_client\n    \n    def inspect_function(x, y):\n        \"\"\"Function to inspect.\"\"\"\n        z = x @ y\n        return jnp.sum(z * z)\n    \n    x = jnp.ones((3, 3))\n    y = jnp.ones((3, 3))\n    \n    # JAX expression\n    jaxpr = make_jaxpr(inspect_function)(x, y)\n    print(\"\\n  JAX expression (simplified IR):\")\n    print(\"  \" + str(jaxpr).split('\\n')[0][:60] + \"...\")\n    \n    # Lowered to XLA\n    lowered = jit(inspect_function).lower(x, y)\n    print(\"\\n  XLA HLO module available for inspection\")\n    \n    # 4. Debug prints and assertions\n    print(\"\\n4. DEBUGGING TOOLS:\")\n    \n    @jit\n    def debug_example(x):\n        \"\"\"Using debug utilities inside JIT.\"\"\"\n        \n        # Debug print (works inside JIT)\n        jax.debug.print(\"Input shape: {}\", x.shape)\n        \n        # Intermediate values\n        y = jnp.sin(x)\n        jax.debug.print(\"After sin - mean: {:.3f}, std: {:.3f}\", \n                       jnp.mean(y), jnp.std(y))\n        \n        # Assertions (converted to runtime checks)\n        # jax.debug.assert_(jnp.all(jnp.isfinite(y)), \"NaN detected!\")\n        \n        z = y @ y.T\n        jax.debug.print(\"Final shape: {}\", z.shape)\n        \n        return z\n    \n    x = random.normal(random.PRNGKey(2), (5, 5))\n    result = debug_example(x)\n    \n    # 5. Finding bottlenecks\n    print(\"\\n5. IDENTIFYING BOTTLENECKS:\")\n    \n    def find_bottlenecks():\n        \"\"\"Strategies for finding performance issues.\"\"\"\n        \n        # Common bottlenecks:\n        bottlenecks = {\n            \"Python loops\": \"Use scan/fori_loop instead\",\n            \"Small operations\": \"Batch into larger operations\",\n            \"Recompilation\": \"Check for changing shapes/types\",\n            \"Host-device transfer\": \"Minimize data movement\",\n            \"Unintended float64\": \"Use float32 by default\",\n            \"Missing JIT\": \"JIT-compile hot functions\",\n            \"Bad memory access\": \"Optimize data layout\"\n        }\n        \n        for issue, solution in bottlenecks.items():\n            print(f\"    {issue}: {solution}\")\n    \n    find_bottlenecks()\n\nprofiling_and_debugging()","type":"content","url":"/jax-advanced-continued#performance-analysis-tools","position":13},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization (Continued)","lvl2":"Real-World Example: Galaxy Simulation"},"type":"lvl2","url":"/jax-advanced-continued#real-world-example-galaxy-simulation","position":14},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization (Continued)","lvl2":"Real-World Example: Galaxy Simulation"},"content":"","type":"content","url":"/jax-advanced-continued#real-world-example-galaxy-simulation","position":15},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization (Continued)","lvl3":"Production-Ready N-Body Code","lvl2":"Real-World Example: Galaxy Simulation"},"type":"lvl3","url":"/jax-advanced-continued#production-ready-n-body-code","position":16},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization (Continued)","lvl3":"Production-Ready N-Body Code","lvl2":"Real-World Example: Galaxy Simulation"},"content":"def galaxy_simulation_production():\n    \"\"\"\n    Production-ready galaxy simulation using all JAX features.\n    Demonstrates best practices for scientific computing.\n    \"\"\"\n    \n    print(\"\\nPRODUCTION GALAXY SIMULATION\")\n    print(\"=\" * 50)\n    \n    # Configuration\n    @jit\n    def create_galaxy(n_stars, key, galaxy_type='spiral'):\n        \"\"\"Initialize a galaxy with realistic structure.\"\"\"\n        \n        key1, key2, key3, key4 = random.split(key, 4)\n        \n        if galaxy_type == 'spiral':\n            # Spiral galaxy with disk and bulge\n            \n            # Disk component (exponential profile)\n            radii = random.exponential(key1, (int(0.8 * n_stars),)) * 10.0\n            angles = random.uniform(key2, (int(0.8 * n_stars),), \n                                   minval=0, maxval=2*jnp.pi)\n            \n            # Add spiral structure\n            spiral_phase = 2.0 * jnp.log(radii + 1)\n            angles = angles + spiral_phase\n            \n            disk_x = radii * jnp.cos(angles)\n            disk_y = radii * jnp.sin(angles)\n            disk_z = random.normal(key3, (int(0.8 * n_stars),)) * 0.5\n            \n            # Bulge component (Hernquist profile)\n            n_bulge = n_stars - int(0.8 * n_stars)\n            r_bulge = random.uniform(key4, (n_bulge,)) ** (1/3) * 3.0\n            theta = jnp.arccos(1 - 2 * random.uniform(key1, (n_bulge,)))\n            phi = random.uniform(key2, (n_bulge,), minval=0, maxval=2*jnp.pi)\n            \n            bulge_x = r_bulge * jnp.sin(theta) * jnp.cos(phi)\n            bulge_y = r_bulge * jnp.sin(theta) * jnp.sin(phi)\n            bulge_z = r_bulge * jnp.cos(theta)\n            \n            # Combine\n            positions = jnp.concatenate([\n                jnp.stack([disk_x, disk_y, disk_z], axis=1),\n                jnp.stack([bulge_x, bulge_y, bulge_z], axis=1)\n            ])\n            \n            # Circular velocities (simplified)\n            all_radii = jnp.sqrt(positions[:, 0]**2 + positions[:, 1]**2)\n            v_circ = jnp.sqrt(all_radii / (all_radii + 1.0))  # Rotation curve\n            \n            velocities = jnp.zeros_like(positions)\n            velocities = velocities.at[:, 0].set(-positions[:, 1] / all_radii * v_circ)\n            velocities = velocities.at[:, 1].set(positions[:, 0] / all_radii * v_circ)\n            \n            # Masses (IMF-like distribution)\n            masses = random.pareto(key3, 2.35, (n_stars,)) * 0.1 + 0.1\n            masses = jnp.clip(masses, 0.1, 10.0)\n            \n        return positions, velocities, masses\n    \n    # Optimized force calculation\n    @jit\n    def compute_forces_fast(positions, masses, softening=0.1):\n        \"\"\"\n        Fast force calculation using vectorization.\n        O(N²) but highly optimized.\n        \"\"\"\n        n = len(masses)\n        \n        # Compute all pairwise vectors at once\n        r_ij = positions[:, None, :] - positions[None, :, :]  # (n, n, 3)\n        \n        # Distances\n        r2 = jnp.sum(r_ij**2, axis=2) + softening**2  # (n, n)\n        r3 = r2 ** 1.5\n        \n        # Mask diagonal (self-interaction)\n        mask = 1.0 - jnp.eye(n)\n        \n        # Forces\n        F_ij = r_ij / r3[:, :, None] * mask[:, :, None]  # (n, n, 3)\n        forces = jnp.sum(masses[None, :, None] * F_ij, axis=1)  # (n, 3)\n        \n        return forces * masses[:, None]\n    \n    # Adaptive time-stepping\n    @jit\n    def adaptive_timestep(positions, velocities, masses, base_dt=0.01):\n        \"\"\"Compute adaptive timestep based on local dynamics.\"\"\"\n        \n        forces = compute_forces_fast(positions, masses)\n        accelerations = forces / masses[:, None]\n        \n        # Criteria for timestep\n        v_mag = jnp.linalg.norm(velocities, axis=1)\n        a_mag = jnp.linalg.norm(accelerations, axis=1)\n        \n        # Courant condition\n        dt_courant = jnp.min(0.1 / (v_mag + 1e-10))\n        \n        # Acceleration condition\n        dt_accel = jnp.min(jnp.sqrt(0.1 / (a_mag + 1e-10)))\n        \n        # Take minimum\n        dt = jnp.minimum(dt_courant, dt_accel)\n        dt = jnp.minimum(dt, base_dt)\n        \n        return dt\n    \n    # Main evolution with all features\n    @partial(jit, static_argnums=(3,))\n    def evolve_galaxy(positions, velocities, masses, n_steps, checkpoint_every=100):\n        \"\"\"\n        Full galaxy evolution with checkpointing.\n        \"\"\"\n        \n        def step(carry, i):\n            pos, vel = carry\n            \n            # Adaptive timestep\n            dt = adaptive_timestep(pos, vel, masses)\n            \n            # Leapfrog integration\n            forces = compute_forces_fast(pos, masses)\n            acc = forces / masses[:, None]\n            \n            vel_half = vel + 0.5 * dt * acc\n            pos_new = pos + dt * vel_half\n            \n            forces_new = compute_forces_fast(pos_new, masses)\n            acc_new = forces_new / masses[:, None]\n            \n            vel_new = vel_half + 0.5 * dt * acc_new\n            \n            # Energy for monitoring\n            ke = 0.5 * jnp.sum(masses[:, None] * vel_new**2)\n            \n            # Checkpoint decision (would save to disk in practice)\n            should_checkpoint = (i % checkpoint_every) == 0\n            \n            return (pos_new, vel_new), (pos_new, ke, dt)\n        \n        final_state, (trajectory, energies, timesteps) = scan(\n            step, (positions, velocities), jnp.arange(n_steps)\n        )\n        \n        return final_state, trajectory[::checkpoint_every], energies[::checkpoint_every]\n    \n    # Initialize and run\n    key = random.PRNGKey(42)\n    n_stars = 1000\n    \n    print(f\"\\nInitializing spiral galaxy with {n_stars} stars...\")\n    positions, velocities, masses = create_galaxy(n_stars, key)\n    \n    print(\"Running simulation...\")\n    start_time = time.perf_counter()\n    \n    final_state, checkpoints, energies = evolve_galaxy(\n        positions, velocities, masses, n_steps=1000, checkpoint_every=100\n    )\n    \n    elapsed = time.perf_counter() - start_time\n    print(f\"  Completed in {elapsed:.2f} seconds\")\n    print(f\"  Steps per second: {1000/elapsed:.0f}\")\n    \n    # Analysis\n    initial_energy = energies[0]\n    final_energy = energies[-1]\n    print(f\"\\nEnergy conservation:\")\n    print(f\"  Initial: {initial_energy:.2f}\")\n    print(f\"  Final: {final_energy:.2f}\")\n    print(f\"  Drift: {abs(final_energy - initial_energy) / abs(initial_energy) * 100:.2f}%\")\n    \n    # Visualize\n    fig = plt.figure(figsize=(15, 5))\n    \n    # Initial configuration\n    ax1 = fig.add_subplot(131)\n    ax1.scatter(positions[:, 0], positions[:, 1], s=1, alpha=0.5, c=masses, cmap='YlOrRd')\n    ax1.set_xlim(-30, 30)\n    ax1.set_ylim(-30, 30)\n    ax1.set_xlabel('X [kpc]')\n    ax1.set_ylabel('Y [kpc]')\n    ax1.set_title('Initial Galaxy')\n    ax1.set_aspect('equal')\n    \n    # Final configuration\n    ax2 = fig.add_subplot(132)\n    final_pos = final_state[0]\n    ax2.scatter(final_pos[:, 0], final_pos[:, 1], s=1, alpha=0.5, c=masses, cmap='YlOrRd')\n    ax2.set_xlim(-30, 30)\n    ax2.set_ylim(-30, 30)\n    ax2.set_xlabel('X [kpc]')\n    ax2.set_ylabel('Y [kpc]')\n    ax2.set_title('Final Galaxy')\n    ax2.set_aspect('equal')\n    \n    # Energy evolution\n    ax3 = fig.add_subplot(133)\n    ax3.plot(energies)\n    ax3.set_xlabel('Checkpoint')\n    ax3.set_ylabel('Total Energy')\n    ax3.set_title('Energy Conservation')\n    ax3.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return checkpoints\n\n# Run the production simulation\ncheckpoints = galaxy_simulation_production()","type":"content","url":"/jax-advanced-continued#production-ready-n-body-code","position":17},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization (Continued)","lvl2":"Best Practices Summary"},"type":"lvl2","url":"/jax-advanced-continued#best-practices-summary","position":18},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization (Continued)","lvl2":"Best Practices Summary"},"content":"","type":"content","url":"/jax-advanced-continued#best-practices-summary","position":19},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization (Continued)","lvl3":"JAX Do’s and Don’ts","lvl2":"Best Practices Summary"},"type":"lvl3","url":"/jax-advanced-continued#jax-dos-and-donts","position":20},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization (Continued)","lvl3":"JAX Do’s and Don’ts","lvl2":"Best Practices Summary"},"content":"def best_practices():\n    \"\"\"Summary of JAX best practices for astronomical computing.\"\"\"\n    \n    print(\"\\nJAX BEST PRACTICES FOR ASTRONOMY\")\n    print(\"=\" * 50)\n    \n    practices = {\n        \"✅ DO\": [\n            \"Use JIT on hot loops and expensive functions\",\n            \"Write pure functional code without side effects\",\n            \"Use scan for sequential computations\",\n            \"Vectorize with vmap instead of loops\",\n            \"Profile and measure performance\",\n            \"Use float32 by default for speed\",\n            \"Leverage automatic differentiation\",\n            \"Think in terms of array operations\",\n            \"Use static_argnums for compile-time constants\",\n            \"Test numerical stability and accuracy\"\n        ],\n        \n        \"❌ DON'T\": [\n            \"Use Python loops inside JIT functions\",\n            \"Mutate arrays (use .at[].set() instead)\",\n            \"Use varying shapes (causes recompilation)\",\n            \"Forget to block_until_ready() when timing\",\n            \"Mix NumPy and JAX arrays carelessly\",\n            \"Use float64 unless necessary\",\n            \"Ignore memory consumption\",\n            \"Use global variables in JIT functions\",\n            \"Forget to split random keys\",\n            \"Skip validation of gradients\"\n        ],\n        \n        \"🚀 ADVANCED\": [\n            \"Custom VJP rules for numerical stability\",\n            \"Checkpointing for memory efficiency\",\n            \"Sharding for multi-GPU parallelism\",\n            \"Pipeline parallelism for deep models\",\n            \"Mixed precision for performance\",\n            \"Custom linear algebra primitives\",\n            \"Optimize data layout for cache\",\n            \"Use donation to reuse buffers\",\n            \"Profile XLA compilation\",\n            \"Implement custom CUDA kernels\"\n        ]\n    }\n    \n    for category, items in practices.items():\n        print(f\"\\n{category}:\")\n        for item in items:\n            print(f\"  • {item}\")\n\nbest_practices()","type":"content","url":"/jax-advanced-continued#jax-dos-and-donts","position":21},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization (Continued)","lvl2":"Exercises"},"type":"lvl2","url":"/jax-advanced-continued#exercises","position":22},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization (Continued)","lvl2":"Exercises"},"content":"","type":"content","url":"/jax-advanced-continued#exercises","position":23},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization (Continued)","lvl3":"Exercise 1: Adaptive Mesh Refinement","lvl2":"Exercises"},"type":"lvl3","url":"/jax-advanced-continued#exercise-1-adaptive-mesh-refinement","position":24},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization (Continued)","lvl3":"Exercise 1: Adaptive Mesh Refinement","lvl2":"Exercises"},"content":"def adaptive_mesh_refinement():\n    \"\"\"\n    Implement AMR for solving Poisson equation in JAX.\n    \n    Requirements:\n    - Use while_loop for adaptive refinement\n    - Custom VJP for interpolation operators\n    - JIT compile the multigrid solver\n    - Verify convergence with manufactured solution\n    \"\"\"\n    # Your code here\n    pass","type":"content","url":"/jax-advanced-continued#exercise-1-adaptive-mesh-refinement","position":25},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization (Continued)","lvl3":"Exercise 2: Differentiable Radiative Transfer","lvl2":"Exercises"},"type":"lvl3","url":"/jax-advanced-continued#exercise-2-differentiable-radiative-transfer","position":26},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization (Continued)","lvl3":"Exercise 2: Differentiable Radiative Transfer","lvl2":"Exercises"},"content":"def differentiable_rt():\n    \"\"\"\n    Build differentiable radiative transfer solver.\n    \n    Tasks:\n    - Implement ray marching with scan\n    - Use custom derivatives for optical depth\n    - Optimize source function parameters via gradient descent\n    - Compare with Monte Carlo solution\n    \"\"\"\n    # Your code here\n    pass","type":"content","url":"/jax-advanced-continued#exercise-2-differentiable-radiative-transfer","position":27},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization (Continued)","lvl3":"Exercise 3: Parallel MCMC Sampler","lvl2":"Exercises"},"type":"lvl3","url":"/jax-advanced-continued#exercise-3-parallel-mcmc-sampler","position":28},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization (Continued)","lvl3":"Exercise 3: Parallel MCMC Sampler","lvl2":"Exercises"},"content":"def parallel_mcmc():\n    \"\"\"\n    Implement parallel tempered MCMC in JAX.\n    \n    Requirements:\n    - Use pmap for parallel chains\n    - Implement replica exchange with collective ops\n    - JIT compile the likelihood and proposals\n    - Test on cosmological parameter estimation\n    \"\"\"\n    # Your code here\n    pass","type":"content","url":"/jax-advanced-continued#exercise-3-parallel-mcmc-sampler","position":29},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization (Continued)","lvl2":"Key Takeaways"},"type":"lvl2","url":"/jax-advanced-continued#key-takeaways","position":30},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization (Continued)","lvl2":"Key Takeaways"},"content":"✅ Control flow - Use lax.cond, scan, while_loop for conditionals and loops✅ Custom derivatives - Define VJP/JVP rules for numerical stability✅ Memory optimization - Checkpointing, donation, and chunking strategies✅ Parallelism - pmap for multi-device, sharding for large arrays✅ Profiling - Always measure, use block_until_ready, inspect compilation✅ Production code - Combine all features for real scientific applications","type":"content","url":"/jax-advanced-continued#key-takeaways","position":31},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization (Continued)","lvl2":"Next Steps"},"type":"lvl2","url":"/jax-advanced-continued#next-steps","position":32},{"hierarchy":{"lvl1":"JAX Advanced Patterns: Control Flow and Optimization (Continued)","lvl2":"Next Steps"},"content":"With these advanced patterns, you’re ready to:\n\nPort existing astronomical codes to JAX\n\nBuild differentiable physical models\n\nScale to multi-GPU clusters\n\nContribute to JAX ecosystem libraries\n\nDevelop novel computational methods\n\nRemember: JAX rewards thinking in terms of transformations and functional programming. The initial learning curve pays off with unprecedented performance and capabilities for scientific computing!","type":"content","url":"/jax-advanced-continued#next-steps","position":33},{"hierarchy":{"lvl1":"Computational Methods"},"type":"lvl1","url":"/index-2","position":0},{"hierarchy":{"lvl1":"Computational Methods"},"content":"Comprehensive guides to the numerical methods and programming techniques used throughout ASTR 596.","type":"content","url":"/index-2","position":1},{"hierarchy":{"lvl1":"Computational Methods","lvl2":"Overview"},"type":"lvl2","url":"/index-2#overview","position":2},{"hierarchy":{"lvl1":"Computational Methods","lvl2":"Overview"},"content":"This section provides deep-dive explanations of the computational methods you’ll implement in the course projects. Each topic builds from mathematical foundations to practical implementation.","type":"content","url":"/index-2#overview","position":3},{"hierarchy":{"lvl1":"Computational Methods","lvl2":"Topics Covered"},"type":"lvl2","url":"/index-2#topics-covered","position":4},{"hierarchy":{"lvl1":"Computational Methods","lvl2":"Topics Covered"},"content":"","type":"content","url":"/index-2#topics-covered","position":5},{"hierarchy":{"lvl1":"Computational Methods","lvl3":"🐍 Python Foundations","lvl2":"Topics Covered"},"type":"lvl3","url":"/index-2#id-python-foundations","position":6},{"hierarchy":{"lvl1":"Computational Methods","lvl3":"🐍 Python Foundations","lvl2":"Topics Covered"},"content":"Essential Python programming concepts for scientific computing:\n\nObject-oriented programming principles\n\nNumPy array operations and broadcasting\n\nScientific Python ecosystem (SciPy, matplotlib, astropy)\n\nProfessional development practices\n\nDebugging and testing strategies","type":"content","url":"/index-2#id-python-foundations","position":7},{"hierarchy":{"lvl1":"Computational Methods","lvl3":"🔢 Numerical Methods","lvl2":"Topics Covered"},"type":"lvl3","url":"/index-2#id-numerical-methods","position":8},{"hierarchy":{"lvl1":"Computational Methods","lvl3":"🔢 Numerical Methods","lvl2":"Topics Covered"},"content":"Core algorithms for solving mathematical problems computationally:\n\nNumerical integration (Euler, Runge-Kutta methods)\n\nRoot finding and optimization\n\nLinear algebra operations\n\nInterpolation and approximation\n\nError analysis and stability","type":"content","url":"/index-2#id-numerical-methods","position":9},{"hierarchy":{"lvl1":"Computational Methods","lvl3":"🤖 Machine Learning","lvl2":"Topics Covered"},"type":"lvl3","url":"/index-2#id-machine-learning","position":10},{"hierarchy":{"lvl1":"Computational Methods","lvl3":"🤖 Machine Learning","lvl2":"Topics Covered"},"content":"Statistical learning methods implemented from first principles:\n\nLinear and polynomial regression\n\nGradient descent optimization\n\nCross-validation and model selection\n\nBayesian inference and MCMC\n\nNeural network fundamentals","type":"content","url":"/index-2#id-machine-learning","position":11},{"hierarchy":{"lvl1":"Computational Methods","lvl3":"⚡ Modern Frameworks","lvl2":"Topics Covered"},"type":"lvl3","url":"/index-2#id-modern-frameworks","position":12},{"hierarchy":{"lvl1":"Computational Methods","lvl3":"⚡ Modern Frameworks","lvl2":"Topics Covered"},"content":"Cutting-edge tools for high-performance scientific computing:\n\nJAX ecosystem and automatic differentiation\n\nGPU acceleration and vectorization\n\nFunctional programming paradigms\n\nPerformance optimization techniques\n\nIntegration with existing codebases","type":"content","url":"/index-2#id-modern-frameworks","position":13},{"hierarchy":{"lvl1":"Computational Methods","lvl2":"Learning Philosophy"},"type":"lvl2","url":"/index-2#learning-philosophy","position":14},{"hierarchy":{"lvl1":"Computational Methods","lvl2":"Learning Philosophy"},"content":"These materials follow our “glass box” approach:\n\nMathematical Foundation: Understand the theory behind each method\n\nManual Implementation: Code algorithms from scratch in NumPy\n\nFramework Integration: Leverage modern tools like JAX for performance\n\nReal Applications: Apply methods to genuine astrophysical problems","type":"content","url":"/index-2#learning-philosophy","position":15},{"hierarchy":{"lvl1":"Computational Methods","lvl2":"How to Use This Section"},"type":"lvl2","url":"/index-2#how-to-use-this-section","position":16},{"hierarchy":{"lvl1":"Computational Methods","lvl2":"How to Use This Section"},"content":"Before Projects: Read relevant theory and examples\n\nDuring Implementation: Reference syntax and algorithms\n\nAfter Completion: Deepen understanding with advanced topics\n\nFor Research: Use as reference for future work","type":"content","url":"/index-2#how-to-use-this-section","position":17},{"hierarchy":{"lvl1":"Computational Methods","lvl2":"Connection to Projects"},"type":"lvl2","url":"/index-2#connection-to-projects","position":18},{"hierarchy":{"lvl1":"Computational Methods","lvl2":"Connection to Projects"},"content":"Each computational method directly supports the course projects:\n\nProject 1: Python foundations and OOP design\n\nProject 2: Numerical integration and Monte Carlo methods\n\nProject 3: Machine learning and optimization\n\nFinal Project: Modern frameworks and neural networks","type":"content","url":"/index-2#connection-to-projects","position":19},{"hierarchy":{"lvl1":"Computational Methods","lvl2":"Prerequisites"},"type":"lvl2","url":"/index-2#prerequisites","position":20},{"hierarchy":{"lvl1":"Computational Methods","lvl2":"Prerequisites"},"content":"Basic understanding of:\n\nPython programming fundamentals\n\nLinear algebra and calculus\n\nElementary physics concepts\n\nCommand-line interface usage","type":"content","url":"/index-2#prerequisites","position":21},{"hierarchy":{"lvl1":"Computational Methods","lvl2":"Getting Help"},"type":"lvl2","url":"/index-2#getting-help","position":22},{"hierarchy":{"lvl1":"Computational Methods","lvl2":"Getting Help"},"content":"If you encounter difficulties with any computational method:\n\nReview the theoretical background first\n\nCheck implementation examples and code snippets\n\nAttend office hours for conceptual clarification\n\nUse course Slack for specific syntax questions\n\nApply AI tools following course guidelines\n\nThese computational skills form the foundation for modern astrophysical research and will serve you throughout your career in science or technology.","type":"content","url":"/index-2#getting-help","position":23},{"hierarchy":{"lvl1":"Machine Learning"},"type":"lvl1","url":"/index-3","position":0},{"hierarchy":{"lvl1":"Machine Learning"},"content":"Content coming soon!","type":"content","url":"/index-3","position":1},{"hierarchy":{"lvl1":"Modern Frameworks"},"type":"lvl1","url":"/index-4","position":0},{"hierarchy":{"lvl1":"Modern Frameworks"},"content":"Content coming soon!","type":"content","url":"/index-4","position":1},{"hierarchy":{"lvl1":"Numerical Methods"},"type":"lvl1","url":"/index-5","position":0},{"hierarchy":{"lvl1":"Numerical Methods"},"content":"Content coming soon!","type":"content","url":"/index-5","position":1},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator"},"type":"lvl1","url":"/chapter1-python-calculator","position":0},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator"},"content":"“Before you run, you must walk. Before you compute galaxies, you must compute numbers.”","type":"content","url":"/chapter1-python-calculator","position":1},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl2":"Table of Contents"},"type":"lvl2","url":"/chapter1-python-calculator#table-of-contents","position":2},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl2":"Table of Contents"},"content":"Learning Objectives\n\nWhy Start Here?\n\nBasic Operations & Data Types\n\nNumbers in Python\n\nBasic Math Operations\n\nScientific Notation\n\nThe Reality of Computer Math\n\nMachine Precision\n\nWhy 0.1 + 0.2 ≠ 0.3\n\nImplications for Astronomy\n\nVariables & Memory\n\nNaming Your Data\n\nMultiple Assignment\n\nConstants in Science\n\nStrings & Scientific Formatting\n\nString Basics\n\nF-strings for Science\n\nFormatting Numbers\n\nType Conversion & Checking\n\nExercises\n\nKey Takeaways","type":"content","url":"/chapter1-python-calculator#table-of-contents","position":3},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl2":"Learning Objectives"},"type":"lvl2","url":"/chapter1-python-calculator#learning-objectives","position":4},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl2":"Learning Objectives"},"content":"By the end of this chapter you will:\n\nUse Python as a powerful scientific calculator\n\nUnderstand fundamental data types (int, float, complex, string)\n\nRecognize and handle floating-point precision limitations\n\nCreate meaningful variable names following scientific conventions\n\nFormat numerical output for scientific communication\n\nConvert between data types safely","type":"content","url":"/chapter1-python-calculator#learning-objectives","position":5},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl2":"Why Start Here?"},"type":"lvl2","url":"/chapter1-python-calculator#why-start-here","position":6},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl2":"Why Start Here?"},"content":"Every simulation, every data analysis, every model starts with basic calculations. Python can replace your scientific calculator, but it’s far more powerful—and has some quirks you need to understand.\n\n# Python as calculator - try these!\nprint(2**10)  # Powers\nprint(355/113)  # Better approximation of pi than 22/7\nprint(1.23e-7)  # Scientific notation\n\n","type":"content","url":"/chapter1-python-calculator#why-start-here","position":7},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl2":"Basic Operations & Data Types"},"type":"lvl2","url":"/chapter1-python-calculator#basic-operations-data-types","position":8},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl2":"Basic Operations & Data Types"},"content":"","type":"content","url":"/chapter1-python-calculator#basic-operations-data-types","position":9},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl3":"Numbers in Python","lvl2":"Basic Operations & Data Types"},"type":"lvl3","url":"/chapter1-python-calculator#numbers-in-python","position":10},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl3":"Numbers in Python","lvl2":"Basic Operations & Data Types"},"content":"Python has three numeric types, each with a specific purpose:\n\n# Integers - exact whole numbers\nphoton_count = 1000000\nprint(f\"Type: {type(photon_count)}, Value: {photon_count}\")\n\n# Floats - decimal numbers (approximate!)\nredshift = 2.345\nprint(f\"Type: {type(redshift)}, Value: {redshift}\")\n\n# Complex - for wave functions, Fourier transforms\nwave = 3 + 4j\nprint(f\"Type: {type(wave)}, Magnitude: {abs(wave)}\")\n\nWhy Different Types?\n\nIntegers: Exact counts, array indices, loop counters\n\nFloats: Measurements, calculations, anything with decimals\n\nComplex: Quantum mechanics, signal processing, Fourier analysis","type":"content","url":"/chapter1-python-calculator#numbers-in-python","position":11},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl3":"Basic Math Operations","lvl2":"Basic Operations & Data Types"},"type":"lvl3","url":"/chapter1-python-calculator#basic-math-operations","position":12},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl3":"Basic Math Operations","lvl2":"Basic Operations & Data Types"},"content":"\n\n# Standard operations\na, b = 10, 3\n\nprint(f\"Addition:       {a} + {b} = {a + b}\")\nprint(f\"Subtraction:    {a} - {b} = {a - b}\")\nprint(f\"Multiplication: {a} * {b} = {a * b}\")\nprint(f\"Division:       {a} / {b} = {a / b:.4f}\")  # Always returns float\nprint(f\"Integer div:    {a} // {b} = {a // b}\")    # Floors the result\nprint(f\"Remainder:      {a} % {b} = {a % b}\")      # Modulo\nprint(f\"Power:          {a} ** {b} = {a ** b}\")\n\nOrder of Operations\n\nWhat does Python calculate for: 2 + 3 * 4 ** 2 / 8 - 1\n\nThink through it step by step, then verify.Python follows PEMDAS (Parentheses, Exponents, Multiplication/Division, Addition/Subtraction):\n- First: `4 ** 2 = 16`\n- Then: `3 * 16 = 48`\n- Then: `48 / 8 = 6.0`\n- Then: `2 + 6.0 = 8.0`\n- Finally: `8.0 - 1 = 7.0`\n\n```python\nresult = 2 + 3 * 4 ** 2 / 8 - 1\nprint(result)  # 7.0\n### Scientific Notation\n\nIn astronomy, we deal with enormous and tiny numbers:\n\n```{code-cell} ipython3\n# Scientific notation using 'e'\nlight_speed = 3e8  # m/s\nplanck_constant = 6.626e-34  # J⋅s\nsolar_mass = 1.989e30  # kg\n\n# Python preserves precision\ndistance_to_andromeda = 2.537e6  # light years\nprint(f\"Distance to Andromeda: {distance_to_andromeda:.3e} ly\")\nprint(f\"In meters: {distance_to_andromeda * 9.461e15:.3e} m\")","type":"content","url":"/chapter1-python-calculator#basic-math-operations","position":13},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl2":"The Reality of Computer Math"},"type":"lvl2","url":"/chapter1-python-calculator#the-reality-of-computer-math","position":14},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl2":"The Reality of Computer Math"},"content":"","type":"content","url":"/chapter1-python-calculator#the-reality-of-computer-math","position":15},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl3":"Machine Precision","lvl2":"The Reality of Computer Math"},"type":"lvl3","url":"/chapter1-python-calculator#machine-precision","position":16},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl3":"Machine Precision","lvl2":"The Reality of Computer Math"},"content":"Critical Concept\n\nComputers cannot store infinite precision. Every calculation is approximate!\n\nimport sys\n\n# What's the smallest number we can add to 1.0 and see a difference?\nepsilon = sys.float_info.epsilon\nprint(f\"Machine epsilon: {epsilon}\")\nprint(f\"1.0 + epsilon/2 == 1.0? {1.0 + epsilon/2 == 1.0}\")\nprint(f\"1.0 + epsilon == 1.0? {1.0 + epsilon == 1.0}\")\n\n# Floats have limits\nprint(f\"\\nLargest float: {sys.float_info.max:.3e}\")\nprint(f\"Smallest positive float: {sys.float_info.min:.3e}\")\n\n","type":"content","url":"/chapter1-python-calculator#machine-precision","position":17},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl3":"Why 0.1 + 0.2 ≠ 0.3","lvl2":"The Reality of Computer Math"},"type":"lvl3","url":"/chapter1-python-calculator#why-0-1-0-2-0-3","position":18},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl3":"Why 0.1 + 0.2 ≠ 0.3","lvl2":"The Reality of Computer Math"},"content":"The most famous example of floating-point weirdness:\n\n# The shocking truth\na = 0.1\nb = 0.2\nc = a + b\n\nprint(f\"0.1 + 0.2 = {c}\")\nprint(f\"0.1 + 0.2 == 0.3? {c == 0.3}\")\nprint(f\"Actual value: {c:.20f}\")\n\n# Why? Binary representation can't exactly store decimal 0.1\nprint(f\"\\nBinary approximation of 0.1: {0.1:.55f}\")\n\nWhy This Happens\n\nComputers store numbers in binary (base 2). Just like 1/3 = 0.333... repeats forever in decimal,\n0.1 in binary is 0.0001100110011... repeating forever. The computer truncates this, causing tiny errors.\n\nRule: Never use == to compare floats! Use “close enough”:\n\n# The right way to compare floats\nimport math\n\ndef floats_are_equal(a, b, tolerance=1e-9):\n    \"\"\"Check if two floats are 'close enough'.\"\"\"\n    return abs(a - b) < tolerance\n\n# Or use Python's built-in\nprint(f\"Using custom function: {floats_are_equal(0.1 + 0.2, 0.3)}\")\nprint(f\"Using math.isclose: {math.isclose(0.1 + 0.2, 0.3)}\")\n\n","type":"content","url":"/chapter1-python-calculator#why-0-1-0-2-0-3","position":19},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl3":"Implications for Astronomy","lvl2":"The Reality of Computer Math"},"type":"lvl3","url":"/chapter1-python-calculator#implications-for-astronomy","position":20},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl3":"Implications for Astronomy","lvl2":"The Reality of Computer Math"},"content":"These tiny errors matter in astronomical computations:\n\n# Simulating orbital mechanics for 1 year\ndays_per_year = 365.25\nseconds_per_day = 86400\ntime_step = 0.1  # seconds\n\n# Naive approach - accumulating error\ntime_naive = 0.0\nsteps = int(days_per_year * seconds_per_day / time_step)\nfor _ in range(steps):\n    time_naive += time_step\n\n# Better approach - minimize accumulation\ntime_better = steps * time_step\n\nprint(f\"Expected time: {days_per_year * seconds_per_day} seconds\")\nprint(f\"Naive approach: {time_naive} seconds\")\nprint(f\"Better approach: {time_better} seconds\")\nprint(f\"Error in naive: {time_naive - days_per_year * seconds_per_day} seconds\")\nprint(f\"Error in better: {time_better - days_per_year * seconds_per_day} seconds\")\n\nCatastrophic Cancellation\n\nWhen you subtract two nearly equal numbers, you lose precision. Try this:a = 1.0000001\nb = 1.0000000\nprint(f\"Difference: {a - b}\")\nprint(f\"Relative error affects {___}% of the result\")  # Fill in\n\nWhy might this be a problem when calculating small changes in stellar positions?\n```{solution}\n:class: dropdown\n\nThe difference is 1e-7, but we started with 7 significant figures and ended with 1. \nThat's a loss of 6 significant figures - 86% precision loss!\n\nThis is critical in astronomy when calculating:\n- Proper motions (tiny changes in position)\n- Radial velocities (small Doppler shifts)\n- Parallax measurements (minuscule angular changes)\n\nBetter approach: Reformulate to avoid subtraction of similar numbers when possible.","type":"content","url":"/chapter1-python-calculator#implications-for-astronomy","position":21},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl2":"Variables & Memory"},"type":"lvl2","url":"/chapter1-python-calculator#variables-memory","position":22},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl2":"Variables & Memory"},"content":"","type":"content","url":"/chapter1-python-calculator#variables-memory","position":23},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl3":"Naming Your Data","lvl2":"Variables & Memory"},"type":"lvl3","url":"/chapter1-python-calculator#naming-your-data","position":24},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl3":"Naming Your Data","lvl2":"Variables & Memory"},"content":"Variable names are documentation. Choose wisely:\n\n# Bad - what does this mean?\nx = 5.972e24\nv = 7.9e3\nt = 86400\n\n# Good - self-documenting code\nearth_mass_kg = 5.972e24\norbital_velocity_ms = 7.9e3  # m/s\nseconds_per_day = 86400\n\n# Python naming conventions (PEP 8)\nstellar_temperature = 5778  # lowercase_with_underscores\nMAX_ITERATIONS = 1000000   # CONSTANTS_IN_CAPS\nStarClass = \"G2V\"          # CapitalizedWords for classes (later)\n\nPython Keywords\n\nNever use these as variable names:\nand, as, assert, break, class, continue, def, del, elif, else, except, False, finally, for, from, global, if, import, in, is, lambda, None, nonlocal, not, or, pass, raise, return, True, try, while, with, yield","type":"content","url":"/chapter1-python-calculator#naming-your-data","position":25},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl3":"Multiple Assignment","lvl2":"Variables & Memory"},"type":"lvl3","url":"/chapter1-python-calculator#multiple-assignment","position":26},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl3":"Multiple Assignment","lvl2":"Variables & Memory"},"content":"Python allows elegant multiple assignment:\n\n# Simultaneous assignment\nra, dec = 266.405, -28.936  # Galactic center coordinates\nprint(f\"Sgr A* position: RA={ra}°, Dec={dec}°\")\n\n# Swapping without temporary variable\na, b = 10, 20\nprint(f\"Before swap: a={a}, b={b}\")\na, b = b, a\nprint(f\"After swap: a={a}, b={b}\")\n\n# Unpacking from calculations\nquotient, remainder = divmod(100, 7)\nprint(f\"100 = 7 * {quotient} + {remainder}\")\n\n","type":"content","url":"/chapter1-python-calculator#multiple-assignment","position":27},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl3":"Constants in Science","lvl2":"Variables & Memory"},"type":"lvl3","url":"/chapter1-python-calculator#constants-in-science","position":28},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl3":"Constants in Science","lvl2":"Variables & Memory"},"content":"Python doesn’t have true constants, but we use CAPS by convention:\n\n# Physical constants (SI units)\nSPEED_OF_LIGHT = 299792458  # m/s (exact)\nGRAVITATIONAL_CONSTANT = 6.67430e-11  # m³/kg/s²\nPLANCK_CONSTANT = 6.62607015e-34  # J⋅s (exact as of 2019)\n\n# Astronomical constants\nSOLAR_MASS = 1.98892e30  # kg\nPARSEC = 3.0857e16  # m\nASTRONOMICAL_UNIT = 1.495978707e11  # m\n\n# Using constants in calculations\nschwarzschild_radius_sun = 2 * GRAVITATIONAL_CONSTANT * SOLAR_MASS / SPEED_OF_LIGHT**2\nprint(f\"Schwarzschild radius of Sun: {schwarzschild_radius_sun:.1f} m\")\nprint(f\"That's about {schwarzschild_radius_sun/1000:.1f} km\")\n\n","type":"content","url":"/chapter1-python-calculator#constants-in-science","position":29},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl2":"Strings & Scientific Formatting"},"type":"lvl2","url":"/chapter1-python-calculator#strings-scientific-formatting","position":30},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl2":"Strings & Scientific Formatting"},"content":"","type":"content","url":"/chapter1-python-calculator#strings-scientific-formatting","position":31},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl3":"String Basics","lvl2":"Strings & Scientific Formatting"},"type":"lvl3","url":"/chapter1-python-calculator#string-basics","position":32},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl3":"String Basics","lvl2":"Strings & Scientific Formatting"},"content":"Strings store text data - essential for labels, filenames, and output:\n\n# Creating strings\nobject_name = \"PSR J0348+0432\"  # Single or double quotes\nobservation_note = 'High-mass pulsar in binary system'\n\n# String concatenation\nfull_description = object_name + \": \" + observation_note\nprint(full_description)\n\n# String methods\nprint(f\"Uppercase: {object_name.upper()}\")\nprint(f\"Is it PSR? {object_name.startswith('PSR')}\")\nprint(f\"Replace: {object_name.replace('PSR', 'Pulsar')}\")\n\n","type":"content","url":"/chapter1-python-calculator#string-basics","position":33},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl3":"F-strings for Science","lvl2":"Strings & Scientific Formatting"},"type":"lvl3","url":"/chapter1-python-calculator#f-strings-for-science","position":34},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl3":"F-strings for Science","lvl2":"Strings & Scientific Formatting"},"content":"F-strings (formatted string literals) are Python’s best formatting tool:\n\n# Basic f-string formatting\nstar_name = \"Betelgeuse\"\ntemperature = 3500  # Kelvin\nradius = 887  # Solar radii\n\nprint(f\"{star_name}: T = {temperature} K, R = {radius} R☉\")\n\n# Controlling decimal places\npi = 3.14159265359\nprint(f\"π to 2 decimals: {pi:.2f}\")\nprint(f\"π to 5 decimals: {pi:.5f}\")\n\n# Scientific notation\navogadro = 6.02214076e23\nprint(f\"Avogadro's number: {avogadro:.3e}\")\nprint(f\"Also written as: {avogadro:.3E}\")\n\n","type":"content","url":"/chapter1-python-calculator#f-strings-for-science","position":35},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl3":"Formatting Numbers","lvl2":"Strings & Scientific Formatting"},"type":"lvl3","url":"/chapter1-python-calculator#formatting-numbers","position":36},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl3":"Formatting Numbers","lvl2":"Strings & Scientific Formatting"},"content":"Advanced formatting for scientific output:\n\n# Alignment and padding\ndata = [\n    (\"Sirius A\", 1.711, 9940),\n    (\"Vega\", 2.135, 9602),\n    (\"Arcturus\", 1.08, 4286)\n]\n\nprint(\"Star Name    Mass (M☉)  Temp (K)\")\nprint(\"-\" * 35)\nfor name, mass, temp in data:\n    print(f\"{name:<12} {mass:>9.3f}  {temp:>8d}\")\n\n# Percentage formatting\ndetection_efficiency = 0.8765\nprint(f\"Detection efficiency: {detection_efficiency:.1%}\")\n\n# Adding thousand separators\ngalaxy_count = 2000000000000\nprint(f\"Observable galaxies: {galaxy_count:,}\")\nprint(f\"In scientific notation: {galaxy_count:.2e}\")\n\nFormat the Output\n\nCreate a nicely formatted table of planetary data:\n\nPlanet name (left-aligned, 10 characters)\n\nMass (in Earth masses, 2 decimals, right-aligned)\n\nOrbital period (in days, 1 decimal)\n\nData: [(“Mercury”, 0.055, 87.969), (“Venus”, 0.815, 224.701), (“Earth”, 1.0, 365.256)]```python\nplanets = [\n    (\"Mercury\", 0.055, 87.969),\n    (\"Venus\", 0.815, 224.701),\n    (\"Earth\", 1.0, 365.256)\n]\n\nprint(f\"{'Planet':<10} {'Mass (M⊕)':>10} {'Period (days)':>13}\")\nprint(\"-\" * 35)\nfor name, mass, period in planets:\n    print(f\"{name:<10} {mass:>10.2f} {period:>13.1f}\")\n\nOutput:Planet      Mass (M⊕)  Period (days)\n-----------------------------------\nMercury          0.06          88.0\nVenus            0.82         224.7\nEarth            1.00         365.3\n---\n\n## Type Conversion & Checking\n\nConverting between types is common when processing data:\n\n```{code-cell} ipython3\n# Type checking\nvalue = 42\nprint(f\"Is {value} an integer? {isinstance(value, int)}\")\nprint(f\"Is {value} a float? {isinstance(value, float)}\")\nprint(f\"Is {value} a number? {isinstance(value, (int, float))}\")\n\n# Explicit conversion\nuser_input = \"123.45\"  # Simulating input\nnumber = float(user_input)\nprint(f\"String '{user_input}' converted to float: {number}\")\n\n# Integer conversion truncates!\npi = 3.14159\nprint(f\"int({pi}) = {int(pi)}\")  # Doesn't round!\n\n# Rounding properly\nprint(f\"round({pi}) = {round(pi)}\")\nprint(f\"round({pi}, 2) = {round(pi, 2)}\")\n\nCommon Conversion Pitfalls\n\nint() truncates, doesn’t round - use round() first if needed\n\nfloat('inf') and float('nan') are valid!\n\nConverting to int can overflow in other languages, but Python handles arbitrary precision\n\n# Special float values\nimport math\n\ninfinity = float('inf')\nnot_a_number = float('nan')\n\nprint(f\"Infinity > 1e308? {infinity > 1e308}\")\nprint(f\"NaN == NaN? {not_a_number == not_a_number}\")  # Always False!\nprint(f\"Check for NaN: {math.isnan(not_a_number)}\")\nprint(f\"Check for infinity: {math.isinf(infinity)}\")\n\n","type":"content","url":"/chapter1-python-calculator#formatting-numbers","position":37},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl2":"Exercises"},"type":"lvl2","url":"/chapter1-python-calculator#exercises","position":38},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl2":"Exercises"},"content":"","type":"content","url":"/chapter1-python-calculator#exercises","position":39},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl3":"Exercise 1: Stellar Magnitude Calculator","lvl2":"Exercises"},"type":"lvl3","url":"/chapter1-python-calculator#exercise-1-stellar-magnitude-calculator","position":40},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl3":"Exercise 1: Stellar Magnitude Calculator","lvl2":"Exercises"},"content":"Magnitude and Flux\n\nThe magnitude system in astronomy uses a logarithmic scale. The relationship between two stars’ magnitudes and fluxes is:m_1 - m_2 = -2.5 \\log_{10}(F_1/F_2)\n\nCalculate the flux ratio between a magnitude 1 star and a magnitude 6 star\n\nIf Star A is 2.5 magnitudes brighter than Star B, what’s their flux ratio?\n\nWhy do we use this “backwards” system where brighter objects have smaller numbers?","type":"content","url":"/chapter1-python-calculator#exercise-1-stellar-magnitude-calculator","position":41},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl3":"Exercise 2: Floating Point Detective","lvl2":"Exercises"},"type":"lvl3","url":"/chapter1-python-calculator#exercise-2-floating-point-detective","position":42},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl3":"Exercise 2: Floating Point Detective","lvl2":"Exercises"},"content":"Finding the Limits\n\nWrite code to determine:\n\nThe smallest positive float Python can represent\n\nThe largest integer that can be stored exactly in a float\n\nWhat happens when you exceed these limits?\n\nHint: Try powers of 2, and remember that floats use 53 bits for the mantissa.","type":"content","url":"/chapter1-python-calculator#exercise-2-floating-point-detective","position":43},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl3":"Exercise 3: Scientific Constants Library","lvl2":"Exercises"},"type":"lvl3","url":"/chapter1-python-calculator#exercise-3-scientific-constants-library","position":44},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl3":"Exercise 3: Scientific Constants Library","lvl2":"Exercises"},"content":"Build Your Constants Module\n\nCreate variables for these astronomical constants with appropriate names:\n\nSpeed of light (c)\n\nGravitational constant (G)\n\nSolar luminosity (L☉)\n\nEarth mass (M⊕)\n\nHubble constant (H₀)\n\nThen calculate:\n\nThe Schwarzschild radius of Earth\n\nThe critical density of the universe\n\nHow many Earth masses equal one Solar mass","type":"content","url":"/chapter1-python-calculator#exercise-3-scientific-constants-library","position":45},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl2":"Key Takeaways"},"type":"lvl2","url":"/chapter1-python-calculator#key-takeaways","position":46},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl2":"Key Takeaways"},"content":"Chapter 1 Summary\n\n✅ Python as Calculator: Python handles basic math, scientific notation, and complex numbers naturally\n\n✅ Floating-Point Reality: All calculations have limited precision (~15-17 decimal digits)\n\n✅ Never Compare Floats with ==: Use math.isclose() or check if difference < tolerance\n\n✅ Variable Names Matter: stellar_temperature_kelvin beats temp every time\n\n✅ F-strings for Formatting: f\"{value:.3e}\" gives you control over scientific output\n\n✅ Type Conversion: Be explicit and careful, especially with user input\n\n✅ Constants Convention: USE_CAPS for values that shouldn’t change\n\nNext Chapter Preview\n\nChapter 2: Control Flow & Logic - Teaching your computer to make decisions based on data","type":"content","url":"/chapter1-python-calculator#key-takeaways","position":47},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl2":"Quick Reference Card"},"type":"lvl2","url":"/chapter1-python-calculator#quick-reference-card","position":48},{"hierarchy":{"lvl1":"Chapter 1: Python as a Scientific Calculator","lvl2":"Quick Reference Card"},"content":"# Numbers\ninteger = 42\nfloating = 3.14159\nscientific = 6.626e-34\ncomplex_num = 3 + 4j\n\n# Operations\nx ** y  # Power\nx // y  # Integer division\nx % y   # Remainder/modulo\n\n# Formatting\nf\"{value:.3f}\"   # 3 decimal places\nf\"{value:.2e}\"   # Scientific notation\nf\"{value:>10}\"   # Right-align, width 10\nf\"{value:,}\"     # Thousands separator\n\n# Type Conversion\nint(3.14)        # → 3 (truncates!)\nfloat(\"3.14\")    # → 3.14\nround(3.14159, 2)  # → 3.14\n\n# Floating-point comparison\nimport math\nmath.isclose(a, b, rel_tol=1e-9)","type":"content","url":"/chapter1-python-calculator#quick-reference-card","position":49},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think"},"type":"lvl1","url":"/chapter2-control-flow","position":0},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think"},"content":"“Logic is the beginning of wisdom, not the end.” - Spock","type":"content","url":"/chapter2-control-flow","position":1},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl2":"Table of Contents"},"type":"lvl2","url":"/chapter2-control-flow#table-of-contents","position":2},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl2":"Table of Contents"},"content":"Learning Objectives\n\nWhy Logic Matters\n\nFrom Philosophy to Circuits\n\nBoolean Algebra: The Mathematics of Decision\n\nTruth and Conditions\n\nComparison Operators\n\nBoolean Values and Truthiness\n\nLogical Operators\n\nIf-Then Logic: Making Decisions\n\nBasic If Statements\n\nIf-Elif-Else Chains\n\nGuard Clauses vs Nested Ifs\n\nLoops: Repetition with Purpose\n\nFor Loops - When You Know How Many\n\nWhile Loops - Until a Condition\n\nLoop Control: Break, Continue, Else\n\nComprehensions: Elegant Iteration\n\nList Comprehensions\n\nWhen to Use Comprehensions\n\nCommon Patterns in Scientific Computing\n\nExercises\n\nKey Takeaways","type":"content","url":"/chapter2-control-flow#table-of-contents","position":3},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl2":"Learning Objectives"},"type":"lvl2","url":"/chapter2-control-flow#learning-objectives","position":4},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl2":"Learning Objectives"},"content":"By the end of this chapter you will:\n\nUnderstand Boolean logic and its connection to mathematics and philosophy\n\nWrite conditional statements that make intelligent decisions\n\nUse loops to automate repetitive calculations\n\nRecognize when to use different loop patterns\n\nApply logical thinking to solve astronomical problems\n\nDebug logical errors in program flow","type":"content","url":"/chapter2-control-flow#learning-objectives","position":5},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl2":"Why Logic Matters"},"type":"lvl2","url":"/chapter2-control-flow#why-logic-matters","position":6},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl2":"Why Logic Matters"},"content":"","type":"content","url":"/chapter2-control-flow#why-logic-matters","position":7},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl3":"From Philosophy to Circuits","lvl2":"Why Logic Matters"},"type":"lvl3","url":"/chapter2-control-flow#from-philosophy-to-circuits","position":8},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl3":"From Philosophy to Circuits","lvl2":"Why Logic Matters"},"content":"Logic isn’t just programming—it’s the foundation of rational thought, mathematics, and computation itself.\n\n# Logic has a rich history\nprint(\"Aristotle (384 BCE): Syllogistic logic - All stars are suns, Proxima is a star, therefore...\")\nprint(\"Boole (1854): Boolean algebra - True/False as 1/0\")\nprint(\"Shannon (1937): Logic gates in circuits\")\nprint(\"Today: Every if-statement in your code\")\n\n# It all reduces to True and False\nprint(f\"\\nIn Python: True = {int(True)}, False = {int(False)}\")\nprint(f\"This is why: True + True + False = {True + True + False}\")\n\nThe Philosophical Connection\n\nLogical reasoning forms the basis of the scientific method:\n\nDeduction: If all stars fuse hydrogen (premise) and the Sun is a star (premise), then the Sun fuses hydrogen (conclusion)\n\nInduction: We’ve observed 1000 pulsars rotating rapidly, therefore all pulsars probably rotate rapidly\n\nAbduction: The CMB has these properties, the best explanation is the Big Bang\n\nYour code embodies logical reasoning!","type":"content","url":"/chapter2-control-flow#from-philosophy-to-circuits","position":9},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl3":"Boolean Algebra: The Mathematics of Decision","lvl2":"Why Logic Matters"},"type":"lvl3","url":"/chapter2-control-flow#boolean-algebra-the-mathematics-of-decision","position":10},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl3":"Boolean Algebra: The Mathematics of Decision","lvl2":"Why Logic Matters"},"content":"Before computers, George Boole showed that logic follows mathematical rules:\n\n# Boolean algebra in action\nA = True\nB = False\n\n# Basic operations (same as logic gates in circuits!)\nprint(f\"NOT A = {not A}\")\nprint(f\"A AND B = {A and B}\")\nprint(f\"A OR B = {A or B}\")\nprint(f\"A XOR B = {A != B}\")  # Exclusive OR\n\n# De Morgan's Laws - fundamental to logic\nprint(\"\\nDe Morgan's Laws:\")\nprint(f\"not (A and B) = (not A) or (not B): {not (A and B) == (not A) or (not B)}\")\nprint(f\"not (A or B) = (not A) and (not B): {not (A or B) == (not A) and (not B)}\")\n\n","type":"content","url":"/chapter2-control-flow#boolean-algebra-the-mathematics-of-decision","position":11},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl2":"Truth and Conditions"},"type":"lvl2","url":"/chapter2-control-flow#truth-and-conditions","position":12},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl2":"Truth and Conditions"},"content":"","type":"content","url":"/chapter2-control-flow#truth-and-conditions","position":13},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl3":"Comparison Operators","lvl2":"Truth and Conditions"},"type":"lvl3","url":"/chapter2-control-flow#comparison-operators","position":14},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl3":"Comparison Operators","lvl2":"Truth and Conditions"},"content":"Every decision starts with a comparison:\n\n# Astronomical example: Is this a habitable zone planet?\nstar_luminosity = 0.5  # Solar units\nplanet_distance = 0.7  # AU\ninner_habitable = 0.95 * (star_luminosity ** 0.5)\nouter_habitable = 1.37 * (star_luminosity ** 0.5)\n\nprint(f\"Star luminosity: {star_luminosity} L☉\")\nprint(f\"Planet distance: {planet_distance} AU\")\nprint(f\"Habitable zone: {inner_habitable:.2f} - {outer_habitable:.2f} AU\")\nprint()\n\n# All comparison operators\nprint(f\"Distance > inner edge? {planet_distance > inner_habitable}\")\nprint(f\"Distance < outer edge? {planet_distance < outer_habitable}\")\nprint(f\"Distance >= inner? {planet_distance >= inner_habitable}\")\nprint(f\"Distance <= outer? {planet_distance <= outer_habitable}\")\nprint(f\"Exactly at inner edge? {planet_distance == inner_habitable}\")\nprint(f\"Not at inner edge? {planet_distance != inner_habitable}\")\n\nFloating Point Comparisons (Again!)\n\nRemember from Chapter 1: Never use == with floats!# WRONG\nif orbit_period == 365.25:\n    \n# RIGHT\nif abs(orbit_period - 365.25) < 0.01:\n### Boolean Values and Truthiness\n\nPython has a broader concept of \"truth\" than just True/False:\n\n```{code-cell} ipython3\n# What's considered True or False?\nvalues_to_test = [\n    True, False,           # Booleans\n    1, 0, -1,             # Numbers\n    \"\", \"hello\",          # Strings\n    [], [1, 2, 3],        # Lists\n    None,                 # Special null value\n    0.0, 0.000001,        # Floats\n]\n\nprint(\"Value\".ljust(15), \"Bool\".ljust(8), \"Type\")\nprint(\"-\" * 35)\nfor value in values_to_test:\n    print(f\"{str(value):15} {bool(value)!s:8} {type(value).__name__}\")\n\nThe Truthiness Rule\n\nFalsy values: False, 0, 0.0, \"\", [], {}, None\nEverything else is Truthy!\n\nThis enables elegant code:# Instead of: if len(observations) > 0:\nif observations:  # Empty list is False!\n    process(observations)\n### Logical Operators\n\nCombine conditions to express complex logic:\n\n```{code-cell} ipython3\n# Stellar classification logic\ntemperature = 5800  # Kelvin\nluminosity = 1.0    # Solar units\nmass = 1.0          # Solar masses\n\n# Complex conditions\nis_main_sequence = 0.08 < mass < 150  # Stars have mass limits\nis_sun_like = 5300 < temperature < 6000 and 0.8 < luminosity < 1.2\nis_giant = luminosity > 10 and temperature < 5000\nis_white_dwarf = luminosity < 0.01 and temperature > 8000\n\nprint(f\"Temperature: {temperature}K, Luminosity: {luminosity}L☉, Mass: {mass}M☉\")\nprint(f\"Main sequence? {is_main_sequence}\")\nprint(f\"Sun-like? {is_sun_like}\")\nprint(f\"Giant? {is_giant}\")\nprint(f\"White dwarf? {is_white_dwarf}\")\n\n# Operator precedence (like math!)\n# not > and > or\nresult = True or False and False  # What's this?\nprint(f\"\\nTrue or False and False = {result}\")\nprint(\"Because 'and' binds tighter than 'or': True or (False and False)\")\n\nShort-Circuit Evaluation\n\nPython stops evaluating as soon as it knows the answer. Why do these matter?# This is safe even if divisor is 0\nif divisor != 0 and value/divisor > 10:\n    print(\"Large ratio\")\n\n# This would crash!\nif value/divisor > 10 and divisor != 0:\n    print(\"Large ratio\")\n\nWhen might this be useful in astronomy code?\n```{solution}\n:class: dropdown\n\nShort-circuit evaluation is crucial for:\n\n1. **Avoiding division by zero**:\n```python\nif parallax != 0 and 1/parallax < 100:  # Safe!\n    print(\"Nearby star\")\n\nChecking existence before access:if spectrum is not None and spectrum.max() > threshold:\n    print(\"Bright source\")\n\nPerformance - expensive operations last:if quick_check() and expensive_calculation():\n    process()\n---\n\n## If-Then Logic: Making Decisions\n\n### Basic If Statements\n\nThe fundamental decision structure:\n\n```{code-cell} ipython3\ndef classify_star(temperature):\n    \"\"\"Classify star by temperature using Harvard spectral classification.\"\"\"\n    \n    spectral_class = \"Unknown\"\n    \n    if temperature > 30000:\n        spectral_class = \"O\"  # Blue\n    elif temperature > 10000:\n        spectral_class = \"B\"  # Blue-white\n    elif temperature > 7500:\n        spectral_class = \"A\"  # White\n    elif temperature > 6000:\n        spectral_class = \"F\"  # Yellow-white\n    elif temperature > 5200:\n        spectral_class = \"G\"  # Yellow (Sun)\n    elif temperature > 3700:\n        spectral_class = \"K\"  # Orange\n    elif temperature > 2400:\n        spectral_class = \"M\"  # Red\n    else:\n        spectral_class = \"L/T/Y\"  # Brown dwarfs\n    \n    return spectral_class\n\n# Test the classifier\ntest_temps = [40000, 9700, 5778, 3500, 1000]\nfor temp in test_temps:\n    print(f\"{temp:5}K -> Class {classify_star(temp)}\")","type":"content","url":"/chapter2-control-flow#comparison-operators","position":15},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl3":"If-Elif-Else Chains","lvl2":"Truth and Conditions"},"type":"lvl3","url":"/chapter2-control-flow#if-elif-else-chains","position":16},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl3":"If-Elif-Else Chains","lvl2":"Truth and Conditions"},"content":"Order matters in elif chains!\n\ndef determine_evolution_stage(mass, luminosity, temperature):\n    \"\"\"Determine stellar evolution stage from observable parameters.\"\"\"\n    \n    # Check in order of likelihood/importance\n    if luminosity < 0.01 and temperature > 10000:\n        return \"White Dwarf\"\n    elif luminosity > 1000:\n        return \"Supergiant\"\n    elif luminosity > 100 and temperature < 4500:\n        return \"Red Giant\"\n    elif luminosity > 100:\n        return \"Blue Giant\"\n    elif 0.08 < mass < 0.5 and luminosity < 0.08:\n        return \"Red Dwarf\"\n    elif abs(luminosity - mass**3.5) < 0.5 * mass**3.5:  # Within 50% of main sequence\n        return \"Main Sequence\"\n    else:\n        return \"Peculiar/Variable\"\n\n# Test cases\nstars = [\n    (1.0, 1.0, 5778),      # Sun\n    (0.1, 0.001, 3000),    # Red dwarf\n    (10, 10000, 20000),    # Supergiant\n    (0.6, 0.0001, 15000),  # White dwarf\n]\n\nfor mass, lum, temp in stars:\n    stage = determine_evolution_stage(mass, lum, temp)\n    print(f\"M={mass:4.1f}M☉, L={lum:7.4f}L☉, T={temp:5}K -> {stage}\")\n\n","type":"content","url":"/chapter2-control-flow#if-elif-else-chains","position":17},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl3":"Guard Clauses vs Nested Ifs","lvl2":"Truth and Conditions"},"type":"lvl3","url":"/chapter2-control-flow#guard-clauses-vs-nested-ifs","position":18},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl3":"Guard Clauses vs Nested Ifs","lvl2":"Truth and Conditions"},"content":"Write cleaner code with guard clauses:\n\n# ❌ Nested approach - hard to follow\ndef process_observation_nested(data):\n    if data is not None:\n        if len(data) > 0:\n            if data.max() > 0:\n                if data.min() < 1000:\n                    # Finally do the work!\n                    return data.mean()\n                else:\n                    return \"Signal too strong\"\n            else:\n                return \"No positive values\"\n        else:\n            return \"Empty dataset\"\n    else:\n        return \"No data\"\n\n# ✅ Guard clause approach - much cleaner!\ndef process_observation_clean(data):\n    # Handle error cases first and exit early\n    if data is None:\n        return \"No data\"\n    if len(data) == 0:\n        return \"Empty dataset\"\n    if data.max() <= 0:\n        return \"No positive values\"\n    if data.min() >= 1000:\n        return \"Signal too strong\"\n    \n    # Main logic is unindented and clear\n    return data.mean()\n\n# Both give same results, but one is much more readable!\n\nCode Philosophy: Fail Fast\n\nGuard clauses embody the “fail fast” principle:\n\nCheck for error conditions first\n\nReturn/exit immediately if something’s wrong\n\nKeep the “happy path” unindented and clear\n\nThis matches how we think: “If this is wrong, stop. If that’s wrong, stop. Otherwise, proceed.”","type":"content","url":"/chapter2-control-flow#guard-clauses-vs-nested-ifs","position":19},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl2":"Loops: Repetition with Purpose"},"type":"lvl2","url":"/chapter2-control-flow#loops-repetition-with-purpose","position":20},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl2":"Loops: Repetition with Purpose"},"content":"","type":"content","url":"/chapter2-control-flow#loops-repetition-with-purpose","position":21},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl3":"For Loops - When You Know How Many","lvl2":"Loops: Repetition with Purpose"},"type":"lvl3","url":"/chapter2-control-flow#for-loops-when-you-know-how-many","position":22},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl3":"For Loops - When You Know How Many","lvl2":"Loops: Repetition with Purpose"},"content":"For loops are perfect when you know the iterations in advance:\n\n# Classic for loop with range\nprint(\"Counting photons in bins:\")\nfor bin_number in range(5):\n    photon_count = 100 + bin_number * 50  # Simulated data\n    print(f\"Bin {bin_number}: {photon_count} photons\")\n\nprint(\"\\n\" + \"=\"*40 + \"\\n\")\n\n# Iterating over data directly\nwavelengths = [656.3, 486.1, 434.0, 410.2]  # Hydrogen Balmer series\nprint(\"Balmer series wavelengths (nm):\")\nfor wavelength in wavelengths:\n    print(f\"  λ = {wavelength} nm\")\n\nprint(\"\\n\" + \"=\"*40 + \"\\n\")\n\n# Enumerate when you need index AND value\nelements = [\"Hydrogen\", \"Helium\", \"Carbon\", \"Oxygen\"]\nprint(\"Cosmic abundances (by number):\")\nabundances = [0.92, 0.078, 0.0003, 0.0005]\nfor i, element in enumerate(elements):\n    print(f\"  {i+1}. {element}: {abundances[i]:.4%}\")\n\n# Range variations\nprint(\"range(5):\", list(range(5)))            # 0 to 4\nprint(\"range(2, 8):\", list(range(2, 8)))      # 2 to 7\nprint(\"range(0, 10, 2):\", list(range(0, 10, 2)))  # Even numbers\nprint(\"range(10, 0, -1):\", list(range(10, 0, -1)))  # Countdown\n\n# Practical example: Observing schedule\nprint(\"\\nObservation schedule (hours from midnight):\")\nfor hour in range(20, 28, 2):  # 8pm to 4am, every 2 hours\n    actual_hour = hour if hour < 24 else hour - 24\n    am_pm = \"PM\" if 12 <= hour < 24 else \"AM\"\n    display_hour = actual_hour if actual_hour <= 12 else actual_hour - 12\n    if display_hour == 0:\n        display_hour = 12\n    print(f\"  Observation at {display_hour:2d}:00 {am_pm}\")\n\n","type":"content","url":"/chapter2-control-flow#for-loops-when-you-know-how-many","position":23},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl3":"While Loops - Until a Condition","lvl2":"Loops: Repetition with Purpose"},"type":"lvl3","url":"/chapter2-control-flow#while-loops-until-a-condition","position":24},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl3":"While Loops - Until a Condition","lvl2":"Loops: Repetition with Purpose"},"content":"While loops continue until a condition becomes false:\n\n# Newton's method for finding square roots\ndef sqrt_newton(n, tolerance=1e-10):\n    \"\"\"Calculate square root using Newton's method.\"\"\"\n    if n < 0:\n        return None\n    \n    guess = n / 2  # Initial guess\n    iterations = 0\n    \n    while abs(guess * guess - n) > tolerance:\n        guess = (guess + n/guess) / 2  # Newton's formula\n        iterations += 1\n        \n        # Safety check\n        if iterations > 100:\n            print(\"Warning: Max iterations reached\")\n            break\n    \n    return guess, iterations\n\n# Test it\nimport math\nnumber = 2.0\nmy_sqrt, iters = sqrt_newton(number)\nprint(f\"Newton's sqrt({number}): {my_sqrt} after {iters} iterations\")\nprint(f\"Python's sqrt({number}): {math.sqrt(number)}\")\nprint(f\"Difference: {abs(my_sqrt - math.sqrt(number)):.2e}\")\n\nInfinite Loop Danger!\n\nAlways ensure your while loop condition will eventually become False!# DANGER: This runs forever!\nwhile True:\n    print(\"Help, I'm stuck!\")\n    \n# SAFE: Always have an exit strategy\nmax_iterations = 1000\ncount = 0\nwhile condition and count < max_iterations:\n    # do work\n    count += 1\n### Loop Control: Break, Continue, Else\n\nFine-tune loop behavior:\n\n```{code-cell} ipython3\n# Break: Exit loop early\ndef find_first_giant_planet(planets):\n    \"\"\"Find first planet with mass > 50 Earth masses.\"\"\"\n    for i, planet in enumerate(planets):\n        if planet['mass'] > 50:\n            print(f\"Found giant planet at index {i}: {planet['name']}\")\n            break\n    else:  # This runs if loop completes without break!\n        print(\"No giant planets found\")\n\nplanets = [\n    {'name': 'Kepler-452b', 'mass': 5},\n    {'name': 'HD 209458 b', 'mass': 220},\n    {'name': 'Proxima b', 'mass': 1.3}\n]\n\nfind_first_giant_planet(planets)\n\nprint(\"\\n\" + \"=\"*40 + \"\\n\")\n\n# Continue: Skip to next iteration\nprint(\"Processing observations (skipping bad data):\")\nobservations = [100, -5, 200, 0, 150, -999, 300]\n\nfor obs in observations:\n    if obs <= 0:  # Bad data\n        print(f\"  Skipping invalid value: {obs}\")\n        continue\n    \n    # Process valid data\n    magnitude = -2.5 * math.log10(obs/100)\n    print(f\"  Flux: {obs:3d} -> Magnitude: {magnitude:+5.2f}\")\n\nThe Mysterious Loop-Else\n\nPython’s else clause on loops is unique:\n\nExecutes if loop completes normally (no break)\n\nUseful for search patterns: “Find X, else report not found”\n\nWorks with both for and while loops","type":"content","url":"/chapter2-control-flow#while-loops-until-a-condition","position":25},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl2":"Comprehensions: Elegant Iteration"},"type":"lvl2","url":"/chapter2-control-flow#comprehensions-elegant-iteration","position":26},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl2":"Comprehensions: Elegant Iteration"},"content":"","type":"content","url":"/chapter2-control-flow#comprehensions-elegant-iteration","position":27},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl3":"List Comprehensions","lvl2":"Comprehensions: Elegant Iteration"},"type":"lvl3","url":"/chapter2-control-flow#list-comprehensions","position":28},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl3":"List Comprehensions","lvl2":"Comprehensions: Elegant Iteration"},"content":"Transform loops into concise expressions:\n\n# Traditional loop approach\nmagnitudes = [5.2, 3.1, 6.8, 4.5, 2.3]\nfluxes_loop = []\nfor mag in magnitudes:\n    flux = 10**(-0.4 * mag)\n    fluxes_loop.append(flux)\n\n# List comprehension - same result, one line!\nfluxes_comp = [10**(-0.4 * mag) for mag in magnitudes]\n\nprint(\"Traditional loop result:\", len(fluxes_loop), \"values\")\nprint(\"Comprehension result:\", len(fluxes_comp), \"values\")\nprint(f\"Results identical? {fluxes_loop == fluxes_comp}\")\n\nprint(\"\\n\" + \"=\"*40 + \"\\n\")\n\n# Comprehensions with conditions\nall_stars = [\n    {'name': 'Sirius', 'mag': -1.46, 'type': 'A'},\n    {'name': 'Betelgeuse', 'mag': 0.42, 'type': 'M'},\n    {'name': 'Rigel', 'mag': 0.13, 'type': 'B'},\n    {'name': 'Aldebaran', 'mag': 0.85, 'type': 'K'},\n    {'name': 'Vega', 'mag': 0.03, 'type': 'A'},\n]\n\n# Get bright stars (mag < 0.5)\nbright_stars = [star['name'] for star in all_stars if star['mag'] < 0.5]\nprint(f\"Bright stars: {bright_stars}\")\n\n# Get blue stars with their magnitudes\nblue_stars = [(s['name'], s['mag']) for s in all_stars if s['type'] in ['O', 'B', 'A']]\nprint(f\"Blue stars: {blue_stars}\")\n\n","type":"content","url":"/chapter2-control-flow#list-comprehensions","position":29},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl3":"When to Use Comprehensions","lvl2":"Comprehensions: Elegant Iteration"},"type":"lvl3","url":"/chapter2-control-flow#when-to-use-comprehensions","position":30},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl3":"When to Use Comprehensions","lvl2":"Comprehensions: Elegant Iteration"},"content":"\n\n# ✅ GOOD: Simple transformation\n# Clear and concise\nsquares = [x**2 for x in range(10)]\n\n# ✅ GOOD: Filtering with simple condition\npositives = [x for x in data if x > 0]\n\n# ❌ BAD: Too complex - use regular loop\n# Hard to read and debug\n# result = [process(x) if complex_condition(x) and other_check(x) \n#          else alternate_process(x) for x in data if pre_filter(x)]\n\n# ❌ BAD: Side effects - comprehensions shouldn't print or modify external state\n# Don't do this:\n# [print(x) for x in range(10)]  # Use regular loop instead\n\n# ✅ GOOD: Nested comprehensions for matrices\n# Create a 3x3 identity matrix\nidentity = [[1 if i == j else 0 for j in range(3)] for i in range(3)]\nprint(\"\\nIdentity matrix:\")\nfor row in identity:\n    print(row)\n\nComprehension Philosophy\n\nUse comprehensions when:\n\nThe operation is simple and clear\n\nYou’re building a new list from an existing iterable\n\nThe logic fits comfortably on 1-2 lines\n\nUse regular loops when:\n\nLogic is complex or multi-step\n\nYou need to break or handle errors\n\nYou’re not building a list (just processing)","type":"content","url":"/chapter2-control-flow#when-to-use-comprehensions","position":31},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl2":"Common Patterns in Scientific Computing"},"type":"lvl2","url":"/chapter2-control-flow#common-patterns-in-scientific-computing","position":32},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl2":"Common Patterns in Scientific Computing"},"content":"","type":"content","url":"/chapter2-control-flow#common-patterns-in-scientific-computing","position":33},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl3":"Pattern 1: Accumulator","lvl2":"Common Patterns in Scientific Computing"},"type":"lvl3","url":"/chapter2-control-flow#pattern-1-accumulator","position":34},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl3":"Pattern 1: Accumulator","lvl2":"Common Patterns in Scientific Computing"},"content":"\n\n# Sum pattern\ndef calculate_total_luminosity(star_luminosities):\n    \"\"\"Sum luminosities of star cluster.\"\"\"\n    total = 0  # Initialize accumulator\n    for luminosity in star_luminosities:\n        total += luminosity  # Accumulate\n    return total\n\n# Product pattern  \ndef calculate_probability_all_detect(individual_probs):\n    \"\"\"Probability that ALL telescopes detect the source.\"\"\"\n    combined = 1  # Initialize for product\n    for prob in individual_probs:\n        combined *= prob  # Accumulate via multiplication\n    return combined\n\ncluster = [1.0, 0.5, 0.3, 2.1, 0.8]  # Solar luminosities\nprint(f\"Total cluster luminosity: {calculate_total_luminosity(cluster):.1f} L☉\")\n\ndetection_probs = [0.9, 0.85, 0.95]\nprint(f\"Combined detection probability: {calculate_probability_all_detect(detection_probs):.3f}\")\n\n","type":"content","url":"/chapter2-control-flow#pattern-1-accumulator","position":35},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl3":"Pattern 2: Search/Filter","lvl2":"Common Patterns in Scientific Computing"},"type":"lvl3","url":"/chapter2-control-flow#pattern-2-search-filter","position":36},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl3":"Pattern 2: Search/Filter","lvl2":"Common Patterns in Scientific Computing"},"content":"\n\ndef find_habitable_planets(planets):\n    \"\"\"Find all planets in the habitable zone.\"\"\"\n    habitable = []\n    \n    for planet in planets:\n        # Calculate habitable zone for planet's star\n        inner = 0.95 * math.sqrt(planet['star_luminosity'])\n        outer = 1.37 * math.sqrt(planet['star_luminosity'])\n        \n        # Check if planet is in zone\n        if inner <= planet['distance'] <= outer:\n            habitable.append(planet)\n    \n    return habitable\n\n# Example exoplanet data\nexoplanets = [\n    {'name': 'Proxima b', 'distance': 0.05, 'star_luminosity': 0.0017},\n    {'name': 'Kepler-452b', 'distance': 1.05, 'star_luminosity': 1.2},\n    {'name': 'TRAPPIST-1e', 'distance': 0.029, 'star_luminosity': 0.000524},\n]\n\nhabitable = find_habitable_planets(exoplanets)\nprint(f\"Potentially habitable: {[p['name'] for p in habitable]}\")\n\n","type":"content","url":"/chapter2-control-flow#pattern-2-search-filter","position":37},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl3":"Pattern 3: Convergence","lvl2":"Common Patterns in Scientific Computing"},"type":"lvl3","url":"/chapter2-control-flow#pattern-3-convergence","position":38},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl3":"Pattern 3: Convergence","lvl2":"Common Patterns in Scientific Computing"},"content":"\n\ndef calculate_pi_leibniz(tolerance=1e-6):\n    \"\"\"Calculate π using Leibniz formula until convergence.\"\"\"\n    # π/4 = 1 - 1/3 + 1/5 - 1/7 + ...\n    \n    pi_estimate = 0\n    term_number = 0\n    \n    while True:\n        term = (-1)**term_number / (2*term_number + 1)\n        pi_estimate += term\n        \n        # Check convergence\n        if abs(term) < tolerance:\n            break\n            \n        term_number += 1\n    \n    return 4 * pi_estimate, term_number\n\npi_approx, iterations = calculate_pi_leibniz()\nprint(f\"π ≈ {pi_approx:.8f} after {iterations} iterations\")\nprint(f\"Error: {abs(pi_approx - math.pi):.2e}\")\n\n","type":"content","url":"/chapter2-control-flow#pattern-3-convergence","position":39},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl2":"Exercises"},"type":"lvl2","url":"/chapter2-control-flow#exercises","position":40},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl2":"Exercises"},"content":"","type":"content","url":"/chapter2-control-flow#exercises","position":41},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl3":"Exercise 1: Logical Thinking","lvl2":"Exercises"},"type":"lvl3","url":"/chapter2-control-flow#exercise-1-logical-thinking","position":42},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl3":"Exercise 1: Logical Thinking","lvl2":"Exercises"},"content":"Stellar Classification Logic\n\nWrite a function that takes temperature, luminosity, and mass as inputs and returns:\n\n“Impossible” if the star violates basic physics (e.g., luminosity > 10^6 * mass^3.5)\n\n“White Dwarf” if high temp, low luminosity\n\n“Main Sequence” if it follows L ∝ M^3.5 approximately\n\n“Giant” if luminosity is much higher than expected for its mass\n\n“Not classified” otherwise\n\nTest with:\n\nSun: T=5778K, L=1.0, M=1.0\n\nSirius B: T=25000K, L=0.026, M=1.0\n\nBetelgeuse: T=3500K, L=100000, M=20","type":"content","url":"/chapter2-control-flow#exercise-1-logical-thinking","position":43},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl3":"Exercise 2: Prime Number Sieve","lvl2":"Exercises"},"type":"lvl3","url":"/chapter2-control-flow#exercise-2-prime-number-sieve","position":44},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl3":"Exercise 2: Prime Number Sieve","lvl2":"Exercises"},"content":"Sieve of Eratosthenes\n\nImplement the ancient algorithm for finding prime numbers:\n\nCreate a list of numbers from 2 to n\n\nStart with the first number (2)\n\nMark all its multiples as composite\n\nMove to the next unmarked number\n\nRepeat until you’ve processed all numbers\n\nFind all primes less than 100. How many are there?\n\nBonus: Why might astronomers care about prime numbers? (Hint: think about periodic signals and aliases)","type":"content","url":"/chapter2-control-flow#exercise-2-prime-number-sieve","position":45},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl3":"Exercise 3: Monte Carlo Integration","lvl2":"Exercises"},"type":"lvl3","url":"/chapter2-control-flow#exercise-3-monte-carlo-integration","position":46},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl3":"Exercise 3: Monte Carlo Integration","lvl2":"Exercises"},"content":"Escape Velocity Distribution\n\nA globular cluster has stars with random velocities. Use a loop to:\n\nGenerate 1000 random velocities (Gaussian distribution, mean=10 km/s, std=3 km/s)\n\nCount how many exceed the cluster’s escape velocity (15 km/s)\n\nCalculate what fraction will escape\n\nUse a comprehension to get the list of escaping velocities\n\nHow does this relate to cluster evaporation over time?","type":"content","url":"/chapter2-control-flow#exercise-3-monte-carlo-integration","position":47},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl3":"Exercise 4: Convergence Testing","lvl2":"Exercises"},"type":"lvl3","url":"/chapter2-control-flow#exercise-4-convergence-testing","position":48},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl3":"Exercise 4: Convergence Testing","lvl2":"Exercises"},"content":"Iterative Orbit Calculation\n\nWhen calculating orbits, we often need to solve Kepler’s equation iteratively:\nE - e*sin(E) = M\n\nWhere E is eccentric anomaly, e is eccentricity, M is mean anomaly.\n\nWrite a function that:\n\nUses a while loop to solve for E given M and e\n\nStops when the change in E is less than 1e-10\n\nLimits iterations to prevent infinite loops\n\nReturns both E and the number of iterations\n\nTest with e=0.5, M=π/4. How many iterations does it take?","type":"content","url":"/chapter2-control-flow#exercise-4-convergence-testing","position":49},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl2":"Key Takeaways"},"type":"lvl2","url":"/chapter2-control-flow#key-takeaways","position":50},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl2":"Key Takeaways"},"content":"Chapter 2 Summary\n\n✅ Logic is Universal: From Aristotle to CPUs, logic underlies all reasoning\n\n✅ Boolean Algebra: True/False operations follow mathematical rules (De Morgan’s Laws)\n\n✅ Truthiness: Empty containers and zeros are False; most everything else is True\n\n✅ Guard Clauses: Check error conditions first, keep happy path unindented\n\n✅ For vs While: Use for when iterations are known, while for conditions\n\n✅ Break/Continue/Else: Control loop flow precisely\n\n✅ Comprehensions: Elegant one-liners for simple transformations\n\n✅ Patterns: Accumulator, Search/Filter, and Convergence appear everywhere\n\nNext Chapter Preview\n\nChapter 3: Functions - Your First Abstraction. Learn to write reusable, testable code that does one thing well.","type":"content","url":"/chapter2-control-flow#key-takeaways","position":51},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl2":"Quick Reference Card"},"type":"lvl2","url":"/chapter2-control-flow#quick-reference-card","position":52},{"hierarchy":{"lvl1":"Chapter 2: Control Flow & Logic - Teaching Computers to Think","lvl2":"Quick Reference Card"},"content":"# Comparisons\n<, >, <=, >=, ==, !=\nmath.isclose(a, b)  # For floats!\n\n# Logical operators (in order of precedence)\nnot x\nx and y  # Short-circuits\nx or y   # Short-circuits\n\n# If statements\nif condition:\n    pass\nelif other_condition:\n    pass\nelse:\n    pass\n\n# Guard clause pattern\nif error_condition:\n    return early\n# Happy path here\n\n# For loops\nfor i in range(n):  # 0 to n-1\nfor i, val in enumerate(list):  # Index and value\nfor item in collection:  # Direct iteration\n\n# While loops\nwhile condition:\n    # work\n    if done:\n        break\n    if skip:\n        continue\nelse:\n    # Runs if no break\n\n# Comprehensions\n[expr for item in iterable if condition]\n\n# Common patterns\ntotal = 0\nfor x in data:\n    total += x  # Accumulator","type":"content","url":"/chapter2-control-flow#quick-reference-card","position":53},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction"},"type":"lvl1","url":"/chapter3-functions","position":0},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction"},"content":"“The function of good software is to make the complex appear to be simple.” - Grady Booch","type":"content","url":"/chapter3-functions","position":1},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl2":"Table of Contents"},"type":"lvl2","url":"/chapter3-functions#table-of-contents","position":2},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl2":"Table of Contents"},"content":"Learning Objectives\n\nWhy Functions Matter\n\nThe Philosophy of Abstraction\n\nDRY: Don’t Repeat Yourself\n\nFunction Fundamentals\n\nBasic Syntax and Return\n\nParameters vs Arguments\n\nThe None Return\n\nFunction Arguments: The Full Story\n\nPositional Arguments\n\nDefault Arguments and Their Dangers\n\nKeyword Arguments\n\n*args: Variable Positional Arguments\n\n**kwargs: Variable Keyword Arguments\n\nThe Complete Order\n\nScope and Namespaces\n\nThe LEGB Rule\n\nGlobal Variables: Handle with Care\n\nNonlocal: Nested Function Scopes\n\nFunctions as First-Class Objects\n\nFunctions as Arguments\n\nFunctions Returning Functions\n\nLambda Functions\n\nDecorators: Function Transformers\n\nDocumentation and Type Hints\n\nDocstrings That Matter\n\nType Hints for Clarity\n\nAdvanced Patterns\n\nRecursion\n\nClosures\n\nPartial Functions\n\nCommon Pitfalls and Best Practices\n\nExercises\n\nKey Takeaways","type":"content","url":"/chapter3-functions#table-of-contents","position":3},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl2":"Learning Objectives"},"type":"lvl2","url":"/chapter3-functions#learning-objectives","position":4},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl2":"Learning Objectives"},"content":"By the end of this chapter you will:\n\nWrite functions that are modular, reusable, and testable\n\nMaster all argument types: positional, keyword, *args, **kwargs\n\nUnderstand scope rules and avoid common namespace pitfalls\n\nUse functions as first-class objects (pass, return, transform)\n\nWrite comprehensive docstrings and use type hints\n\nApply functional programming concepts to scientific code\n\nRecognize when and how to use advanced patterns like decorators","type":"content","url":"/chapter3-functions#learning-objectives","position":5},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl2":"Why Functions Matter"},"type":"lvl2","url":"/chapter3-functions#why-functions-matter","position":6},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl2":"Why Functions Matter"},"content":"","type":"content","url":"/chapter3-functions#why-functions-matter","position":7},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"The Philosophy of Abstraction","lvl2":"Why Functions Matter"},"type":"lvl3","url":"/chapter3-functions#the-philosophy-of-abstraction","position":8},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"The Philosophy of Abstraction","lvl2":"Why Functions Matter"},"content":"Functions are humanity’s tool for managing complexity. They let us think at different levels:\n\n# Level 1: The messy details\ndef calculate_schwarzschild_radius_verbose():\n    G = 6.67430e-11  # gravitational constant in m³/kg/s²\n    c = 299792458    # speed of light in m/s\n    M = 1.98892e30   # solar mass in kg\n    \n    # Schwarzschild radius formula\n    numerator = 2 * G * M\n    denominator = c * c\n    r_s = numerator / denominator\n    \n    print(f\"For calculation:\")\n    print(f\"  G = {G:.5e} m³/kg/s²\")\n    print(f\"  c = {c:.5e} m/s\")\n    print(f\"  M = {M:.5e} kg\")\n    print(f\"  r_s = 2GM/c² = {r_s:.3f} m\")\n    \n    return r_s\n\n# Level 2: The abstraction\ndef schwarzschild_radius(mass_kg):\n    \"\"\"Calculate Schwarzschild radius for given mass.\"\"\"\n    G = 6.67430e-11\n    c = 299792458\n    return 2 * G * mass_kg / c**2\n\n# Level 3: The application\ndef is_black_hole(mass_kg, radius_m):\n    \"\"\"Check if object is within its Schwarzschild radius.\"\"\"\n    return radius_m < schwarzschild_radius(mass_kg)\n\n# Now we can think at the problem level, not the math level\nsolar_mass = 1.98892e30\nprint(f\"Sun compressed to 1km: Black hole? {is_black_hole(solar_mass, 1000)}\")\nprint(f\"Sun compressed to 10km: Black hole? {is_black_hole(solar_mass, 10000)}\")\n\n","type":"content","url":"/chapter3-functions#the-philosophy-of-abstraction","position":9},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"DRY: Don’t Repeat Yourself","lvl2":"Why Functions Matter"},"type":"lvl3","url":"/chapter3-functions#dry-dont-repeat-yourself","position":10},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"DRY: Don’t Repeat Yourself","lvl2":"Why Functions Matter"},"content":"Every duplicated piece of code is a bug waiting to happen:\n\n# ❌ BAD: Repeated code\nmag1 = 5.2\nflux1 = 100 * 10**(-0.4 * mag1)  # Pogson's formula\nprint(f\"Magnitude {mag1} → Flux {flux1:.2f}\")\n\nmag2 = 3.7\nflux2 = 100 * 10**(-0.4 * mag2)  # Same formula, repeated\nprint(f\"Magnitude {mag2} → Flux {flux2:.2f}\")\n\nmag3 = 6.1\nflux3 = 100 * 10**(-0.4 * mag3)  # And again...\nprint(f\"Magnitude {mag3} → Flux {flux3:.2f}\")\n\nprint(\"\\n\" + \"=\"*40 + \"\\n\")\n\n# ✅ GOOD: Function eliminates repetition\ndef magnitude_to_flux(magnitude, zero_point_flux=100):\n    \"\"\"Convert astronomical magnitude to flux using Pogson's formula.\"\"\"\n    return zero_point_flux * 10**(-0.4 * magnitude)\n\n# Now if we need to change the formula, we change it in ONE place\nfor mag in [5.2, 3.7, 6.1]:\n    flux = magnitude_to_flux(mag)\n    print(f\"Magnitude {mag} → Flux {flux:.2f}\")\n\n","type":"content","url":"/chapter3-functions#dry-dont-repeat-yourself","position":11},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl2":"Function Fundamentals"},"type":"lvl2","url":"/chapter3-functions#function-fundamentals","position":12},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl2":"Function Fundamentals"},"content":"","type":"content","url":"/chapter3-functions#function-fundamentals","position":13},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"Basic Syntax and Return","lvl2":"Function Fundamentals"},"type":"lvl3","url":"/chapter3-functions#basic-syntax-and-return","position":14},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"Basic Syntax and Return","lvl2":"Function Fundamentals"},"content":"\n\ndef kinetic_energy(mass, velocity):\n    \"\"\"\n    Calculate kinetic energy.\n    \n    Parameters\n    ----------\n    mass : float\n        Mass in kg\n    velocity : float\n        Velocity in m/s\n        \n    Returns\n    -------\n    float\n        Kinetic energy in Joules\n    \"\"\"\n    return 0.5 * mass * velocity**2\n\n# Multiple return values\ndef orbital_parameters(semi_major_axis_au, eccentricity):\n    \"\"\"Calculate perihelion and aphelion distances.\"\"\"\n    perihelion = semi_major_axis_au * (1 - eccentricity)\n    aphelion = semi_major_axis_au * (1 + eccentricity)\n    return perihelion, aphelion  # Returns a tuple\n\n# Using the functions\nenergy = kinetic_energy(1000, 7900)  # 1000kg at orbital velocity\nprint(f\"Kinetic energy: {energy:.2e} J\")\n\nperi, aph = orbital_parameters(1.0, 0.0167)  # Earth's orbit\nprint(f\"Earth: Perihelion = {peri:.3f} AU, Aphelion = {aph:.3f} AU\")\n\n","type":"content","url":"/chapter3-functions#basic-syntax-and-return","position":15},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"Parameters vs Arguments","lvl2":"Function Fundamentals"},"type":"lvl3","url":"/chapter3-functions#parameters-vs-arguments","position":16},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"Parameters vs Arguments","lvl2":"Function Fundamentals"},"content":"\n\n# Parameters are the variables in the function definition\ndef greet(name, greeting=\"Hello\"):  # 'name' and 'greeting' are parameters\n    return f\"{greeting}, {name}!\"\n\n# Arguments are the actual values passed when calling\nresult = greet(\"Andromeda\", \"Greetings\")  # \"Andromeda\" and \"Greetings\" are arguments\nprint(result)\n\n# This distinction matters when discussing function behavior!\n\n","type":"content","url":"/chapter3-functions#parameters-vs-arguments","position":17},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"The None Return","lvl2":"Function Fundamentals"},"type":"lvl3","url":"/chapter3-functions#the-none-return","position":18},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"The None Return","lvl2":"Function Fundamentals"},"content":"Functions always return something, even if you don’t specify:\n\ndef print_only(message):\n    \"\"\"This function doesn't explicitly return anything.\"\"\"\n    print(f\"Message: {message}\")\n    # Implicit: return None\n\nresult = print_only(\"Testing\")\nprint(f\"Return value: {result}\")\nprint(f\"Type: {type(result)}\")\n\n# Explicit None return for early exit\ndef safe_divide(a, b):\n    \"\"\"Divide with safety check.\"\"\"\n    if b == 0:\n        return None  # Explicit None for error case\n    return a / b\n\nprint(f\"10/2 = {safe_divide(10, 2)}\")\nprint(f\"10/0 = {safe_divide(10, 0)}\")\n\n","type":"content","url":"/chapter3-functions#the-none-return","position":19},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl2":"Function Arguments: The Full Story"},"type":"lvl2","url":"/chapter3-functions#function-arguments-the-full-story","position":20},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl2":"Function Arguments: The Full Story"},"content":"","type":"content","url":"/chapter3-functions#function-arguments-the-full-story","position":21},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"Positional Arguments","lvl2":"Function Arguments: The Full Story"},"type":"lvl3","url":"/chapter3-functions#positional-arguments","position":22},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"Positional Arguments","lvl2":"Function Arguments: The Full Story"},"content":"Order matters for positional arguments:\n\ndef calculate_redshift(observed_wavelength, rest_wavelength):\n    \"\"\"Calculate redshift z from wavelengths.\"\"\"\n    return (observed_wavelength - rest_wavelength) / rest_wavelength\n\n# Order matters!\nz1 = calculate_redshift(656.3, 486.1)  # Wrong order\nz2 = calculate_redshift(486.1, 656.3)  # Also wrong\nz_correct = calculate_redshift(750.0, 656.3)  # Correct: observed, then rest\n\nprint(f\"Wrong: z = {z1:.3f}\")\nprint(f\"Also wrong: z = {z2:.3f}\")\nprint(f\"Correct (H-alpha redshifted): z = {z_correct:.3f}\")\n\n","type":"content","url":"/chapter3-functions#positional-arguments","position":23},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"Default Arguments and Their Dangers","lvl2":"Function Arguments: The Full Story"},"type":"lvl3","url":"/chapter3-functions#default-arguments-and-their-dangers","position":24},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"Default Arguments and Their Dangers","lvl2":"Function Arguments: The Full Story"},"content":"Default arguments are evaluated ONCE when the function is defined:\n\n# ⚠️ DANGER: Mutable default argument\ndef add_observation_bad(obs, obs_list=[]):  # DON'T DO THIS!\n    obs_list.append(obs)\n    return obs_list\n\n# Watch what happens:\nlist1 = add_observation_bad(\"Galaxy A\")\nlist2 = add_observation_bad(\"Galaxy B\")  # Where did Galaxy A come from?!\nprint(f\"list1: {list1}\")\nprint(f\"list2: {list2}\")\nprint(f\"Same object? {list1 is list2}\")  # They're the same list!\n\nprint(\"\\n\" + \"=\"*40 + \"\\n\")\n\n# ✅ CORRECT: Use None as default for mutable objects\ndef add_observation_good(obs, obs_list=None):\n    if obs_list is None:\n        obs_list = []  # Create new list each time\n    obs_list.append(obs)\n    return obs_list\n\n# Now it works correctly:\nlist3 = add_observation_good(\"Galaxy C\")\nlist4 = add_observation_good(\"Galaxy D\")\nprint(f\"list3: {list3}\")\nprint(f\"list4: {list4}\")\nprint(f\"Same object? {list3 is list4}\")  # Different lists!\n\nThe Mutable Default Trap\n\nThis is one of Python’s most common gotchas!\n\nDefault values are evaluated ONCE when the function is defined\n\nLists, dicts, and sets are mutable and will be shared across calls\n\nAlways use None as default for mutable objects","type":"content","url":"/chapter3-functions#default-arguments-and-their-dangers","position":25},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"Keyword Arguments","lvl2":"Function Arguments: The Full Story"},"type":"lvl3","url":"/chapter3-functions#keyword-arguments","position":26},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"Keyword Arguments","lvl2":"Function Arguments: The Full Story"},"content":"Use names for clarity and flexibility:\n\ndef simulate_orbit(\n    mass1, mass2,  # Positional arguments\n    eccentricity=0,  # Keyword arguments with defaults\n    inclination=0,\n    time_steps=1000,\n    integrator=\"verlet\"\n):\n    \"\"\"Simulate a two-body orbit.\"\"\"\n    print(f\"Simulating: M1={mass1}, M2={mass2}\")\n    print(f\"  e={eccentricity}, i={inclination}°\")\n    print(f\"  {time_steps} steps using {integrator}\")\n    return f\"Orbit data for {time_steps} steps\"\n\n# Can use positional and keyword arguments\nresult1 = simulate_orbit(1.0, 0.5)  # Just positional\nresult2 = simulate_orbit(1.0, 0.5, eccentricity=0.3)  # Mix\nresult3 = simulate_orbit(1.0, 0.5, time_steps=5000, eccentricity=0.1)  # Any order for keywords!\n\n","type":"content","url":"/chapter3-functions#keyword-arguments","position":27},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"*args: Variable Positional Arguments","lvl2":"Function Arguments: The Full Story"},"type":"lvl3","url":"/chapter3-functions#id-args-variable-positional-arguments","position":28},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"*args: Variable Positional Arguments","lvl2":"Function Arguments: The Full Story"},"content":"Accept any number of positional arguments:\n\ndef total_luminosity(*star_luminosities):\n    \"\"\"\n    Calculate total luminosity of multiple stars.\n    \n    Parameters\n    ----------\n    *star_luminosities : float\n        Variable number of luminosity values (solar units)\n    \"\"\"\n    print(f\"Received {len(star_luminosities)} stars\")\n    print(f\"Type of args: {type(star_luminosities)}\")  # It's a tuple!\n    \n    total = sum(star_luminosities)\n    return total\n\n# Can call with any number of arguments\nprint(f\"Single star: {total_luminosity(1.0)} L☉\")\nprint(f\"Binary: {total_luminosity(1.0, 0.5)} L☉\")\nprint(f\"Cluster: {total_luminosity(1.0, 0.5, 2.3, 0.1, 3.5)} L☉\")\n\n# Can also unpack a list with *\ncluster = [1.0, 0.5, 2.3, 0.1, 3.5]\nprint(f\"From list: {total_luminosity(*cluster)} L☉\")  # Note the *\n\n","type":"content","url":"/chapter3-functions#id-args-variable-positional-arguments","position":29},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"**kwargs: Variable Keyword Arguments","lvl2":"Function Arguments: The Full Story"},"type":"lvl3","url":"/chapter3-functions#id-kwargs-variable-keyword-arguments","position":30},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"**kwargs: Variable Keyword Arguments","lvl2":"Function Arguments: The Full Story"},"content":"Accept any number of keyword arguments:\n\ndef create_star_catalog(**star_properties):\n    \"\"\"\n    Create a star catalog with arbitrary properties.\n    \n    Parameters\n    ----------\n    **star_properties : various\n        Arbitrary keyword arguments for star properties\n    \"\"\"\n    print(f\"Type of kwargs: {type(star_properties)}\")  # It's a dict!\n    \n    catalog = \"Star Catalog:\\n\"\n    for property_name, value in star_properties.items():\n        catalog += f\"  {property_name}: {value}\\n\"\n    \n    return catalog\n\n# Can pass any keyword arguments\nprint(create_star_catalog(\n    name=\"Sirius A\",\n    spectral_type=\"A1V\",\n    magnitude=-1.46,\n    distance_pc=2.64\n))\n\n# Can also unpack a dictionary with **\nvega_data = {\n    'name': 'Vega',\n    'spectral_type': 'A0V',\n    'magnitude': 0.03,\n    'distance_pc': 7.68,\n    'rotation_km_s': 236\n}\nprint(create_star_catalog(**vega_data))  # Note the **\n\n","type":"content","url":"/chapter3-functions#id-kwargs-variable-keyword-arguments","position":31},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"The Complete Order","lvl2":"Function Arguments: The Full Story"},"type":"lvl3","url":"/chapter3-functions#the-complete-order","position":32},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"The Complete Order","lvl2":"Function Arguments: The Full Story"},"content":"When combining all argument types, they must appear in this order:\n\ndef ultimate_function(\n    pos1, pos2,           # Regular positional arguments\n    *args,                # Variable positional arguments\n    kwonly1, kwonly2=10,  # Keyword-only arguments (after *args)\n    **kwargs              # Variable keyword arguments\n):\n    \"\"\"Demonstrates the complete argument order.\"\"\"\n    print(f\"Positional: {pos1}, {pos2}\")\n    print(f\"*args: {args}\")\n    print(f\"Keyword-only: {kwonly1}, {kwonly2}\")\n    print(f\"**kwargs: {kwargs}\")\n\n# Must provide keyword-only arguments by name\nultimate_function(\n    1, 2,                    # Positional\n    3, 4, 5,                 # Extra positional (*args)\n    kwonly1=\"required\",      # Keyword-only (required)\n    kwonly2=\"optional\",      # Keyword-only (has default)\n    extra1=\"bonus\",          # Extra keywords (**kwargs)\n    extra2=\"more\"\n)\n\n# Force keyword-only without *args using bare *\ndef keyword_only_example(*, name, value=0):\n    \"\"\"After *, all arguments must be passed by name.\"\"\"\n    return f\"{name} = {value}\"\n\n# keyword_only_example(\"test\", 5)  # ERROR: won't work\nprint(keyword_only_example(name=\"test\", value=5))  # Must use names\n\n","type":"content","url":"/chapter3-functions#the-complete-order","position":33},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl2":"Scope and Namespaces"},"type":"lvl2","url":"/chapter3-functions#scope-and-namespaces","position":34},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl2":"Scope and Namespaces"},"content":"","type":"content","url":"/chapter3-functions#scope-and-namespaces","position":35},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"The LEGB Rule","lvl2":"Scope and Namespaces"},"type":"lvl3","url":"/chapter3-functions#the-legb-rule","position":36},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"The LEGB Rule","lvl2":"Scope and Namespaces"},"content":"Python searches for variables in this order: Local → Enclosing → Global → Built-in\n\n# Global scope\ngalaxy_name = \"Milky Way\"  # Global variable\n\ndef outer_function():\n    # Enclosing scope\n    star_count = 400_000_000_000  # Enclosing for inner_function\n    \n    def inner_function():\n        # Local scope\n        planet_count = 8  # Local variable\n        \n        # LEGB in action\n        print(f\"Local: {planet_count} planets\")\n        print(f\"Enclosing: {star_count} stars\")\n        print(f\"Global: In the {galaxy_name}\")\n        print(f\"Built-in: sum function is {sum}\")\n        \n    inner_function()\n\nouter_function()\n\n# Shadowing: Local variables can hide outer ones\ndef shadow_example():\n    galaxy_name = \"Andromeda\"  # Shadows global galaxy_name\n    print(f\"Inside function: {galaxy_name}\")\n\nshadow_example()\nprint(f\"Outside function: {galaxy_name}\")  # Global unchanged\n\n","type":"content","url":"/chapter3-functions#the-legb-rule","position":37},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"Global Variables: Handle with Care","lvl2":"Scope and Namespaces"},"type":"lvl3","url":"/chapter3-functions#global-variables-handle-with-care","position":38},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"Global Variables: Handle with Care","lvl2":"Scope and Namespaces"},"content":"\n\n# Global variable (generally avoid these!)\nobservation_count = 0\n\ndef add_observation_global():\n    global observation_count  # Declare we're modifying global\n    observation_count += 1\n    return observation_count\n\n# Modifying global state (usually bad practice)\nprint(f\"Initial count: {observation_count}\")\nadd_observation_global()\nadd_observation_global()\nprint(f\"After two calls: {observation_count}\")\n\n# Better approach: Pass and return state\ndef add_observation_pure(count):\n    \"\"\"Pure function - no side effects.\"\"\"\n    return count + 1\n\n# Much cleaner and testable\ncount = 0\ncount = add_observation_pure(count)\ncount = add_observation_pure(count)\nprint(f\"Pure function result: {count}\")\n\nWhy Avoid Global Variables?\n\nMakes code hard to test (tests affect each other)\n\nCreates hidden dependencies\n\nConcurrent code becomes dangerous\n\nDebugging becomes difficult\n\nBetter alternatives:\n\nPass parameters explicitly\n\nUse classes to encapsulate state\n\nReturn updated values","type":"content","url":"/chapter3-functions#global-variables-handle-with-care","position":39},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"Nonlocal: Nested Function Scopes","lvl2":"Scope and Namespaces"},"type":"lvl3","url":"/chapter3-functions#nonlocal-nested-function-scopes","position":40},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"Nonlocal: Nested Function Scopes","lvl2":"Scope and Namespaces"},"content":"\n\ndef make_counter():\n    \"\"\"Create a closure that counts calls.\"\"\"\n    count = 0\n    \n    def increment():\n        nonlocal count  # Modify enclosing scope variable\n        count += 1\n        return count\n    \n    return increment\n\n# Create independent counters\ncounter1 = make_counter()\ncounter2 = make_counter()\n\nprint(f\"Counter1: {counter1()}, {counter1()}, {counter1()}\")\nprint(f\"Counter2: {counter2()}, {counter2()}\")  # Independent!\n\n","type":"content","url":"/chapter3-functions#nonlocal-nested-function-scopes","position":41},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl2":"Functions as First-Class Objects"},"type":"lvl2","url":"/chapter3-functions#functions-as-first-class-objects","position":42},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl2":"Functions as First-Class Objects"},"content":"In Python, functions are objects like any other - you can pass them, return them, and store them:","type":"content","url":"/chapter3-functions#functions-as-first-class-objects","position":43},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"Functions as Arguments","lvl2":"Functions as First-Class Objects"},"type":"lvl3","url":"/chapter3-functions#functions-as-arguments","position":44},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"Functions as Arguments","lvl2":"Functions as First-Class Objects"},"content":"\n\nimport math\n\ndef apply_to_list(data, function):\n    \"\"\"Apply a function to each element in a list.\"\"\"\n    return [function(x) for x in data]\n\n# Different functions to apply\nmagnitudes = [1.5, 2.3, 0.8, 3.1]\n\n# Pass different functions\nfluxes = apply_to_list(magnitudes, lambda m: 10**(-0.4 * m))\nlogs = apply_to_list(magnitudes, math.log10)\nsquares = apply_to_list(magnitudes, lambda x: x**2)\n\nprint(f\"Magnitudes: {magnitudes}\")\nprint(f\"To fluxes: {[f'{f:.3f}' for f in fluxes]}\")\nprint(f\"Logarithms: {[f'{l:.3f}' for l in logs]}\")\nprint(f\"Squares: {[f'{s:.3f}' for s in squares]}\")\n\n# Real example: Numerical integration with different functions\ndef integrate(func, a, b, n=1000):\n    \"\"\"Simple numerical integration using rectangles.\"\"\"\n    dx = (b - a) / n\n    total = 0\n    for i in range(n):\n        x = a + i * dx\n        total += func(x) * dx\n    return total\n\n# Integrate different functions\nresult1 = integrate(math.sin, 0, math.pi)  # ∫sin(x) from 0 to π\nresult2 = integrate(lambda x: x**2, 0, 1)  # ∫x² from 0 to 1\n\nprint(f\"\\n∫sin(x) from 0 to π = {result1:.4f} (expected: 2)\")\nprint(f\"∫x² from 0 to 1 = {result2:.4f} (expected: 0.333...)\")\n\n","type":"content","url":"/chapter3-functions#functions-as-arguments","position":45},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"Functions Returning Functions","lvl2":"Functions as First-Class Objects"},"type":"lvl3","url":"/chapter3-functions#functions-returning-functions","position":46},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"Functions Returning Functions","lvl2":"Functions as First-Class Objects"},"content":"\n\ndef make_magnitude_converter(zero_point_flux):\n    \"\"\"\n    Create a magnitude-to-flux converter for a specific zero point.\n    \n    This is a 'function factory' - it returns customized functions.\n    \"\"\"\n    def converter(magnitude):\n        return zero_point_flux * 10**(-0.4 * magnitude)\n    \n    # Return the inner function\n    return converter\n\n# Create converters for different photometric systems\nvega_converter = make_magnitude_converter(3.631e-20)  # Vega system\nab_converter = make_magnitude_converter(3.631e-23)    # AB system\n\nmag = 20.0\nprint(f\"Magnitude {mag}:\")\nprint(f\"  Vega system: {vega_converter(mag):.3e} W/m²/Hz\")\nprint(f\"  AB system: {ab_converter(mag):.3e} W/m²/Hz\")\n\n# Another example: Creating custom filters\ndef make_filter(lower, upper):\n    \"\"\"Create a filter function for a wavelength range.\"\"\"\n    def filter_func(wavelength):\n        return lower <= wavelength <= upper\n    \n    filter_func.__name__ = f\"filter_{lower}_{upper}\"\n    return filter_func\n\n# Create filters for different bands\nu_band = make_filter(300, 400)  # U band in nm\ng_band = make_filter(400, 550)  # G band in nm\n\nwavelength = 450\nprint(f\"\\nWavelength {wavelength}nm:\")\nprint(f\"  In U band? {u_band(wavelength)}\")\nprint(f\"  In G band? {g_band(wavelength)}\")\n\n","type":"content","url":"/chapter3-functions#functions-returning-functions","position":47},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"Lambda Functions","lvl2":"Functions as First-Class Objects"},"type":"lvl3","url":"/chapter3-functions#lambda-functions","position":48},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"Lambda Functions","lvl2":"Functions as First-Class Objects"},"content":"Anonymous functions for simple operations:\n\n# Lambda syntax: lambda arguments: expression\n\n# Regular function\ndef square(x):\n    return x**2\n\n# Equivalent lambda\nsquare_lambda = lambda x: x**2\n\nprint(f\"Regular: {square(5)}\")\nprint(f\"Lambda: {square_lambda(5)}\")\n\n# Lambdas shine in functional programming\ndata = [\n    {'name': 'Sirius', 'mag': -1.46},\n    {'name': 'Canopus', 'mag': -0.74},\n    {'name': 'Arcturus', 'mag': -0.05},\n    {'name': 'Vega', 'mag': 0.03}\n]\n\n# Sort by magnitude\ndata_sorted = sorted(data, key=lambda star: star['mag'])\nprint(\"\\nStars by brightness:\")\nfor star in data_sorted:\n    print(f\"  {star['name']}: {star['mag']}\")\n\n# Filter bright stars\nbright = filter(lambda s: s['mag'] < 0, data)\nprint(\"\\nBright stars (mag < 0):\")\nfor star in bright:\n    print(f\"  {star['name']}\")\n\n","type":"content","url":"/chapter3-functions#lambda-functions","position":49},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"Decorators: Function Transformers","lvl2":"Functions as First-Class Objects"},"type":"lvl3","url":"/chapter3-functions#decorators-function-transformers","position":50},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"Decorators: Function Transformers","lvl2":"Functions as First-Class Objects"},"content":"Decorators modify or enhance functions:\n\nimport time\nimport functools\n\n# Simple decorator to time function execution\ndef timer(func):\n    \"\"\"Decorator to measure function execution time.\"\"\"\n    @functools.wraps(func)  # Preserves function metadata\n    def wrapper(*args, **kwargs):\n        start = time.perf_counter()\n        result = func(*args, **kwargs)\n        end = time.perf_counter()\n        print(f\"{func.__name__} took {end-start:.6f} seconds\")\n        return result\n    return wrapper\n\n# Apply decorator with @\n@timer\ndef slow_calculation(n):\n    \"\"\"Simulate a slow calculation.\"\"\"\n    total = 0\n    for i in range(n):\n        total += i**2\n    return total\n\nresult = slow_calculation(1000000)\nprint(f\"Result: {result}\")\n\n# Decorator with arguments\ndef validate_range(min_val, max_val):\n    \"\"\"Decorator factory that validates input range.\"\"\"\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(value):\n            if not min_val <= value <= max_val:\n                raise ValueError(f\"Value {value} outside range [{min_val}, {max_val}]\")\n            return func(value)\n        return wrapper\n    return decorator\n\n@validate_range(0, 1)\ndef calculate_probability(p):\n    \"\"\"Calculate something with probability.\"\"\"\n    return p * (1 - p)\n\nprint(f\"\\nProbability(0.3) = {calculate_probability(0.3):.3f}\")\n# calculate_probability(1.5)  # Would raise ValueError\n\n","type":"content","url":"/chapter3-functions#decorators-function-transformers","position":51},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl2":"Documentation and Type Hints"},"type":"lvl2","url":"/chapter3-functions#documentation-and-type-hints","position":52},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl2":"Documentation and Type Hints"},"content":"","type":"content","url":"/chapter3-functions#documentation-and-type-hints","position":53},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"Docstrings That Matter","lvl2":"Documentation and Type Hints"},"type":"lvl3","url":"/chapter3-functions#docstrings-that-matter","position":54},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"Docstrings That Matter","lvl2":"Documentation and Type Hints"},"content":"\n\ndef calculate_orbital_period(semi_major_axis: float, \n                           total_mass: float) -> float:\n    \"\"\"\n    Calculate orbital period using Kepler's third law.\n    \n    For a two-body system, calculates the orbital period given\n    the semi-major axis and total system mass.\n    \n    Parameters\n    ----------\n    semi_major_axis : float\n        Semi-major axis in AU\n    total_mass : float\n        Total mass of system in solar masses\n    \n    Returns\n    -------\n    float\n        Orbital period in years\n        \n    Notes\n    -----\n    Uses the simplified form of Kepler's third law:\n    P² = a³/M where P is in years, a in AU, M in solar masses\n    \n    Examples\n    --------\n    >>> calculate_orbital_period(1.0, 1.0)  # Earth around Sun\n    1.0\n    >>> calculate_orbital_period(5.2, 1.0)  # Jupiter around Sun\n    11.86\n    \n    References\n    ----------\n    .. [1] Carroll & Ostlie, \"An Introduction to Modern Astrophysics\"\n    \"\"\"\n    return (semi_major_axis**3 / total_mass)**0.5\n\n# Access docstring\nprint(calculate_orbital_period.__doc__)\n\n","type":"content","url":"/chapter3-functions#docstrings-that-matter","position":55},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"Type Hints for Clarity","lvl2":"Documentation and Type Hints"},"type":"lvl3","url":"/chapter3-functions#type-hints-for-clarity","position":56},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"Type Hints for Clarity","lvl2":"Documentation and Type Hints"},"content":"\n\nfrom typing import List, Tuple, Optional, Union, Callable, Dict\n\ndef process_spectrum(\n    wavelengths: List[float],\n    fluxes: List[float],\n    normalize: bool = True,\n    smooth_window: Optional[int] = None\n) -> Tuple[List[float], List[float]]:\n    \"\"\"\n    Process a spectrum with optional normalization and smoothing.\n    \n    Type hints make the expected inputs and outputs clear!\n    \"\"\"\n    # Processing would happen here\n    return wavelengths, fluxes\n\ndef find_spectral_lines(\n    spectrum: Dict[str, List[float]],\n    threshold: float = 3.0,\n    method: Callable[[List[float]], float] = max\n) -> Union[List[float], None]:\n    \"\"\"\n    Find spectral lines in a spectrum.\n    \n    Shows complex type hints including Callable and Union.\n    \"\"\"\n    if not spectrum:\n        return None\n    \n    # Line finding logic here\n    return [656.3, 486.1]  # H-alpha, H-beta\n\n# Type hints help IDEs provide better autocomplete and catch errors!\n\n","type":"content","url":"/chapter3-functions#type-hints-for-clarity","position":57},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl2":"Advanced Patterns"},"type":"lvl2","url":"/chapter3-functions#advanced-patterns","position":58},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl2":"Advanced Patterns"},"content":"","type":"content","url":"/chapter3-functions#advanced-patterns","position":59},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"Recursion","lvl2":"Advanced Patterns"},"type":"lvl3","url":"/chapter3-functions#recursion","position":60},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"Recursion","lvl2":"Advanced Patterns"},"content":"Functions calling themselves - elegant but use with care:\n\ndef factorial(n: int) -> int:\n    \"\"\"Calculate factorial recursively.\"\"\"\n    if n <= 1:  # Base case\n        return 1\n    return n * factorial(n - 1)  # Recursive case\n\nprint(f\"5! = {factorial(5)}\")\n\n# More complex: Binary tree traversal for hierarchical structures\ndef calculate_cluster_mass(cluster):\n    \"\"\"\n    Recursively calculate mass of hierarchical structure.\n    \n    Each cluster can contain stars or sub-clusters.\n    \"\"\"\n    if isinstance(cluster, (int, float)):  # Base case: single star\n        return cluster\n    \n    # Recursive case: sum all components\n    total_mass = 0\n    for component in cluster:\n        total_mass += calculate_cluster_mass(component)\n    return total_mass\n\n# Hierarchical cluster structure\nglobular_cluster = [\n    1.5,  # Single star\n    [0.8, 1.2],  # Binary system\n    [0.5, [0.3, 0.4]],  # Triple system\n    [[0.9, 1.1], [1.3, 0.7]]  # Two binaries\n]\n\ntotal = calculate_cluster_mass(globular_cluster)\nprint(f\"Total cluster mass: {total:.1f} M☉\")\n\nRecursion Limits\n\nPython has a recursion limit (usually 1000) to prevent stack overflow:import sys\nprint(sys.getrecursionlimit())  # Usually 1000\n\nFor deep recursion, use iteration or increase the limit carefully.\n### Closures\n\nFunctions that \"remember\" their environment:\n\n```{code-cell} ipython3\ndef make_doppler_calculator(rest_wavelength):\n    \"\"\"\n    Create a Doppler shift calculator for a specific spectral line.\n    \n    This is a closure - the inner function 'remembers' rest_wavelength.\n    \"\"\"\n    def calculate_velocity(observed_wavelength):\n        # This function has access to rest_wavelength from outer scope\n        z = (observed_wavelength - rest_wavelength) / rest_wavelength\n        c = 299792.458  # km/s\n        return z * c\n    \n    # Add some metadata\n    calculate_velocity.rest_wavelength = rest_wavelength\n    calculate_velocity.__name__ = f\"doppler_{rest_wavelength}\"\n    \n    return calculate_velocity\n\n# Create specialized calculators\nh_alpha_doppler = make_doppler_calculator(656.28)  # H-alpha line\nh_beta_doppler = make_doppler_calculator(486.13)   # H-beta line\n\n# Use them\nobserved = 658.0\nprint(f\"Observed wavelength: {observed} nm\")\nprint(f\"H-alpha velocity: {h_alpha_doppler(observed):.1f} km/s\")\nprint(f\"H-beta velocity: {h_beta_doppler(observed):.1f} km/s\")\nprint(f\"H-alpha rest λ: {h_alpha_doppler.rest_wavelength} nm\")","type":"content","url":"/chapter3-functions#recursion","position":61},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"Partial Functions","lvl2":"Advanced Patterns"},"type":"lvl3","url":"/chapter3-functions#partial-functions","position":62},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"Partial Functions","lvl2":"Advanced Patterns"},"content":"Pre-fill some arguments of a function:\n\nfrom functools import partial\n\ndef planck_law(wavelength, temperature, scale=1e-9):\n    \"\"\"\n    Planck's law for blackbody radiation.\n    \n    Parameters\n    ----------\n    wavelength : float\n        Wavelength in meters\n    temperature : float\n        Temperature in Kelvin\n    scale : float\n        Scale factor for units\n    \"\"\"\n    import math\n    h = 6.626e-34  # Planck constant\n    c = 2.998e8    # Speed of light\n    k = 1.381e-23  # Boltzmann constant\n    \n    numerator = 2 * h * c**2 / wavelength**5\n    denominator = math.exp(h * c / (wavelength * k * temperature)) - 1\n    return scale * numerator / denominator\n\n# Create specialized functions for specific temperatures\nsun_spectrum = partial(planck_law, temperature=5778)  # Sun\nsirius_spectrum = partial(planck_law, temperature=9940)  # Sirius\n\n# Now we can use them easily\nwavelength = 500e-9  # 500 nm (green light)\nprint(f\"At {wavelength*1e9:.0f} nm:\")\nprint(f\"  Sun: {sun_spectrum(wavelength):.2e} W/m²/m\")\nprint(f\"  Sirius: {sirius_spectrum(wavelength):.2e} W/m²/m\")\n\n","type":"content","url":"/chapter3-functions#partial-functions","position":63},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl2":"Common Pitfalls and Best Practices"},"type":"lvl2","url":"/chapter3-functions#common-pitfalls-and-best-practices","position":64},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl2":"Common Pitfalls and Best Practices"},"content":"\n\n# Pitfall 1: Modifying arguments\ndef bad_append(item, lst=[]):  # DON'T: mutable default\n    lst.append(item)\n    return lst\n\ndef good_append(item, lst=None):  # DO: None default\n    if lst is None:\n        lst = []\n    lst.append(item)\n    return lst\n\n# Pitfall 2: Too many parameters\ndef bad_function(a, b, c, d, e, f, g, h):  # Too many!\n    pass\n\ndef good_function(config_dict):  # Group related parameters\n    pass\n\n# Pitfall 3: Side effects in unexpected places\ntotal = 0\ndef bad_accumulator(value):\n    global total  # Hidden side effect!\n    total += value\n    return total\n\ndef good_accumulator(value, running_total):  # Explicit\n    return running_total + value\n\n# Pitfall 4: Functions doing too much\ndef bad_do_everything(data):\n    # Load data\n    # Process data\n    # Analyze data\n    # Plot results\n    # Save output\n    pass  # Too many responsibilities!\n\n# Better: Single responsibility\ndef load_data(filename): pass\ndef process_data(data): pass\ndef analyze_data(processed): pass\ndef plot_results(analysis): pass\ndef save_output(results, filename): pass\n\n","type":"content","url":"/chapter3-functions#common-pitfalls-and-best-practices","position":65},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl2":"Exercises"},"type":"lvl2","url":"/chapter3-functions#exercises","position":66},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl2":"Exercises"},"content":"","type":"content","url":"/chapter3-functions#exercises","position":67},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"Exercise 1: Advanced Argument Handling","lvl2":"Exercises"},"type":"lvl3","url":"/chapter3-functions#exercise-1-advanced-argument-handling","position":68},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"Exercise 1: Advanced Argument Handling","lvl2":"Exercises"},"content":"Flexible Data Processor\n\nCreate a function that:\n\nTakes required positional arguments for data and method\n\nAccepts any number of filters as *args\n\nTakes optional keyword arguments for configuration\n\nAccepts any additional metadata as **kwargs\n\nThe function should:\n\nApply all filters to the data\n\nProcess using the specified method\n\nReturn results with metadata attached\n\nTest with astronomical data filtering scenarios.","type":"content","url":"/chapter3-functions#exercise-1-advanced-argument-handling","position":69},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"Exercise 2: Function Factory","lvl2":"Exercises"},"type":"lvl3","url":"/chapter3-functions#exercise-2-function-factory","position":70},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"Exercise 2: Function Factory","lvl2":"Exercises"},"content":"Custom Integrator Generator\n\nWrite a function that returns customized numerical integrators:\n\nTakes integration method (‘euler’, ‘rk4’, ‘verlet’) as input\n\nReturns a function configured for that method\n\nThe returned function should integrate any differential equation\n\nExample usage:euler_integrator = make_integrator('euler')\nresult = euler_integrator(dydt, y0, t_span)\n\nTest with orbital dynamics equations.\n### Exercise 3: Decorator Challenge\n\n```{exercise} Performance Monitor Decorator\nCreate a decorator that:\n1. Times function execution\n2. Logs input arguments\n3. Catches and logs exceptions\n4. Can be configured with verbosity level\n\nApply to various astronomical calculations and analyze performance.","type":"content","url":"/chapter3-functions#exercise-2-function-factory","position":71},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"Exercise 4: Recursive Tree Search","lvl2":"Exercises"},"type":"lvl3","url":"/chapter3-functions#exercise-4-recursive-tree-search","position":72},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl3":"Exercise 4: Recursive Tree Search","lvl2":"Exercises"},"content":"Galaxy Cluster Finder\n\nGalaxies form hierarchical structures. Write a recursive function that:\n\nTakes a tree structure of galaxy positions\n\nFinds all groups within a given distance threshold\n\nReturns nested structure of identified clusters\n\nHandle edge cases like empty regions and single galaxies.","type":"content","url":"/chapter3-functions#exercise-4-recursive-tree-search","position":73},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl2":"Key Takeaways"},"type":"lvl2","url":"/chapter3-functions#key-takeaways","position":74},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl2":"Key Takeaways"},"content":"Chapter 3 Summary\n\n✅ Functions are abstractions: Hide complexity, expose simplicity\n\n✅ DRY Principle: Don’t Repeat Yourself - factor out common code\n\n✅ Argument mastery: Positional, keyword, *args, **kwargs - know when to use each\n\n✅ Beware mutable defaults: Use None and create inside function\n\n✅ LEGB scope rule: Local → Enclosing → Global → Built-in\n\n✅ Functions are objects: Pass them, return them, transform them\n\n✅ Decorators enhance functions: Add functionality without modifying code\n\n✅ Type hints clarify intent: Make your code self-documenting\n\n✅ Single responsibility: Each function should do one thing well\n\nNext Chapter Preview\n\nChapter 4: Data Structures & Algorithms - Choosing the right tool for astronomical data","type":"content","url":"/chapter3-functions#key-takeaways","position":75},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl2":"Quick Reference Card"},"type":"lvl2","url":"/chapter3-functions#quick-reference-card","position":76},{"hierarchy":{"lvl1":"Chapter 3: Functions - The Art of Abstraction","lvl2":"Quick Reference Card"},"content":"# Function definition\ndef func(pos, default=None, *args, **kwargs):\n    \"\"\"Docstring here.\"\"\"\n    return result\n\n# Argument unpacking\nfunc(*list_args, **dict_kwargs)\n\n# Lambda functions\nlambda x: x**2\n\n# Decorators\n@decorator\ndef func():\n    pass\n\n# Type hints\ndef func(x: int, y: float = 0.0) -> str:\n    pass\n\n# Scope modifiers\nglobal var_name\nnonlocal var_name\n\n# Function as argument\nmap(func, iterable)\nfilter(func, iterable)\nsorted(items, key=func)\n\n# Partial functions\nfrom functools import partial\nnew_func = partial(old_func, arg1=value)\n\n# Common patterns\nif param is None:\n    param = []  # Mutable default pattern","type":"content","url":"/chapter3-functions#quick-reference-card","position":77},{"hierarchy":{"lvl1":"Python Fundamentals"},"type":"lvl1","url":"/index-6","position":0},{"hierarchy":{"lvl1":"Python Fundamentals"},"content":"Content coming soon!","type":"content","url":"/index-6","position":1},{"hierarchy":{"lvl1":"Advanced Topics"},"type":"lvl1","url":"/index-8","position":0},{"hierarchy":{"lvl1":"Advanced Topics"},"content":"Content coming soon!","type":"content","url":"/index-8","position":1},{"hierarchy":{"lvl1":"Gravitational Dynamics"},"type":"lvl1","url":"/index-9","position":0},{"hierarchy":{"lvl1":"Gravitational Dynamics"},"content":"Content coming soon!","type":"content","url":"/index-9","position":1},{"hierarchy":{"lvl1":"Astrophysics Applications"},"type":"lvl1","url":"/index-7","position":0},{"hierarchy":{"lvl1":"Astrophysics Applications"},"content":"Real astronomical phenomena and data analysis techniques that provide scientific context for computational methods.","type":"content","url":"/index-7","position":1},{"hierarchy":{"lvl1":"Astrophysics Applications","lvl2":"Overview"},"type":"lvl2","url":"/index-7#overview","position":2},{"hierarchy":{"lvl1":"Astrophysics Applications","lvl2":"Overview"},"content":"This section explores the astrophysical motivation behind every computational technique in ASTR 596. You’ll understand not just how to implement algorithms, but why they’re essential for modern astronomical research.","type":"content","url":"/index-7#overview","position":3},{"hierarchy":{"lvl1":"Astrophysics Applications","lvl2":"Physical Contexts"},"type":"lvl2","url":"/index-7#physical-contexts","position":4},{"hierarchy":{"lvl1":"Astrophysics Applications","lvl2":"Physical Contexts"},"content":"","type":"content","url":"/index-7#physical-contexts","position":5},{"hierarchy":{"lvl1":"Astrophysics Applications","lvl3":"⭐ Stellar Physics","lvl2":"Physical Contexts"},"type":"lvl3","url":"/index-7#id-stellar-physics","position":6},{"hierarchy":{"lvl1":"Astrophysics Applications","lvl3":"⭐ Stellar Physics","lvl2":"Physical Contexts"},"content":"The life cycles of stars provide rich computational problems:\n\nStellar Structure: Hydrostatic equilibrium and energy transport\n\nNuclear Physics: Fusion rates and element synthesis\n\nStellar Evolution: Main sequence to white dwarf/neutron star/black hole\n\nObservational Data: HR diagrams and stellar classification\n\nComputational Methods: ODE solving, boundary value problems\nProject Connection: Project 1 implements stellar evolution models","type":"content","url":"/index-7#id-stellar-physics","position":7},{"hierarchy":{"lvl1":"Astrophysics Applications","lvl3":"🌌 Gravitational Dynamics","lvl2":"Physical Contexts"},"type":"lvl3","url":"/index-7#id-gravitational-dynamics","position":8},{"hierarchy":{"lvl1":"Astrophysics Applications","lvl3":"🌌 Gravitational Dynamics","lvl2":"Physical Contexts"},"content":"N-body systems from planetary motion to galaxy formation:\n\nClassical Mechanics: Newton’s laws in astronomical contexts\n\nOrbital Dynamics: Kepler’s laws and perturbation theory\n\nN-Body Systems: Star clusters, galaxy interactions\n\nNumerical Integration: Leapfrog, symplectic methods\n\nDark Matter: Structure formation and cosmological simulations\nProject Connection: Project 2 builds gravitational N-body simulators","type":"content","url":"/index-7#id-gravitational-dynamics","position":9},{"hierarchy":{"lvl1":"Astrophysics Applications","lvl3":"🌟 Radiative Transfer","lvl2":"Physical Contexts"},"type":"lvl3","url":"/index-7#id-radiative-transfer","position":10},{"hierarchy":{"lvl1":"Astrophysics Applications","lvl3":"🌟 Radiative Transfer","lvl2":"Physical Contexts"},"content":"How light travels through astronomical environments:\n\nPhoton Physics: Emission, absorption, and scattering processes\n\nStellar Atmospheres: Spectral line formation\n\nInterstellar Medium: Dust extinction and emission\n\nMonte Carlo Methods: Photon transport simulations\n\nObservational Astronomy: From photons to physical parameters\nProject Connection: Projects 2-3 use Monte Carlo radiative transfer","type":"content","url":"/index-7#id-radiative-transfer","position":11},{"hierarchy":{"lvl1":"Astrophysics Applications","lvl3":"🔬 Advanced Topics","lvl2":"Physical Contexts"},"type":"lvl3","url":"/index-7#id-advanced-topics","position":12},{"hierarchy":{"lvl1":"Astrophysics Applications","lvl3":"🔬 Advanced Topics","lvl2":"Physical Contexts"},"content":"Cutting-edge astrophysical applications:\n\nMagnetohydrodynamics: Plasma physics in stellar and galactic environments\n\nGeneral Relativity: Black holes, gravitational waves, cosmology\n\nAstrostatistics: Bayesian methods for astronomical data analysis\n\nMachine Learning: Neural networks for classification and discovery\n\nMulti-Messenger Astronomy: Combining electromagnetic, gravitational, and neutrino observations\nProject Connection: Final project applies ML to modern astrophysical datasets","type":"content","url":"/index-7#id-advanced-topics","position":13},{"hierarchy":{"lvl1":"Astrophysics Applications","lvl2":"Scientific Method Integration"},"type":"lvl2","url":"/index-7#scientific-method-integration","position":14},{"hierarchy":{"lvl1":"Astrophysics Applications","lvl2":"Scientific Method Integration"},"content":"Each topic demonstrates how computational astrophysics follows the scientific method:\n\nObservation: Real astronomical data and phenomena\n\nHypothesis: Physical theories and mathematical models\n\nPrediction: Computational simulations and calculations\n\nTesting: Comparison with observations and experiments\n\nIteration: Model refinement and new predictions","type":"content","url":"/index-7#scientific-method-integration","position":15},{"hierarchy":{"lvl1":"Astrophysics Applications","lvl2":"Mathematical Physics Foundation"},"type":"lvl2","url":"/index-7#mathematical-physics-foundation","position":16},{"hierarchy":{"lvl1":"Astrophysics Applications","lvl2":"Mathematical Physics Foundation"},"content":"The astrophysical applications reinforce key physics concepts:\n\nConservation Laws: Energy, momentum, angular momentum\n\nThermodynamics: Stellar interiors and planetary atmospheres\n\nElectromagnetism: Radiation processes and magnetic fields\n\nQuantum Mechanics: Atomic physics and stellar nucleosynthesis\n\nStatistical Mechanics: Kinetic theory and plasma physics","type":"content","url":"/index-7#mathematical-physics-foundation","position":17},{"hierarchy":{"lvl1":"Astrophysics Applications","lvl2":"Observational Connection"},"type":"lvl2","url":"/index-7#observational-connection","position":18},{"hierarchy":{"lvl1":"Astrophysics Applications","lvl2":"Observational Connection"},"content":"Every computational method connects to real observations:\n\nPhotometry: Measuring stellar brightness and colors\n\nSpectroscopy: Analyzing chemical composition and kinematics\n\nAstrometry: Precise positional measurements\n\nTime-Domain: Variable stars, supernovae, exoplanet transits\n\nMulti-Wavelength: Radio, infrared, optical, X-ray, gamma-ray astronomy","type":"content","url":"/index-7#observational-connection","position":19},{"hierarchy":{"lvl1":"Astrophysics Applications","lvl2":"Research Preparation"},"type":"lvl2","url":"/index-7#research-preparation","position":20},{"hierarchy":{"lvl1":"Astrophysics Applications","lvl2":"Research Preparation"},"content":"These applications prepare you for:\n\nGraduate Research: Understanding current astrophysical problems\n\nLiterature Review: Reading and interpreting research papers\n\nData Analysis: Working with real astronomical datasets\n\nScientific Communication: Presenting results to scientific audiences\n\nCareer Readiness: Skills applicable to academia or industry","type":"content","url":"/index-7#research-preparation","position":21},{"hierarchy":{"lvl1":"Astrophysics Applications","lvl2":"Course Integration"},"type":"lvl2","url":"/index-7#course-integration","position":22},{"hierarchy":{"lvl1":"Astrophysics Applications","lvl2":"Course Integration"},"content":"The astrophysical context enhances every aspect of ASTR 596:\n\nMotivation: Understanding why computational methods matter\n\nIntuition: Developing physical insight for debugging\n\nValidation: Testing code against known astronomical results\n\nApplication: Solving genuine research problems\n\nCommunication: Explaining technical work to scientific audiences","type":"content","url":"/index-7#course-integration","position":23},{"hierarchy":{"lvl1":"Astrophysics Applications","lvl2":"Beyond the Course"},"type":"lvl2","url":"/index-7#beyond-the-course","position":24},{"hierarchy":{"lvl1":"Astrophysics Applications","lvl2":"Beyond the Course"},"content":"These foundations prepare you for:\n\nAdvanced astrophysics coursework.\n\nGraduate school applications and research.\n\nCareers in astronomy, data science, or technology.\n\nLifelong learning in rapidly evolving fields.\n\nThe universe provides an inexhaustible source of computational challenges. Master these tools, and you’ll be ready to tackle questions about the fundamental nature of reality itself.","type":"content","url":"/index-7#beyond-the-course","position":25},{"hierarchy":{"lvl1":"Radiative Transfer"},"type":"lvl1","url":"/index-10","position":0},{"hierarchy":{"lvl1":"Radiative Transfer"},"content":"Content coming soon!","type":"content","url":"/index-10","position":1},{"hierarchy":{"lvl1":"Stellar Physics"},"type":"lvl1","url":"/index-11","position":0},{"hierarchy":{"lvl1":"Stellar Physics"},"content":"Content coming soon!","type":"content","url":"/index-11","position":1},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide"},"type":"lvl1","url":"/project-submission-guide","position":0},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide"},"content":"","type":"content","url":"/project-submission-guide","position":1},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl2":"Project Schedule & Deadlines"},"type":"lvl2","url":"/project-submission-guide#project-schedule-deadlines","position":2},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl2":"Project Schedule & Deadlines"},"content":"","type":"content","url":"/project-submission-guide#project-schedule-deadlines","position":3},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl3":"Project Timeline","lvl2":"Project Schedule & Deadlines"},"type":"lvl3","url":"/project-submission-guide#project-timeline","position":4},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl3":"Project Timeline","lvl2":"Project Schedule & Deadlines"},"content":"Projects are assigned on Mondays (posted to GitHub Classroom) and due the following Monday at 11:59 PM. This schedule allows you to review requirements before Friday’s class, where we’ll work on implementation together.\n\nProject\n\nAssigned\n\nDue Date\n\nTopic\n\nKey Concepts\n\nProject 1\n\nAug 25 (Mon)\n\nSept 8 (Mon)\n\nPython/OOP/Stellar Physics Basics\n\nClasses, inheritance, HR diagrams\n\nProject 2\n\nSept 8 (Mon)\n\nSept 22 (Mon)\n\nODE Integration + N-Body Dynamics + Monte Carlo Sampling\n\nEuler, RK4, Leapfrog, IMF sampling\n\nProject 3\n\nSept 22 (Mon)\n\nOct 6 (Mon)\n\nRegression/ML Fundamentals\n\nGradient descent, loss functions, optimization\n\nProject 4\n\nOct 6 (Mon)\n\nOct 20 (Mon)\n\nMonte Carlo Radiative Transfer\n\nPhoton packets, scattering, absorption\n\nProject 5\n\nOct 20 (Mon)\n\nNov 3 (Mon)\n\nBayesian/MCMC\n\nPriors, likelihood, Metropolis-Hastings\n\nProject 6\n\nNov 3 (Mon)\n\nNov 17 (Mon)\n\nGaussian Processes\n\nKernels, hyperparameters, regression\n\nFinal Project\n\nNov 17 (Mon)\n\nDec 18 (Thu)\n\nNeural Networks (From Scratch + JAX)\n\nBackprop, autodiff, JAX ecosystem","type":"content","url":"/project-submission-guide#project-timeline","position":5},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl3":"Two-Week Project Workflow","lvl2":"Project Schedule & Deadlines"},"type":"lvl3","url":"/project-submission-guide#two-week-project-workflow","position":6},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl3":"Two-Week Project Workflow","lvl2":"Project Schedule & Deadlines"},"content":"Week 1: Understanding & Initial Implementation\n\nDay 1-2 (Mon-Tue): Read assignment thoroughly, understand requirements, review relevant JupyterBook chapter\n\nDay 3-4 (Wed-Thu): Begin implementation, focus on core functionality\n\nDay 5 (Fri): Class session - ask questions, pair programming, debug with peers\n\nDay 6-7 (Sat-Sun): Continue implementation based on class insights\n\nWeek 2: Refinement & Completion\n\nDay 8-9 (Mon-Tue): Complete base requirements, begin mandatory extensions\n\nDay 10-11 (Wed-Thu): Test edge cases, optimize performance\n\nDay 12 (Fri): Class session - final debugging, optimization discussions\n\nDay 13 (Sat-Sun): Polish code, write documentation, complete project memo\n\nDay 14 (Mon): Final review, submit by 11:59 PM","type":"content","url":"/project-submission-guide#two-week-project-workflow","position":7},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl2":"Submission Requirements"},"type":"lvl2","url":"/project-submission-guide#submission-requirements","position":8},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl2":"Submission Requirements"},"content":"","type":"content","url":"/project-submission-guide#submission-requirements","position":9},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl3":"Each Project Must Include","lvl2":"Submission Requirements"},"type":"lvl3","url":"/project-submission-guide#each-project-must-include","position":10},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl3":"Each Project Must Include","lvl2":"Submission Requirements"},"content":"","type":"content","url":"/project-submission-guide#each-project-must-include","position":11},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl4":"1. Code Components","lvl3":"Each Project Must Include","lvl2":"Submission Requirements"},"type":"lvl4","url":"/project-submission-guide#id-1-code-components","position":12},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl4":"1. Code Components","lvl3":"Each Project Must Include","lvl2":"Submission Requirements"},"content":"project_N/\n├── src/\n│   ├── __init__.py\n│   ├── main.py           # Entry point with clear argument parsing\n│   ├── physics.py        # Physics calculations\n│   ├── numerics.py       # Numerical methods\n│   ├── utils.py          # Helper functions\n│   └── visualization.py  # Plotting functions\n├── tests/\n│   └── test_core.py      # At least basic tests\n├── outputs/\n│   ├── figures/          # All generated plots\n│   └── data/             # Any output data files\n├── README.md             # Installation and usage instructions\n├── requirements.txt      # All dependencies with versions\n├── project_memo.md       # Your analysis and reflection\n└── .gitignore           # Properly configured\n\nCode Standards:\n\nModular design with clear separation of concerns\n\nNo God functions (functions should do one thing well)\n\nMeaningful variable names (no single letters except for indices)\n\nType hints encouraged for function signatures\n\nNo global variables unless absolutely necessary\n\nError handling for edge cases","type":"content","url":"/project-submission-guide#id-1-code-components","position":13},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl4":"2. Project Memo (Markdown Format)","lvl3":"Each Project Must Include","lvl2":"Submission Requirements"},"type":"lvl4","url":"/project-submission-guide#id-2-project-memo-markdown-format","position":14},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl4":"2. Project Memo (Markdown Format)","lvl3":"Each Project Must Include","lvl2":"Submission Requirements"},"content":"Your memo should be 2-5 pages and include:# Project N: [Title] - Memo\nAuthor: [Your Name]\nDate: [Submission Date]\n\n## Executive Summary\n[1-2 paragraphs summarizing what you did and key findings]\n\n## Methodology\n[How you approached the problem, key algorithmic choices]\n\n## Results\n[Key findings with embedded plots using relative paths]\n![Description](outputs/figures/plot1.png)\n\n## Computational Performance\n[Runtime analysis, bottlenecks identified, optimizations made]\n\n## Challenges & Solutions\n[What was hard, how you solved it, what you learned]\n\n## Extensions Implemented\n[Description of mandatory extensions completed]\n\n## Reflection\n[What you learned about computational physics and programming]","type":"content","url":"/project-submission-guide#id-2-project-memo-markdown-format","position":15},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl4":"3. Documentation Requirements","lvl3":"Each Project Must Include","lvl2":"Submission Requirements"},"type":"lvl4","url":"/project-submission-guide#id-3-documentation-requirements","position":16},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl4":"3. Documentation Requirements","lvl3":"Each Project Must Include","lvl2":"Submission Requirements"},"content":"README.md must include:# Project N: [Descriptive Title]\n\n## Description\n[Brief description of what this project does]\n\n## Installation\n```bash\nconda create -n proj_n python=3.10\nconda activate proj_n\npip install -r requirements.txt","type":"content","url":"/project-submission-guide#id-3-documentation-requirements","position":17},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl2":"Usage"},"type":"lvl2","url":"/project-submission-guide#usage","position":18},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl2":"Usage"},"content":"python src/main.py --input data.txt --output results.png","type":"content","url":"/project-submission-guide#usage","position":19},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl2":"Project Structure"},"type":"lvl2","url":"/project-submission-guide#project-structure","position":20},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl2":"Project Structure"},"content":"[Brief description of each file’s purpose]","type":"content","url":"/project-submission-guide#project-structure","position":21},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl2":"Key Results"},"type":"lvl2","url":"/project-submission-guide#key-results","position":22},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl2":"Key Results"},"content":"[Summary of main findings]","type":"content","url":"/project-submission-guide#key-results","position":23},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl2":"Dependencies"},"type":"lvl2","url":"/project-submission-guide#dependencies","position":24},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl2":"Dependencies"},"content":"Python 3.10+\n\nNumPy, SciPy, Matplotlib\n\n[Any other specific packages]\n**Function Docstrings Example:**\n```python\ndef integrate_orbit(initial_conditions, time_span, method='RK4', dt=0.01):\n    \"\"\"\n    Integrate orbital dynamics using specified numerical method.\n    \n    Parameters\n    ----------\n    initial_conditions : np.ndarray\n        Shape (6,) array of [x, y, z, vx, vy, vz]\n    time_span : tuple\n        (t_start, t_end) for integration\n    method : str, optional\n        Integration method: 'Euler', 'RK4', or 'Leapfrog'\n    dt : float, optional\n        Time step size\n    \n    Returns\n    -------\n    trajectory : np.ndarray\n        Shape (n_steps, 6) array of positions and velocities\n    times : np.ndarray\n        Shape (n_steps,) array of time points\n    \n    Raises\n    ------\n    ValueError\n        If method is not recognized or dt <= 0\n    \n    Examples\n    --------\n    >>> ic = np.array([1, 0, 0, 0, 1, 0])\n    >>> traj, t = integrate_orbit(ic, (0, 10))\n    \"\"\"\n    # Implementation here","type":"content","url":"/project-submission-guide#dependencies","position":25},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl4":"4. GitHub Classroom Requirements","lvl2":"Dependencies"},"type":"lvl4","url":"/project-submission-guide#id-4-github-classroom-requirements","position":26},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl4":"4. GitHub Classroom Requirements","lvl2":"Dependencies"},"content":"Commit Practices:\n\nCommit early and often (minimum 5-10 meaningful commits per project)\n\nEach commit should represent a logical unit of work\n\nCommit messages should be descriptive:\n\n✅ Good: “Add RK4 integration method with adaptive timestep”\n\n❌ Bad: “Update code” or “Fix stuff”\n\n.gitignore must include:# Python\n__pycache__/\n*.py[cod]\n*$py.class\n*.so\n.Python\n\n# Virtual Environment\nvenv/\nenv/\nENV/\n\n# IDE\n.vscode/\n.idea/\n*.swp\n.DS_Store\n\n# Project specific\noutputs/figures/*.png\noutputs/data/*.txt\n*.log\n\n# But track\n!outputs/figures/.gitkeep\n!outputs/data/.gitkeep","type":"content","url":"/project-submission-guide#id-4-github-classroom-requirements","position":27},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl2":"Grading Rubric"},"type":"lvl2","url":"/project-submission-guide#grading-rubric","position":28},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl2":"Grading Rubric"},"content":"","type":"content","url":"/project-submission-guide#grading-rubric","position":29},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl3":"Project Grading Breakdown (100 points total)","lvl2":"Grading Rubric"},"type":"lvl3","url":"/project-submission-guide#project-grading-breakdown-100-points-total","position":30},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl3":"Project Grading Breakdown (100 points total)","lvl2":"Grading Rubric"},"content":"Component\n\nPoints\n\nCriteria\n\nCore Implementation\n\n40\n\nCorrectness, completeness, follows specifications\n\nMandatory Extensions\n\n30\n\nAll required extensions implemented and working\n\nCode Quality\n\n15\n\nStructure, readability, documentation, style\n\nProject Memo\n\n15\n\nAnalysis quality, reflection depth, visualization","type":"content","url":"/project-submission-guide#project-grading-breakdown-100-points-total","position":31},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl3":"Detailed Rubric","lvl2":"Grading Rubric"},"type":"lvl3","url":"/project-submission-guide#detailed-rubric","position":32},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl3":"Detailed Rubric","lvl2":"Grading Rubric"},"content":"","type":"content","url":"/project-submission-guide#detailed-rubric","position":33},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl4":"Core Implementation (40 points)","lvl3":"Detailed Rubric","lvl2":"Grading Rubric"},"type":"lvl4","url":"/project-submission-guide#core-implementation-40-points","position":34},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl4":"Core Implementation (40 points)","lvl3":"Detailed Rubric","lvl2":"Grading Rubric"},"content":"Excellent (36-40): All requirements met, code runs without errors, produces correct results, handles edge cases\n\nGood (32-35): Most requirements met, minor bugs, generally correct results\n\nSatisfactory (28-31): Core functionality works, some requirements missing, several bugs\n\nNeeds Improvement (0-27): Major functionality missing, significant bugs, incorrect results","type":"content","url":"/project-submission-guide#core-implementation-40-points","position":35},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl4":"Mandatory Extensions (30 points)","lvl3":"Detailed Rubric","lvl2":"Grading Rubric"},"type":"lvl4","url":"/project-submission-guide#mandatory-extensions-30-points","position":36},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl4":"Mandatory Extensions (30 points)","lvl3":"Detailed Rubric","lvl2":"Grading Rubric"},"content":"Excellent (27-30): All extensions complete, creative implementation, goes beyond minimum\n\nGood (24-26): All extensions complete, solid implementation\n\nSatisfactory (21-23): Most extensions complete, basic implementation\n\nNeeds Improvement (0-20): Extensions missing or non-functional","type":"content","url":"/project-submission-guide#mandatory-extensions-30-points","position":37},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl4":"Code Quality (15 points)","lvl3":"Detailed Rubric","lvl2":"Grading Rubric"},"type":"lvl4","url":"/project-submission-guide#code-quality-15-points","position":38},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl4":"Code Quality (15 points)","lvl3":"Detailed Rubric","lvl2":"Grading Rubric"},"content":"Structure (5 pts): Modular design, appropriate file organization\n\nDocumentation (5 pts): Clear docstrings, helpful comments, complete README\n\nStyle (5 pts): Consistent formatting, meaningful names, follows Python conventions","type":"content","url":"/project-submission-guide#code-quality-15-points","position":39},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl4":"Project Memo (15 points)","lvl3":"Detailed Rubric","lvl2":"Grading Rubric"},"type":"lvl4","url":"/project-submission-guide#project-memo-15-points","position":40},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl4":"Project Memo (15 points)","lvl3":"Detailed Rubric","lvl2":"Grading Rubric"},"content":"Analysis (7 pts): Demonstrates understanding, interprets results correctly\n\nReflection (4 pts): Thoughtful discussion of challenges and learning\n\nPresentation (4 pts): Clear writing, effective visualizations, proper formatting","type":"content","url":"/project-submission-guide#project-memo-15-points","position":41},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl2":"Mandatory Extensions"},"type":"lvl2","url":"/project-submission-guide#mandatory-extensions","position":42},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl2":"Mandatory Extensions"},"content":"Each project includes required extensions that push you beyond the base implementation. These are NOT optional and constitute 30% of your project grade.","type":"content","url":"/project-submission-guide#mandatory-extensions","position":43},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl3":"Types of Extensions","lvl2":"Mandatory Extensions"},"type":"lvl3","url":"/project-submission-guide#types-of-extensions","position":44},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl3":"Types of Extensions","lvl2":"Mandatory Extensions"},"content":"Performance Extensions:\n\nOptimize algorithms for speed (vectorization, better algorithms)\n\nMemory optimization for large-scale problems\n\nParallel processing implementation\n\nScientific Extensions:\n\nParameter studies and sensitivity analysis\n\nComparison with analytical solutions where available\n\nError analysis and convergence studies\n\nMethodological Extensions:\n\nImplement alternative algorithms and compare\n\nAdd adaptive methods (timestep, resolution, etc.)\n\nExtend to more complex physics\n\nVisualization Extensions:\n\nInteractive plots\n\nAnimations of time evolution\n\n3D visualizations where appropriate\n\nSpecific extensions for each project will be detailed in the individual project assignments.","type":"content","url":"/project-submission-guide#types-of-extensions","position":45},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl2":"Common Issues & Solutions"},"type":"lvl2","url":"/project-submission-guide#common-issues-solutions","position":46},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl2":"Common Issues & Solutions"},"content":"","type":"content","url":"/project-submission-guide#common-issues-solutions","position":47},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl3":"Git/GitHub Issues","lvl2":"Common Issues & Solutions"},"type":"lvl3","url":"/project-submission-guide#git-github-issues","position":48},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl3":"Git/GitHub Issues","lvl2":"Common Issues & Solutions"},"content":"Problem: “I accidentally committed large files”git rm --cached large_file.dat\ngit commit -m \"Remove large file\"\ngit push\n\nProblem: “I forgot to commit regularly”\n\nStart committing now! Better late than never\n\nBreak your current code into logical pieces and commit each\n\nProblem: “I want to undo my last commit”git reset --soft HEAD~1  # Keeps changes\ngit reset --hard HEAD~1  # Discards changes (careful!)","type":"content","url":"/project-submission-guide#git-github-issues","position":49},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl3":"Python Issues","lvl2":"Common Issues & Solutions"},"type":"lvl3","url":"/project-submission-guide#python-issues","position":50},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl3":"Python Issues","lvl2":"Common Issues & Solutions"},"content":"Problem: “ImportError: No module named...”\n\nCheck your virtual environment is activated\n\nVerify package is in requirements.txt\n\nInstall with: pip install package_name\n\nProblem: “My code is slow”\n\nProfile first: python -m cProfile -s time your_script.py\n\nVectorize NumPy operations\n\nAvoid loops where possible\n\nConsider numba for critical sections\n\nProblem: “Memory error with large arrays”\n\nUse generators instead of lists where possible\n\nProcess data in chunks\n\nUse np.float32 instead of np.float64 if precision allows","type":"content","url":"/project-submission-guide#python-issues","position":51},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl2":"Submission Checklist"},"type":"lvl2","url":"/project-submission-guide#submission-checklist","position":52},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl2":"Submission Checklist"},"content":"Before submitting, verify:\n\nCode runs without errors on a clean environment\n\nAll required files are present and properly named\n\nREADME includes clear installation and usage instructions\n\nAt least 5 meaningful commits in Git history\n\nProject memo includes all required sections\n\nAll plots are generated and saved in outputs/figures/\n\nMandatory extensions are complete and documented\n\nCode follows style guidelines (no IDE AI assistance used)\n\nFinal push completed before Monday 11:59 PM deadline","type":"content","url":"/project-submission-guide#submission-checklist","position":53},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl2":"Late Policy Reminder"},"type":"lvl2","url":"/project-submission-guide#late-policy-reminder","position":54},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl2":"Late Policy Reminder"},"content":"One no-questions-asked 2-day extension per semester\n\nMust be requested at least 24 hours before deadline\n\n10% penalty per day after grace period\n\nSubmit early if complete—no bonus, but peace of mind!","type":"content","url":"/project-submission-guide#late-policy-reminder","position":55},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl2":"Getting Help"},"type":"lvl2","url":"/project-submission-guide#getting-help","position":56},{"hierarchy":{"lvl1":"ASTR 596: Project Submission Guide","lvl2":"Getting Help"},"content":"When to seek help:\n\nAfter 20-30 minutes of genuine effort on a bug\n\nWhen you don’t understand the physics/math despite reading\n\nIf you’re unsure about project requirements\n\nHow to ask for help effectively:\n\nDescribe what you’re trying to do\n\nShow what you’ve tried (code snippets, error messages)\n\nExplain what you expected vs. what happened\n\nInclude minimal reproducible example if possible\n\nResources:\n\nCourse Slack (fastest response)\n\nOffice hours (for complex issues)\n\nPair programming sessions (learn from peers)\n\nAI tutors (for concepts, not code generation)\n\nRemember: Struggling is part of learning. But struggling alone for too long is inefficient. Ask for help!","type":"content","url":"/project-submission-guide#getting-help","position":57},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics"},"type":"lvl1","url":"/project1-description","position":0},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics"},"content":"Duration: 3 weeks\nWeight: 12% of course grade\nTheme: “From Observations to Blackbody Physics”","type":"content","url":"/project1-description","position":1},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl3":"Project Overview"},"type":"lvl3","url":"/project1-description#project-overview","position":2},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl3":"Project Overview"},"content":"This project establishes the computational and theoretical foundations for the entire course. You will develop essential Python programming skills, implement numerical analysis methods from scratch, and apply them to fundamental stellar physics problems. By the end, you’ll have a complete toolkit for stellar analysis that will be used throughout subsequent projects.","type":"content","url":"/project1-description#project-overview","position":3},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl3":"Learning Objectives"},"type":"lvl3","url":"/project1-description#learning-objectives","position":4},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl3":"Learning Objectives"},"content":"By completing this project, you will:\n\nMaster Python fundamentals: Functions, classes, data structures, and scientific computing libraries\n\nImplement numerical methods: Integration techniques and root-finding algorithms\n\nUnderstand stellar physics: Blackbody radiation, Wien’s law, and stellar classification\n\nDevelop professional practices: Version control, testing, documentation, and code organization\n\nBuild modular software: Create reusable components for astrophysical calculations","type":"content","url":"/project1-description#learning-objectives","position":5},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl2":"Week 1: Development Environment and Stellar Data Analysis"},"type":"lvl2","url":"/project1-description#week-1-development-environment-and-stellar-data-analysis","position":6},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl2":"Week 1: Development Environment and Stellar Data Analysis"},"content":"","type":"content","url":"/project1-description#week-1-development-environment-and-stellar-data-analysis","position":7},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl3":"Conceptual Introduction (30 min)","lvl2":"Week 1: Development Environment and Stellar Data Analysis"},"type":"lvl3","url":"/project1-description#conceptual-introduction-30-min","position":8},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl3":"Conceptual Introduction (30 min)","lvl2":"Week 1: Development Environment and Stellar Data Analysis"},"content":"Course overview and computational astrophysics in modern research\n\nSoftware development workflow: Git, GitHub, and collaborative coding\n\nPython ecosystem for astronomy: NumPy, Matplotlib, Pandas, Astropy\n\nIntroduction to stellar observations and the Hertzsprung-Russell diagram","type":"content","url":"/project1-description#conceptual-introduction-30-min","position":9},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl3":"Lab Session Objectives","lvl2":"Week 1: Development Environment and Stellar Data Analysis"},"type":"lvl3","url":"/project1-description#lab-session-objectives","position":10},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl3":"Lab Session Objectives","lvl2":"Week 1: Development Environment and Stellar Data Analysis"},"content":"Set up professional development environment and begin astronomical data analysis.","type":"content","url":"/project1-description#lab-session-objectives","position":11},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl4":"Task 1: Environment Setup (30 min)","lvl3":"Lab Session Objectives","lvl2":"Week 1: Development Environment and Stellar Data Analysis"},"type":"lvl4","url":"/project1-description#task-1-environment-setup-30-min","position":12},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl4":"Task 1: Environment Setup (30 min)","lvl3":"Lab Session Objectives","lvl2":"Week 1: Development Environment and Stellar Data Analysis"},"content":"Goal: Establish reproducible computational environment\n\nInstructions:\n\nInstall Miniconda/Anaconda\n\nDownload from official site\n\nCreate course-specific environment: conda create -n astr596 python=3.11\n\nInstall essential packages: numpy matplotlib pandas jupyter astropy\n\nGit and GitHub Setup\n\nCreate GitHub account if needed\n\nConfigure Git with your name and email\n\nFork the course repository template\n\nClone your fork locally: git clone <your-repo-url>\n\nIDE Configuration\n\nInstall VS Code or preferred editor\n\nConfigure Python interpreter to use conda environment\n\nInstall useful extensions: Python, Jupyter, GitLens\n\nDeliverable: Screenshot of successful environment test","type":"content","url":"/project1-description#task-1-environment-setup-30-min","position":13},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl4":"Task 2: Python Fundamentals Review (60 min)","lvl3":"Lab Session Objectives","lvl2":"Week 1: Development Environment and Stellar Data Analysis"},"type":"lvl4","url":"/project1-description#task-2-python-fundamentals-review-60-min","position":14},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl4":"Task 2: Python Fundamentals Review (60 min)","lvl3":"Lab Session Objectives","lvl2":"Week 1: Development Environment and Stellar Data Analysis"},"content":"Goal: Refresh/establish core Python programming skills\n\nCore Concepts to Implement:# Data types and operations\ndef basic_arithmetic_operations():\n    \"\"\"Practice with numbers, strings, lists, dictionaries.\"\"\"\n    \n# Control structures\ndef stellar_magnitude_classifier(magnitude):\n    \"\"\"Classify stars by brightness using if/elif/else.\"\"\"\n    \n# File I/O\ndef load_stellar_catalog(filename):\n    \"\"\"Read CSV data using both built-in and pandas methods.\"\"\"\n    \n# List comprehensions and basic algorithms\ndef filter_stars_by_criteria(catalog, min_magnitude, max_magnitude):\n    \"\"\"Filter stellar data using comprehensions and boolean indexing.\"\"\"\n\nPractice Dataset: Hipparcos catalog subset (provided)\n\nValidation: Compare your results with provided reference outputs","type":"content","url":"/project1-description#task-2-python-fundamentals-review-60-min","position":15},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl4":"Task 3: First Astronomical Analysis (40 min)","lvl3":"Lab Session Objectives","lvl2":"Week 1: Development Environment and Stellar Data Analysis"},"type":"lvl4","url":"/project1-description#task-3-first-astronomical-analysis-40-min","position":16},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl4":"Task 3: First Astronomical Analysis (40 min)","lvl3":"Lab Session Objectives","lvl2":"Week 1: Development Environment and Stellar Data Analysis"},"content":"Goal: Apply Python skills to real astronomical data\n\nImplementation Requirements:import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef load_hipparcos_data(filename):\n    \"\"\"Load and clean Hipparcos stellar catalog.\"\"\"\n    # Handle missing data and outliers\n    # Convert magnitude and color data to proper types\n    # Calculate distances from parallax measurements\n    \ndef basic_stellar_statistics(catalog):\n    \"\"\"Calculate fundamental statistics of stellar sample.\"\"\"\n    # Mean, median, standard deviation of key parameters\n    # Magnitude distributions\n    # Color distributions\n    \ndef create_color_magnitude_diagram(catalog):\n    \"\"\"Generate first HR diagram.\"\"\"\n    # Plot B-V color vs absolute magnitude\n    # Add proper axis labels and title\n    # Include error bars where appropriate\n\nAnalysis Questions:\n\nWhat is the range of stellar magnitudes in the sample?\n\nHow many stars have reliable parallax measurements?\n\nWhat patterns do you observe in the color-magnitude diagram?\n\nWeek 1 Deliverable: Jupyter notebook with environment setup, Python exercises, and basic stellar analysis","type":"content","url":"/project1-description#task-3-first-astronomical-analysis-40-min","position":17},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl2":"Week 2: Numerical Integration and Blackbody Physics"},"type":"lvl2","url":"/project1-description#week-2-numerical-integration-and-blackbody-physics","position":18},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl2":"Week 2: Numerical Integration and Blackbody Physics"},"content":"","type":"content","url":"/project1-description#week-2-numerical-integration-and-blackbody-physics","position":19},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl3":"Conceptual Introduction (25 min)","lvl2":"Week 2: Numerical Integration and Blackbody Physics"},"type":"lvl3","url":"/project1-description#conceptual-introduction-25-min","position":20},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl3":"Conceptual Introduction (25 min)","lvl2":"Week 2: Numerical Integration and Blackbody Physics"},"content":"Blackbody radiation and the Planck function\n\nStefan-Boltzmann law and Wien’s displacement law\n\nNumerical integration: why and when we need it\n\nTrapezoidal rule, Simpson’s rule, and Gaussian quadrature","type":"content","url":"/project1-description#conceptual-introduction-25-min","position":21},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl3":"Lab Session Objectives","lvl2":"Week 2: Numerical Integration and Blackbody Physics"},"type":"lvl3","url":"/project1-description#lab-session-objectives-1","position":22},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl3":"Lab Session Objectives","lvl2":"Week 2: Numerical Integration and Blackbody Physics"},"content":"Implement numerical integration methods and apply them to stellar radiation calculations.","type":"content","url":"/project1-description#lab-session-objectives-1","position":23},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl4":"Task 1: Numerical Integration Library (45 min)","lvl3":"Lab Session Objectives","lvl2":"Week 2: Numerical Integration and Blackbody Physics"},"type":"lvl4","url":"/project1-description#task-1-numerical-integration-library-45-min","position":24},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl4":"Task 1: Numerical Integration Library (45 min)","lvl3":"Lab Session Objectives","lvl2":"Week 2: Numerical Integration and Blackbody Physics"},"content":"Goal: Build integration toolkit from first principles\n\nRequired Implementations:import numpy as np\n\ndef trapezoid_rule(func, a, b, n_points):\n    \"\"\"\n    Implement trapezoidal rule integration.\n    \n    Parameters:\n    -----------\n    func : callable\n        Function to integrate\n    a, b : float\n        Integration limits\n    n_points : int\n        Number of grid points\n        \n    Returns:\n    --------\n    integral : float\n        Numerical approximation of integral\n    \"\"\"\n    # YOUR IMPLEMENTATION HERE\n    # Calculate spacing: h = (b-a)/(n_points-1)\n    # Create grid points\n    # Apply trapezoidal rule formula\n    \ndef simpson_rule(func, a, b, n_points):\n    \"\"\"\n    Implement Simpson's rule (requires odd number of points).\n    More accurate than trapezoidal rule for smooth functions.\n    \"\"\"\n    # YOUR IMPLEMENTATION HERE\n    # Ensure n_points is odd\n    # Apply Simpson's 1/3 rule\n    \ndef gaussian_quadrature(func, a, b, n_points):\n    \"\"\"\n    Implement Gaussian quadrature for high accuracy.\n    Use numpy.polynomial.legendre.leggauss for weights and nodes.\n    \"\"\"\n    # YOUR IMPLEMENTATION HERE\n    # Transform from [-1,1] to [a,b]\n    # Apply Gaussian quadrature formula\n\nValidation Tests:\n\nTest on functions with known integrals: x², sin(x), exp(x)\n\nCompare accuracy vs computational cost\n\nPlot convergence as function of n_points","type":"content","url":"/project1-description#task-1-numerical-integration-library-45-min","position":25},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl4":"Task 2: Blackbody Radiation Functions (60 min)","lvl3":"Lab Session Objectives","lvl2":"Week 2: Numerical Integration and Blackbody Physics"},"type":"lvl4","url":"/project1-description#task-2-blackbody-radiation-functions-60-min","position":26},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl4":"Task 2: Blackbody Radiation Functions (60 min)","lvl3":"Lab Session Objectives","lvl2":"Week 2: Numerical Integration and Blackbody Physics"},"content":"Goal: Implement Planck function and related stellar physics\n\nCore Physics Implementation:# Physical constants (use astropy.constants for precision)\nh = 6.626e-34      # Planck constant [J⋅s]\nc = 2.998e8        # Speed of light [m/s]\nk_B = 1.381e-23    # Boltzmann constant [J/K]\nsigma_SB = 5.67e-8 # Stefan-Boltzmann constant [W/m²/K⁴]\n\ndef planck_function_frequency(nu, T):\n    \"\"\"\n    Planck function B_ν(T) in frequency units.\n    \n    Parameters:\n    -----------\n    nu : float or array\n        Frequency [Hz]\n    T : float\n        Temperature [K]\n        \n    Returns:\n    --------\n    B_nu : float or array\n        Spectral radiance [W/m²/Hz/sr]\n    \"\"\"\n    # B_ν(T) = (2hν³/c²) × 1/(exp(hν/kT) - 1)\n    # Handle numerical issues for small and large arguments\n    \ndef planck_function_wavelength(wavelength, T):\n    \"\"\"\n    Planck function B_λ(T) in wavelength units.\n    \n    Note: B_λ dλ = B_ν dν, so B_λ = B_ν × (dν/dλ) = B_ν × (c/λ²)\n    \"\"\"\n    # Convert wavelength to frequency and apply conversion\n    \ndef stellar_luminosity_integral(T_eff, R_star):\n    \"\"\"\n    Calculate stellar luminosity by integrating Planck function.\n    \n    L = 4πR² ∫ π B_ν(T) dν = 4πR² σT⁴\n    \n    Verify Stefan-Boltzmann law numerically.\n    \"\"\"\n    # Integrate Planck function over all frequencies\n    # Compare with analytical Stefan-Boltzmann result\n    # Calculate relative error\n\nStellar Applications:def frequency_integrated_intensity(T_eff, nu_min, nu_max):\n    \"\"\"\n    Integrate Planck function over frequency range.\n    Critical for radiation pressure calculations in Project 3.\n    \"\"\"\n    \ndef stellar_flux_at_distance(L_star, distance):\n    \"\"\"\n    Calculate observed flux: F = L/(4πd²)\n    \"\"\"\n    \ndef main_sequence_relations():\n    \"\"\"\n    Implement empirical mass-luminosity and mass-temperature relations.\n    L ∝ M³⋅⁵ for M > 1 M☉\n    T_eff ∝ M⁰⋅⁵ for main sequence stars\n    \"\"\"","type":"content","url":"/project1-description#task-2-blackbody-radiation-functions-60-min","position":27},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl4":"Task 3: Integration Method Comparison (30 min)","lvl3":"Lab Session Objectives","lvl2":"Week 2: Numerical Integration and Blackbody Physics"},"type":"lvl4","url":"/project1-description#task-3-integration-method-comparison-30-min","position":28},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl4":"Task 3: Integration Method Comparison (30 min)","lvl3":"Lab Session Objectives","lvl2":"Week 2: Numerical Integration and Blackbody Physics"},"content":"Goal: Understand computational trade-offs in numerical methods\n\nAnalysis Requirements:\n\nAccuracy Study: For each integration method, plot error vs n_points\n\nPerformance Study: Time each method for various n_points\n\nFunction Sensitivity: How do methods perform on oscillatory vs smooth functions?\n\nTest Functions:\n\nSmooth: Planck function at various temperatures\n\nOscillatory: Wien displacement law integral\n\nSharp features: Planck function at low temperatures\n\nWeek 2 Deliverable: Integration library with comprehensive testing and stellar luminosity calculations","type":"content","url":"/project1-description#task-3-integration-method-comparison-30-min","position":29},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl2":"Week 3: Root-Finding and Object-Oriented Design"},"type":"lvl2","url":"/project1-description#week-3-root-finding-and-object-oriented-design","position":30},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl2":"Week 3: Root-Finding and Object-Oriented Design"},"content":"","type":"content","url":"/project1-description#week-3-root-finding-and-object-oriented-design","position":31},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl3":"Conceptual Introduction (25 min)","lvl2":"Week 3: Root-Finding and Object-Oriented Design"},"type":"lvl3","url":"/project1-description#conceptual-introduction-25-min-1","position":32},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl3":"Conceptual Introduction (25 min)","lvl2":"Week 3: Root-Finding and Object-Oriented Design"},"content":"Root-finding problems in astrophysics\n\nBisection method: guaranteed convergence\n\nNewton-Raphson method: fast convergence with derivatives\n\nSecant method: fast convergence without derivatives\n\nObject-oriented programming: when and why to use classes","type":"content","url":"/project1-description#conceptual-introduction-25-min-1","position":33},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl3":"Lab Session Objectives","lvl2":"Week 3: Root-Finding and Object-Oriented Design"},"type":"lvl3","url":"/project1-description#lab-session-objectives-2","position":34},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl3":"Lab Session Objectives","lvl2":"Week 3: Root-Finding and Object-Oriented Design"},"content":"Implement root-finding algorithms and design object-oriented stellar analysis framework.","type":"content","url":"/project1-description#lab-session-objectives-2","position":35},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl4":"Task 1: Root-Finding Algorithm Library (45 min)","lvl3":"Lab Session Objectives","lvl2":"Week 3: Root-Finding and Object-Oriented Design"},"type":"lvl4","url":"/project1-description#task-1-root-finding-algorithm-library-45-min","position":36},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl4":"Task 1: Root-Finding Algorithm Library (45 min)","lvl3":"Lab Session Objectives","lvl2":"Week 3: Root-Finding and Object-Oriented Design"},"content":"Goal: Build robust root-finding toolkit\n\nRequired Implementations:def bisection_method(func, a, b, tolerance=1e-6, max_iterations=100):\n    \"\"\"\n    Find root using bisection method.\n    \n    Guaranteed to converge if func(a) and func(b) have opposite signs.\n    Slow but robust.\n    \"\"\"\n    # Check preconditions: func(a) * func(b) < 0\n    # Implement bisection algorithm\n    # Return root, number of iterations, convergence status\n    \ndef newton_raphson(func, func_derivative, x0, tolerance=1e-6, max_iterations=50):\n    \"\"\"\n    Find root using Newton-Raphson method.\n    \n    Fast convergence but requires derivative and good initial guess.\n    \"\"\"\n    # Implement Newton-Raphson: x_{n+1} = x_n - f(x_n)/f'(x_n)\n    # Handle cases where derivative is zero\n    # Return root, number of iterations, convergence status\n    \ndef secant_method(func, x0, x1, tolerance=1e-6, max_iterations=50):\n    \"\"\"\n    Find root using secant method.\n    \n    Fast convergence without requiring derivative.\n    \"\"\"\n    # Implement secant method using finite difference approximation\n    # Handle cases where function values are too close\n    # Return root, number of iterations, convergence status","type":"content","url":"/project1-description#task-1-root-finding-algorithm-library-45-min","position":37},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl4":"Task 2: Wien’s Law and Stellar Physics Applications (60 min)","lvl3":"Lab Session Objectives","lvl2":"Week 3: Root-Finding and Object-Oriented Design"},"type":"lvl4","url":"/project1-description#task-2-wiens-law-and-stellar-physics-applications-60-min","position":38},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl4":"Task 2: Wien’s Law and Stellar Physics Applications (60 min)","lvl3":"Lab Session Objectives","lvl2":"Week 3: Root-Finding and Object-Oriented Design"},"content":"Goal: Apply root-finding to solve transcendental equations in stellar physics\n\nWien’s Displacement Law Implementation:def wien_displacement_equation(x):\n    \"\"\"\n    Wien's law equation: 5 - x = 5*exp(-x)\n    where x = hc/(λ_max * k_B * T)\n    \n    Root occurs at x ≈ 4.965114\n    \"\"\"\n    return 5 - x - 5*np.exp(-x)\n\ndef wien_displacement_derivative(x):\n    \"\"\"Analytical derivative for Newton-Raphson method.\"\"\"\n    return -1 + 5*np.exp(-x)\n\ndef find_peak_wavelength(temperature):\n    \"\"\"\n    Find wavelength of peak emission for given temperature.\n    \n    Uses root-finding to solve Wien's displacement law.\n    \"\"\"\n    # Solve Wien equation for x\n    # Convert to wavelength: λ_max = hc/(x * k_B * T)\n    # Validate with Wien's constant: λ_max * T = 2.898e-3 m⋅K\n    \ndef temperature_from_color_index(color_bv, color_system='Johnson'):\n    \"\"\"\n    Invert empirical color-temperature relations using root-finding.\n    \n    Example relation: log(T_eff) = 3.979 - 0.654*log(B-V + 1.334)\n    \"\"\"\n    # Define equation to solve: observed_color - predicted_color(T) = 0\n    # Use appropriate root-finding method\n    # Handle edge cases and invalid inputs","type":"content","url":"/project1-description#task-2-wiens-law-and-stellar-physics-applications-60-min","position":39},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl4":"Task 3: Object-Oriented Stellar Analysis Framework (30 min)","lvl3":"Lab Session Objectives","lvl2":"Week 3: Root-Finding and Object-Oriented Design"},"type":"lvl4","url":"/project1-description#task-3-object-oriented-stellar-analysis-framework-30-min","position":40},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl4":"Task 3: Object-Oriented Stellar Analysis Framework (30 min)","lvl3":"Lab Session Objectives","lvl2":"Week 3: Root-Finding and Object-Oriented Design"},"content":"Goal: Design clean, extensible code architecture\n\nClass Design:class Star:\n    \"\"\"\n    Represents individual star with physical and observational properties.\n    \"\"\"\n    \n    def __init__(self, name=None, mass=None, temperature=None, radius=None, \n                 magnitude_v=None, color_bv=None, parallax=None):\n        \"\"\"Initialize star with observational or theoretical data.\"\"\"\n        self.name = name\n        self.mass = mass  # Solar masses\n        self.temperature = temperature  # Kelvin\n        self.radius = radius  # Solar radii\n        self.magnitude_v = magnitude_v  # Apparent V magnitude\n        self.color_bv = color_bv  # B-V color index\n        self.parallax = parallax  # arcseconds\n        \n    def distance(self):\n        \"\"\"Calculate distance from parallax [pc].\"\"\"\n        if self.parallax is None or self.parallax <= 0:\n            return None\n        return 1.0 / self.parallax\n    \n    def absolute_magnitude(self):\n        \"\"\"Calculate absolute magnitude.\"\"\"\n        d = self.distance()\n        if d is None or self.magnitude_v is None:\n            return None\n        return self.magnitude_v - 5*np.log10(d/10)\n    \n    def luminosity(self):\n        \"\"\"Calculate luminosity using Stefan-Boltzmann law.\"\"\"\n        if self.temperature is None or self.radius is None:\n            return None\n        return 4*np.pi*(self.radius*R_sun)**2 * sigma_SB * self.temperature**4\n    \n    def estimate_temperature_from_color(self):\n        \"\"\"Estimate temperature from B-V color using root-finding.\"\"\"\n        if self.color_bv is None:\n            return None\n        return temperature_from_color_index(self.color_bv)\n    \n    def stellar_type(self):\n        \"\"\"Classify star based on temperature or color.\"\"\"\n        # Implement spectral classification (O, B, A, F, G, K, M)\n        \n    def radiation_pressure_luminosity(self):\n        \"\"\"\n        Calculate luminosity for radiation pressure calculations.\n        This method will be used in Project 3.\n        \"\"\"\n        return self.luminosity()\n\nclass StellarCatalog:\n    \"\"\"\n    Collection of stars with analysis capabilities.\n    \"\"\"\n    \n    def __init__(self, stars=None):\n        \"\"\"Initialize with list of Star objects.\"\"\"\n        self.stars = stars if stars is not None else []\n    \n    @classmethod\n    def from_file(cls, filename):\n        \"\"\"Load catalog from CSV file.\"\"\"\n        # Read data and create Star objects\n        \n    def filter_by_criteria(self, **criteria):\n        \"\"\"Filter stars based on various criteria.\"\"\"\n        # magnitude_range, color_range, distance_range, etc.\n        \n    def create_hr_diagram(self, save_path=None):\n        \"\"\"\n        Generate publication-quality HR diagram.\n        \"\"\"\n        # Plot absolute magnitude vs color or temperature\n        # Add theoretical main sequence track\n        # Include stellar classification regions\n        \n    def statistical_summary(self):\n        \"\"\"Generate comprehensive statistical analysis.\"\"\"\n        # Distributions of key parameters\n        # Correlations between observables\n\nWeek 3 Deliverable: Complete stellar analysis package with object-oriented design, root-finding applications, and advanced HR diagram analysis","type":"content","url":"/project1-description#task-3-object-oriented-stellar-analysis-framework-30-min","position":41},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl2":"Assessment and Grading"},"type":"lvl2","url":"/project1-description#assessment-and-grading","position":42},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl2":"Assessment and Grading"},"content":"","type":"content","url":"/project1-description#assessment-and-grading","position":43},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl3":"Grading Breakdown","lvl2":"Assessment and Grading"},"type":"lvl3","url":"/project1-description#grading-breakdown","position":44},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl3":"Grading Breakdown","lvl2":"Assessment and Grading"},"content":"Week 1: Environment setup and Python fundamentals (30%)\n\nWeek 2: Numerical integration and blackbody physics (35%)\n\nWeek 3: Root-finding and OOP implementation (35%)","type":"content","url":"/project1-description#grading-breakdown","position":45},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl3":"Evaluation Criteria","lvl2":"Assessment and Grading"},"type":"lvl3","url":"/project1-description#evaluation-criteria","position":46},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl3":"Evaluation Criteria","lvl2":"Assessment and Grading"},"content":"","type":"content","url":"/project1-description#evaluation-criteria","position":47},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl4":"Technical Implementation (60%)","lvl3":"Evaluation Criteria","lvl2":"Assessment and Grading"},"type":"lvl4","url":"/project1-description#technical-implementation-60","position":48},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl4":"Technical Implementation (60%)","lvl3":"Evaluation Criteria","lvl2":"Assessment and Grading"},"content":"Correctness: Do algorithms produce accurate results?\n\nEfficiency: Are implementations reasonably optimized?\n\nRobustness: Does code handle edge cases and errors gracefully?\n\nTesting: Are functions validated against known results?","type":"content","url":"/project1-description#technical-implementation-60","position":49},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl4":"Code Quality (25%)","lvl3":"Evaluation Criteria","lvl2":"Assessment and Grading"},"type":"lvl4","url":"/project1-description#code-quality-25","position":50},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl4":"Code Quality (25%)","lvl3":"Evaluation Criteria","lvl2":"Assessment and Grading"},"content":"Documentation: Clear docstrings and comments\n\nOrganization: Logical file structure and function design\n\nStyle: Follows Python conventions (PEP 8)\n\nVersion Control: Meaningful commit messages and regular commits","type":"content","url":"/project1-description#code-quality-25","position":51},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl4":"Scientific Understanding (15%)","lvl3":"Evaluation Criteria","lvl2":"Assessment and Grading"},"type":"lvl4","url":"/project1-description#scientific-understanding-15","position":52},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl4":"Scientific Understanding (15%)","lvl3":"Evaluation Criteria","lvl2":"Assessment and Grading"},"content":"Physics: Correct implementation of stellar physics\n\nValidation: Appropriate comparison with analytical results\n\nInterpretation: Understanding of numerical method trade-offs","type":"content","url":"/project1-description#scientific-understanding-15","position":53},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl3":"Deliverables","lvl2":"Assessment and Grading"},"type":"lvl3","url":"/project1-description#deliverables","position":54},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl3":"Deliverables","lvl2":"Assessment and Grading"},"content":"","type":"content","url":"/project1-description#deliverables","position":55},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl4":"Final Submission Requirements","lvl3":"Deliverables","lvl2":"Assessment and Grading"},"type":"lvl4","url":"/project1-description#final-submission-requirements","position":56},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl4":"Final Submission Requirements","lvl3":"Deliverables","lvl2":"Assessment and Grading"},"content":"Complete Python Package:\n\nstellar_physics.py: Blackbody and stellar property functions\n\nnumerical_methods.py: Integration and root-finding algorithms\n\nstellar_analysis.py: Star and StellarCatalog classes\n\ntests/: Comprehensive test suite\n\nREADME.md: Installation and usage instructions\n\nAnalysis Notebooks:\n\nweek1_python_fundamentals.ipynb: Environment setup and basic analysis\n\nweek2_numerical_integration.ipynb: Integration methods and stellar applications\n\nweek3_stellar_classification.ipynb: Advanced HR diagram analysis\n\nValidation Report: Document comparing your results with literature values","type":"content","url":"/project1-description#final-submission-requirements","position":57},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl3":"Connection to Future Projects","lvl2":"Assessment and Grading"},"type":"lvl3","url":"/project1-description#connection-to-future-projects","position":58},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl3":"Connection to Future Projects","lvl2":"Assessment and Grading"},"content":"This project establishes foundations used throughout the course:\n\nProject 2: Stellar mass-luminosity relations for realistic N-body clusters\n\nProject 3: Blackbody stellar spectra for radiation heating calculations\n\nProject 4: Numerical integration techniques translated to JAX\n\nFinal Project: Object-oriented design principles for research-grade software","type":"content","url":"/project1-description#connection-to-future-projects","position":59},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl3":"Getting Help","lvl2":"Assessment and Grading"},"type":"lvl3","url":"/project1-description#getting-help","position":60},{"hierarchy":{"lvl1":"ASTR 596 Project 1: Python Fundamentals + Numerical Analysis + Stellar Physics","lvl3":"Getting Help","lvl2":"Assessment and Grading"},"content":"Office Hours: Use for conceptual questions and debugging assistance\n\nPair Programming: Collaborate during lab sessions but submit individual work\n\nDiscussion Forum: Share general questions and solutions to common issues\n\nOnline Resources: Python documentation, NumPy tutorials, Astropy guides\n\nThis project sets the stage for sophisticated computational astrophysics while ensuring students master fundamental programming and numerical analysis skills.","type":"content","url":"/project1-description#getting-help","position":61},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems"},"type":"lvl1","url":"/project2-description","position":0},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems"},"content":"Duration: 3 weeks\nWeight: 15% of course grade\nTheme: “Realistic Stellar Clusters with Gravitational Dynamics”","type":"content","url":"/project2-description","position":1},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl3":"Project Overview"},"type":"lvl3","url":"/project2-description#project-overview","position":2},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl3":"Project Overview"},"content":"This project builds sophisticated N-body gravitational dynamics simulations with realistic stellar populations. You will implement multiple ODE integration schemes, master statistical sampling from astrophysical distributions, and create evolving stellar clusters that serve as input for radiation calculations in Project 3. The emphasis is on vectorization, performance optimization, and adaptive numerical methods.","type":"content","url":"/project2-description#project-overview","position":3},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl3":"Learning Objectives"},"type":"lvl3","url":"/project2-description#learning-objectives","position":4},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl3":"Learning Objectives"},"content":"By completing this project, you will:\n\nMaster ODE integration: Implement and compare multiple numerical integration schemes\n\nUnderstand gravitational dynamics: N-body physics, energy conservation, and cluster evolution\n\nLearn statistical sampling: Sample from Initial Mass Function and spatial distributions\n\nDevelop vectorization skills: Efficient NumPy operations for computational performance\n\nImplement adaptive methods: Energy-controlled timestep adjustment\n\nGenerate realistic astrophysical data: Stellar clusters for radiation modeling","type":"content","url":"/project2-description#learning-objectives","position":5},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl3":"Prerequisites from Project 1"},"type":"lvl3","url":"/project2-description#prerequisites-from-project-1","position":6},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl3":"Prerequisites from Project 1"},"content":"Numerical integration techniques (trapezoid, Simpson’s, Gaussian quadrature)\n\nRoot-finding methods (Newton-Raphson for energy balance)\n\nObject-oriented programming (Star class design)\n\nBlackbody physics and stellar luminosity calculations","type":"content","url":"/project2-description#prerequisites-from-project-1","position":7},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl2":"Week 1: ODE Solvers and Energy Conservation"},"type":"lvl2","url":"/project2-description#week-1-ode-solvers-and-energy-conservation","position":8},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl2":"Week 1: ODE Solvers and Energy Conservation"},"content":"","type":"content","url":"/project2-description#week-1-ode-solvers-and-energy-conservation","position":9},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl3":"Conceptual Introduction (25 min)","lvl2":"Week 1: ODE Solvers and Energy Conservation"},"type":"lvl3","url":"/project2-description#conceptual-introduction-25-min","position":10},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl3":"Conceptual Introduction (25 min)","lvl2":"Week 1: ODE Solvers and Energy Conservation"},"content":"Newton’s laws and gravitational force in astrophysical contexts\n\nConverting 2nd order ODEs to 1st order systems\n\nIntegration methods: explicit vs implicit, stability vs accuracy\n\nSymplectic integrators for Hamiltonian systems\n\nEnergy and angular momentum conservation in gravitational systems","type":"content","url":"/project2-description#conceptual-introduction-25-min","position":11},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl3":"Lab Session Objectives","lvl2":"Week 1: ODE Solvers and Energy Conservation"},"type":"lvl3","url":"/project2-description#lab-session-objectives","position":12},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl3":"Lab Session Objectives","lvl2":"Week 1: ODE Solvers and Energy Conservation"},"content":"Build comprehensive ODE solver library and validate on two-body dynamics.","type":"content","url":"/project2-description#lab-session-objectives","position":13},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl4":"Task 1: ODE Solver Framework (45 min)","lvl3":"Lab Session Objectives","lvl2":"Week 1: ODE Solvers and Energy Conservation"},"type":"lvl4","url":"/project2-description#task-1-ode-solver-framework-45-min","position":14},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl4":"Task 1: ODE Solver Framework (45 min)","lvl3":"Lab Session Objectives","lvl2":"Week 1: ODE Solvers and Energy Conservation"},"content":"Goal: Create abstract base class and implement multiple integration methods\n\nFramework Design:from abc import ABC, abstractmethod\nimport numpy as np\n\nclass ODESolver(ABC):\n    \"\"\"\n    Abstract base class for ODE integration methods.\n    \n    Solves system: dy/dt = f(t, y) where y can be vector-valued\n    \"\"\"\n    \n    def __init__(self, derivatives_func, initial_conditions, initial_time=0.0):\n        \"\"\"\n        Parameters:\n        -----------\n        derivatives_func : callable\n            Function f(t, y) returning dy/dt\n        initial_conditions : array_like\n            Initial values y(t0)\n        initial_time : float\n            Initial time t0\n        \"\"\"\n        self.f = derivatives_func\n        self.y = np.array(initial_conditions, dtype=float)\n        self.t = initial_time\n        self.history = {'t': [initial_time], 'y': [self.y.copy()]}\n    \n    @abstractmethod\n    def step(self, dt):\n        \"\"\"Take single integration step of size dt.\"\"\"\n        pass\n    \n    def evolve(self, t_final, dt):\n        \"\"\"Evolve system from current time to t_final.\"\"\"\n        while self.t < t_final:\n            step_size = min(dt, t_final - self.t)\n            self.step(step_size)\n            self.history['t'].append(self.t)\n            self.history['y'].append(self.y.copy())\n        return np.array(self.history['t']), np.array(self.history['y'])\n\nclass EulerSolver(ODESolver):\n    \"\"\"First-order Euler method: y_{n+1} = y_n + dt * f(t_n, y_n)\"\"\"\n    \n    def step(self, dt):\n        \"\"\"Implement Euler step.\"\"\"\n        dydt = self.f(self.t, self.y)\n        self.y += dt * dydt\n        self.t += dt\n\nclass RungeKutta4Solver(ODESolver):\n    \"\"\"Fourth-order Runge-Kutta method.\"\"\"\n    \n    def step(self, dt):\n        \"\"\"Implement RK4 step with four evaluations.\"\"\"\n        k1 = self.f(self.t, self.y)\n        k2 = self.f(self.t + dt/2, self.y + dt*k1/2)\n        k3 = self.f(self.t + dt/2, self.y + dt*k2/2)\n        k4 = self.f(self.t + dt, self.y + dt*k3)\n        \n        self.y += dt * (k1 + 2*k2 + 2*k3 + k4) / 6\n        self.t += dt\n\nclass LeapfrogSolver(ODESolver):\n    \"\"\"\n    Leapfrog integrator for Hamiltonian systems.\n    Particularly good for gravitational dynamics.\n    \"\"\"\n    \n    def __init__(self, force_func, positions, velocities, masses, initial_time=0.0):\n        \"\"\"\n        Specialized for N-body problems.\n        \n        Parameters:\n        -----------\n        force_func : callable\n            Function returning accelerations given (positions, masses)\n        positions : array\n            Initial positions [N, 3]\n        velocities : array  \n            Initial velocities [N, 3]\n        masses : array\n            Particle masses [N]\n        \"\"\"\n        self.force_func = force_func\n        self.positions = np.array(positions)\n        self.velocities = np.array(velocities)\n        self.masses = np.array(masses)\n        self.t = initial_time\n        self.history = {\n            't': [initial_time],\n            'positions': [self.positions.copy()],\n            'velocities': [self.velocities.copy()]\n        }\n    \n    def step(self, dt):\n        \"\"\"Leapfrog integration step.\"\"\"\n        # Kick: v_{1/2} = v_0 + (dt/2) * a_0\n        accelerations = self.force_func(self.positions, self.masses)\n        self.velocities += 0.5 * dt * accelerations\n        \n        # Drift: x_1 = x_0 + dt * v_{1/2}\n        self.positions += dt * self.velocities\n        \n        # Kick: v_1 = v_{1/2} + (dt/2) * a_1\n        accelerations = self.force_func(self.positions, self.masses)\n        self.velocities += 0.5 * dt * accelerations\n        \n        self.t += dt\n        self.history['t'].append(self.t)\n        self.history['positions'].append(self.positions.copy())\n        self.history['velocities'].append(self.velocities.copy())","type":"content","url":"/project2-description#task-1-ode-solver-framework-45-min","position":15},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl4":"Task 2: Two-Body Gravitational Dynamics (60 min)","lvl3":"Lab Session Objectives","lvl2":"Week 1: ODE Solvers and Energy Conservation"},"type":"lvl4","url":"/project2-description#task-2-two-body-gravitational-dynamics-60-min","position":16},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl4":"Task 2: Two-Body Gravitational Dynamics (60 min)","lvl3":"Lab Session Objectives","lvl2":"Week 1: ODE Solvers and Energy Conservation"},"content":"Goal: Validate integrators on Kepler problem with known analytical solution\n\nImplementation Requirements:def gravitational_derivatives(t, state):\n    \"\"\"\n    Derivatives for two-body problem.\n    \n    state = [x1, y1, z1, vx1, vy1, vz1, x2, y2, z2, vx2, vy2, vz2]\n    \"\"\"\n    # Extract positions and velocities\n    pos1 = state[0:3]\n    vel1 = state[3:6]\n    pos2 = state[6:9]\n    vel2 = state[9:12]\n    \n    # Calculate separation and force\n    r_vec = pos2 - pos1\n    r_mag = np.linalg.norm(r_vec)\n    \n    # Gravitational acceleration\n    G = 6.674e-11  # m³/kg/s²\n    m1, m2 = 1.0, 1.0  # masses\n    \n    acc_magnitude = G * (m1 + m2) / r_mag**3\n    acc1 = acc_magnitude * r_vec\n    acc2 = -acc_magnitude * r_vec\n    \n    # Return derivatives: [vel1, acc1, vel2, acc2]\n    return np.concatenate([vel1, acc1, vel2, acc2])\n\ndef kepler_orbit_validation():\n    \"\"\"\n    Test integrators on Earth-Sun system.\n    Compare with analytical solution for energy and angular momentum.\n    \"\"\"\n    # Earth-Sun system (simplified units)\n    AU = 1.496e11  # m\n    year = 365.25 * 24 * 3600  # s\n    \n    # Initial conditions: Earth at aphelion\n    initial_state = [\n        1.017*AU, 0, 0,      # Earth position\n        0, 29.29e3, 0,       # Earth velocity\n        0, 0, 0,             # Sun position (at origin)\n        0, 0, 0              # Sun velocity\n    ]\n    \n    # Test each integrator\n    methods = {\n        'Euler': EulerSolver,\n        'RK4': RungeKutta4Solver\n    }\n    \n    results = {}\n    for name, SolverClass in methods.items():\n        solver = SolverClass(gravitational_derivatives, initial_state)\n        t_vals, y_vals = solver.evolve(t_final=year, dt=year/1000)\n        results[name] = {'t': t_vals, 'y': y_vals}\n    \n    return results\n\ndef calculate_orbital_energy(positions, velocities, masses):\n    \"\"\"Calculate total energy: kinetic + potential.\"\"\"\n    # Kinetic energy: (1/2) * m * v²\n    ke = 0.5 * np.sum(masses * np.sum(velocities**2, axis=1))\n    \n    # Potential energy: -G * m1 * m2 / r\n    G = 6.674e-11\n    pe = 0\n    for i in range(len(masses)):\n        for j in range(i+1, len(masses)):\n            r_ij = np.linalg.norm(positions[i] - positions[j])\n            pe -= G * masses[i] * masses[j] / r_ij\n    \n    return ke + pe\n\ndef orbital_validation_analysis(results):\n    \"\"\"\n    Analyze energy conservation and orbital accuracy.\n    Plot energy drift and orbital trajectories.\n    \"\"\"\n    # Calculate energy conservation for each method\n    # Plot trajectories and energy vs time\n    # Compare with analytical orbital period","type":"content","url":"/project2-description#task-2-two-body-gravitational-dynamics-60-min","position":17},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl4":"Task 3: Error Analysis and Method Comparison (30 min)","lvl3":"Lab Session Objectives","lvl2":"Week 1: ODE Solvers and Energy Conservation"},"type":"lvl4","url":"/project2-description#task-3-error-analysis-and-method-comparison-30-min","position":18},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl4":"Task 3: Error Analysis and Method Comparison (30 min)","lvl3":"Lab Session Objectives","lvl2":"Week 1: ODE Solvers and Energy Conservation"},"content":"Goal: Understand trade-offs between accuracy, stability, and computational cost\n\nAnalysis Requirements:\n\nConvergence Study: Plot error vs timestep for each method\n\nEnergy Conservation: Track relative energy drift over multiple orbits\n\nComputational Cost: Time each method for various timestep sizes\n\nLong-term Stability: Run for 10+ orbital periods\n\nWeek 1 Deliverable: ODE solver library with comprehensive validation on Kepler orbits","type":"content","url":"/project2-description#task-3-error-analysis-and-method-comparison-30-min","position":19},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl2":"Week 2: Statistical Sampling and Multi-Body Systems"},"type":"lvl2","url":"/project2-description#week-2-statistical-sampling-and-multi-body-systems","position":20},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl2":"Week 2: Statistical Sampling and Multi-Body Systems"},"content":"","type":"content","url":"/project2-description#week-2-statistical-sampling-and-multi-body-systems","position":21},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl3":"Conceptual Introduction (25 min)","lvl2":"Week 2: Statistical Sampling and Multi-Body Systems"},"type":"lvl3","url":"/project2-description#conceptual-introduction-25-min-1","position":22},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl3":"Conceptual Introduction (25 min)","lvl2":"Week 2: Statistical Sampling and Multi-Body Systems"},"content":"Stellar Initial Mass Function: Salpeter, Kroupa, Chabrier prescriptions\n\nSpatial distributions in star clusters: Plummer sphere, King profiles\n\nStatistical sampling techniques: inverse transform, rejection sampling\n\nVirial equilibrium and cluster dynamics","type":"content","url":"/project2-description#conceptual-introduction-25-min-1","position":23},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl3":"Lab Session Objectives","lvl2":"Week 2: Statistical Sampling and Multi-Body Systems"},"type":"lvl3","url":"/project2-description#lab-session-objectives-1","position":24},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl3":"Lab Session Objectives","lvl2":"Week 2: Statistical Sampling and Multi-Body Systems"},"content":"Implement realistic stellar cluster initialization and scale to many-body systems.","type":"content","url":"/project2-description#lab-session-objectives-1","position":25},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl4":"Task 1: Initial Mass Function Implementation (50 min)","lvl3":"Lab Session Objectives","lvl2":"Week 2: Statistical Sampling and Multi-Body Systems"},"type":"lvl4","url":"/project2-description#task-1-initial-mass-function-implementation-50-min","position":26},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl4":"Task 1: Initial Mass Function Implementation (50 min)","lvl3":"Lab Session Objectives","lvl2":"Week 2: Statistical Sampling and Multi-Body Systems"},"content":"Goal: Sample realistic stellar mass distributions\n\nIMF Theory and Implementation:class StellarIMF:\n    \"\"\"\n    Stellar Initial Mass Function implementation.\n    \n    Supports multiple functional forms used in astrophysics.\n    \"\"\"\n    \n    def __init__(self, imf_type='kroupa', mass_range=(0.08, 120)):\n        \"\"\"\n        Parameters:\n        -----------\n        imf_type : str\n            'salpeter', 'kroupa', or 'chabrier'\n        mass_range : tuple\n            (minimum_mass, maximum_mass) in solar masses\n        \"\"\"\n        self.imf_type = imf_type\n        self.m_min, self.m_max = mass_range\n        self.normalization = self._calculate_normalization()\n    \n    def pdf(self, mass):\n        \"\"\"\n        Probability density function dN/dM.\n        \n        Salpeter (1955): dN/dM ∝ M^(-2.35)\n        Kroupa (2001): dN/dM ∝ M^(-1.3) for M < 0.5 M☉\n                               M^(-2.3) for M > 0.5 M☉\n        \"\"\"\n        mass = np.asarray(mass)\n        \n        if self.imf_type == 'salpeter':\n            return mass**(-2.35)\n        \n        elif self.imf_type == 'kroupa':\n            # Broken power law\n            result = np.zeros_like(mass)\n            low_mass = mass < 0.5\n            high_mass = mass >= 0.5\n            \n            result[low_mass] = mass[low_mass]**(-1.3)\n            # Ensure continuity at M = 0.5\n            normalization = 0.5**(-1.3 + 2.3)\n            result[high_mass] = normalization * mass[high_mass]**(-2.3)\n            \n            return result\n        \n        elif self.imf_type == 'chabrier':\n            # Log-normal for low masses + power law for high masses\n            # Implementation left as advanced exercise\n            pass\n    \n    def cdf(self, mass):\n        \"\"\"Cumulative distribution function.\"\"\"\n        # Analytical when possible, numerical integration otherwise\n        if self.imf_type == 'salpeter':\n            # CDF ∝ M^(-1.35)\n            return (mass**(-1.35) - self.m_min**(-1.35)) / \\\n                   (self.m_max**(-1.35) - self.m_min**(-1.35))\n    \n    def sample_rejection(self, n_stars):\n        \"\"\"Sample using rejection method.\"\"\"\n        masses = []\n        max_pdf = self.pdf(self.m_min)  # Maximum of PDF\n        \n        while len(masses) < n_stars:\n            # Propose random mass in range\n            m_proposal = self.m_min + (self.m_max - self.m_min) * np.random.random()\n            \n            # Accept with probability proportional to PDF\n            if np.random.random() < self.pdf(m_proposal) / max_pdf:\n                masses.append(m_proposal)\n        \n        return np.array(masses)\n    \n    def sample_inverse_transform(self, n_stars):\n        \"\"\"Sample using inverse CDF (when available).\"\"\"\n        if self.imf_type == 'salpeter':\n            u = np.random.random(n_stars)\n            # Invert CDF analytically\n            return (self.m_min**(-1.35) + u * (self.m_max**(-1.35) - self.m_min**(-1.35)))**(-1/1.35)\n        else:\n            # Fall back to rejection sampling\n            return self.sample_rejection(n_stars)\n    \n    def validate_distribution(self, masses, n_bins=50):\n        \"\"\"Compare sampled masses with theoretical IMF.\"\"\"\n        # Create histogram and compare with PDF\n        # Plot and calculate goodness-of-fit statistics\n        pass\n\ndef mass_to_stellar_properties(masses):\n    \"\"\"\n    Convert stellar masses to observable properties.\n    Uses Project 1 stellar physics relationships.\n    \"\"\"\n    # Mass-luminosity relation\n    luminosities = np.where(masses > 1.0, \n                           masses**3.5,  # High mass: L ∝ M^3.5\n                           masses**4.0)  # Low mass: L ∝ M^4.0\n    \n    # Mass-temperature relation (main sequence)\n    temperatures = 5778 * (masses)**0.5  # Rough approximation\n    \n    # Mass-radius relation\n    radii = np.where(masses > 1.0,\n                    masses**0.8,   # High mass\n                    masses**0.9)   # Low mass\n    \n    return {\n        'luminosities': luminosities,\n        'temperatures': temperatures,\n        'radii': radii\n    }","type":"content","url":"/project2-description#task-1-initial-mass-function-implementation-50-min","position":27},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl4":"Task 2: Plummer Sphere Spatial Distribution (45 min)","lvl3":"Lab Session Objectives","lvl2":"Week 2: Statistical Sampling and Multi-Body Systems"},"type":"lvl4","url":"/project2-description#task-2-plummer-sphere-spatial-distribution-45-min","position":28},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl4":"Task 2: Plummer Sphere Spatial Distribution (45 min)","lvl3":"Lab Session Objectives","lvl2":"Week 2: Statistical Sampling and Multi-Body Systems"},"content":"Goal: Sample realistic 3D stellar cluster geometry\n\nPlummer Model Implementation:class PlummerSphere:\n    \"\"\"\n    Plummer sphere model for stellar cluster spatial distribution.\n    \n    Density profile: ρ(r) = (3M/4πa³) * (1 + r²/a²)^(-5/2)\n    where a is the scale radius.\n    \"\"\"\n    \n    def __init__(self, total_mass=1000, scale_radius=1.0):\n        \"\"\"\n        Parameters:\n        -----------\n        total_mass : float\n            Total cluster mass [M☉]\n        scale_radius : float\n            Plummer scale radius [pc]\n        \"\"\"\n        self.M = total_mass\n        self.a = scale_radius\n    \n    def density(self, r):\n        \"\"\"Density at radius r.\"\"\"\n        return (3*self.M/(4*np.pi*self.a**3)) * (1 + (r/self.a)**2)**(-5/2)\n    \n    def mass_enclosed(self, r):\n        \"\"\"Mass within radius r.\"\"\"\n        return self.M * (r/self.a)**3 / (1 + (r/self.a)**2)**(3/2)\n    \n    def sample_radial_positions(self, n_stars):\n        \"\"\"\n        Sample radial distances using inverse CDF method.\n        \n        CDF: M(r)/M_total = (r/a)³ / (1 + (r/a)²)^(3/2)\n        Inverse: r = a / sqrt(u^(-2/3) - 1)\n        \"\"\"\n        u = np.random.random(n_stars)\n        # Prevent u=0 which gives infinite radius\n        u = np.clip(u, 1e-10, 1-1e-10)\n        \n        radii = self.a / np.sqrt(u**(-2/3) - 1)\n        return radii\n    \n    def sample_positions(self, n_stars):\n        \"\"\"Sample 3D positions from Plummer distribution.\"\"\"\n        radii = self.sample_radial_positions(n_stars)\n        \n        # Sample isotropic directions\n        cos_theta = 2*np.random.random(n_stars) - 1  # cos(θ) uniform in [-1,1]\n        phi = 2*np.pi*np.random.random(n_stars)      # φ uniform in [0,2π]\n        \n        sin_theta = np.sqrt(1 - cos_theta**2)\n        \n        # Convert to Cartesian coordinates\n        x = radii * sin_theta * np.cos(phi)\n        y = radii * sin_theta * np.sin(phi)\n        z = radii * cos_theta\n        \n        return np.column_stack([x, y, z])\n    \n    def calculate_virial_velocities(self, positions, masses):\n        \"\"\"\n        Calculate velocities for virial equilibrium.\n        \n        Uses virial theorem: 2T + U = 0 for bound system\n        where T = kinetic energy, U = potential energy\n        \"\"\"\n        n_stars = len(masses)\n        velocities = np.zeros_like(positions)\n        \n        # Calculate potential energy\n        U = 0\n        for i in range(n_stars):\n            for j in range(i+1, n_stars):\n                r_ij = np.linalg.norm(positions[i] - positions[j])\n                U -= G * masses[i] * masses[j] / r_ij\n        \n        # Virial theorem: total kinetic energy = -U/2\n        T_total = -U / 2\n        \n        # Distribute kinetic energy among particles\n        # Simple approach: assume isotropic velocity dispersion\n        for i in range(n_stars):\n            # Individual kinetic energy proportional to mass\n            T_i = T_total * masses[i] / np.sum(masses)\n            v_mag = np.sqrt(2 * T_i / masses[i])\n            \n            # Random direction\n            cos_theta = 2*np.random.random() - 1\n            phi = 2*np.pi*np.random.random()\n            sin_theta = np.sqrt(1 - cos_theta**2)\n            \n            velocities[i] = v_mag * np.array([\n                sin_theta * np.cos(phi),\n                sin_theta * np.sin(phi),\n                cos_theta\n            ])\n        \n        return velocities","type":"content","url":"/project2-description#task-2-plummer-sphere-spatial-distribution-45-min","position":29},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl4":"Task 3: Vectorized N-Body Force Calculation (40 min)","lvl3":"Lab Session Objectives","lvl2":"Week 2: Statistical Sampling and Multi-Body Systems"},"type":"lvl4","url":"/project2-description#task-3-vectorized-n-body-force-calculation-40-min","position":30},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl4":"Task 3: Vectorized N-Body Force Calculation (40 min)","lvl3":"Lab Session Objectives","lvl2":"Week 2: Statistical Sampling and Multi-Body Systems"},"content":"Goal: Implement efficient O(N²) force computation\n\nVectorized Implementation:def gravitational_forces_vectorized(positions, masses, softening=0.01):\n    \"\"\"\n    Calculate gravitational forces between all particle pairs.\n    \n    Parameters:\n    -----------\n    positions : array [N, 3]\n        Particle positions\n    masses : array [N]\n        Particle masses\n    softening : float\n        Softening parameter to avoid singularities\n        \n    Returns:\n    --------\n    forces : array [N, 3]\n        Gravitational forces on each particle\n    \"\"\"\n    N = len(masses)\n    G = 4.3e-3  # pc³/M☉/Myr² (convenient units)\n    \n    # Calculate all pairwise separations using broadcasting\n    # positions[i,j] - positions[k,j] for all i,k pairs\n    r_vectors = positions[:, np.newaxis, :] - positions[np.newaxis, :, :]  # [N, N, 3]\n    \n    # Distance magnitudes with softening\n    r_magnitudes = np.sqrt(np.sum(r_vectors**2, axis=2) + softening**2)  # [N, N]\n    \n    # Avoid self-interaction\n    np.fill_diagonal(r_magnitudes, np.inf)\n    \n    # Force magnitudes: F = G*m1*m2/r²\n    mass_products = masses[:, np.newaxis] * masses[np.newaxis, :]  # [N, N]\n    force_magnitudes = G * mass_products / r_magnitudes**2  # [N, N]\n    \n    # Force directions: unit vectors\n    r_unit = r_vectors / r_magnitudes[:, :, np.newaxis]  # [N, N, 3]\n    \n    # Total forces: sum over all other particles\n    forces = np.sum(force_magnitudes[:, :, np.newaxis] * r_unit, axis=1)  # [N, 3]\n    \n    return forces\n\ndef performance_comparison():\n    \"\"\"Compare vectorized vs nested loop implementations.\"\"\"\n    import time\n    \n    # Test different cluster sizes\n    N_values = [10, 50, 100, 200, 500]\n    \n    for N in N_values:\n        # Generate test data\n        positions = np.random.randn(N, 3)\n        masses = np.random.uniform(0.1, 10, N)\n        \n        # Time vectorized version\n        start = time.time()\n        forces_vec = gravitational_forces_vectorized(positions, masses)\n        time_vec = time.time() - start\n        \n        # Time nested loop version (for comparison)\n        start = time.time()\n        forces_loop = gravitational_forces_nested_loops(positions, masses)\n        time_loop = time.time() - start\n        \n        print(f\"N={N}: Vectorized={time_vec:.4f}s, Loops={time_loop:.4f}s, \"\n              f\"Speedup={time_loop/time_vec:.1f}x\")\n\nWeek 2 Deliverable: Realistic stellar cluster initialization with IMF masses and Plummer positions, plus efficient force calculations","type":"content","url":"/project2-description#task-3-vectorized-n-body-force-calculation-40-min","position":31},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl2":"Week 3: Adaptive Timestepping and Cluster Evolution"},"type":"lvl2","url":"/project2-description#week-3-adaptive-timestepping-and-cluster-evolution","position":32},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl2":"Week 3: Adaptive Timestepping and Cluster Evolution"},"content":"","type":"content","url":"/project2-description#week-3-adaptive-timestepping-and-cluster-evolution","position":33},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl3":"Conceptual Introduction (25 min)","lvl2":"Week 3: Adaptive Timestepping and Cluster Evolution"},"type":"lvl3","url":"/project2-description#conceptual-introduction-25-min-2","position":34},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl3":"Conceptual Introduction (25 min)","lvl2":"Week 3: Adaptive Timestepping and Cluster Evolution"},"content":"Energy conservation as accuracy criterion\n\nAdaptive timestep algorithms\n\nMulti-mass cluster dynamics: mass segregation, two-body relaxation\n\nStellar escape and cluster dissolution\n\nComputational complexity and optimization strategies","type":"content","url":"/project2-description#conceptual-introduction-25-min-2","position":35},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl3":"Lab Session Objectives","lvl2":"Week 3: Adaptive Timestepping and Cluster Evolution"},"type":"lvl3","url":"/project2-description#lab-session-objectives-2","position":36},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl3":"Lab Session Objectives","lvl2":"Week 3: Adaptive Timestepping and Cluster Evolution"},"content":"Implement energy-controlled adaptive integration and study realistic cluster evolution.","type":"content","url":"/project2-description#lab-session-objectives-2","position":37},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl4":"Task 1: Adaptive Timestep Control (50 min)","lvl3":"Lab Session Objectives","lvl2":"Week 3: Adaptive Timestepping and Cluster Evolution"},"type":"lvl4","url":"/project2-description#task-1-adaptive-timestep-control-50-min","position":38},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl4":"Task 1: Adaptive Timestep Control (50 min)","lvl3":"Lab Session Objectives","lvl2":"Week 3: Adaptive Timestepping and Cluster Evolution"},"content":"Goal: Implement robust adaptive timestep algorithm based on energy conservation\n\nAdaptive Integration Framework:class AdaptiveNBodySimulator:\n    \"\"\"\n    N-body simulator with adaptive timestep control.\n    \n    Uses energy conservation to monitor accuracy and adjust timestep.\n    \"\"\"\n    \n    def __init__(self, positions, velocities, masses, initial_dt=0.01, \n                 energy_tolerance=1e-6):\n        \"\"\"\n        Parameters:\n        -----------\n        positions : array [N, 3]\n            Initial positions [pc]\n        velocities : array [N, 3] \n            Initial velocities [km/s]\n        masses : array [N]\n            Particle masses [M☉]\n        initial_dt : float\n            Initial timestep [Myr]\n        energy_tolerance : float\n            Relative energy error tolerance\n        \"\"\"\n        self.positions = np.array(positions)\n        self.velocities = np.array(velocities) \n        self.masses = np.array(masses)\n        self.dt = initial_dt\n        self.tolerance = energy_tolerance\n        \n        # Calculate initial energy\n        self.initial_energy = self.total_energy()\n        \n        # Statistics tracking\n        self.n_accepted = 0\n        self.n_rejected = 0\n        self.energy_errors = []\n        self.timesteps = []\n        \n        # History storage\n        self.time = 0.0\n        self.history = {\n            'time': [0.0],\n            'positions': [self.positions.copy()],\n            'velocities': [self.velocities.copy()],\n            'energy': [self.initial_energy],\n            'timestep': [self.dt]\n        }\n    \n    def total_energy(self):\n        \"\"\"Calculate total energy: kinetic + potential.\"\"\"\n        # Kinetic energy\n        ke = 0.5 * np.sum(self.masses * np.sum(self.velocities**2, axis=1))\n        \n        # Potential energy\n        pe = 0\n        for i in range(len(self.masses)):\n            for j in range(i+1, len(self.masses)):\n                r_ij = np.linalg.norm(self.positions[i] - self.positions[j])\n                pe -= G * self.masses[i] * self.masses[j] / r_ij\n        \n        return ke + pe\n    \n    def energy_error(self):\n        \"\"\"Calculate relative energy error from initial value.\"\"\"\n        current_energy = self.total_energy()\n        return abs((current_energy - self.initial_energy) / self.initial_energy)\n    \n    def leapfrog_step(self, dt):\n        \"\"\"Take single leapfrog integration step.\"\"\"\n        # Store initial state for potential rollback\n        old_positions = self.positions.copy()\n        old_velocities = self.velocities.copy()\n        \n        # Leapfrog integration\n        forces = gravitational_forces_vectorized(self.positions, self.masses)\n        accelerations = forces / self.masses[:, np.newaxis]\n        \n        # Kick-drift-kick\n        self.velocities += 0.5 * dt * accelerations\n        self.positions += dt * self.velocities\n        \n        forces = gravitational_forces_vectorized(self.positions, self.masses)\n        accelerations = forces / self.masses[:, np.newaxis]\n        self.velocities += 0.5 * dt * accelerations\n        \n        return old_positions, old_velocities\n    \n    def adaptive_step(self):\n        \"\"\"\n        Take adaptive timestep with error control.\n        \n        Algorithm:\n        1. Attempt step with current timestep\n        2. Check energy conservation\n        3. If error too large: reduce timestep and retry\n        4. If error acceptable: possibly increase timestep for next step\n        \"\"\"\n        max_attempts = 5\n        \n        for attempt in range(max_attempts):\n            # Store state before step\n            old_positions, old_velocities = self.leapfrog_step(self.dt)\n            \n            # Check energy conservation\n            error = self.energy_error()\n            \n            if error <= self.tolerance:\n                # Step accepted\n                self.time += self.dt\n                self.n_accepted += 1\n                \n                # Store results\n                self.history['time'].append(self.time)\n                self.history['positions'].append(self.positions.copy())\n                self.history['velocities'].append(self.velocities.copy())\n                self.history['energy'].append(self.total_energy())\n                self.history['timestep'].append(self.dt)\n                \n                self.energy_errors.append(error)\n                self.timesteps.append(self.dt)\n                \n                # Possibly increase timestep for next step\n                if error < self.tolerance / 10:\n                    self.dt = min(self.dt * 1.1, 0.1)  # Don't let it grow too large\n                \n                return True\n            \n            else:\n                # Step rejected - restore state and reduce timestep\n                self.positions = old_positions\n                self.velocities = old_velocities\n                self.dt *= 0.5\n                self.n_rejected += 1\n                \n                if attempt == max_attempts - 1:\n                    print(f\"Warning: Max attempts reached at t={self.time:.3f}\")\n                    return False\n        \n        return False\n    \n    def evolve(self, t_final, max_steps=10000):\n        \"\"\"Evolve system to final time using adaptive timesteps.\"\"\"\n        step_count = 0\n        \n        while self.time < t_final and step_count < max_steps:\n            success = self.adaptive_step()\n            if not success:\n                print(\"Simulation failed - energy errors too large\")\n                break\n            \n            step_count += 1\n            \n            # Progress reporting\n            if step_count % 100 == 0:\n                acceptance_rate = self.n_accepted / (self.n_accepted + self.n_rejected)\n                print(f\"t={self.time:.2f}, dt={self.dt:.4f}, \"\n                      f\"E_error={self.energy_errors[-1]:.2e}, \"\n                      f\"acceptance={acceptance_rate:.2f}\")\n        \n        return self.get_results()\n    \n    def get_results(self):\n        \"\"\"Return simulation results as arrays.\"\"\"\n        return {\n            'time': np.array(self.history['time']),\n            'positions': np.array(self.history['positions']),\n            'velocities': np.array(self.history['velocities']),\n            'energy': np.array(self.history['energy']),\n            'timesteps': np.array(self.history['timestep'])\n        }","type":"content","url":"/project2-description#task-1-adaptive-timestep-control-50-min","position":39},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl4":"Task 2: Cluster Physics and Evolution (55 min)","lvl3":"Lab Session Objectives","lvl2":"Week 3: Adaptive Timestepping and Cluster Evolution"},"type":"lvl4","url":"/project2-description#task-2-cluster-physics-and-evolution-55-min","position":40},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl4":"Task 2: Cluster Physics and Evolution (55 min)","lvl3":"Lab Session Objectives","lvl2":"Week 3: Adaptive Timestepping and Cluster Evolution"},"content":"Goal: Study realistic stellar cluster evolution phenomena\n\nMass Segregation Analysis:def analyze_mass_segregation(positions, masses, times):\n    \"\"\"\n    Track mass segregation: massive stars sink to cluster center.\n    \n    Quantify using mass-weighted radial distribution.\n    \"\"\"\n    segregation_ratios = []\n    \n    for i, pos in enumerate(positions):\n        # Calculate distance from cluster center\n        center = np.average(pos, weights=masses, axis=0)\n        distances = np.linalg.norm(pos - center, axis=1)\n        \n        # Sort by mass\n        mass_order = np.argsort(masses)[::-1]  # Heaviest first\n        \n        # Compare radial distribution of most vs least massive stars\n        n_heavy = len(masses) // 10  # Top 10%\n        n_light = len(masses) // 10  # Bottom 10%\n        \n        r_heavy = np.mean(distances[mass_order[:n_heavy]])\n        r_light = np.mean(distances[mass_order[-n_light:]])\n        \n        segregation_ratios.append(r_light / r_heavy)\n    \n    return segregation_ratios\n\ndef calculate_virial_ratio(positions, velocities, masses):\n    \"\"\"\n    Calculate virial ratio: 2T/|U|\n    \n    For bound system in equilibrium, should equal 1.\n    \"\"\"\n    # Kinetic energy\n    T = 0.5 * np.sum(masses * np.sum(velocities**2, axis=1))\n    \n    # Potential energy\n    U = 0\n    for i in range(len(masses)):\n        for j in range(i+1, len(masses)):\n            r_ij = np.linalg.norm(positions[i] - positions[j])\n            U -= G * masses[i] * masses[j] / r_ij\n    \n    return 2 * T / abs(U)\n\ndef identify_escaping_stars(positions, velocities, masses, escape_criterion=2.0):\n    \"\"\"\n    Identify stars with velocities exceeding escape velocity.\n    \n    v_escape = sqrt(2 * |U| / m) at each star's location\n    \"\"\"\n    escaping_stars = []\n    \n    for i in range(len(masses)):\n        # Calculate potential at star i due to all other stars\n        phi_i = 0\n        for j in range(len(masses)):\n            if i != j:\n                r_ij = np.linalg.norm(positions[i] - positions[j])\n                phi_i -= G * masses[j] / r_ij\n        \n        # Escape velocity at this location\n        v_escape = np.sqrt(-2 * phi_i)\n        v_star = np.linalg.norm(velocities[i])\n        \n        if v_star > escape_criterion * v_escape:\n            escaping_stars.append(i)\n    \n    return escaping_stars\n\nCluster Snapshot Generation for Project 3:def generate_cluster_snapshots(cluster_mass=1000, n_stars=200, \n                              evolution_times=[0, 5, 20, 50]):\n    \"\"\"\n    Generate stellar cluster at multiple evolutionary phases.\n    These snapshots will be used in Project 3 for radiation calculations.\n    \n    Parameters:\n    -----------\n    cluster_mass : float\n        Total cluster mass [M☉]\n    n_stars : int\n        Number of stars in cluster\n    evolution_times : list\n        Times to save snapshots [Myr]\n        \n    Returns:\n    --------\n    snapshots : list of dict\n        Each dict contains stellar properties at one time\n    \"\"\"\n    # Initialize cluster\n    imf = StellarIMF(imf_type='kroupa')\n    masses = imf.sample_inverse_transform(n_stars)\n    masses = masses * (cluster_mass / np.sum(masses))  # Normalize total mass\n    \n    plummer = PlummerSphere(total_mass=cluster_mass, scale_radius=1.0)\n    positions = plummer.sample_positions(n_stars)\n    velocities = plummer.calculate_virial_velocities(positions, masses)\n    \n    # Calculate stellar properties for radiation (from Project 1)\n    stellar_props = mass_to_stellar_properties(masses)\n    \n    # Set up adaptive simulator\n    simulator = AdaptiveNBodySimulator(\n        positions, velocities, masses,\n        initial_dt=0.01, energy_tolerance=1e-6\n    )\n    \n    snapshots = []\n    \n    for t_target in evolution_times:\n        if t_target == 0:\n            # Initial conditions\n            snapshot = create_snapshot(\n                time=0, \n                positions=simulator.positions,\n                velocities=simulator.velocities,\n                masses=masses,\n                stellar_props=stellar_props\n            )\n        else:\n            # Evolve to target time\n            results = simulator.evolve(t_target)\n            \n            # Extract final state\n            final_positions = results['positions'][-1]\n            final_velocities = results['velocities'][-1]\n            \n            snapshot = create_snapshot(\n                time=t_target,\n                positions=final_positions,\n                velocities=final_velocities,\n                masses=masses,\n                stellar_props=stellar_props\n            )\n        \n        snapshots.append(snapshot)\n        print(f\"Snapshot created at t = {t_target} Myr\")\n    \n    return snapshots\n\ndef create_snapshot(time, positions, velocities, masses, stellar_props):\n    \"\"\"Create comprehensive cluster snapshot.\"\"\"\n    # Calculate cluster center and properties\n    center = np.average(positions, weights=masses, axis=0)\n    centered_positions = positions - center\n    \n    # Structural parameters\n    distances = np.linalg.norm(centered_positions, axis=1)\n    half_mass_radius = np.median(distances)\n    \n    # Core radius (radius containing 10% of mass)\n    mass_order = np.argsort(distances)\n    core_mass_index = int(0.1 * len(masses))\n    core_radius = distances[mass_order[core_mass_index]]\n    \n    snapshot = {\n        'time': time,\n        'n_stars': len(masses),\n        'total_mass': np.sum(masses),\n        \n        # Stellar properties\n        'positions': centered_positions,  # Centered on cluster\n        'velocities': velocities,\n        'masses': masses,\n        'luminosities': stellar_props['luminosities'],\n        'temperatures': stellar_props['temperatures'],\n        'radii': stellar_props['radii'],\n        \n        # Cluster structure\n        'center': center,\n        'half_mass_radius': half_mass_radius,\n        'core_radius': core_radius,\n        'virial_ratio': calculate_virial_ratio(positions, velocities, masses),\n        \n        # Evolution diagnostics\n        'mass_segregation_ratio': analyze_mass_segregation([positions], masses, [time])[0],\n        'escaping_stars': identify_escaping_stars(positions, velocities, masses)\n    }\n    \n    return snapshot\n\ndef save_snapshots_for_project3(snapshots, filename='cluster_evolution.pkl'):\n    \"\"\"Save snapshots in format suitable for Project 3.\"\"\"\n    import pickle\n    \n    with open(filename, 'wb') as f:\n        pickle.dump(snapshots, f)\n    \n    print(f\"Saved {len(snapshots)} cluster snapshots to {filename}\")\n    print(\"These will be used as radiation sources in Project 3\")","type":"content","url":"/project2-description#task-2-cluster-physics-and-evolution-55-min","position":41},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl4":"Task 3: Performance Analysis and Optimization (30 min)","lvl3":"Lab Session Objectives","lvl2":"Week 3: Adaptive Timestepping and Cluster Evolution"},"type":"lvl4","url":"/project2-description#task-3-performance-analysis-and-optimization-30-min","position":42},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl4":"Task 3: Performance Analysis and Optimization (30 min)","lvl3":"Lab Session Objectives","lvl2":"Week 3: Adaptive Timestepping and Cluster Evolution"},"content":"Goal: Analyze computational efficiency and identify optimization opportunities\n\nPerformance Studies:def scaling_analysis():\n    \"\"\"Study how computational cost scales with cluster size.\"\"\"\n    import time\n    \n    N_values = [50, 100, 200, 400]\n    times_force = []\n    times_integration = []\n    \n    for N in N_values:\n        # Generate test cluster\n        masses = np.random.uniform(0.1, 10, N)\n        positions = np.random.randn(N, 3)\n        velocities = np.random.randn(N, 3)\n        \n        # Time force calculation\n        start = time.time()\n        for _ in range(10):  # Multiple iterations for averaging\n            forces = gravitational_forces_vectorized(positions, masses)\n        times_force.append((time.time() - start) / 10)\n        \n        # Time full integration step\n        simulator = AdaptiveNBodySimulator(positions, velocities, masses)\n        start = time.time()\n        for _ in range(10):\n            simulator.adaptive_step()\n        times_integration.append((time.time() - start) / 10)\n    \n    # Analyze scaling: should be O(N²) for force calculation\n    print(\"Scaling Analysis:\")\n    for i, N in enumerate(N_values):\n        print(f\"N={N}: Force={times_force[i]:.4f}s, Integration={times_integration[i]:.4f}s\")\n\ndef memory_optimization_analysis():\n    \"\"\"Analyze memory usage and suggest optimizations.\"\"\"\n    # Profile memory usage during simulation\n    # Identify opportunities for optimization\n    pass\n\nWeek 3 Deliverable: Complete adaptive N-body simulator with realistic cluster evolution and snapshots for Project 3","type":"content","url":"/project2-description#task-3-performance-analysis-and-optimization-30-min","position":43},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl2":"Assessment and Grading"},"type":"lvl2","url":"/project2-description#assessment-and-grading","position":44},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl2":"Assessment and Grading"},"content":"","type":"content","url":"/project2-description#assessment-and-grading","position":45},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl3":"Grading Breakdown","lvl2":"Assessment and Grading"},"type":"lvl3","url":"/project2-description#grading-breakdown","position":46},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl3":"Grading Breakdown","lvl2":"Assessment and Grading"},"content":"Week 1: ODE solvers and validation (30%)\n\nWeek 2: Statistical sampling and vectorization (35%)\n\nWeek 3: Adaptive methods and cluster evolution (35%)","type":"content","url":"/project2-description#grading-breakdown","position":47},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl3":"Evaluation Criteria","lvl2":"Assessment and Grading"},"type":"lvl3","url":"/project2-description#evaluation-criteria","position":48},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl3":"Evaluation Criteria","lvl2":"Assessment and Grading"},"content":"","type":"content","url":"/project2-description#evaluation-criteria","position":49},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl4":"Technical Implementation (60%)","lvl3":"Evaluation Criteria","lvl2":"Assessment and Grading"},"type":"lvl4","url":"/project2-description#technical-implementation-60","position":50},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl4":"Technical Implementation (60%)","lvl3":"Evaluation Criteria","lvl2":"Assessment and Grading"},"content":"Algorithm Correctness: Do integrators conserve energy appropriately?\n\nSampling Accuracy: Do distributions match theoretical expectations?\n\nVectorization Efficiency: Significant speedup over naive implementations\n\nAdaptive Control: Proper timestep adjustment based on energy errors","type":"content","url":"/project2-description#technical-implementation-60","position":51},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl4":"Scientific Understanding (25%)","lvl3":"Evaluation Criteria","lvl2":"Assessment and Grading"},"type":"lvl4","url":"/project2-description#scientific-understanding-25","position":52},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl4":"Scientific Understanding (25%)","lvl3":"Evaluation Criteria","lvl2":"Assessment and Grading"},"content":"Physics Validation: Energy conservation, virial equilibrium, orbital mechanics\n\nStatistical Analysis: IMF and spatial distribution validation\n\nCluster Evolution: Understanding of mass segregation and stellar escape","type":"content","url":"/project2-description#scientific-understanding-25","position":53},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl4":"Code Quality and Performance (15%)","lvl3":"Evaluation Criteria","lvl2":"Assessment and Grading"},"type":"lvl4","url":"/project2-description#code-quality-and-performance-15","position":54},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl4":"Code Quality and Performance (15%)","lvl3":"Evaluation Criteria","lvl2":"Assessment and Grading"},"content":"Documentation: Clear docstrings and code organization\n\nTesting: Validation against analytical solutions\n\nOptimization: Efficient use of NumPy vectorization\n\nReproducibility: Proper random seed handling","type":"content","url":"/project2-description#code-quality-and-performance-15","position":55},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl3":"Connection to Project 3","lvl2":"Assessment and Grading"},"type":"lvl3","url":"/project2-description#connection-to-project-3","position":56},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl3":"Connection to Project 3","lvl2":"Assessment and Grading"},"content":"The stellar cluster snapshots generated in this project become the radiation sources for Project 3:\n\nStellar positions: Spatial distribution for radiation field calculations\n\nStellar masses and luminosities: Heating source strengths\n\nCluster evolution: How radiation field changes with time\n\nRealistic populations: IMF-sampled masses give proper luminosity functions","type":"content","url":"/project2-description#connection-to-project-3","position":57},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl3":"Deliverables","lvl2":"Assessment and Grading"},"type":"lvl3","url":"/project2-description#deliverables","position":58},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl3":"Deliverables","lvl2":"Assessment and Grading"},"content":"","type":"content","url":"/project2-description#deliverables","position":59},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl4":"Final Submission","lvl3":"Deliverables","lvl2":"Assessment and Grading"},"type":"lvl4","url":"/project2-description#final-submission","position":60},{"hierarchy":{"lvl1":"ASTR 596 Project 2: N-Body Dynamics + Statistical Sampling + Stellar Systems","lvl4":"Final Submission","lvl3":"Deliverables","lvl2":"Assessment and Grading"},"content":"N-Body Simulation Library:\n\node_solvers.py: Integration method implementations\n\nstellar_sampling.py: IMF and Plummer sphere classes\n\nnbody_simulator.py: Complete adaptive N-body framework\n\ncluster_analysis.py: Evolution analysis tools\n\nValidation Notebooks:\n\norbital_mechanics_validation.ipynb: Two-body problem tests\n\nsampling_validation.ipynb: IMF and spatial distribution verification\n\ncluster_evolution_analysis.ipynb: Mass segregation and dynamics\n\nProject 3 Interface:\n\ncluster_snapshots.pkl: Saved stellar cluster evolution data\n\nsnapshot_format.md: Documentation of data structure\n\nThis project establishes the realistic stellar systems needed for sophisticated radiation calculations while teaching essential computational physics skills: numerical integration, statistical sampling, vectorization, and adaptive methods.","type":"content","url":"/project2-description#final-submission","position":61},{"hierarchy":{"lvl1":"ASTR 596 Project 3: Monte Carlo Radiative Transfer + MCMC + Bayesian Statistics"},"type":"lvl1","url":"/project3-description","position":0},{"hierarchy":{"lvl1":"ASTR 596 Project 3: Monte Carlo Radiative Transfer + MCMC + Bayesian Statistics"},"content":"Duration: 4 weeks\nWeight: 18% of course grade\nTheme: “Rosen (2016) Direct Radiation + Deep Bayesian Inference”","type":"content","url":"/project3-description","position":1},{"hierarchy":{"lvl1":"ASTR 596 Project 3: Monte Carlo Radiative Transfer + MCMC + Bayesian Statistics","lvl3":"Project Overview"},"type":"lvl3","url":"/project3-description#project-overview","position":2},{"hierarchy":{"lvl1":"ASTR 596 Project 3: Monte Carlo Radiative Transfer + MCMC + Bayesian Statistics","lvl3":"Project Overview"},"content":"This project implements sophisticated Monte Carlo radiative transfer following the Rosen (2016) methodology for direct stellar radiation, combined with comprehensive Bayesian parameter estimation. You will build a complete pipeline from stellar cluster heating calculations to statistical inference of dust properties. This project emphasizes both physical understanding of radiative processes and modern statistical methods.","type":"content","url":"/project3-description#project-overview","position":3},{"hierarchy":{"lvl1":"ASTR 596 Project 3: Monte Carlo Radiative Transfer + MCMC + Bayesian Statistics","lvl3":"Learning Objectives"},"type":"lvl3","url":"/project3-description#learning-objectives","position":4},{"hierarchy":{"lvl1":"ASTR 596 Project 3: Monte Carlo Radiative Transfer + MCMC + Bayesian Statistics","lvl3":"Learning Objectives"},"content":"By completing this project, you will:\n\nMaster radiative transfer physics: Direct radiation field calculation and dust heating\n\nImplement Monte Carlo methods: Photon transport and statistical sampling\n\nUnderstand Bayesian statistics: Priors, likelihoods, posteriors, and model comparison\n\nDevelop MCMC expertise: Multiple sampling algorithms with convergence diagnostics\n\nApply advanced inference: Parameter estimation for complex astrophysical models\n\nConnect theory to observations: Synthetic observational data analysis","type":"content","url":"/project3-description#learning-objectives","position":5},{"hierarchy":{"lvl1":"ASTR 596 Project 3: Monte Carlo Radiative Transfer + MCMC + Bayesian Statistics","lvl3":"Prerequisites from Previous Projects"},"type":"lvl3","url":"/project3-description#prerequisites-from-previous-projects","position":6},{"hierarchy":{"lvl1":"ASTR 596 Project 3: Monte Carlo Radiative Transfer + MCMC + Bayesian Statistics","lvl3":"Prerequisites from Previous Projects"},"content":"Project 1: Numerical integration (Planck function), root-finding (temperature balance), blackbody physics\n\nProject 2: Stellar cluster snapshots with realistic mass/luminosity distributions\n\nMathematical Tools: Statistical sampling, error analysis, performance optimization","type":"content","url":"/project3-description#prerequisites-from-previous-projects","position":7},{"hierarchy":{"lvl1":"ASTR 596 Project 3: Monte Carlo Radiative Transfer + MCMC + Bayesian Statistics","lvl2":"Week 1: Direct Radiation Monte Carlo Framework"},"type":"lvl2","url":"/project3-description#week-1-direct-radiation-monte-carlo-framework","position":8},{"hierarchy":{"lvl1":"ASTR 596 Project 3: Monte Carlo Radiative Transfer + MCMC + Bayesian Statistics","lvl2":"Week 1: Direct Radiation Monte Carlo Framework"},"content":"","type":"content","url":"/project3-description#week-1-direct-radiation-monte-carlo-framework","position":9},{"hierarchy":{"lvl1":"ASTR 596 Project 3: Monte Carlo Radiative Transfer + MCMC + Bayesian Statistics","lvl3":"Conceptual Introduction (30 min)","lvl2":"Week 1: Direct Radiation Monte Carlo Framework"},"type":"lvl3","url":"/project3-description#conceptual-introduction-30-min","position":10},{"hierarchy":{"lvl1":"ASTR 596 Project 3: Monte Carlo Radiative Transfer + MCMC + Bayesian Statistics","lvl3":"Conceptual Introduction (30 min)","lvl2":"Week 1: Direct Radiation Monte Carlo Framework"},"content":"Radiative Transfer Theory: Detailed mathematical foundation (see extended theory section)\n\nRosen (2016) Innovation: Direct radiation field vs diffusion approximation\n\nMonte Carlo Philosophy: Statistical approach to complex integral equations\n\nDust Physics: Absorption, scattering, and thermal re-emission processes","type":"content","url":"/project3-description#conceptual-introduction-30-min","position":11},{"hierarchy":{"lvl1":"ASTR 596 Project 3: Monte Carlo Radiative Transfer + MCMC + Bayesian Statistics","lvl3":"Lab Session Objectives","lvl2":"Week 1: Direct Radiation Monte Carlo Framework"},"type":"lvl3","url":"/project3-description#lab-session-objectives","position":12},{"hierarchy":{"lvl1":"ASTR 596 Project 3: Monte Carlo Radiative Transfer + MCMC + Bayesian Statistics","lvl3":"Lab Session Objectives","lvl2":"Week 1: Direct Radiation Monte Carlo Framework"},"content":"Implement direct radiation field calculation using Project 2 stellar clusters.","type":"content","url":"/project3-description#lab-session-objectives","position":13},{"hierarchy":{"lvl1":"ASTR 596 Project 3: Monte Carlo Radiative Transfer + MCMC + Bayesian Statistics","lvl4":"Task 1: Direct Radiation Physics Implementation (60 min)","lvl3":"Lab Session Objectives","lvl2":"Week 1: Direct Radiation Monte Carlo Framework"},"type":"lvl4","url":"/project3-description#task-1-direct-radiation-physics-implementation-60-min","position":14},{"hierarchy":{"lvl1":"ASTR 596 Project 3: Monte Carlo Radiative Transfer + MCMC + Bayesian Statistics","lvl4":"Task 1: Direct Radiation Physics Implementation (60 min)","lvl3":"Lab Session Objectives","lvl2":"Week 1: Direct Radiation Monte Carlo Framework"},"content":"Goal: Build foundation for accurate stellar heating calculations\n\nCore Physics Modules:import numpy as np\nfrom scipy.optimize import brentq\nimport pickle\n\n# Load stellar cluster data from Project 2\ndef load_cluster_snapshot(filename, snapshot_index=0):\n    \"\"\"Load stellar cluster from Project 2.\"\"\"","type":"content","url":"/project3-description#task-1-direct-radiation-physics-implementation-60-min","position":15},{"hierarchy":{"lvl1":"Short Projects Overview"},"type":"lvl1","url":"/index-12","position":0},{"hierarchy":{"lvl1":"Short Projects Overview"},"content":"This section contains the core programming projects for ASTR 596: Modeling the Universe.","type":"content","url":"/index-12","position":1},{"hierarchy":{"lvl1":"Short Projects Overview","lvl2":"Project Philosophy"},"type":"lvl2","url":"/index-12#project-philosophy","position":2},{"hierarchy":{"lvl1":"Short Projects Overview","lvl2":"Project Philosophy"},"content":"Each project implements fundamental algorithms from scratch using our “glass box” approach. You’ll build understanding through manual implementation before leveraging modern frameworks.","type":"content","url":"/index-12#project-philosophy","position":3},{"hierarchy":{"lvl1":"Short Projects Overview","lvl2":"Project Structure"},"type":"lvl2","url":"/index-12#project-structure","position":4},{"hierarchy":{"lvl1":"Short Projects Overview","lvl2":"Project Structure"},"content":"The three short projects progress from foundations to advanced methods:","type":"content","url":"/index-12#project-structure","position":5},{"hierarchy":{"lvl1":"Short Projects Overview","lvl3":"🐍 Project 1: Python Foundations & Stellar Physics","lvl2":"Project Structure"},"type":"lvl3","url":"/index-12#id-project-1-python-foundations-stellar-physics","position":6},{"hierarchy":{"lvl1":"Short Projects Overview","lvl3":"🐍 Project 1: Python Foundations & Stellar Physics","lvl2":"Project Structure"},"content":"Duration: 2 weeks | Focus: OOP design and stellar evolution modeling\n\nProfessional development environment setup\n\nObject-oriented programming principles\n\nStellar physics implementation\n\nGit workflow and documentation\n\n→ Project 1 Details","type":"content","url":"/index-12#id-project-1-python-foundations-stellar-physics","position":7},{"hierarchy":{"lvl1":"Short Projects Overview","lvl3":"🪐 Project 2: N-Body Dynamics & Monte Carlo","lvl2":"Project Structure"},"type":"lvl3","url":"/index-12#id-project-2-n-body-dynamics-monte-carlo","position":8},{"hierarchy":{"lvl1":"Short Projects Overview","lvl3":"🪐 Project 2: N-Body Dynamics & Monte Carlo","lvl2":"Project Structure"},"content":"Duration: 2 weeks | Focus: Gravitational systems and statistical sampling\n\nNumerical integration methods\n\nN-body gravitational dynamics\n\nMonte Carlo sampling techniques\n\nPerformance optimization\n\n→ Project 2 Details","type":"content","url":"/index-12#id-project-2-n-body-dynamics-monte-carlo","position":9},{"hierarchy":{"lvl1":"Short Projects Overview","lvl3":"📈 Project 3: Linear Regression & Machine Learning","lvl2":"Project Structure"},"type":"lvl3","url":"/index-12#id-project-3-linear-regression-machine-learning","position":10},{"hierarchy":{"lvl1":"Short Projects Overview","lvl3":"📈 Project 3: Linear Regression & Machine Learning","lvl2":"Project Structure"},"content":"Duration: 2 weeks | Focus: Statistical modeling from first principles\n\nLinear regression implementation\n\nGradient descent optimization\n\nCross-validation and model selection\n\nAstronomical data analysis\n\n→ Project 3 Details","type":"content","url":"/index-12#id-project-3-linear-regression-machine-learning","position":11},{"hierarchy":{"lvl1":"Short Projects Overview","lvl2":"Submission Guidelines"},"type":"lvl2","url":"/index-12#submission-guidelines","position":12},{"hierarchy":{"lvl1":"Short Projects Overview","lvl2":"Submission Guidelines"},"content":"All projects follow consistent submission requirements designed to build professional development skills:\n\nModular .py scripts (no Jupyter notebooks after Project 1)\n\nGitHub version control with meaningful commit history\n\nProfessional documentation with clear README files\n\nProject memos explaining methodology and results\n\n→ Complete Submission Guide","type":"content","url":"/index-12#submission-guidelines","position":13},{"hierarchy":{"lvl1":"Short Projects Overview","lvl2":"Skills Development"},"type":"lvl2","url":"/index-12#skills-development","position":14},{"hierarchy":{"lvl1":"Short Projects Overview","lvl2":"Skills Development"},"content":"Through these projects, you’ll develop:\n\nComputational thinking for complex astrophysical problems\n\nProfessional coding practices used in research and industry\n\nMathematical intuition behind numerical methods\n\nScientific communication through documentation and reports","type":"content","url":"/index-12#skills-development","position":15},{"hierarchy":{"lvl1":"Short Projects Overview","lvl2":"Getting Help"},"type":"lvl2","url":"/index-12#getting-help","position":16},{"hierarchy":{"lvl1":"Short Projects Overview","lvl2":"Getting Help"},"content":"Pair programming sessions during Friday lab time\n\nOffice hours for conceptual guidance and debugging\n\nCourse Slack for quick questions and peer support\n\nAI tools integration following our three-phase policy\n\nEach project builds the foundation for the final neural networks project where you’ll apply these skills to cutting-edge machine learning implementations.","type":"content","url":"/index-12#getting-help","position":17},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression"},"type":"lvl1","url":"/astr596-project-outline","position":0},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression"},"content":"","type":"content","url":"/astr596-project-outline","position":1},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl2":"Project 1: Stellar Structure & Astrophysical Foundations"},"type":"lvl2","url":"/astr596-project-outline#project-1-stellar-structure-astrophysical-foundations","position":2},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl2":"Project 1: Stellar Structure & Astrophysical Foundations"},"content":"Duration: 2 weeks (Aug 25 - Sept 8) | Skills Focus: Python/OOP foundations, fundamental astronomy","type":"content","url":"/astr596-project-outline#project-1-stellar-structure-astrophysical-foundations","position":3},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Science Challenge","lvl2":"Project 1: Stellar Structure & Astrophysical Foundations"},"type":"lvl3","url":"/astr596-project-outline#science-challenge","position":4},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Science Challenge","lvl2":"Project 1: Stellar Structure & Astrophysical Foundations"},"content":"Build a comprehensive Star class that calculates stellar properties with metallicity dependence and implements fundamental astronomical relations for synthetic data generation.","type":"content","url":"/astr596-project-outline#science-challenge","position":5},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Core Physics Components","lvl2":"Project 1: Stellar Structure & Astrophysical Foundations"},"type":"lvl3","url":"/astr596-project-outline#core-physics-components","position":6},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Core Physics Components","lvl2":"Project 1: Stellar Structure & Astrophysical Foundations"},"content":"Stellar Structure: Implement full Tout et al. (1996) metallicity-dependent ZAMS relations for L(M,Z) and R(M,Z)\n\nStellar Evolution: Basic stellar lifetime calculations, main sequence evolution tracks\n\nFundamental Astronomy: Wien’s law, blackbody function, parallax-distance relation, angular size\n\nColor Systems: B-V colors, color-magnitude diagrams, surface brightness calculations\n\nH-R Diagram: Multi-metallicity stellar evolutionary tracks and zero-age main sequence","type":"content","url":"/astr596-project-outline#core-physics-components","position":7},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Technical Implementation","lvl2":"Project 1: Stellar Structure & Astrophysical Foundations"},"type":"lvl3","url":"/astr596-project-outline#technical-implementation","position":8},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Technical Implementation","lvl2":"Project 1: Stellar Structure & Astrophysical Foundations"},"content":"Object-Oriented Design:\n\nStar class with properties (mass, radius, temperature, luminosity, metallicity, age)\n\nStellarPopulation class managing multiple stars with different metallicities\n\nAstrophysical Toolkit Functions:def wien_displacement_law(T): # Peak wavelength of blackbody\ndef blackbody_flux(T, wavelength): # Planck function\ndef parallax_distance(parallax_mas): # Distance from parallax\ndef angular_size(physical_size, distance): # Angular size in arcseconds\ndef luminosity_distance(z, H0=70): # Cosmological distances\ndef surface_brightness(luminosity, angular_area): # Extended object brightness\n\nPython Fundamentals: NumPy arrays, matplotlib visualization, proper documentation\n\nSoftware Practices: Git workflow, modular code organization, unit testing","type":"content","url":"/astr596-project-outline#technical-implementation","position":9},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Expected Deliverables","lvl2":"Project 1: Stellar Structure & Astrophysical Foundations"},"type":"lvl3","url":"/astr596-project-outline#expected-deliverables","position":10},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Expected Deliverables","lvl2":"Project 1: Stellar Structure & Astrophysical Foundations"},"content":"Working Star and StellarPopulation classes with full Tout metallicity dependence\n\nComplete astrophysical toolkit for synthetic observations\n\nH-R diagrams across metallicity range (Z = 0.0001 to 0.03)\n\nColor-magnitude diagrams showing metallicity effects\n\nMass-lifetime relationship analysis for different stellar populations","type":"content","url":"/astr596-project-outline#expected-deliverables","position":11},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Exploration Opportunities","lvl2":"Project 1: Stellar Structure & Astrophysical Foundations"},"type":"lvl3","url":"/astr596-project-outline#exploration-opportunities","position":12},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Exploration Opportunities","lvl2":"Project 1: Stellar Structure & Astrophysical Foundations"},"content":"Creative Experiments: “How would H-R diagram look for stars with Z = 0.1?” “What if stellar lifetimes scaled differently with mass?”\n\nParameter Exploration: Compare low-metallicity (Population II) vs high-metallicity (Population I) stellar populations\n\nConnections to Observations: Calculate properties of nearby stars (Vega, Sirius, etc.)","type":"content","url":"/astr596-project-outline#exploration-opportunities","position":13},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Learning Outcomes","lvl2":"Project 1: Stellar Structure & Astrophysical Foundations"},"type":"lvl3","url":"/astr596-project-outline#learning-outcomes","position":14},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Learning Outcomes","lvl2":"Project 1: Stellar Structure & Astrophysical Foundations"},"content":"Master object-oriented programming through stellar physics\n\nUnderstand stellar structure, evolution, and metallicity effects\n\nBuild foundational astronomical calculation toolkit\n\nDevelop professional software development practices","type":"content","url":"/astr596-project-outline#learning-outcomes","position":15},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl2":"Project 2: N-Body Dynamics & Statistical Stellar Systems"},"type":"lvl2","url":"/astr596-project-outline#project-2-n-body-dynamics-statistical-stellar-systems","position":16},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl2":"Project 2: N-Body Dynamics & Statistical Stellar Systems"},"content":"Duration: 2 weeks (Sept 8 - Sept 22) | Skills Focus: Numerical integration, Monte Carlo sampling, stellar clusters","type":"content","url":"/astr596-project-outline#project-2-n-body-dynamics-statistical-stellar-systems","position":17},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Science Challenge","lvl2":"Project 2: N-Body Dynamics & Statistical Stellar Systems"},"type":"lvl3","url":"/astr596-project-outline#science-challenge-1","position":18},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Science Challenge","lvl2":"Project 2: N-Body Dynamics & Statistical Stellar Systems"},"content":"Simulate realistic gravitational N-body stellar systems by sampling from Initial Mass Functions (IMF) and spatial distributions, enabling exploration of diverse cluster configurations.","type":"content","url":"/astr596-project-outline#science-challenge-1","position":19},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Core Physics Components","lvl2":"Project 2: N-Body Dynamics & Statistical Stellar Systems"},"type":"lvl3","url":"/astr596-project-outline#core-physics-components-1","position":20},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Core Physics Components","lvl2":"Project 2: N-Body Dynamics & Statistical Stellar Systems"},"content":"Gravitational Dynamics: N-body Newton’s laws with softened gravity for close encounters\n\nStellar Cluster Physics: King profiles, virial equilibrium, relaxation timescales, escape velocities\n\nIMF Sampling: Monte Carlo sampling from Salpeter, Kroupa, and custom/top-heavy IMFs\n\nSpatial Distributions: King profiles, Plummer spheres, uniform distributions\n\nCluster Evolution: Mass segregation, evaporation, core collapse timescales","type":"content","url":"/astr596-project-outline#core-physics-components-1","position":21},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Technical Implementation","lvl2":"Project 2: N-Body Dynamics & Statistical Stellar Systems"},"type":"lvl3","url":"/astr596-project-outline#technical-implementation-1","position":22},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Technical Implementation","lvl2":"Project 2: N-Body Dynamics & Statistical Stellar Systems"},"content":"Extensions from Project 1: Use realistic stellar masses and properties from Star class\n\nMonte Carlo Integration:\n\nSample stellar masses from IMF (provided functions: sample_salpeter_imf(), sample_kroupa_imf())\n\nSample positions from King profile (provided: sample_king_profile())\n\nInclude binary fraction as adjustable parameter\n\nNumerical Integration: Euler, RK4, Leapfrog integrators with stability analysis\n\nEnergy Tracking: Kinetic, potential, and total energy conservation\n\nPerformance Optimization: Efficient force calculations, adaptive timesteps","type":"content","url":"/astr596-project-outline#technical-implementation-1","position":23},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Expected Deliverables","lvl2":"Project 2: N-Body Dynamics & Statistical Stellar Systems"},"type":"lvl3","url":"/astr596-project-outline#expected-deliverables-1","position":24},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Expected Deliverables","lvl2":"Project 2: N-Body Dynamics & Statistical Stellar Systems"},"content":"N-body integrator with multiple algorithms (20-100 star systems)\n\nIMF sampling and cluster initialization tools\n\nSolar system simulation demonstrating long-term stability\n\nStellar cluster evolution with realistic mass spectrum and spatial structure\n\nEnergy conservation analysis and algorithmic performance comparison","type":"content","url":"/astr596-project-outline#expected-deliverables-1","position":25},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Creative Experimental Opportunities","lvl2":"Project 2: N-Body Dynamics & Statistical Stellar Systems"},"type":"lvl3","url":"/astr596-project-outline#creative-experimental-opportunities","position":26},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Creative Experimental Opportunities","lvl2":"Project 2: N-Body Dynamics & Statistical Stellar Systems"},"content":"“Design a cluster that evaporates in 1 Myr”: Explore cluster binding energy\n\n“What if all stars were 10 M☉?”: Test equipartition and stellar interactions\n\n“Super dense globular cluster”: Push density limits, explore core collapse\n\n“Primordial star formation”: Top-heavy IMF with zero metallicity stars\n\n“Binary-dominated cluster”: High binary fraction effects on dynamics","type":"content","url":"/astr596-project-outline#creative-experimental-opportunities","position":27},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Parameter Exploration Suggestions","lvl2":"Project 2: N-Body Dynamics & Statistical Stellar Systems"},"type":"lvl3","url":"/astr596-project-outline#parameter-exploration-suggestions","position":28},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Parameter Exploration Suggestions","lvl2":"Project 2: N-Body Dynamics & Statistical Stellar Systems"},"content":"Compare different IMF slopes and mass ranges\n\nVary metallicity across the cluster (metallicity gradients)\n\nExperiment with extreme density configurations\n\nTest different binary fractions and orbital distributions\n\nExplore clusters with initial rotation or turbulence","type":"content","url":"/astr596-project-outline#parameter-exploration-suggestions","position":29},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Learning Outcomes","lvl2":"Project 2: N-Body Dynamics & Statistical Stellar Systems"},"type":"lvl3","url":"/astr596-project-outline#learning-outcomes-1","position":30},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Learning Outcomes","lvl2":"Project 2: N-Body Dynamics & Statistical Stellar Systems"},"content":"Understand gravitational dynamics and stellar cluster physics\n\nMaster Monte Carlo sampling and statistical methods\n\nLearn numerical integration theory and algorithm comparison\n\nDevelop intuition for stellar system evolution through experimentation","type":"content","url":"/astr596-project-outline#learning-outcomes-1","position":31},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl2":"Project 3: Monte Carlo Radiative Transfer & Synthetic Observations"},"type":"lvl2","url":"/astr596-project-outline#project-3-monte-carlo-radiative-transfer-synthetic-observations","position":32},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl2":"Project 3: Monte Carlo Radiative Transfer & Synthetic Observations"},"content":"Duration: 2 weeks (Sept 22 - Oct 6) | Skills Focus: Complex algorithms, radiative physics, data generation","type":"content","url":"/astr596-project-outline#project-3-monte-carlo-radiative-transfer-synthetic-observations","position":33},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Science Challenge","lvl2":"Project 3: Monte Carlo Radiative Transfer & Synthetic Observations"},"type":"lvl3","url":"/astr596-project-outline#science-challenge-2","position":34},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Science Challenge","lvl2":"Project 3: Monte Carlo Radiative Transfer & Synthetic Observations"},"content":"Implement Monte Carlo Radiative Transfer (MCRT) for stellar atmospheres and dusty environments, generating realistic synthetic observations for statistical analysis.","type":"content","url":"/astr596-project-outline#science-challenge-2","position":35},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Core Physics Components","lvl2":"Project 3: Monte Carlo Radiative Transfer & Synthetic Observations"},"type":"lvl3","url":"/astr596-project-outline#core-physics-components-2","position":36},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Core Physics Components","lvl2":"Project 3: Monte Carlo Radiative Transfer & Synthetic Observations"},"content":"Radiative Transfer Physics: Photon transport through absorbing and scattering media\n\nDust Properties: Wavelength-dependent opacity, scattering albedo, phase functions\n\nStellar Atmosphere Models: Plane-parallel atmospheres, limb darkening, temperature gradients\n\nScattering Physics: Isotropic vs anisotropic scattering, single vs multiple scattering\n\nObservational Effects: Line-of-sight variations, dust geometry effects","type":"content","url":"/astr596-project-outline#core-physics-components-2","position":37},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Technical Implementation","lvl2":"Project 3: Monte Carlo Radiative Transfer & Synthetic Observations"},"type":"lvl3","url":"/astr596-project-outline#technical-implementation-2","position":38},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Technical Implementation","lvl2":"Project 3: Monte Carlo Radiative Transfer & Synthetic Observations"},"content":"Builds on Projects 1-2: Use stellar properties and Monte Carlo experience\n\nMCRT Algorithm:\n\nPhoton packet tracking with statistical weights\n\nAbsorption and scattering event handling\n\nOptical depth calculations and Beer’s law\n\nMultiple scattering implementations\n\nAdvanced Features:\n\nStart with isotropic scattering, add anisotropic as extension\n\nInclude both absorption and scattering components\n\nVariable dust grain size distributions\n\nOptional: polarization for ambitious students\n\nData Generation: Create comprehensive synthetic datasets for Projects 4-5","type":"content","url":"/astr596-project-outline#technical-implementation-2","position":39},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Expected Deliverables","lvl2":"Project 3: Monte Carlo Radiative Transfer & Synthetic Observations"},"type":"lvl3","url":"/astr596-project-outline#expected-deliverables-2","position":40},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Expected Deliverables","lvl2":"Project 3: Monte Carlo Radiative Transfer & Synthetic Observations"},"content":"Working MCRT code for plane-parallel stellar atmospheres\n\nSynthetic multi-wavelength spectral energy distributions\n\nParameter space exploration (dust density, optical depth, geometry)\n\nComparison of different scattering assumptions\n\nHigh-quality synthetic datasets with realistic noise for statistical analysis","type":"content","url":"/astr596-project-outline#expected-deliverables-2","position":41},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Science Case Connections","lvl2":"Project 3: Monte Carlo Radiative Transfer & Synthetic Observations"},"type":"lvl3","url":"/astr596-project-outline#science-case-connections","position":42},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Science Case Connections","lvl2":"Project 3: Monte Carlo Radiative Transfer & Synthetic Observations"},"content":"“Why do stars appear redder through dust?”: Explore wavelength-dependent extinction\n\n“How does dust geometry affect observations?”: Compare different dust distributions\n\n“What creates the 2175 Å bump?”: Investigate specific dust features","type":"content","url":"/astr596-project-outline#science-case-connections","position":43},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Creative Experiments","lvl2":"Project 3: Monte Carlo Radiative Transfer & Synthetic Observations"},"type":"lvl3","url":"/astr596-project-outline#creative-experiments","position":44},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Creative Experiments","lvl2":"Project 3: Monte Carlo Radiative Transfer & Synthetic Observations"},"content":"“Dust only in outer atmosphere”: Explore layered dust distributions\n\n“Extreme optical depths”: Push into optically thick regime\n\n“Binary star with circumbinary dust”: Complex geometric configurations\n\n“Variable dust properties”: Time-dependent or spatially varying extinction","type":"content","url":"/astr596-project-outline#creative-experiments","position":45},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Learning Outcomes","lvl2":"Project 3: Monte Carlo Radiative Transfer & Synthetic Observations"},"type":"lvl3","url":"/astr596-project-outline#learning-outcomes-2","position":46},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Learning Outcomes","lvl2":"Project 3: Monte Carlo Radiative Transfer & Synthetic Observations"},"content":"Master advanced Monte Carlo algorithm implementation\n\nUnderstand radiative transfer physics and stellar atmospheres\n\nDevelop complex scientific algorithm debugging and validation\n\nGenerate realistic synthetic datasets for statistical analysis","type":"content","url":"/astr596-project-outline#learning-outcomes-2","position":47},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl2":"Project 4: Linear Regression & Frequentist Parameter Estimation"},"type":"lvl2","url":"/astr596-project-outline#project-4-linear-regression-frequentist-parameter-estimation","position":48},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl2":"Project 4: Linear Regression & Frequentist Parameter Estimation"},"content":"Duration: 2 weeks (Oct 6 - Oct 20) | Skills Focus: ML from scratch, statistical modeling","type":"content","url":"/astr596-project-outline#project-4-linear-regression-frequentist-parameter-estimation","position":49},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Science Challenge","lvl2":"Project 4: Linear Regression & Frequentist Parameter Estimation"},"type":"lvl3","url":"/astr596-project-outline#science-challenge-3","position":50},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Science Challenge","lvl2":"Project 4: Linear Regression & Frequentist Parameter Estimation"},"content":"Implement linear regression from mathematical foundations and apply to recovering physical parameters from Project 3’s MCRT synthetic observations.","type":"content","url":"/astr596-project-outline#science-challenge-3","position":51},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Core Physics & Data Science Components","lvl2":"Project 4: Linear Regression & Frequentist Parameter Estimation"},"type":"lvl3","url":"/astr596-project-outline#core-physics-data-science-components","position":52},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Core Physics & Data Science Components","lvl2":"Project 4: Linear Regression & Frequentist Parameter Estimation"},"content":"Parameter Recovery: Extract dust properties (optical depth, grain size, geometry) from synthetic spectra\n\nDegeneracy Analysis: Understand parameter correlations and measurement limitations\n\nModel Selection: Compare linear vs polynomial fits, regularization techniques\n\nError Propagation: Handle observational uncertainties and systematic effects\n\nAstrophysical Applications: Dust-to-gas ratios, extinction curve fitting, stellar parameter recovery","type":"content","url":"/astr596-project-outline#core-physics-data-science-components","position":53},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Technical Implementation","lvl2":"Project 4: Linear Regression & Frequentist Parameter Estimation"},"type":"lvl3","url":"/astr596-project-outline#technical-implementation-3","position":54},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Technical Implementation","lvl2":"Project 4: Linear Regression & Frequentist Parameter Estimation"},"content":"ML from Mathematical Foundations:\n\nDerive normal equations: (X^T X)β = X^T y\n\nImplement gradient descent optimization from scratch\n\nNo scikit-learn - build complete understanding\n\nData Integration: Use identical MCRT synthetic observations for direct comparison with Project 5\n\nStatistical Analysis:\n\nConfidence intervals, cross-validation, model comparison\n\nOutlier detection and robust fitting techniques\n\nFeature engineering for astrophysical problems","type":"content","url":"/astr596-project-outline#technical-implementation-3","position":55},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Expected Deliverables","lvl2":"Project 4: Linear Regression & Frequentist Parameter Estimation"},"type":"lvl3","url":"/astr596-project-outline#expected-deliverables-3","position":56},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Expected Deliverables","lvl2":"Project 4: Linear Regression & Frequentist Parameter Estimation"},"content":"Complete linear regression implementation without external ML libraries\n\nParameter recovery pipeline for MCRT synthetic observations\n\nAnalysis of parameter degeneracies and measurement uncertainties\n\nModel validation framework with statistical diagnostics\n\nConfidence interval calculations and model comparison metrics","type":"content","url":"/astr596-project-outline#expected-deliverables-3","position":57},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Exploration Components","lvl2":"Project 4: Linear Regression & Frequentist Parameter Estimation"},"type":"lvl3","url":"/astr596-project-outline#exploration-components","position":58},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Exploration Components","lvl2":"Project 4: Linear Regression & Frequentist Parameter Estimation"},"content":"“Which parameters are most degenerate?”: Explore parameter correlation matrices\n\n“How much noise can the method handle?”: Test robustness against observational errors\n\n“Can we recover non-linear relationships?”: Experiment with polynomial features","type":"content","url":"/astr596-project-outline#exploration-components","position":59},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Connection to Real Astronomy","lvl2":"Project 4: Linear Regression & Frequentist Parameter Estimation"},"type":"lvl3","url":"/astr596-project-outline#connection-to-real-astronomy","position":60},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Connection to Real Astronomy","lvl2":"Project 4: Linear Regression & Frequentist Parameter Estimation"},"content":"Compare recovered parameters with known input values\n\nInvestigate which observational setups break degeneracies\n\nExplore how wavelength coverage affects parameter recovery","type":"content","url":"/astr596-project-outline#connection-to-real-astronomy","position":61},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Learning Outcomes","lvl2":"Project 4: Linear Regression & Frequentist Parameter Estimation"},"type":"lvl3","url":"/astr596-project-outline#learning-outcomes-3","position":62},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Learning Outcomes","lvl2":"Project 4: Linear Regression & Frequentist Parameter Estimation"},"content":"Master machine learning fundamentals and optimization theory\n\nUnderstand statistical model validation and comparison\n\nLearn scientific data analysis and uncertainty quantification\n\nDevelop intuition for parameter estimation challenges in astronomy","type":"content","url":"/astr596-project-outline#learning-outcomes-3","position":63},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl2":"Project 5: Bayesian Inference & MCMC - Statistical Method Comparison"},"type":"lvl2","url":"/astr596-project-outline#project-5-bayesian-inference-mcmc-statistical-method-comparison","position":64},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl2":"Project 5: Bayesian Inference & MCMC - Statistical Method Comparison"},"content":"Duration: 2 weeks (Oct 20 - Nov 3) | Skills Focus: Bayesian methods, advanced statistical inference","type":"content","url":"/astr596-project-outline#project-5-bayesian-inference-mcmc-statistical-method-comparison","position":65},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Science Challenge","lvl2":"Project 5: Bayesian Inference & MCMC - Statistical Method Comparison"},"type":"lvl3","url":"/astr596-project-outline#science-challenge-4","position":66},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Science Challenge","lvl2":"Project 5: Bayesian Inference & MCMC - Statistical Method Comparison"},"content":"Implement MCMC from scratch and apply Bayesian inference to the identical MCRT data from Projects 3-4, enabling direct comparison of frequentist vs Bayesian approaches.","type":"content","url":"/astr596-project-outline#science-challenge-4","position":67},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Core Physics & Statistical Components","lvl2":"Project 5: Bayesian Inference & MCMC - Statistical Method Comparison"},"type":"lvl3","url":"/astr596-project-outline#core-physics-statistical-components","position":68},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Core Physics & Statistical Components","lvl2":"Project 5: Bayesian Inference & MCMC - Statistical Method Comparison"},"content":"Bayesian Framework: Prior specification using astrophysical knowledge of dust and stellar properties\n\nMCMC Implementation: Metropolis-Hastings algorithm with adaptive step sizing\n\nPhysical Priors: Incorporate realistic constraints (dust properties, stellar physics, IMF)\n\nPosterior Analysis: Full probability distributions vs point estimates\n\nMethod Comparison: Direct statistical comparison with Project 4 results on identical data","type":"content","url":"/astr596-project-outline#core-physics-statistical-components","position":69},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Technical Implementation","lvl2":"Project 5: Bayesian Inference & MCMC - Statistical Method Comparison"},"type":"lvl3","url":"/astr596-project-outline#technical-implementation-4","position":70},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Technical Implementation","lvl2":"Project 5: Bayesian Inference & MCMC - Statistical Method Comparison"},"content":"Same Dataset: Apply to identical MCRT synthetic observations from Project 3\n\nMCMC from Scratch:\n\nMetropolis-Hastings with proposal distributions\n\nAdaptive step size algorithms\n\nMultiple chain implementation\n\nConvergence diagnostics (Gelman-Rubin, autocorrelation)\n\nPrior Engineering:\n\nPhysical priors on dust-to-gas ratios\n\nStellar parameter priors from Project 1\n\nHierarchical priors for population studies\n\nPosterior Analysis: Corner plots, credible intervals, model comparison","type":"content","url":"/astr596-project-outline#technical-implementation-4","position":71},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Expected Deliverables","lvl2":"Project 5: Bayesian Inference & MCMC - Statistical Method Comparison"},"type":"lvl3","url":"/astr596-project-outline#expected-deliverables-4","position":72},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Expected Deliverables","lvl2":"Project 5: Bayesian Inference & MCMC - Statistical Method Comparison"},"content":"Complete MCMC sampler implementation from mathematical principles\n\nBayesian parameter estimation pipeline for dust and stellar properties\n\nDirect statistical comparison: Bayesian posteriors vs frequentist confidence intervals\n\nAnalysis of prior sensitivity and robustness\n\nConvergence diagnostics and computational efficiency analysis","type":"content","url":"/astr596-project-outline#expected-deliverables-4","position":73},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Statistical Method Comparison Focus","lvl2":"Project 5: Bayesian Inference & MCMC - Statistical Method Comparison"},"type":"lvl3","url":"/astr596-project-outline#statistical-method-comparison-focus","position":74},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Statistical Method Comparison Focus","lvl2":"Project 5: Bayesian Inference & MCMC - Statistical Method Comparison"},"content":"Plot both confidence intervals and credible intervals on same plots\n\nModel selection comparison: Which approach handles degeneracies better?\n\nOutlier robustness: Compare method performance with contaminated data\n\nComputational efficiency: Runtime and convergence comparison","type":"content","url":"/astr596-project-outline#statistical-method-comparison-focus","position":75},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Advanced Extensions","lvl2":"Project 5: Bayesian Inference & MCMC - Statistical Method Comparison"},"type":"lvl3","url":"/astr596-project-outline#advanced-extensions","position":76},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Advanced Extensions","lvl2":"Project 5: Bayesian Inference & MCMC - Statistical Method Comparison"},"content":"Hierarchical Bayesian modeling: Population-level parameters\n\nModel selection: Bayesian evidence vs frequentist model comparison\n\nRobust likelihood functions: Handle outliers and systematic errors","type":"content","url":"/astr596-project-outline#advanced-extensions","position":77},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Learning Outcomes","lvl2":"Project 5: Bayesian Inference & MCMC - Statistical Method Comparison"},"type":"lvl3","url":"/astr596-project-outline#learning-outcomes-4","position":78},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Learning Outcomes","lvl2":"Project 5: Bayesian Inference & MCMC - Statistical Method Comparison"},"content":"Master Bayesian statistical inference and MCMC implementation\n\nUnderstand the fundamental differences between statistical paradigms\n\nLearn advanced uncertainty quantification and model comparison\n\nDevelop intuition for when to use Bayesian vs frequentist approaches","type":"content","url":"/astr596-project-outline#learning-outcomes-4","position":79},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl2":"Project 6: Multi-Wavelength Stellar Extinction with Gaussian Processes"},"type":"lvl2","url":"/astr596-project-outline#project-6-multi-wavelength-stellar-extinction-with-gaussian-processes","position":80},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl2":"Project 6: Multi-Wavelength Stellar Extinction with Gaussian Processes"},"content":"Duration: 2 weeks (Nov 3 - Nov 17) | Skills Focus: Advanced ML, multi-wavelength astrophysics","type":"content","url":"/astr596-project-outline#project-6-multi-wavelength-stellar-extinction-with-gaussian-processes","position":81},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Science Challenge","lvl2":"Project 6: Multi-Wavelength Stellar Extinction with Gaussian Processes"},"type":"lvl3","url":"/astr596-project-outline#science-challenge-5","position":82},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Science Challenge","lvl2":"Project 6: Multi-Wavelength Stellar Extinction with Gaussian Processes"},"content":"Implement Gaussian Processes from scratch for multi-wavelength stellar extinction modeling, enabling rapid parameter recovery and uncertainty quantification.","type":"content","url":"/astr596-project-outline#science-challenge-5","position":83},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Core Physics & ML Components","lvl2":"Project 6: Multi-Wavelength Stellar Extinction with Gaussian Processes"},"type":"lvl3","url":"/astr596-project-outline#core-physics-ml-components","position":84},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Core Physics & ML Components","lvl2":"Project 6: Multi-Wavelength Stellar Extinction with Gaussian Processes"},"content":"Advanced Stellar Physics: Full Tout et al. metallicity-dependent stellar populations\n\nMulti-Wavelength Extinction: Frequency-dependent dust opacities (Weingartner & Draine 2001)\n\nRealistic Observations: Include photometric uncertainties, systematic effects, filter responses\n\nGP Applications: Spectral interpolation, uncertainty quantification, active learning\n\nSurrogate Modeling: Fast prediction vs full stellar atmosphere calculations","type":"content","url":"/astr596-project-outline#core-physics-ml-components","position":85},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Technical Implementation","lvl2":"Project 6: Multi-Wavelength Stellar Extinction with Gaussian Processes"},"type":"lvl3","url":"/astr596-project-outline#technical-implementation-5","position":86},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Technical Implementation","lvl2":"Project 6: Multi-Wavelength Stellar Extinction with Gaussian Processes"},"content":"Building from Project 1: Extend Tout relations and astrophysical toolkit\n\nRadiative Transfer Connection: Apply Project 3 concepts to realistic dust extinction\n\nGP from Scratch:\n\nImplement kernel functions (RBF, Matérn, periodic combinations)\n\nHyperparameter optimization via marginal likelihood\n\nUncertainty quantification and prediction intervals\n\nMulti-Wavelength Physics:\n\nModel I_observed(ν) = I_intrinsic(ν) × e^(-τ_ν) across UV/optical/IR\n\nBinned opacity approach (6-10 frequency bins)\n\nRealistic filter convolutions","type":"content","url":"/astr596-project-outline#technical-implementation-5","position":87},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Expected Deliverables","lvl2":"Project 6: Multi-Wavelength Stellar Extinction with Gaussian Processes"},"type":"lvl3","url":"/astr596-project-outline#expected-deliverables-5","position":88},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Expected Deliverables","lvl2":"Project 6: Multi-Wavelength Stellar Extinction with Gaussian Processes"},"content":"Complete GP implementation for multi-wavelength stellar data\n\nStellar population synthesis with realistic extinction modeling\n\nFast SED prediction pipeline with uncertainty quantification\n\nPerformance comparison: GP interpolation vs traditional stellar atmosphere grids\n\nAnalysis of GP performance across parameter space (mass, metallicity, extinction)","type":"content","url":"/astr596-project-outline#expected-deliverables-5","position":89},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Advanced Extensions & Experiments","lvl2":"Project 6: Multi-Wavelength Stellar Extinction with Gaussian Processes"},"type":"lvl3","url":"/astr596-project-outline#advanced-extensions-experiments","position":90},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Advanced Extensions & Experiments","lvl2":"Project 6: Multi-Wavelength Stellar Extinction with Gaussian Processes"},"content":"Kernel Combinations: Experiment with different kernel combinations for spectral features\n\nActive Learning: GP chooses which observations would be most informative\n\nTime-Domain Applications: Variable star light curves, stellar pulsations\n\nGP Emulators: Train GP to emulate expensive MCRT calculations from Project 3","type":"content","url":"/astr596-project-outline#advanced-extensions-experiments","position":91},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Creative Exploration Opportunities","lvl2":"Project 6: Multi-Wavelength Stellar Extinction with Gaussian Processes"},"type":"lvl3","url":"/astr596-project-outline#creative-exploration-opportunities","position":92},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Creative Exploration Opportunities","lvl2":"Project 6: Multi-Wavelength Stellar Extinction with Gaussian Processes"},"content":"“Design optimal filter sets”: Use GP uncertainty to choose best wavelength coverage\n\n“Extreme stellar populations”: Very metal-poor or super-metal-rich stars\n\n“Non-standard extinction laws”: Explore deviations from standard dust properties","type":"content","url":"/astr596-project-outline#creative-exploration-opportunities","position":93},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Connection to Modern Astronomy","lvl2":"Project 6: Multi-Wavelength Stellar Extinction with Gaussian Processes"},"type":"lvl3","url":"/astr596-project-outline#connection-to-modern-astronomy","position":94},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Connection to Modern Astronomy","lvl2":"Project 6: Multi-Wavelength Stellar Extinction with Gaussian Processes"},"content":"Applications to large astronomical surveys (Gaia, LSST, Euclid)\n\nReal-time parameter estimation for massive datasets\n\nUncertainty-aware predictions for follow-up observations","type":"content","url":"/astr596-project-outline#connection-to-modern-astronomy","position":95},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Learning Outcomes","lvl2":"Project 6: Multi-Wavelength Stellar Extinction with Gaussian Processes"},"type":"lvl3","url":"/astr596-project-outline#learning-outcomes-5","position":96},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Learning Outcomes","lvl2":"Project 6: Multi-Wavelength Stellar Extinction with Gaussian Processes"},"content":"Master advanced machine learning (non-parametric methods)\n\nUnderstand multi-wavelength stellar astrophysics and realistic observations\n\nLearn scientific surrogate modeling and computational efficiency\n\nConnect modern ML techniques to practical astronomical applications","type":"content","url":"/astr596-project-outline#learning-outcomes-5","position":97},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl2":"Final Project: Neural Networks with JAX - Modern Computational Astrophysics"},"type":"lvl2","url":"/astr596-project-outline#final-project-neural-networks-with-jax-modern-computational-astrophysics","position":98},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl2":"Final Project: Neural Networks with JAX - Modern Computational Astrophysics"},"content":"Duration: 4 weeks (Nov 17 - Dec 18) | Skills Focus: Modern frameworks, neural networks, research integration","type":"content","url":"/astr596-project-outline#final-project-neural-networks-with-jax-modern-computational-astrophysics","position":99},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Science Challenge","lvl2":"Final Project: Neural Networks with JAX - Modern Computational Astrophysics"},"type":"lvl3","url":"/astr596-project-outline#science-challenge-6","position":100},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Science Challenge","lvl2":"Final Project: Neural Networks with JAX - Modern Computational Astrophysics"},"content":"Build neural networks from mathematical foundations using JAX, then apply to a research-level astronomical problem integrating multiple course techniques.","type":"content","url":"/astr596-project-outline#science-challenge-6","position":101},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Phase 1: Neural Networks from JAX Fundamentals (Week 1)","lvl2":"Final Project: Neural Networks with JAX - Modern Computational Astrophysics"},"type":"lvl3","url":"/astr596-project-outline#phase-1-neural-networks-from-jax-fundamentals-week-1","position":102},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Phase 1: Neural Networks from JAX Fundamentals (Week 1)","lvl2":"Final Project: Neural Networks with JAX - Modern Computational Astrophysics"},"content":"Technical Focus: Implement NN mathematics from scratch using JAX arrays\n\nGlass Box Philosophy: Build feedforward networks, backpropagation, gradient descent using only JAX arrays\n\nStructural Support: Possibly use Equinox for PyTree organization while implementing all mathematics by hand\n\nMathematical Mastery: Complete understanding of neural network algorithms before using frameworks\n\nSimple Validation: Function approximation or basic astronomical classification problems","type":"content","url":"/astr596-project-outline#phase-1-neural-networks-from-jax-fundamentals-week-1","position":103},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Phase 2: JAX Ecosystem Integration (Week 2)","lvl2":"Final Project: Neural Networks with JAX - Modern Computational Astrophysics"},"type":"lvl3","url":"/astr596-project-outline#phase-2-jax-ecosystem-integration-week-2","position":104},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Phase 2: JAX Ecosystem Integration (Week 2)","lvl2":"Final Project: Neural Networks with JAX - Modern Computational Astrophysics"},"content":"Technical Focus: Transition to modern frameworks with full understanding\n\nFramework Transition: Convert hand-built JAX implementation to Flax/Optax ecosystem\n\nAlgorithm Translation: Convert previous algorithm (N-body, MCRT, or regression) to JAX\n\nPerformance Analysis: Benchmark hand-built vs framework implementations\n\nJAX Transformations: Master jit, grad, vmap through practical applications","type":"content","url":"/astr596-project-outline#phase-2-jax-ecosystem-integration-week-2","position":105},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Phase 3: Research-Level Application (Weeks 3-4)","lvl2":"Final Project: Neural Networks with JAX - Modern Computational Astrophysics"},"type":"lvl3","url":"/astr596-project-outline#phase-3-research-level-application-weeks-3-4","position":106},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Phase 3: Research-Level Application (Weeks 3-4)","lvl2":"Final Project: Neural Networks with JAX - Modern Computational Astrophysics"},"content":"Science Focus: Choose ONE application integrating multiple course concepts\n\nOption A: Multi-Method Stellar Parameter Estimation\n\nData Integration: Combine stellar models (Project 1), extinction (Project 6), synthetic observations\n\nMethod Comparison: Neural networks vs GP (Project 6) vs Bayesian (Project 5) approaches\n\nScientific Validation: Test on realistic stellar survey data, quantify systematic differences\n\nAdvanced Features: Bayesian neural networks, uncertainty quantification, ensemble methods\n\nOption B: Neural Surrogate for Complex Simulations\n\nPhysics Integration: Use MCRT (Project 3) or N-body (Project 2) as expensive “truth” calculations\n\nSurrogate Development: Train neural networks to emulate full physics simulations\n\nPerformance Demonstration: Orders-of-magnitude speedup with quantified accuracy\n\nActive Learning: Neural network chooses which simulations to run for optimal training\n\nOption C: Hierarchical Bayesian Neural Networks\n\nAdvanced Integration: Combine Bayesian inference (Project 5) with neural network flexibility\n\nPopulation Studies: Apply to stellar populations or cluster properties\n\nUncertainty Quantification: Full Bayesian treatment of neural network parameters\n\nScientific Application: Real astronomical datasets with hierarchical structure","type":"content","url":"/astr596-project-outline#phase-3-research-level-application-weeks-3-4","position":107},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Expected Deliverables","lvl2":"Final Project: Neural Networks with JAX - Modern Computational Astrophysics"},"type":"lvl3","url":"/astr596-project-outline#expected-deliverables-6","position":108},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Expected Deliverables","lvl2":"Final Project: Neural Networks with JAX - Modern Computational Astrophysics"},"content":"Complete neural network implementation from mathematical foundations\n\nJAX ecosystem integration with performance benchmarking\n\nResearch-level final application with scientific validation\n\nProfessional presentation comparing modern vs traditional methods\n\nComputational portfolio demonstrating technical and scientific growth","type":"content","url":"/astr596-project-outline#expected-deliverables-6","position":109},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Research Preparation Components","lvl2":"Final Project: Neural Networks with JAX - Modern Computational Astrophysics"},"type":"lvl3","url":"/astr596-project-outline#research-preparation-components","position":110},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Research Preparation Components","lvl2":"Final Project: Neural Networks with JAX - Modern Computational Astrophysics"},"content":"Literature Integration: Reference relevant astronomical applications and recent papers\n\nMethod Validation: Compare with established techniques, quantify advantages/limitations\n\nComputational Efficiency: Analyze scaling, memory usage, and optimization\n\nScientific Impact: Discuss applications to real astronomical problems","type":"content","url":"/astr596-project-outline#research-preparation-components","position":111},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Learning Outcomes","lvl2":"Final Project: Neural Networks with JAX - Modern Computational Astrophysics"},"type":"lvl3","url":"/astr596-project-outline#learning-outcomes-6","position":112},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Learning Outcomes","lvl2":"Final Project: Neural Networks with JAX - Modern Computational Astrophysics"},"content":"Master neural network theory and modern computational frameworks\n\nIntegrate multiple course techniques into research-level applications\n\nDevelop skills for computational research careers in academia and industry\n\nUnderstand the progression from classical methods to cutting-edge approaches","type":"content","url":"/astr596-project-outline#learning-outcomes-6","position":113},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl2":"Course Integration & Pedagogical Framework"},"type":"lvl2","url":"/astr596-project-outline#course-integration-pedagogical-framework","position":114},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl2":"Course Integration & Pedagogical Framework"},"content":"","type":"content","url":"/astr596-project-outline#course-integration-pedagogical-framework","position":115},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Three-Phase Learning Progression","lvl2":"Course Integration & Pedagogical Framework"},"type":"lvl3","url":"/astr596-project-outline#three-phase-learning-progression","position":116},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Three-Phase Learning Progression","lvl2":"Course Integration & Pedagogical Framework"},"content":"Phase 1: Computational Physics Foundations (Projects 1-3)\n\nCharacter: Classical computational astrophysics with modern software practices\n\nFocus: Physics understanding through code implementation\n\nSkills: Object-oriented programming, numerical methods, complex algorithms\n\nMindset: “How do I translate physics equations into working code?”\n\nPhase 2: Statistical Analysis & Machine Learning (Projects 4-6)\n\nCharacter: Modern data analysis applied to astronomical problems\n\nFocus: Statistical inference and machine learning from mathematical foundations\n\nSkills: Parameter estimation, uncertainty quantification, advanced ML\n\nMindset: “How do I extract knowledge from complex datasets?”\n\nPhase 3: Modern Framework Integration (Final Project)\n\nCharacter: Cutting-edge computational tools for research applications\n\nFocus: Integration of physics understanding with modern ML frameworks\n\nSkills: JAX ecosystem, neural networks, research-level problem solving\n\nMindset: “How do I solve research problems using state-of-the-art methods?”","type":"content","url":"/astr596-project-outline#three-phase-learning-progression","position":117},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Cross-Project Integration Strategy","lvl2":"Course Integration & Pedagogical Framework"},"type":"lvl3","url":"/astr596-project-outline#cross-project-integration-strategy","position":118},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Cross-Project Integration Strategy","lvl2":"Course Integration & Pedagogical Framework"},"content":"Data Flow & Reuse:\n\nProjects 1→2: Stellar properties and masses flow into cluster simulations\n\nProjects 3→4→5: Identical MCRT data enables direct statistical method comparison\n\nProjects 1→6: Stellar physics extended to multi-wavelength applications\n\nAll→Final: Students can build on any previous project for final application\n\nSkill Accumulation:\n\nProgramming Fundamentals (Projects 1-2) → Advanced Algorithms (Project 3)\n\nStatistical Foundations (Projects 4-5) → Advanced ML (Project 6)\n\nClassical Methods (Projects 1-6) → Modern Frameworks (Final Project)\n\nPhysics Understanding Development:\n\nStellar Structure → Stellar Systems → Stellar Observations → Data Analysis → Modern Applications\n\nEach project adds complexity while reinforcing previous concepts","type":"content","url":"/astr596-project-outline#cross-project-integration-strategy","position":119},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Assessment Philosophy & Expectations","lvl2":"Course Integration & Pedagogical Framework"},"type":"lvl3","url":"/astr596-project-outline#assessment-philosophy-expectations","position":120},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Assessment Philosophy & Expectations","lvl2":"Course Integration & Pedagogical Framework"},"content":"Understanding Over Performance:\n\nStudents must explain every line of code they submit\n\nEmphasis on physical intuition and mathematical foundations\n\nCreativity and experimentation valued over “correct” results\n\nResearch Preparation Focus:\n\nProfessional software development practices throughout\n\nLiterature connections and scientific context for each project\n\nFinal presentations mirror research conference talks\n\nComputational Thinking Development:\n\nExplicit debugging challenges and optimization exercises\n\nPeer code review sessions for collaborative learning\n\nPortfolio development for career preparation","type":"content","url":"/astr596-project-outline#assessment-philosophy-expectations","position":121},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Creative Experimentation Guidelines","lvl2":"Course Integration & Pedagogical Framework"},"type":"lvl3","url":"/astr596-project-outline#creative-experimentation-guidelines","position":122},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Creative Experimentation Guidelines","lvl2":"Course Integration & Pedagogical Framework"},"content":"Encourage Scientific Curiosity:\n\n“What happens if...” questions drive exploration\n\nParameter space exploration over rigid problem sets\n\nStudents design their own scientific experiments\n\nSupport Creative Risk-Taking:\n\nWeird parameter choices and extreme cases welcomed\n\nFailed experiments are learning opportunities\n\nPeer sharing of surprising results and discoveries\n\nResearch Skills Integration:\n\nLiterature connections for each project\n\nScientific communication through presentations\n\nProfessional portfolio development","type":"content","url":"/astr596-project-outline#creative-experimentation-guidelines","position":123},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl2":"Detailed Implementation Feedback & Extensions"},"type":"lvl2","url":"/astr596-project-outline#detailed-implementation-feedback-extensions","position":124},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl2":"Detailed Implementation Feedback & Extensions"},"content":"","type":"content","url":"/astr596-project-outline#detailed-implementation-feedback-extensions","position":125},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Project 1 Enhancement Details","lvl2":"Detailed Implementation Feedback & Extensions"},"type":"lvl3","url":"/astr596-project-outline#project-1-enhancement-details","position":126},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Project 1 Enhancement Details","lvl2":"Detailed Implementation Feedback & Extensions"},"content":"Additional Toolkit Functions:def magnitude_system(luminosity, distance, filter_band):\n    \"\"\"Convert to astronomical magnitude system\"\"\"\n\ndef extinction_correction(observed_mag, A_v, extinction_law):\n    \"\"\"Apply dust extinction corrections\"\"\"\n\ndef stellar_density_profile(radius, stellar_type):\n    \"\"\"Radial density profiles for different stellar types\"\"\"\n\nCreative Extension Ideas:\n\n“Alien star systems”: Non-solar metallicity relationships\n\n“Failed stars”: Brown dwarf parameter exploration\n\n“Extreme environments”: High-radiation or low-metallicity galaxies","type":"content","url":"/astr596-project-outline#project-1-enhancement-details","position":127},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Project 2 Advanced Features","lvl2":"Detailed Implementation Feedback & Extensions"},"type":"lvl3","url":"/astr596-project-outline#project-2-advanced-features","position":128},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Project 2 Advanced Features","lvl2":"Detailed Implementation Feedback & Extensions"},"content":"Cluster Physics Extensions:\n\nTidal disruption: External gravitational fields\n\nStellar evolution effects: Supernovae kicks, stellar winds\n\nPrimordial binaries: Formation and evolution in cluster environment\n\nExperimental Suggestions:\n\n“Impossible clusters”: Violate virial theorem, explore consequences\n\n“Time-reversed evolution”: Start with evolved cluster, run backward\n\n“Multi-component systems”: Different stellar populations with different IMFs","type":"content","url":"/astr596-project-outline#project-2-advanced-features","position":129},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Project 3 Advanced Radiative Transfer","lvl2":"Detailed Implementation Feedback & Extensions"},"type":"lvl3","url":"/astr596-project-outline#project-3-advanced-radiative-transfer","position":130},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Project 3 Advanced Radiative Transfer","lvl2":"Detailed Implementation Feedback & Extensions"},"content":"Physical Extensions:\n\nPolarization: Track Stokes parameters through scattering\n\nTime-dependent sources: Variable stars, binary eclipse modeling\n\nComplex geometries: Disk systems, outflow cavities, clumpy media\n\nComputational Challenges:\n\nVariance reduction: Importance sampling, Russian roulette\n\nParallel implementation: Domain decomposition strategies\n\nAdaptive sampling: Focus photons in regions of interest","type":"content","url":"/astr596-project-outline#project-3-advanced-radiative-transfer","position":131},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Projects 4-5 Statistical Method Deep Dive","lvl2":"Detailed Implementation Feedback & Extensions"},"type":"lvl3","url":"/astr596-project-outline#projects-4-5-statistical-method-deep-dive","position":132},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Projects 4-5 Statistical Method Deep Dive","lvl2":"Detailed Implementation Feedback & Extensions"},"content":"Advanced Comparison Studies:\n\nHierarchical modeling: Population vs individual parameter inference\n\nModel selection: Information criteria vs Bayesian evidence\n\nComputational scaling: Performance with increasing dataset size\n\nReal Data Applications:\n\nGaia stellar parameters: Apply methods to real survey data\n\nSystematic error modeling: Handle calibration uncertainties\n\nMissing data treatment: Partial observations and selection effects","type":"content","url":"/astr596-project-outline#projects-4-5-statistical-method-deep-dive","position":133},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Project 6 GP Advanced Applications","lvl2":"Detailed Implementation Feedback & Extensions"},"type":"lvl3","url":"/astr596-project-outline#project-6-gp-advanced-applications","position":134},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Project 6 GP Advanced Applications","lvl2":"Detailed Implementation Feedback & Extensions"},"content":"Modern Astronomical Applications:\n\nSurvey optimization: Design optimal observing strategies\n\nReal-time analysis: Process large datasets efficiently\n\nMulti-fidelity modeling: Combine different simulation accuracies\n\nResearch Connections:\n\nExoplanet detection: GP for stellar activity modeling\n\nSupernova classification: Spectroscopic analysis with uncertainties\n\nGalaxy evolution: Multi-wavelength SED fitting with GP emulators","type":"content","url":"/astr596-project-outline#project-6-gp-advanced-applications","position":135},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Final Project Research Directions","lvl2":"Detailed Implementation Feedback & Extensions"},"type":"lvl3","url":"/astr596-project-outline#final-project-research-directions","position":136},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Final Project Research Directions","lvl2":"Detailed Implementation Feedback & Extensions"},"content":"Cutting-Edge Applications:\n\nPhysics-informed neural networks: Incorporate stellar structure equations\n\nDifferentiable simulations: End-to-end optimization of simulation parameters\n\nFederated learning: Combine datasets across different observatories\n\nGraph neural networks: Analyze astronomical survey catalog structures\n\nIndustry Preparation:\n\nMLOps practices: Model deployment and monitoring\n\nDistributed computing: Large-scale data processing\n\nSoftware engineering: Production-quality code development\n\nTechnical communication: Present to non-technical stakeholders","type":"content","url":"/astr596-project-outline#final-project-research-directions","position":137},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Course Outcome Assessment","lvl2":"Detailed Implementation Feedback & Extensions"},"type":"lvl3","url":"/astr596-project-outline#course-outcome-assessment","position":138},{"hierarchy":{"lvl1":"ASTR596: Modeling the Universe - Complete Scaffolded Project Progression","lvl3":"Course Outcome Assessment","lvl2":"Detailed Implementation Feedback & Extensions"},"content":"Technical Skills Mastery:\n\nImplement complex algorithms from mathematical foundations\n\nUnderstand and apply modern computational frameworks\n\nDevelop professional software engineering practices\n\nMaster statistical inference and machine learning techniques\n\nScientific Thinking Development:\n\nTranslate physical understanding into computational implementations\n\nDesign and execute scientific computational experiments\n\nInterpret and communicate complex technical results\n\nConnect computational methods to real astronomical applications\n\nResearch Preparation:\n\nRead and implement methods from research literature\n\nDevelop original research questions and computational approaches\n\nPresent technical work to scientific audiences\n\nBuild professional portfolio for academic or industry careers\n\nThis comprehensive framework prepares students for the modern landscape of computational astrophysics, where deep physics understanding meets cutting-edge computational techniques. The emphasis on experimentation and creativity ensures students develop both technical skills and scientific intuition essential for research careers.","type":"content","url":"/astr596-project-outline#course-outcome-assessment","position":139},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide"},"type":"lvl1","url":"/final-project-guide","position":0},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide"},"content":"##Neural Networks for Astrophysical Discovery**","type":"content","url":"/final-project-guide","position":1},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"Overview"},"type":"lvl3","url":"/final-project-guide#overview","position":2},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"Overview"},"content":"The final project is your opportunity to synthesize everything you’ve learned by tackling a novel scientific question using neural networks. You’ll extend one of your previous projects (P1-P6), refactor it to JAX, and apply neural network methods to solve a problem that would be difficult or impossible with classical approaches alone.\n\nKey Dates:\n\nNov 17: Project assigned, begin planning\n\nNov 21: Proposal due (2 pages)\n\nDec 5: Progress report due (1 page + preliminary results)\n\nDec 11: Technical Growth Synthesis due\n\nDec 17/18: Final presentations (10 minutes)\n\nDec 18: Final submission (code + 8-12 page report)","type":"content","url":"/final-project-guide#overview","position":3},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl2":"Project Structure"},"type":"lvl2","url":"/final-project-guide#project-structure","position":4},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl2":"Project Structure"},"content":"","type":"content","url":"/final-project-guide#project-structure","position":5},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"Phase 1: Selection & Refactoring (Week 1)","lvl2":"Project Structure"},"type":"lvl3","url":"/final-project-guide#phase-1-selection-refactoring-week-1","position":6},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"Phase 1: Selection & Refactoring (Week 1)","lvl2":"Project Structure"},"content":"Choose one of your previous projects and identify a scientific question that:\n\nExtends beyond the original project scope\n\nBenefits from neural network approaches\n\nCannot be easily solved with classical methods alone\n\nThen refactor your existing code to JAX:\n\nConvert NumPy operations to JAX\n\nImplement automatic differentiation where beneficial\n\nPrepare for GPU acceleration\n\nMaintain modular structure","type":"content","url":"/final-project-guide#phase-1-selection-refactoring-week-1","position":7},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"Phase 2: Neural Network Implementation (Week 2)","lvl2":"Project Structure"},"type":"lvl3","url":"/final-project-guide#phase-2-neural-network-implementation-week-2","position":8},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"Phase 2: Neural Network Implementation (Week 2)","lvl2":"Project Structure"},"content":"Implement your neural network solution with two components:\n\nFrom Scratch: Build the core NN architecture manually (forward pass, backprop)\n\nJAX Ecosystem: Use Equinox/Flax for production implementation","type":"content","url":"/final-project-guide#phase-2-neural-network-implementation-week-2","position":9},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"Phase 3: Science & Analysis (Week 3)","lvl2":"Project Structure"},"type":"lvl3","url":"/final-project-guide#phase-3-science-analysis-week-3","position":10},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"Phase 3: Science & Analysis (Week 3)","lvl2":"Project Structure"},"content":"Run experiments and generate results\n\nCompare NN approach to classical methods\n\nAnalyze what the network learned\n\nPrepare visualizations and presentation","type":"content","url":"/final-project-guide#phase-3-science-analysis-week-3","position":11},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl2":"Project Ideas by Previous Project"},"type":"lvl2","url":"/final-project-guide#project-ideas-by-previous-project","position":12},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl2":"Project Ideas by Previous Project"},"content":"","type":"content","url":"/final-project-guide#project-ideas-by-previous-project","position":13},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"Extending Project 1: Stellar Physics","lvl2":"Project Ideas by Previous Project"},"type":"lvl3","url":"/final-project-guide#extending-project-1-stellar-physics","position":14},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"Extending Project 1: Stellar Physics","lvl2":"Project Ideas by Previous Project"},"content":"Classical Approach: HR diagram analysis, stellar classification\n\nNN Extension Ideas:\n\nStellar Parameter Prediction: Train NN to predict Teff, log(g), [Fe/H] from spectra\n\nEvolutionary Track Interpolation: NN to predict stellar evolution between computed models\n\nVariable Star Classification: Time-series classification of light curves\n\nSpectral Synthesis: Generate synthetic spectra from stellar parameters\n\nWhy NNs? Non-linear relationships in high-dimensional spectral data, pattern recognition in time series","type":"content","url":"/final-project-guide#extending-project-1-stellar-physics","position":15},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"Extending Project 2: N-Body Dynamics","lvl2":"Project Ideas by Previous Project"},"type":"lvl3","url":"/final-project-guide#extending-project-2-n-body-dynamics","position":16},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"Extending Project 2: N-Body Dynamics","lvl2":"Project Ideas by Previous Project"},"content":"Classical Approach: Direct integration of gravitational forces\n\nNN Extension Ideas:\n\nChaos Prediction: Predict long-term stability of multi-body systems\n\nFast Force Approximation: NN to approximate expensive force calculations\n\nOrbit Classification: Classify orbital families in galactic potentials\n\nMissing Mass Inference: Infer dark matter distribution from visible orbits\n\nWhy NNs? Speed up expensive calculations, find patterns in chaotic dynamics, inverse problems","type":"content","url":"/final-project-guide#extending-project-2-n-body-dynamics","position":17},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"Extending Project 3: Regression Fundamentals","lvl2":"Project Ideas by Previous Project"},"type":"lvl3","url":"/final-project-guide#extending-project-3-regression-fundamentals","position":18},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"Extending Project 3: Regression Fundamentals","lvl2":"Project Ideas by Previous Project"},"content":"Classical Approach: Linear/polynomial regression, basic optimization\n\nNN Extension Ideas:\n\nDeep Regression Networks: Multi-layer networks for complex relationships\n\nUncertainty Quantification: Bayesian neural networks for error estimates\n\nFeature Learning: Automatic feature extraction from raw data\n\nTransfer Learning: Pre-train on simulations, fine-tune on observations\n\nWhy NNs? Capture non-linear relationships, automatic feature engineering, uncertainty estimation","type":"content","url":"/final-project-guide#extending-project-3-regression-fundamentals","position":19},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"Extending Project 4: Monte Carlo Radiative Transfer","lvl2":"Project Ideas by Previous Project"},"type":"lvl3","url":"/final-project-guide#extending-project-4-monte-carlo-radiative-transfer","position":20},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"Extending Project 4: Monte Carlo Radiative Transfer","lvl2":"Project Ideas by Previous Project"},"content":"Classical Approach: Photon packet propagation through medium\n\nNN Extension Ideas:\n\nEmulator Networks: NN to approximate expensive MCRT calculations\n\nInverse RT: Infer medium properties from observed spectra\n\nAcceleration Schemes: NN to importance sample photon paths\n\nImage-to-Image Translation: Map observations to physical parameters\n\nWhy NNs? Orders of magnitude speedup, solve inverse problems, learn from simulations","type":"content","url":"/final-project-guide#extending-project-4-monte-carlo-radiative-transfer","position":21},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"Extending Project 5: Bayesian/MCMC","lvl2":"Project Ideas by Previous Project"},"type":"lvl3","url":"/final-project-guide#extending-project-5-bayesian-mcmc","position":22},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"Extending Project 5: Bayesian/MCMC","lvl2":"Project Ideas by Previous Project"},"content":"Classical Approach: Metropolis-Hastings, parameter estimation\n\nNN Extension Ideas:\n\nNormalizing Flows: Learn complex posterior distributions\n\nLikelihood-Free Inference: Neural posterior estimation\n\nProposal Networks: Learn optimal MCMC proposal distributions\n\nVariational Inference: Approximate posteriors with NNs\n\nWhy NNs? Handle high-dimensional posteriors, accelerate inference, avoid likelihood calculations","type":"content","url":"/final-project-guide#extending-project-5-bayesian-mcmc","position":23},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"Extending Project 6: Gaussian Processes","lvl2":"Project Ideas by Previous Project"},"type":"lvl3","url":"/final-project-guide#extending-project-6-gaussian-processes","position":24},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"Extending Project 6: Gaussian Processes","lvl2":"Project Ideas by Previous Project"},"content":"Classical Approach: Kernel-based regression, hyperparameter optimization\nNN Extension Ideas:\n\nDeep Kernel Learning: Learn kernel functions with NNs\n\nNeural Process Models: Combine GP flexibility with NN scalability\n\nAttention Mechanisms: Self-attention for irregular time series\n\nMeta-Learning: Learn to learn from few examples\n\nWhy NNs? Scale to large datasets, learn complex kernels, handle irregular sampling","type":"content","url":"/final-project-guide#extending-project-6-gaussian-processes","position":25},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl2":"Technical Requirements"},"type":"lvl2","url":"/final-project-guide#technical-requirements","position":26},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl2":"Technical Requirements"},"content":"","type":"content","url":"/final-project-guide#technical-requirements","position":27},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"Core Implementation Requirements","lvl2":"Technical Requirements"},"type":"lvl3","url":"/final-project-guide#core-implementation-requirements","position":28},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"Core Implementation Requirements","lvl2":"Technical Requirements"},"content":"JAX Refactoring\n\nConvert core functions to JAX\n\nUse jit compilation where appropriate\n\nImplement vectorized operations\n\nDemonstrate speedup over NumPy version\n\nNeural Network From Scratchclass NeuralNetwork:\n    def __init__(self, layers):\n        self.weights = self.initialize_weights(layers)\n    \n    def forward(self, x):\n        # Implement forward pass\n        \n    def backward(self, x, y, learning_rate):\n        # Implement backpropagation\n        \n    def train(self, X, y, epochs):\n        # Training loop\n\nJAX Ecosystem Implementation\n\nUse Equinox or Flax for model definition\n\nOptax for optimization\n\nProper train/validation/test splits\n\nImplement early stopping and regularization","type":"content","url":"/final-project-guide#core-implementation-requirements","position":29},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"Scientific Requirements","lvl2":"Technical Requirements"},"type":"lvl3","url":"/final-project-guide#scientific-requirements","position":30},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"Scientific Requirements","lvl2":"Technical Requirements"},"content":"Hypothesis: Clear statement of what you’re investigating\n\nBaseline: Compare to non-NN approach from original project\n\nValidation: Demonstrate correctness on known solutions\n\nAnalysis: What did the network learn? Interpretability attempts\n\nLimitations: Where does your approach fail?","type":"content","url":"/final-project-guide#scientific-requirements","position":31},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"Code Structure","lvl2":"Technical Requirements"},"type":"lvl3","url":"/final-project-guide#code-structure","position":32},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"Code Structure","lvl2":"Technical Requirements"},"content":"final_project/\n├── src/\n│   ├── __init__.py\n│   ├── jax_refactor/       # JAX version of original project\n│   │   ├── physics.py\n│   │   └── numerics.py\n│   ├── nn_from_scratch/    # Manual implementation\n│   │   ├── network.py\n│   │   ├── layers.py\n│   │   └── optimizers.py\n│   ├── nn_production/      # Equinox/Flax implementation\n│   │   ├── models.py\n│   │   ├── training.py\n│   │   └── evaluation.py\n│   └── analysis/           # Results analysis\n│       ├── visualize.py\n│       └── interpret.py\n├── data/\n│   ├── raw/\n│   ├── processed/\n│   └── results/\n├── notebooks/              # Exploration only (not submission)\n├── outputs/\n│   ├── figures/\n│   ├── models/            # Saved model checkpoints\n│   └── metrics/           # Training histories\n├── tests/\n├── README.md\n├── requirements.txt\n├── proposal.pdf\n├── progress_report.pdf\n└── final_report.pdf","type":"content","url":"/final-project-guide#code-structure","position":33},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl2":"Deliverables"},"type":"lvl2","url":"/final-project-guide#deliverables","position":34},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl2":"Deliverables"},"content":"","type":"content","url":"/final-project-guide#deliverables","position":35},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"1. Project Proposal (Nov 21) - 2 pages","lvl2":"Deliverables"},"type":"lvl3","url":"/final-project-guide#id-1-project-proposal-nov-21-2-pages","position":36},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"1. Project Proposal (Nov 21) - 2 pages","lvl2":"Deliverables"},"content":"Format: PDF submitted to Canvas\n\nRequired Sections:\n\nScientific Question (0.5 page)\n\nWhat new question will you address?\n\nWhy can’t classical methods solve this?\n\nMethodology (0.75 page)\n\nWhich previous project are you extending?\n\nWhat NN architecture will you use?\n\nHow will you validate results?\n\nTimeline (0.25 page)\n\nWeek-by-week plan\n\nSuccess Metrics (0.5 page)\n\nHow will you know if it worked?\n\nWhat’s your baseline comparison?","type":"content","url":"/final-project-guide#id-1-project-proposal-nov-21-2-pages","position":37},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"2. Progress Report (Dec 5) - 1 page + figures","lvl2":"Deliverables"},"type":"lvl3","url":"/final-project-guide#id-2-progress-report-dec-5-1-page-figures","position":38},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"2. Progress Report (Dec 5) - 1 page + figures","lvl2":"Deliverables"},"content":"Required Elements:\n\nJAX refactoring complete (show timing comparisons)\n\nNN from scratch implemented (show it learns something)\n\nPreliminary results from JAX ecosystem version\n\nAny blocking issues identified","type":"content","url":"/final-project-guide#id-2-progress-report-dec-5-1-page-figures","position":39},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"3. Final Report (Dec 18) - 8-12 pages","lvl2":"Deliverables"},"type":"lvl3","url":"/final-project-guide#id-3-final-report-dec-18-8-12-pages","position":40},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"3. Final Report (Dec 18) - 8-12 pages","lvl2":"Deliverables"},"content":"Format: Research paper style (abstract, intro, methods, results, discussion)\n\nSections:\n\nAbstract (200 words)\n\nIntroduction (1-2 pages)\n\nScientific motivation\n\nPrevious work (cite your original project)\n\nWhy neural networks?\n\nMethods (3-4 pages)\n\nJAX refactoring approach\n\nNetwork architecture choices\n\nTraining procedure\n\nValidation strategy\n\nResults (2-3 pages)\n\nPerformance comparisons (classical vs NN)\n\nScientific findings\n\nComputational benchmarks\n\nDiscussion (2-3 pages)\n\nInterpretation of what network learned\n\nLimitations and failure modes\n\nFuture improvements\n\nBroader implications\n\nConclusion (0.5 page)\n\nReferences (not counted in page limit)\n\nFigure Requirements:\n\nMinimum 5 figures\n\nArchitecture diagram\n\nTraining curves\n\nResults comparison\n\nScientific interpretation plots","type":"content","url":"/final-project-guide#id-3-final-report-dec-18-8-12-pages","position":41},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"4. Final Presentation (Dec 17/18) - 10 minutes","lvl2":"Deliverables"},"type":"lvl3","url":"/final-project-guide#id-4-final-presentation-dec-17-18-10-minutes","position":42},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"4. Final Presentation (Dec 17/18) - 10 minutes","lvl2":"Deliverables"},"content":"Structure:\n\n2 min: Problem setup and motivation\n\n3 min: Methods (focus on NN approach)\n\n3 min: Results and comparison\n\n1 min: What the network learned\n\n1 min: Conclusions and future work\n\nSlides: 10-12 slides maximum, emphasize visuals","type":"content","url":"/final-project-guide#id-4-final-presentation-dec-17-18-10-minutes","position":43},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl2":"Grading Rubric (100 points)"},"type":"lvl2","url":"/final-project-guide#grading-rubric-100-points","position":44},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl2":"Grading Rubric (100 points)"},"content":"Component\n\nPoints\n\nCriteria\n\nJAX Refactoring\n\n15\n\nCorrect implementation, performance gains\n\nNN From Scratch\n\n20\n\nWorking implementation, clear understanding\n\nJAX Ecosystem\n\n20\n\nProper use of tools, advanced features\n\nScientific Merit\n\n15\n\nNovel question, appropriate methods\n\nResults & Analysis\n\n15\n\nThorough comparison, interpretation\n\nReport Quality\n\n10\n\nClear writing, good figures, proper citations\n\nPresentation\n\n5\n\nClear, engaging, on time","type":"content","url":"/final-project-guide#grading-rubric-100-points","position":45},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"Detailed Rubric Descriptions","lvl2":"Grading Rubric (100 points)"},"type":"lvl3","url":"/final-project-guide#detailed-rubric-descriptions","position":46},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"Detailed Rubric Descriptions","lvl2":"Grading Rubric (100 points)"},"content":"","type":"content","url":"/final-project-guide#detailed-rubric-descriptions","position":47},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl4":"JAX Refactoring (15 points)","lvl3":"Detailed Rubric Descriptions","lvl2":"Grading Rubric (100 points)"},"type":"lvl4","url":"/final-project-guide#jax-refactoring-15-points","position":48},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl4":"JAX Refactoring (15 points)","lvl3":"Detailed Rubric Descriptions","lvl2":"Grading Rubric (100 points)"},"content":"Excellent (14-15): Full refactor, significant speedup, uses advanced JAX features\n\nGood (11-13): Most code refactored, some speedup, basic JAX usage\n\nSatisfactory (8-10): Partial refactor, works but minimal optimization\n\nNeeds Improvement (0-7): Minimal refactoring or doesn’t work","type":"content","url":"/final-project-guide#jax-refactoring-15-points","position":49},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl4":"NN From Scratch (20 points)","lvl3":"Detailed Rubric Descriptions","lvl2":"Grading Rubric (100 points)"},"type":"lvl4","url":"/final-project-guide#nn-from-scratch-20-points","position":50},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl4":"NN From Scratch (20 points)","lvl3":"Detailed Rubric Descriptions","lvl2":"Grading Rubric (100 points)"},"content":"Excellent (18-20): Full backprop, multiple layers, advanced features (dropout, batch norm)\n\nGood (14-17): Working backprop, 2+ layers, trains successfully\n\nSatisfactory (10-13): Basic working network, may have limitations\n\nNeeds Improvement (0-9): Doesn’t train or major implementation errors","type":"content","url":"/final-project-guide#nn-from-scratch-20-points","position":51},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl4":"JAX Ecosystem (20 points)","lvl3":"Detailed Rubric Descriptions","lvl2":"Grading Rubric (100 points)"},"type":"lvl4","url":"/final-project-guide#jax-ecosystem-20-points","position":52},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl4":"JAX Ecosystem (20 points)","lvl3":"Detailed Rubric Descriptions","lvl2":"Grading Rubric (100 points)"},"content":"Excellent (18-20): Advanced architectures, proper training pipeline, uses multiple libraries\n\nGood (14-17): Standard implementation, works well, uses Equinox/Flax properly\n\nSatisfactory (10-13): Basic implementation, works but not optimized\n\nNeeds Improvement (0-9): Minimal use of ecosystem or doesn’t work","type":"content","url":"/final-project-guide#jax-ecosystem-20-points","position":53},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl2":"Scientific Merit (15 points)"},"type":"lvl2","url":"/final-project-guide#scientific-merit-15-points","position":54},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl2":"Scientific Merit (15 points)"},"content":"Excellent (14-15): Novel question, clear hypothesis, appropriate for NNs\n\nGood (11-13): Good question, reasonable approach\n\nSatisfactory (8-10): Adequate question but could use classical methods\n\nNeeds Improvement (0-7): Unclear question or inappropriate methods","type":"content","url":"/final-project-guide#scientific-merit-15-points","position":55},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl2":"Tips for Success"},"type":"lvl2","url":"/final-project-guide#tips-for-success","position":56},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl2":"Tips for Success"},"content":"","type":"content","url":"/final-project-guide#tips-for-success","position":57},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"Choosing Your Project","lvl2":"Tips for Success"},"type":"lvl3","url":"/final-project-guide#choosing-your-project","position":58},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"Choosing Your Project","lvl2":"Tips for Success"},"content":"","type":"content","url":"/final-project-guide#choosing-your-project","position":59},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl4":"Good Final Projects:","lvl3":"Choosing Your Project","lvl2":"Tips for Success"},"type":"lvl4","url":"/final-project-guide#good-final-projects","position":60},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl4":"Good Final Projects:","lvl3":"Choosing Your Project","lvl2":"Tips for Success"},"content":"Address a clear scientific question\n\nShow when/why NNs outperform classical methods\n\nBuild naturally on your previous work\n\nAre ambitious but achievable in 3 weeks","type":"content","url":"/final-project-guide#good-final-projects","position":61},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl4":"Avoid:","lvl3":"Choosing Your Project","lvl2":"Tips for Success"},"type":"lvl4","url":"/final-project-guide#avoid","position":62},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl4":"Avoid:","lvl3":"Choosing Your Project","lvl2":"Tips for Success"},"content":"Applying NNs just because you can\n\nProblems with analytical solutions\n\nDatasets too small for NNs to be beneficial\n\nOverly complex architectures you don’t understand","type":"content","url":"/final-project-guide#avoid","position":63},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"Time Management","lvl2":"Tips for Success"},"type":"lvl3","url":"/final-project-guide#time-management","position":64},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"Time Management","lvl2":"Tips for Success"},"content":"","type":"content","url":"/final-project-guide#time-management","position":65},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl4":"Week 1 Focus:","lvl3":"Time Management","lvl2":"Tips for Success"},"type":"lvl4","url":"/final-project-guide#week-1-focus","position":66},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl4":"Week 1 Focus:","lvl3":"Time Management","lvl2":"Tips for Success"},"content":"Days 1-2: Project selection and proposal writing\n\nDays 3-4: JAX refactoring\n\nDays 5-7: NN from scratch implementation","type":"content","url":"/final-project-guide#week-1-focus","position":67},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl4":"Week 2 Focus:","lvl3":"Time Management","lvl2":"Tips for Success"},"type":"lvl4","url":"/final-project-guide#week-2-focus","position":68},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl4":"Week 2 Focus:","lvl3":"Time Management","lvl2":"Tips for Success"},"content":"Days 8-10: JAX ecosystem implementation\n\nDays 11-12: Training and hyperparameter tuning\n\nDays 13-14: Progress report and debugging","type":"content","url":"/final-project-guide#week-2-focus","position":69},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl4":"Week 3 Focus:","lvl3":"Time Management","lvl2":"Tips for Success"},"type":"lvl4","url":"/final-project-guide#week-3-focus","position":70},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl4":"Week 3 Focus:","lvl3":"Time Management","lvl2":"Tips for Success"},"content":"Days 15-17: Final experiments and analysis\n\nDays 18-19: Report writing\n\nDays 20-21: Presentation preparation","type":"content","url":"/final-project-guide#week-3-focus","position":71},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"Common Pitfalls","lvl2":"Tips for Success"},"type":"lvl3","url":"/final-project-guide#common-pitfalls","position":72},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"Common Pitfalls","lvl2":"Tips for Success"},"content":"Overcomplicating: Start simple, add complexity if time permits\n\nPoor Baselines: Always compare to your classical implementation\n\nNo Validation: Must demonstrate correctness on known cases\n\nBlack Box Syndrome: Understand what your network is doing\n\nLast-Minute JAX: Start refactoring early, JAX has a learning curve","type":"content","url":"/final-project-guide#common-pitfalls","position":73},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"Resources","lvl2":"Tips for Success"},"type":"lvl3","url":"/final-project-guide#resources","position":74},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"Resources","lvl2":"Tips for Success"},"content":"","type":"content","url":"/final-project-guide#resources","position":75},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl4":"JAX Tutorials:","lvl3":"Resources","lvl2":"Tips for Success"},"type":"lvl4","url":"/final-project-guide#jax-tutorials","position":76},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl4":"JAX Tutorials:","lvl3":"Resources","lvl2":"Tips for Success"},"content":"Official JAX Documentation\n\nJAX 101 Tutorial\n\nThinking in JAX","type":"content","url":"/final-project-guide#jax-tutorials","position":77},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl4":"Neural Network Theory:","lvl3":"Resources","lvl2":"Tips for Success"},"type":"lvl4","url":"/final-project-guide#neural-network-theory","position":78},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl4":"Neural Network Theory:","lvl3":"Resources","lvl2":"Tips for Success"},"content":"Deep Learning book (Goodfellow et al.) - free online\n\nNeural Networks and Deep Learning (Michael Nielsen) - free online\n\nFast.ai course materials","type":"content","url":"/final-project-guide#neural-network-theory","position":79},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl4":"Astrophysics ML Papers:","lvl3":"Resources","lvl2":"Tips for Success"},"type":"lvl4","url":"/final-project-guide#astrophysics-ml-papers","position":80},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl4":"Astrophysics ML Papers:","lvl3":"Resources","lvl2":"Tips for Success"},"content":"“Machine Learning in Astronomy” (Baron 2019)\n\n“Deep Learning for Observational Cosmology” (Ntampaka+ 2019)\n\nRecent papers using NNs in your subfield","type":"content","url":"/final-project-guide#astrophysics-ml-papers","position":81},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl2":"Getting Help"},"type":"lvl2","url":"/final-project-guide#getting-help","position":82},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl2":"Getting Help"},"content":"","type":"content","url":"/final-project-guide#getting-help","position":83},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"Technical Support:","lvl2":"Getting Help"},"type":"lvl3","url":"/final-project-guide#technical-support","position":84},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"Technical Support:","lvl2":"Getting Help"},"content":"JAX issues: Check documentation first, then ask on Slack\n\nNN architecture questions: Discuss in Friday sessions\n\nDebugging: Use AI tutors for concept help, not code generation","type":"content","url":"/final-project-guide#technical-support","position":85},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"Scientific Guidance:","lvl2":"Getting Help"},"type":"lvl3","url":"/final-project-guide#scientific-guidance","position":86},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl3":"Scientific Guidance:","lvl2":"Getting Help"},"content":"Unsure if your question is appropriate? Discuss in office hours\n\nNeed literature references? Ask on Slack\n\nWant feedback on approach? Submit optional draft to instructor\n\nRemember: This project should showcase your growth as a computational astrophysicist. It’s not about building the most complex network—it’s about demonstrating understanding of when, why, and how neural networks can advance astrophysical research.","type":"content","url":"/final-project-guide#scientific-guidance","position":87},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl2":"Final Thoughts"},"type":"lvl2","url":"/final-project-guide#final-thoughts","position":88},{"hierarchy":{"lvl1":"ASTR 596: Final Project Guide","lvl2":"Final Thoughts"},"content":"This project is your opportunity to:\n\nDemonstrate mastery of course concepts\n\nExplore a scientific question you’re passionate about\n\nBuild something you can show future advisors/employers\n\nPush yourself beyond your comfort zone\n\nEmbrace the challenge, ask for help when needed, and create something you’re proud of!","type":"content","url":"/final-project-guide#final-thoughts","position":89},{"hierarchy":{"lvl1":"Final Project: Neural Networks & JAX"},"type":"lvl1","url":"/index-13","position":0},{"hierarchy":{"lvl1":"Final Project: Neural Networks & JAX"},"content":"The culminating project for ASTR 596 where you implement neural networks from scratch and then translate to the JAX ecosystem.","type":"content","url":"/index-13","position":1},{"hierarchy":{"lvl1":"Final Project: Neural Networks & JAX","lvl2":"Project Overview"},"type":"lvl2","url":"/index-13#project-overview","position":2},{"hierarchy":{"lvl1":"Final Project: Neural Networks & JAX","lvl2":"Project Overview"},"content":"This capstone project synthesizes everything you’ve learned throughout the course:\n\nNeural Network Implementation: Build backpropagation from first principles\n\nJAX Translation: Convert your NumPy implementation to JAX\n\nAstrophysical Application: Apply your networks to real astronomical data\n\nResearch-Quality Output: Publication-ready analysis and documentation","type":"content","url":"/index-13#project-overview","position":3},{"hierarchy":{"lvl1":"Final Project: Neural Networks & JAX","lvl2":"Learning Objectives"},"type":"lvl2","url":"/index-13#learning-objectives","position":4},{"hierarchy":{"lvl1":"Final Project: Neural Networks & JAX","lvl2":"Learning Objectives"},"content":"By completing this project, you will:\n\nUnderstand neural network fundamentals at the mathematical level\n\nImplement automatic differentiation manually before using JAX\n\nMaster the JAX ecosystem for high-performance computing\n\nApply ML to astrophysics with proper scientific methodology\n\nProduce research-quality work suitable for academic or industry contexts","type":"content","url":"/index-13#learning-objectives","position":5},{"hierarchy":{"lvl1":"Final Project: Neural Networks & JAX","lvl2":"Project Components"},"type":"lvl2","url":"/index-13#project-components","position":6},{"hierarchy":{"lvl1":"Final Project: Neural Networks & JAX","lvl2":"Project Components"},"content":"","type":"content","url":"/index-13#project-components","position":7},{"hierarchy":{"lvl1":"Final Project: Neural Networks & JAX","lvl3":"Phase 1: From Scratch Implementation (Weeks 13-14)","lvl2":"Project Components"},"type":"lvl3","url":"/index-13#phase-1-from-scratch-implementation-weeks-13-14","position":8},{"hierarchy":{"lvl1":"Final Project: Neural Networks & JAX","lvl3":"Phase 1: From Scratch Implementation (Weeks 13-14)","lvl2":"Project Components"},"content":"Manual backpropagation in pure NumPy\n\nCustom automatic differentiation\n\nTraining on astronomical datasets","type":"content","url":"/index-13#phase-1-from-scratch-implementation-weeks-13-14","position":9},{"hierarchy":{"lvl1":"Final Project: Neural Networks & JAX","lvl3":"Phase 2: JAX Translation (Week 15)","lvl2":"Project Components"},"type":"lvl3","url":"/index-13#phase-2-jax-translation-week-15","position":10},{"hierarchy":{"lvl1":"Final Project: Neural Networks & JAX","lvl3":"Phase 2: JAX Translation (Week 15)","lvl2":"Project Components"},"content":"Convert NumPy code to JAX\n\nLeverage automatic differentiation\n\nOptimize for performance","type":"content","url":"/index-13#phase-2-jax-translation-week-15","position":11},{"hierarchy":{"lvl1":"Final Project: Neural Networks & JAX","lvl3":"Phase 3: Research Application (Finals Week)","lvl2":"Project Components"},"type":"lvl3","url":"/index-13#phase-3-research-application-finals-week","position":12},{"hierarchy":{"lvl1":"Final Project: Neural Networks & JAX","lvl3":"Phase 3: Research Application (Finals Week)","lvl2":"Project Components"},"content":"Apply to cutting-edge astrophysical problems\n\nGenerate publication-quality figures\n\nWrite formal research report","type":"content","url":"/index-13#phase-3-research-application-finals-week","position":13},{"hierarchy":{"lvl1":"Final Project: Neural Networks & JAX","lvl2":"Resources"},"type":"lvl2","url":"/index-13#resources","position":14},{"hierarchy":{"lvl1":"Final Project: Neural Networks & JAX","lvl2":"Resources"},"content":"Project Guide - Detailed requirements and rubric\n\nJAX Documentation - Official JAX guide\n\nFlax Examples - Neural network implementations","type":"content","url":"/index-13#resources","position":15},{"hierarchy":{"lvl1":"Final Project: Neural Networks & JAX","lvl2":"Timeline"},"type":"lvl2","url":"/index-13#timeline","position":16},{"hierarchy":{"lvl1":"Final Project: Neural Networks & JAX","lvl2":"Timeline"},"content":"Week\n\nMilestone\n\nDeliverable\n\n13\n\nImplement forward pass\n\nWorking neural network\n\n14\n\nAdd backpropagation\n\nTraining pipeline\n\n15\n\nJAX translation\n\nHigh-performance version\n\nFinals\n\nResearch application\n\nFinal report & presentation\n\nThis project represents the culmination of your journey from Python fundamentals to cutting-edge computational astrophysics. You’ll emerge with both deep understanding and practical skills for modern research careers.","type":"content","url":"/index-13#timeline","position":17},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)"},"type":"lvl1","url":"/cli-advanced-guide","position":0},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)"},"content":"","type":"content","url":"/cli-advanced-guide","position":1},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl2":"When You’ll Need This"},"type":"lvl2","url":"/cli-advanced-guide#when-youll-need-this","position":2},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl2":"When You’ll Need This"},"content":"This guide covers advanced topics that aren’t required for ASTR 596 but will be invaluable for:\n\nResearch computing on HPC clusters (like SDSU’s Verne)\n\nRemote work on servers or cloud computing\n\nAutomation of complex data processing pipelines\n\nProfessional development as a computational scientist\n\nFeel free to reference this as needed - you don’t need to master everything at once!","type":"content","url":"/cli-advanced-guide#when-youll-need-this","position":3},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl2":"Text Processing with awk and sed"},"type":"lvl2","url":"/cli-advanced-guide#text-processing-with-awk-and-sed","position":4},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl2":"Text Processing with awk and sed"},"content":"","type":"content","url":"/cli-advanced-guide#text-processing-with-awk-and-sed","position":5},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"awk - Pattern Processing","lvl2":"Text Processing with awk and sed"},"type":"lvl3","url":"/cli-advanced-guide#awk-pattern-processing","position":6},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"awk - Pattern Processing","lvl2":"Text Processing with awk and sed"},"content":"# Extract specific columns from data\nawk '{print $3}' data.txt              # Print 3rd column\nawk '{print $1, $3}' data.txt          # Print columns 1 and 3\nawk '{print $NF}' data.txt             # Print last column\n\n# Conditional processing\nawk '$3 > 100' data.txt                # Print lines where column 3 > 100\nawk '$1 == \"STAR\"' catalog.txt         # Lines where first column is \"STAR\"\n\n# Calculate sum/average\nawk '{sum+=$2} END {print sum}' data.txt           # Sum column 2\nawk '{sum+=$2; n++} END {print sum/n}' data.txt    # Average of column 2\n\n# Field separator\nawk -F',' '{print $2}' data.csv        # Use comma as separator (CSV)\n\nAstronomy example: Processing catalog data# Extract RA, Dec, and magnitude from catalog\nawk '$5 < 15 {print $2, $3, $5}' star_catalog.txt > bright_stars.txt","type":"content","url":"/cli-advanced-guide#awk-pattern-processing","position":7},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"sed - Stream Editor","lvl2":"Text Processing with awk and sed"},"type":"lvl3","url":"/cli-advanced-guide#sed-stream-editor","position":8},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"sed - Stream Editor","lvl2":"Text Processing with awk and sed"},"content":"# Find and replace\nsed 's/old/new/' file.txt              # Replace first occurrence per line\nsed 's/old/new/g' file.txt             # Replace all occurrences\nsed -i 's/old/new/g' file.txt          # Edit file in place\n\n# Delete lines\nsed '1d' file.txt                      # Delete first line\nsed '$d' file.txt                      # Delete last line\nsed '/pattern/d' file.txt              # Delete lines containing pattern\n\n# Insert/append text\nsed '1i\\Header line' file.txt          # Insert at beginning\nsed '$a\\Footer line' file.txt          # Append at end\n\nExample: Cleaning data files# Remove comment lines and blank lines from data\nsed '/^#/d; /^$/d' raw_data.txt > clean_data.txt","type":"content","url":"/cli-advanced-guide#sed-stream-editor","position":9},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl2":"Advanced find with exec"},"type":"lvl2","url":"/cli-advanced-guide#advanced-find-with-exec","position":10},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl2":"Advanced find with exec"},"content":"","type":"content","url":"/cli-advanced-guide#advanced-find-with-exec","position":11},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"Executing Commands on Found Files","lvl2":"Advanced find with exec"},"type":"lvl3","url":"/cli-advanced-guide#executing-commands-on-found-files","position":12},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"Executing Commands on Found Files","lvl2":"Advanced find with exec"},"content":"# Find and perform action\nfind . -name \"*.pyc\" -exec rm {} \\;    # Delete all .pyc files\nfind . -name \"*.py\" -exec wc -l {} \\;  # Count lines in Python files\n\n# Find and copy\nfind . -name \"*.fits\" -exec cp {} /backup/ \\;\n\n# Find with multiple conditions\nfind . -name \"*.txt\" -size +1M -mtime -7   # .txt files > 1MB modified in last week\n\n# Find and rename\nfind . -name \"*.dat\" -exec bash -c 'mv \"$0\" \"${0%.dat}.txt\"' {} \\;\n\nAstronomy example: Processing observation files# Find all FITS files and create thumbnails\nfind . -name \"*.fits\" -exec python make_thumbnail.py {} \\;\n\n# Archive old observations\nfind ./observations -name \"*.fits\" -mtime +365 -exec gzip {} \\;","type":"content","url":"/cli-advanced-guide#executing-commands-on-found-files","position":13},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl2":"SSH and Remote Computing"},"type":"lvl2","url":"/cli-advanced-guide#ssh-and-remote-computing","position":14},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl2":"SSH and Remote Computing"},"content":"","type":"content","url":"/cli-advanced-guide#ssh-and-remote-computing","position":15},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"Basic SSH Connection","lvl2":"SSH and Remote Computing"},"type":"lvl3","url":"/cli-advanced-guide#basic-ssh-connection","position":16},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"Basic SSH Connection","lvl2":"SSH and Remote Computing"},"content":"# Connect to remote server\nssh username@server.sdsu.edu\n\n# Connect with specific port\nssh -p 2222 username@server.edu\n\n# Connect with verbose output (debugging)\nssh -v username@server.edu","type":"content","url":"/cli-advanced-guide#basic-ssh-connection","position":17},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"SSH Key Authentication (No More Passwords!)","lvl2":"SSH and Remote Computing"},"type":"lvl3","url":"/cli-advanced-guide#ssh-key-authentication-no-more-passwords","position":18},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"SSH Key Authentication (No More Passwords!)","lvl2":"SSH and Remote Computing"},"content":"# Generate SSH key pair\nssh-keygen -t ed25519 -C \"your_email@sdsu.edu\"\n\n# Copy public key to server\nssh-copy-id username@server.edu\n\n# Now connect without password\nssh username@server.edu","type":"content","url":"/cli-advanced-guide#ssh-key-authentication-no-more-passwords","position":19},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"File Transfer with SCP and rsync","lvl2":"SSH and Remote Computing"},"type":"lvl3","url":"/cli-advanced-guide#file-transfer-with-scp-and-rsync","position":20},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"File Transfer with SCP and rsync","lvl2":"SSH and Remote Computing"},"content":"# Copy file to remote\nscp local_file.py username@server:~/remote_dir/\n\n# Copy from remote\nscp username@server:~/results.txt ./\n\n# Copy entire directory\nscp -r project/ username@server:~/projects/\n\n# rsync (better for large transfers, resumable)\nrsync -avz local_dir/ username@server:~/remote_dir/\nrsync -avz --progress large_file.fits username@server:~/","type":"content","url":"/cli-advanced-guide#file-transfer-with-scp-and-rsync","position":21},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"SSH Config File","lvl2":"SSH and Remote Computing"},"type":"lvl3","url":"/cli-advanced-guide#ssh-config-file","position":22},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"SSH Config File","lvl2":"SSH and Remote Computing"},"content":"Create ~/.ssh/config:Host verne\n    HostName verne.sdsu.edu\n    User your_username\n    Port 22\n\nHost compute\n    HostName compute.cluster.edu\n    User astro_user\n    IdentityFile ~/.ssh/id_ed25519\n\nNow simply: ssh verne or ssh compute","type":"content","url":"/cli-advanced-guide#ssh-config-file","position":23},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl2":"Screen and tmux for Persistent Sessions"},"type":"lvl2","url":"/cli-advanced-guide#screen-and-tmux-for-persistent-sessions","position":24},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl2":"Screen and tmux for Persistent Sessions"},"content":"","type":"content","url":"/cli-advanced-guide#screen-and-tmux-for-persistent-sessions","position":25},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"Why Use Screen/tmux?","lvl2":"Screen and tmux for Persistent Sessions"},"type":"lvl3","url":"/cli-advanced-guide#why-use-screen-tmux","position":26},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"Why Use Screen/tmux?","lvl2":"Screen and tmux for Persistent Sessions"},"content":"Run long simulations that continue after you disconnect\n\nMultiple terminal windows in one SSH session\n\nResume work exactly where you left off","type":"content","url":"/cli-advanced-guide#why-use-screen-tmux","position":27},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"Screen Basics","lvl2":"Screen and tmux for Persistent Sessions"},"type":"lvl3","url":"/cli-advanced-guide#screen-basics","position":28},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"Screen Basics","lvl2":"Screen and tmux for Persistent Sessions"},"content":"# Start new screen session\nscreen -S simulation\n\n# Detach from screen (leaves it running)\nCtrl+A then D\n\n# List active screens\nscreen -ls\n\n# Reattach to screen\nscreen -r simulation\n\n# Kill a screen session\nscreen -X -S simulation quit\n\n# Commands within screen\nCtrl+A then C    # Create new window\nCtrl+A then N    # Next window\nCtrl+A then P    # Previous window\nCtrl+A then K    # Kill current window","type":"content","url":"/cli-advanced-guide#screen-basics","position":29},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"tmux Basics (More Modern Alternative)","lvl2":"Screen and tmux for Persistent Sessions"},"type":"lvl3","url":"/cli-advanced-guide#tmux-basics-more-modern-alternative","position":30},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"tmux Basics (More Modern Alternative)","lvl2":"Screen and tmux for Persistent Sessions"},"content":"# Start new tmux session\ntmux new -s simulation\n\n# Detach from tmux\nCtrl+B then D\n\n# List sessions\ntmux ls\n\n# Reattach to tmux\ntmux attach -t simulation\n\n# Commands within tmux\nCtrl+B then C    # Create new window\nCtrl+B then N    # Next window\nCtrl+B then %    # Split vertically\nCtrl+B then \"    # Split horizontally\n\nExample: Running overnight simulationssh verne\nscreen -S nbody_run\npython long_simulation.py --particles=1000000\n# Ctrl+A then D to detach\n# Log out, go home\n# Next day:\nssh verne\nscreen -r nbody_run  # Simulation still running!","type":"content","url":"/cli-advanced-guide#tmux-basics-more-modern-alternative","position":31},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl2":"Shell Scripting"},"type":"lvl2","url":"/cli-advanced-guide#shell-scripting","position":32},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl2":"Shell Scripting"},"content":"","type":"content","url":"/cli-advanced-guide#shell-scripting","position":33},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"Basic Script Structure","lvl2":"Shell Scripting"},"type":"lvl3","url":"/cli-advanced-guide#basic-script-structure","position":34},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"Basic Script Structure","lvl2":"Shell Scripting"},"content":"#!/bin/bash\n# This is a comment\n\n# Variables\nNAME=\"simulation\"\nN_PARTICLES=1000\nOUTPUT_DIR=\"./results\"\n\n# Create output directory\nmkdir -p $OUTPUT_DIR\n\n# Loop\nfor i in {1..10}; do\n    echo \"Running iteration $i\"\n    python simulate.py --n=$N_PARTICLES --seed=$i > $OUTPUT_DIR/run_$i.txt\ndone\n\n# Conditional\nif [ -f \"$OUTPUT_DIR/run_1.txt\" ]; then\n    echo \"First run completed successfully\"\nelse\n    echo \"Error: First run failed\"\nfi","type":"content","url":"/cli-advanced-guide#basic-script-structure","position":35},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"Making Scripts Executable","lvl2":"Shell Scripting"},"type":"lvl3","url":"/cli-advanced-guide#making-scripts-executable","position":36},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"Making Scripts Executable","lvl2":"Shell Scripting"},"content":"chmod +x script.sh      # Make executable\n./script.sh            # Run script","type":"content","url":"/cli-advanced-guide#making-scripts-executable","position":37},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"Useful Script Patterns","lvl2":"Shell Scripting"},"type":"lvl3","url":"/cli-advanced-guide#useful-script-patterns","position":38},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"Useful Script Patterns","lvl2":"Shell Scripting"},"content":"# Process command line arguments\n#!/bin/bash\nif [ $# -eq 0 ]; then\n    echo \"Usage: $0 <input_file>\"\n    exit 1\nfi\n\nINPUT_FILE=$1\necho \"Processing $INPUT_FILE\"\n\n# Check if file exists\nif [ ! -f \"$INPUT_FILE\" ]; then\n    echo \"Error: File not found\"\n    exit 1\nfi\n\n# Array of values\nMASSES=(0.5 1.0 2.0 5.0 10.0)\nfor mass in \"${MASSES[@]}\"; do\n    python stellar_evolution.py --mass=$mass\ndone\n\n# Read configuration file\nsource config.sh\n\n# Parallel execution\nfor file in *.dat; do\n    python process.py \"$file\" &\ndone\nwait  # Wait for all background jobs to finish","type":"content","url":"/cli-advanced-guide#useful-script-patterns","position":39},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl2":"Performance Monitoring"},"type":"lvl2","url":"/cli-advanced-guide#performance-monitoring","position":40},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl2":"Performance Monitoring"},"content":"","type":"content","url":"/cli-advanced-guide#performance-monitoring","position":41},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"System Resources","lvl2":"Performance Monitoring"},"type":"lvl3","url":"/cli-advanced-guide#system-resources","position":42},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"System Resources","lvl2":"Performance Monitoring"},"content":"# CPU and memory usage\ntop                     # Interactive process viewer\nhtop                    # Better top (if installed)\n\n# Memory information\nfree -h                 # Memory usage summary\ncat /proc/meminfo      # Detailed memory info\n\n# Disk usage\ndf -h                   # Filesystem usage\ndu -sh *               # Size of each item in current directory\ndu -sh * | sort -hr    # Sorted by size\n\n# Network\nnetstat -tuln          # Open network connections\nss -tuln               # Modern replacement for netstat","type":"content","url":"/cli-advanced-guide#system-resources","position":43},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"Process Information","lvl2":"Performance Monitoring"},"type":"lvl3","url":"/cli-advanced-guide#process-information","position":44},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"Process Information","lvl2":"Performance Monitoring"},"content":"# Your processes\nps aux | grep $USER\n\n# CPU usage by process\nps aux --sort=-%cpu | head\n\n# Memory usage by process\nps aux --sort=-%mem | head\n\n# Process tree\npstree -p\n\n# Kill processes\nkillall python         # Kill all Python processes\nkill -9 PID           # Force kill specific process","type":"content","url":"/cli-advanced-guide#process-information","position":45},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"Monitoring Files and I/O","lvl2":"Performance Monitoring"},"type":"lvl3","url":"/cli-advanced-guide#monitoring-files-and-i-o","position":46},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"Monitoring Files and I/O","lvl2":"Performance Monitoring"},"content":"# Watch file size grow\nwatch -n 1 'ls -lh output.dat'\n\n# Monitor open files\nlsof | grep python\n\n# I/O statistics\niotop                  # Requires sudo\n\n# File system activity\nwatch -n 1 'df -h'","type":"content","url":"/cli-advanced-guide#monitoring-files-and-i-o","position":47},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl2":"Job Management on HPC Clusters"},"type":"lvl2","url":"/cli-advanced-guide#job-management-on-hpc-clusters","position":48},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl2":"Job Management on HPC Clusters"},"content":"","type":"content","url":"/cli-advanced-guide#job-management-on-hpc-clusters","position":49},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"SLURM Basics (Common on HPC)","lvl2":"Job Management on HPC Clusters"},"type":"lvl3","url":"/cli-advanced-guide#slurm-basics-common-on-hpc","position":50},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"SLURM Basics (Common on HPC)","lvl2":"Job Management on HPC Clusters"},"content":"# Submit job\nsbatch job_script.sh\n\n# Check job status\nsqueue -u $USER\n\n# Cancel job\nscancel JOB_ID\n\n# Interactive session\nsrun --pty bash","type":"content","url":"/cli-advanced-guide#slurm-basics-common-on-hpc","position":51},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"Example SLURM Script","lvl2":"Job Management on HPC Clusters"},"type":"lvl3","url":"/cli-advanced-guide#example-slurm-script","position":52},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"Example SLURM Script","lvl2":"Job Management on HPC Clusters"},"content":"#!/bin/bash\n#SBATCH --job-name=nbody_sim\n#SBATCH --output=output_%j.txt\n#SBATCH --error=error_%j.txt\n#SBATCH --time=24:00:00\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=32G\n\nmodule load python/3.11\nconda activate astr596\n\npython nbody_simulation.py --n=1000000","type":"content","url":"/cli-advanced-guide#example-slurm-script","position":53},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl2":"Advanced Data Manipulation"},"type":"lvl2","url":"/cli-advanced-guide#advanced-data-manipulation","position":54},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl2":"Advanced Data Manipulation"},"content":"","type":"content","url":"/cli-advanced-guide#advanced-data-manipulation","position":55},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"Combining Multiple Files","lvl2":"Advanced Data Manipulation"},"type":"lvl3","url":"/cli-advanced-guide#combining-multiple-files","position":56},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"Combining Multiple Files","lvl2":"Advanced Data Manipulation"},"content":"# Concatenate files\ncat file1.txt file2.txt > combined.txt\n\n# Merge sorted files\nsort file1.txt > sorted1.txt\nsort file2.txt > sorted2.txt\ncomm -12 sorted1.txt sorted2.txt  # Common lines\n\n# Join files on common column\njoin -t',' -1 1 -2 1 file1.csv file2.csv","type":"content","url":"/cli-advanced-guide#combining-multiple-files","position":57},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"Data Extraction and Formatting","lvl2":"Advanced Data Manipulation"},"type":"lvl3","url":"/cli-advanced-guide#data-extraction-and-formatting","position":58},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"Data Extraction and Formatting","lvl2":"Advanced Data Manipulation"},"content":"# Extract columns from CSV\ncut -d',' -f2,4 data.csv\n\n# Transpose rows and columns\nawk '{ for (i=1; i<=NF; i++) a[NR,i] = $i } \n     END { for (i=1; i<=NF; i++) \n           { for (j=1; j<=NR; j++) \n             printf \"%s \", a[j,i]; \n             print \"\" }}' file.txt\n\n# Format numbers\nprintf \"%.2f\\n\" 3.14159","type":"content","url":"/cli-advanced-guide#data-extraction-and-formatting","position":59},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl2":"Environment Customization"},"type":"lvl2","url":"/cli-advanced-guide#environment-customization","position":60},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl2":"Environment Customization"},"content":"","type":"content","url":"/cli-advanced-guide#environment-customization","position":61},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"Bash Configuration (~/.bashrc or ~/.bash_profile)","lvl2":"Environment Customization"},"type":"lvl3","url":"/cli-advanced-guide#bash-configuration-bashrc-or-bash-profile","position":62},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"Bash Configuration (~/.bashrc or ~/.bash_profile)","lvl2":"Environment Customization"},"content":"# Custom prompt\nexport PS1=\"\\u@\\h:\\w\\$ \"\n\n# Add to PATH\nexport PATH=\"$HOME/bin:$PATH\"\n\n# Aliases\nalias ll='ls -lah'\nalias gs='git status'\nalias activate='conda activate astr596'\n\n# Functions\nmkcd() {\n    mkdir -p \"$1\" && cd \"$1\"\n}\n\n# History settings\nexport HISTSIZE=10000\nexport HISTFILESIZE=20000\nexport HISTCONTROL=ignoredups","type":"content","url":"/cli-advanced-guide#bash-configuration-bashrc-or-bash-profile","position":63},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"Useful Environment Variables","lvl2":"Environment Customization"},"type":"lvl3","url":"/cli-advanced-guide#useful-environment-variables","position":64},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"Useful Environment Variables","lvl2":"Environment Customization"},"content":"export EDITOR=vim                      # Default editor\nexport PYTHONPATH=\"$HOME/lib/python\"  # Python module path\nexport OMP_NUM_THREADS=8              # OpenMP threads","type":"content","url":"/cli-advanced-guide#useful-environment-variables","position":65},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl2":"Tips and Tricks"},"type":"lvl2","url":"/cli-advanced-guide#tips-and-tricks","position":66},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl2":"Tips and Tricks"},"content":"","type":"content","url":"/cli-advanced-guide#tips-and-tricks","position":67},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"Command Line Efficiency","lvl2":"Tips and Tricks"},"type":"lvl3","url":"/cli-advanced-guide#command-line-efficiency","position":68},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"Command Line Efficiency","lvl2":"Tips and Tricks"},"content":"# Run previous command with sudo\nsudo !!\n\n# Fix typo in previous command\n^old^new\n\n# Run command ignoring aliases\n\\ls\n\n# Time a command\ntime python script.py\n\n# Run command at specific time\nat 2am tomorrow\npython long_simulation.py\nCtrl+D\n\n# Repeat command every N seconds\nwatch -n 2 'ps aux | grep python'","type":"content","url":"/cli-advanced-guide#command-line-efficiency","position":69},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"File Compression","lvl2":"Tips and Tricks"},"type":"lvl3","url":"/cli-advanced-guide#file-compression","position":70},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl3":"File Compression","lvl2":"Tips and Tricks"},"content":"# Compress\ngzip file.txt              # Creates file.txt.gz\ntar -czf archive.tar.gz directory/   # Compress directory\n\n# Decompress\ngunzip file.txt.gz\ntar -xzf archive.tar.gz\n\n# View compressed files without extracting\nzcat file.txt.gz\nzless file.txt.gz","type":"content","url":"/cli-advanced-guide#file-compression","position":71},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl2":"Quick Reference for Research Computing"},"type":"lvl2","url":"/cli-advanced-guide#quick-reference-for-research-computing","position":72},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl2":"Quick Reference for Research Computing"},"content":"# Remote work\nssh user@server            # Connect\nscp file user@server:~/    # Copy file\nscreen -S name            # Persistent session\n\n# Data processing\nawk '{print $2}' file     # Extract column\nsed 's/old/new/g' file    # Replace text\nfind . -name \"*.py\"       # Find files\n\n# Monitoring\ntop                       # System resources\nps aux | grep python      # Check processes\ndu -sh *                  # Directory sizes\n\n# Automation\nfor i in *.dat; do        # Loop over files\n    python process.py $i\ndone","type":"content","url":"/cli-advanced-guide#quick-reference-for-research-computing","position":73},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl2":"Final Thoughts"},"type":"lvl2","url":"/cli-advanced-guide#final-thoughts","position":74},{"hierarchy":{"lvl1":"Advanced CLI Guide (Optional)","lvl2":"Final Thoughts"},"content":"These advanced topics will become relevant as you:\n\nWork with larger datasets\n\nUse HPC resources\n\nCollaborate on remote servers\n\nAutomate complex workflows\n\nDon’t feel pressured to learn everything at once. Bookmark this guide and return to it when you encounter situations that need these tools. The best way to learn is by solving real problems!\n\nRemember: Even experienced users look up syntax. The skill is knowing what tools exist and when to use them.","type":"content","url":"/cli-advanced-guide#final-thoughts","position":75},{"hierarchy":{"lvl1":"External Resources"},"type":"lvl1","url":"/index-15","position":0},{"hierarchy":{"lvl1":"External Resources"},"content":"Content coming soon!","type":"content","url":"/index-15","position":1},{"hierarchy":{"lvl1":"Reference Materials"},"type":"lvl1","url":"/index-14","position":0},{"hierarchy":{"lvl1":"Reference Materials"},"content":"Comprehensive resources and references for ASTR 596: Modeling the Universe.","type":"content","url":"/index-14","position":1},{"hierarchy":{"lvl1":"Reference Materials","lvl2":"Quick Access"},"type":"lvl2","url":"/index-14#quick-access","position":2},{"hierarchy":{"lvl1":"Reference Materials","lvl2":"Quick Access"},"content":"Quick References - Cheat sheets and syntax guides\n\nExternal Resources - Books, papers, and online materials\n\nTroubleshooting Guide - Common issues and solutions","type":"content","url":"/index-14#quick-access","position":3},{"hierarchy":{"lvl1":"Reference Materials","lvl2":"What You’ll Find Here"},"type":"lvl2","url":"/index-14#what-youll-find-here","position":4},{"hierarchy":{"lvl1":"Reference Materials","lvl2":"What You’ll Find Here"},"content":"","type":"content","url":"/index-14#what-youll-find-here","position":5},{"hierarchy":{"lvl1":"Reference Materials","lvl3":"Quick References","lvl2":"What You’ll Find Here"},"type":"lvl3","url":"/index-14#quick-references","position":6},{"hierarchy":{"lvl1":"Reference Materials","lvl3":"Quick References","lvl2":"What You’ll Find Here"},"content":"Fast lookup guides for Python syntax, mathematical formulas, and computational methods used throughout the course.","type":"content","url":"/index-14#quick-references","position":7},{"hierarchy":{"lvl1":"Reference Materials","lvl3":"External Resources","lvl2":"What You’ll Find Here"},"type":"lvl3","url":"/index-14#external-resources","position":8},{"hierarchy":{"lvl1":"Reference Materials","lvl3":"External Resources","lvl2":"What You’ll Find Here"},"content":"Curated collection of textbooks, research papers, online courses, and documentation that complement the course material.","type":"content","url":"/index-14#external-resources","position":9},{"hierarchy":{"lvl1":"Reference Materials","lvl3":"Troubleshooting","lvl2":"What You’ll Find Here"},"type":"lvl3","url":"/index-14#troubleshooting","position":10},{"hierarchy":{"lvl1":"Reference Materials","lvl3":"Troubleshooting","lvl2":"What You’ll Find Here"},"content":"Solutions to common programming, mathematical, and conceptual issues students encounter during projects.","type":"content","url":"/index-14#troubleshooting","position":11},{"hierarchy":{"lvl1":"Reference Materials","lvl2":"Using These Resources"},"type":"lvl2","url":"/index-14#using-these-resources","position":12},{"hierarchy":{"lvl1":"Reference Materials","lvl2":"Using These Resources"},"content":"These materials are designed to support your learning throughout ASTR 596 and serve as a reference for future research work. Bookmark this section for quick access during projects and assignments.","type":"content","url":"/index-14#using-these-resources","position":13},{"hierarchy":{"lvl1":"Quick References"},"type":"lvl1","url":"/index-16","position":0},{"hierarchy":{"lvl1":"Quick References"},"content":"Content coming soon!","type":"content","url":"/index-16","position":1},{"hierarchy":{"lvl1":"Troubleshooting"},"type":"lvl1","url":"/index-17","position":0},{"hierarchy":{"lvl1":"Troubleshooting"},"content":"Content coming soon!","type":"content","url":"/index-17","position":1},{"hierarchy":{"lvl1":"ASTR 596: Modeling the Universe"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"ASTR 596: Modeling the Universe"},"content":"SDSU | Fall 2025 | Fridays 11:00 AM - 1:40 PM | PA 215\n\nWelcome to computational astrophysics! This course takes you on a journey from Python fundamentals to cutting-edge JAX implementations, building transparent “glass box” models that reveal the physics underlying astronomical phenomena.\n\nCourse Philosophy: “Glass Box” Modeling\n\nEarned Complexity: We implement fundamental algorithms from first principles before leveraging advanced frameworks. Every line of code serves understanding—no black boxes allowed until you’ve built the glass box yourself.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"ASTR 596: Modeling the Universe","lvl2":"Course Trajectory"},"type":"lvl2","url":"/#course-trajectory","position":2},{"hierarchy":{"lvl1":"ASTR 596: Modeling the Universe","lvl2":"Course Trajectory"},"content":"🐍 Python Foundations\n\nProfessional development environment, OOP design, and stellar physics modeling\n\n🪐 N-Body Dynamics\n\nGravitational systems, numerical integration, and Monte Carlo sampling\n\n📈 Machine Learning\n\nLinear regression, optimization, and statistical modeling from scratch\n\n🎲 Monte Carlo Methods\n\nRadiative transfer, photon transport, and observational effects\n\n🔍 Bayesian Inference\n\nMCMC sampling, parameter estimation, and uncertainty quantification\n\n🧠 Neural Networks\n\nFrom backpropagation to JAX ecosystem and research applications","type":"content","url":"/#course-trajectory","position":3},{"hierarchy":{"lvl1":"ASTR 596: Modeling the Universe","lvl2":"Quick Navigation"},"type":"lvl2","url":"/#quick-navigation","position":4},{"hierarchy":{"lvl1":"ASTR 596: Modeling the Universe","lvl2":"Quick Navigation"},"content":"📋 Course Information\n\nSyllabus, schedule, policies, and expectations\n\n🎯 Projects\n\nSix progressive projects building computational skills\n\n🤖 AI Guidelines\n\nThree-phase approach to responsible AI integration\n\n🛠️ Resources\n\nSetup guides, references, and learning materials","type":"content","url":"/#quick-navigation","position":5},{"hierarchy":{"lvl1":"ASTR 596: Modeling the Universe","lvl2":"Learning Philosophy"},"type":"lvl2","url":"/#learning-philosophy","position":6},{"hierarchy":{"lvl1":"ASTR 596: Modeling the Universe","lvl2":"Learning Philosophy"},"content":"graph TD\n    A[Manual Implementation] --> B[Deep Understanding]\n    B --> C[Modern AI Tools]\n    C --> D[Research-Ready Skills]\n    \n    A1[NumPy from scratch] --> A\n    A2[MCMC by hand] --> A\n    A3[Neural nets manually] --> A\n    \n    D --> D1[Academic Research]\n    D --> D2[Industry Applications]\n    D --> D3[Computational Discovery]\n    \n    style A fill:#e1f5fe\n    style D fill:#f3e5f5","type":"content","url":"/#learning-philosophy","position":7},{"hierarchy":{"lvl1":"ASTR 596: Modeling the Universe","lvl2":"Course Highlights"},"type":"lvl2","url":"/#course-highlights","position":8},{"hierarchy":{"lvl1":"ASTR 596: Modeling the Universe","lvl2":"Course Highlights"},"content":"Glass Box Methodology: Build understanding from first principles\n\nPair Programming: Collaborative learning with rotating partnerships\n\nModern Frameworks: Bridge from NumPy to JAX ecosystem\n\nReal Astrophysics: Every project addresses genuine astronomical problems\n\nProfessional Skills: Git, testing, documentation, and AI tool integration\n\nResearch Preparation: Publication-quality code and cutting-edge methods","type":"content","url":"/#course-highlights","position":9},{"hierarchy":{"lvl1":"ASTR 596: Modeling the Universe","lvl2":"Responsible AI Integration"},"type":"lvl2","url":"/#responsible-ai-integration","position":10},{"hierarchy":{"lvl1":"ASTR 596: Modeling the Universe","lvl2":"Responsible AI Integration"},"content":"AI as a Learning Partner\n\nThis course teaches you to use AI tools (GitHub Copilot, ChatGPT) as learning amplifiers, not replacement thinking. You’ll develop critical evaluation skills while building the foundational knowledge that makes AI truly powerful.\n\nPhase 1 (Weeks 1-4): Foundation building with limited AI assistancePhase 2 (Weeks 5-8): Strategic AI integration with critical evaluationPhase 3 (Weeks 9-15): Full AI partnership for advanced implementations","type":"content","url":"/#responsible-ai-integration","position":11},{"hierarchy":{"lvl1":"ASTR 596: Modeling the Universe","lvl2":"Getting Started"},"type":"lvl2","url":"/#getting-started","position":12},{"hierarchy":{"lvl1":"ASTR 596: Modeling the Universe","lvl2":"Getting Started"},"content":"📖 Review the \n\nSyllabus and \n\nAI Guidelines\n\n⚙️ Set up your development environment (see \n\nResources)\n\n👥 Join the course GitHub organization\n\n🚀 Start with \n\nProject 1\n\nQuestions or Issues?\n\nInstructor: Professor Anna Rosen\n\nOffice: Physics 239\n\nEmail: \n\nalrosen@sdsu.edu\n\nOffice Hours: TBD in class (also available by appointment)\n\nCourse Issues: \n\nGitHub repository\n\nWelcome to a journey from Python basics to the frontiers of computational astrophysics. Together, we’ll model the universe, one algorithm at a time. ✨","type":"content","url":"/#getting-started","position":13}]}