---
jupytext:
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

# Chapter 5: Functions & Modules - Building Reusable Scientific Code

## Learning Objectives

By the end of this chapter, you will be able to:
- Design functions as clear contracts with well-defined inputs and outputs
- Understand Python's scope rules and how they affect variable access
- Master positional, keyword, and default arguments for flexible interfaces
- Apply functional programming patterns like map, filter, and lambda functions
- Create and import your own modules for code organization
- Document functions properly using docstrings
- Recognize and avoid common function-related bugs
- Build modular, reusable code for scientific applications

## Prerequisites Check

Before starting this chapter, verify you can:

- [ ] Write loops and conditionals fluently (Chapter 3)
- [ ] Choose appropriate data structures for different tasks (Chapter 4)
- [ ] Handle floating-point arithmetic safely (Chapter 2)
- [ ] Use IPython for testing and timing code (Chapter 1)
- [ ] Design algorithms with pseudocode (Chapter 3)

```{code-cell} ipython3
# Quick prerequisite check
data = [2.5, 3.7, 1.2, 4.8]
result = []
for value in data:
    if value > 2.0:
        result.append(value * 2)
print(f"If you got {result}, you're ready!")
# Expected: [5.0, 7.4, 9.6]
```

## Chapter Overview

Functions are the fundamental building blocks of organized code. Without functions, you'd be copying and pasting the same code repeatedly, making bugs harder to fix and improvements impossible to maintain. But functions are more than just a way to avoid repetition‚Äîthey're how we create abstractions, manage complexity, and build reliable software. Whether you're calculating statistical measures, simulating physical systems, or processing experimental data, every computational project starts with well-designed functions.

This chapter teaches you to think about functions as contracts between different parts of your code. When you write a function that calculates energy or performs numerical integration, you're creating a promise: given valid input, the function will reliably return the correct output. This contract mindset helps you write functions that others (including future you) can trust and use effectively. You'll learn how to choose between positional and keyword arguments, when to use default values, and how to design interfaces that are both flexible and clear.

We'll explore Python's scope rules, which determine where variables can be accessed, and learn how seemingly simple concepts like default arguments can create subtle bugs that have plagued even major scientific software packages. You'll discover how Python's flexible parameter system enables powerful interfaces, and how functional programming concepts prepare you for modern scientific computing frameworks. By the end, you'll be organizing your code into modules that can be shared, tested, and maintained professionally‚Äîessential skills for collaborative scientific research.

## 5.1 Defining Functions: The Basics

:::{margin} 
**Function**
A reusable block of code that performs a specific task, taking inputs and optionally returning outputs.
:::

A **function** encapsulates a piece of logic that transforms inputs into outputs. Think of a function as a machine: you feed it raw materials (inputs), it performs some process (the function body), and it produces a product (output). In scientific computing, a function might calculate statistical measures, integrate equations, or transform data‚Äîeach function performs one clear task that can be tested and trusted.

### Your First Function

Let's start with something every scientist needs‚Äîcalculating mean and standard deviation:

```{code-cell} ipython3
def calculate_mean(values):
    """
    Calculate the arithmetic mean of a list of values.
    
    This is our first function - notice the structure!
    """
    # Validate inputs early using assert (raises error if condition is False)
    assert len(values) > 0, "Cannot calculate mean of empty list"
    
    total = sum(values)
    count = len(values)
    mean = total / count
    return mean

# Using the function
measurements = [23.5, 24.1, 23.8, 24.3, 23.9]
avg = calculate_mean(measurements)
print(f"Mean temperature: {avg:.2f}¬∞C")

# The assert would raise an error with empty list:
# empty_mean = calculate_mean([])  # AssertionError: Cannot calculate mean of empty list
```

Let's break down exactly how this works:

1. **`def` keyword**: Tells Python we're defining a function
2. **Function name** (`calculate_mean`): Follows snake_case convention, describes what it does
3. **Parameters** (`values`): Variables that receive values when function is called
4. **Docstring**: Brief description of what the function does (always include this!)
5. **Function body**: Indented code that does the actual work
6. **`return` statement**: Sends a value back to whoever called the function

When Python executes `calculate_mean(measurements)`, it creates a temporary namespace where `values = [23.5, 24.1, ...]`, runs the function body, and returns the result.

:::{admonition} üîç Check Your Understanding #1
:class: question

What will this code print?

```python
def calculate_product(x, y):
    product = x * y
    # Oops, forgot the return statement!

result = calculate_product(3.0, 4.0)
print(f"Product: {result}")
```

<details>
<summary>Answer</summary>

It prints `Product: None`. The function calculates `product` but doesn't return it. Without an explicit `return` statement, Python functions return `None`. This is a common bug in scientific code!

To fix it:
```python
def calculate_product(x, y):
    product = x * y
    return product  # Now it returns the value
```

</details>
:::

### Functions Without Return Values

Not all functions return values. Some perform actions like printing results or saving data:

```{code-cell} ipython3
def report_statistics(data, name="Dataset"):
    """Report basic statistics without returning values."""
    mean = sum(data) / len(data)
    minimum = min(data)
    maximum = max(data)
    
    print(f"Statistics for {name}:")
    print(f"  Mean: {mean:.3f}")
    print(f"  Range: [{minimum:.3f}, {maximum:.3f}]")
    # No return statement - returns None implicitly

# Report some calculations
temperatures = [20.1, 21.5, 19.8, 22.3, 20.9]
report_statistics(temperatures, "Temperature (¬∞C)")
```

### Returning Multiple Values

Python functions can return multiple values using tuples‚Äîperfect for calculations that produce related results:

```{code-cell} ipython3
def analyze_data(values):
    """
    Calculate multiple statistics at once.
    
    Returns:
        mean, std_dev, min_val, max_val
    """
    n = len(values)
    mean = sum(values) / n
    
    # Calculate standard deviation
    variance = sum((x - mean)**2 for x in values) / n
    std_dev = variance ** 0.5
    
    return mean, std_dev, min(values), max(values)

# Analyze experimental data
data = [9.8, 10.1, 9.9, 10.2, 9.7, 10.0]
mean, std, min_val, max_val = analyze_data(data)

print(f"Mean: {mean:.3f}")
print(f"Std Dev: {std:.3f}")
print(f"Range: [{min_val}, {max_val}]")
```

### Kinetic Energy in CGS Units

Now let's calculate kinetic energy using CGS units (centimeters, grams, seconds):

```{code-cell} ipython3
def kinetic_energy_cgs(mass_g, velocity_cms):
    """
    Calculate kinetic energy in ergs.
    
    Parameters:
        mass_g: mass in grams
        velocity_cms: velocity in cm/s
    
    Returns:
        energy in ergs (g‚ãÖcm¬≤/s¬≤)
    """
    energy_ergs = 0.5 * mass_g * velocity_cms**2
    return energy_ergs

# Example: electron moving at 1% speed of light
electron_mass = 9.109e-28  # grams
c_light = 2.998e10  # speed of light in cm/s
electron_velocity = 0.01 * c_light  # 1% of c
ke = kinetic_energy_cgs(electron_mass, electron_velocity)

print(f"Electron kinetic energy: {ke:.2e} ergs")
print(f"In eV: {ke/1.602e-12:.2f} eV")  # 1 eV = 1.602e-12 ergs
```

### The Design Process: From Problem to Function

Before writing any function, design it first. This prevents the common mistake of coding yourself into a corner:

```{code-cell} ipython3
"""
DESIGN: Function to validate numerical calculation

PURPOSE: Ensure numerical results are physically reasonable
INPUT: value, expected_range
OUTPUT: boolean (True if valid)
PROCESS:
    - Check if value is finite
    - Check if value is within range
"""

def validate_result(value, min_val=None, max_val=None):
    """
    Validate a numerical result is reasonable.
    
    Simple validation - we'll expand this pattern later!
    """
    import math
    
    # Check if finite
    if not math.isfinite(value):
        return False
    
    # Check bounds if provided
    if min_val is not None and value < min_val:
        return False
    if max_val is not None and value > max_val:
        return False
    
    return True

# Test validation
energy = kinetic_energy_cgs(1.0, 100.0)
print(f"Energy = {energy} ergs")
print(f"Valid (positive)? {validate_result(energy, min_val=0)}")
print(f"Valid (in range)? {validate_result(energy, 0, 10000)}")
```

:::{admonition} üåü Why This Matters: Validation Saves Missions
:class: important

Validation functions aren't just good practice‚Äîthey prevent catastrophic failures. When NASA's Mars Climate Orbiter was lost in 1999, the root cause was a simple unit mismatch: one team used pound-seconds while another expected Newton-seconds. A validation function checking that thrust values were within expected ranges would have caught this $327 million error immediately.

This is why every function you write should validate its inputs and outputs. The few extra lines of validation code can save years of work and hundreds of millions of dollars!
:::

:::{admonition} üí° Computational Thinking: Function Contract Design
:class: tip

Every well-designed function follows a contract pattern that applies across all programming:

```
CONTRACT PATTERN:
1. Preconditions: What must be true before calling
2. Postconditions: What will be true after calling  
3. Invariants: What stays unchanged
4. Side effects: What else happens

Example for kinetic_energy_cgs():
- Precondition: mass > 0, velocity is numeric
- Postcondition: returns positive energy value
- Invariant: input values unchanged
- Side effects: none (pure function)

This pattern appears in:
- Database transactions (ACID properties)
- API design (REST contracts)
- Parallel computing (thread safety)
- Unit testing (test contracts)
```
:::

## 5.2 Function Arguments In-Depth

:::{margin} 
**Parameter**
A variable in a function definition that receives a value when the function is called.
:::

:::{margin} 
**Argument**
The actual value passed to a function when calling it.
:::

Python provides flexible ways to handle function **parameters**. Understanding the distinction between positional arguments, keyword arguments, and default values is crucial for designing clear, flexible interfaces. Let's explore this flexibility through progressive examples.

### Positional vs Keyword Arguments

```{code-cell} ipython3
def calculate_force(mass_g, acceleration_cms2):
    """
    Calculate force using Newton's second law in CGS units.
    
    Parameters:
        mass_g: mass in grams
        acceleration_cms2: acceleration in cm/s¬≤
    
    Returns:
        force in dynes (g‚ãÖcm/s¬≤)
    """
    force_dynes = mass_g * acceleration_cms2
    return force_dynes

# Different ways to call the same function
f1 = calculate_force(10, 980)                        # Positional only (Earth's gravity)
f2 = calculate_force(mass_g=10, acceleration_cms2=980)  # Keywords (clear!)
f3 = calculate_force(acceleration_cms2=980, mass_g=10)  # Keywords (any order!)

print(f"Positional: {f1:.1f} dynes")
print(f"Keywords: {f2:.1f} dynes")  
print(f"Reversed keywords: {f3:.1f} dynes")

# This would be wrong (arguments reversed):
# f_wrong = calculate_force(980, 10)  # Would give 9800 instead of 98000
```

### Default Arguments Make Functions Flexible

Default arguments let users omit parameters when standard values suffice:

```{code-cell} ipython3
def simulate_decay(initial_atoms, half_life_years=5730, time_years=0):
    """
    Simulate radioactive decay.
    
    Parameters:
        initial_atoms: starting number of atoms
        half_life_years: half-life in years (default: C-14 = 5730)
        time_years: elapsed time in years (default: 0)
    """
    import math
    remaining = initial_atoms * (0.5 ** (time_years / half_life_years))
    return remaining

# Different usage patterns
n0 = 1000000  # One million atoms

# Use all defaults (returns initial count)
print(f"At t=0: {simulate_decay(n0):.0f} atoms")

# Override just time (uses default half-life for C-14)
print(f"After 5730 years: {simulate_decay(n0, time_years=5730):.0f} atoms")

# Override all parameters (U-238 example)
print(f"U-238 after 1 billion years: {simulate_decay(n0, 4.5e9, 1e9):.0f} atoms")
```

:::{admonition} üìö When to Use Different Argument Types
:class: note

**Use Positional Arguments When:**
- The meaning is obvious from context (`power(base, exponent)`)
- There are only 1-2 required parameters
- The order is natural and memorable

**Use Keyword Arguments When:**
- There are many parameters (>3)
- Parameters are optional
- The meaning isn't obvious (`process(True, False, 5)` vs `process(verbose=True, cache=False, retries=5)`)

**Use Default Arguments When:**
- There's a sensible standard value
- Most calls use the same value
- You want backward compatibility when adding features

**Best Practice Progression:**
1. Start with required positional arguments
2. Add defaults for optional behaviors
3. Use keyword-only arguments for clarity (see Advanced section)
:::

### The Mutable Default Trap

Here's a critical bug that has appeared in major scientific software:

```{code-cell} ipython3
# THE TRAP - DON'T DO THIS!
def add_measurement_buggy(value, data=[]):
    """BUGGY VERSION - default list is shared!"""
    data.append(value)
    return data

# Watch the disaster unfold
day1 = add_measurement_buggy(23.5)
print(f"Day 1: {day1}")

day2 = add_measurement_buggy(24.1)  # Surprise!
print(f"Day 2: {day2}")  # Contains BOTH days!

print(f"Same list? {day1 is day2}")  # True - it's the same object!
```

:::{admonition} ‚ö†Ô∏è Common Bug Alert: The Mutable Default Disaster
:class: warning

This bug has appeared in:
- CERN analysis scripts (accumulated all runs' data)  
- NASA trajectory calculations (mixed mission parameters)
- Weather prediction models (combined different forecasts)

The symptom: data from previous runs mysteriously appears in new analyses.

The cause: Python evaluates default arguments **once** when the function is defined, not each time it's called.

The fix: always use `None` as default for mutable arguments.
:::

:::{margin} 
**Sentinel Value**
A special value (like None) used to signal a particular condition, often used as a default for mutable arguments.
:::

Here's the correct pattern:

```{code-cell} ipython3
def add_measurement_fixed(value, data=None):
    """CORRECT VERSION - new list each time if needed."""
    if data is None:
        data = []  # Create new list when not provided
    data.append(value)
    return data

# Now it works correctly
day1 = add_measurement_fixed(23.5)
day2 = add_measurement_fixed(24.1)
print(f"Day 1: {day1}")
print(f"Day 2: {day2}")  # Separate lists!

# Can still provide existing list
combined = []
add_measurement_fixed(23.5, combined)
add_measurement_fixed(24.1, combined)
print(f"Combined: {combined}")
```

### Variable Arguments (*args and **kwargs)

:::{margin}
`*args`
Collects extra positional arguments into a tuple.
:::

:::{margin}
`**kwargs`
Collects extra keyword arguments into a dictionary.
:::

```{code-cell} ipython3
def calculate_statistics(*values, method='all'):
    """
    Calculate statistics for any number of values.
    
    *args collects positional arguments into a tuple.
    """
    if not values:
        return None
    
    n = len(values)
    mean = sum(values) / n
    
    if method == 'mean':
        return mean
    
    # Calculate variance and std dev
    variance = sum((x - mean)**2 for x in values) / n
    std_dev = variance ** 0.5
    
    if method == 'all':
        return {
            'n': n,
            'mean': mean,
            'std': std_dev,
            'min': min(values),
            'max': max(values)
        }

# Works with any number of arguments!
print(calculate_statistics(1, 2, 3, 4, 5))
print(f"Mean only: {calculate_statistics(2.5, 3.7, 1.2, method='mean'):.2f}")
```

```{code-cell} ipython3
def run_experiment(name, **parameters):
    """
    Run experiment with flexible parameters.
    
    **kwargs collects keyword arguments into a dictionary.
    """
    print(f"=== Experiment: {name} ===")
    
    # Default parameters
    defaults = {
        'temperature': 293.15,  # Kelvin
        'pressure': 101325,     # Pascals
        'trials': 10
    }
    
    # Update with user parameters
    config = defaults.copy()
    config.update(parameters)
    
    print("Configuration:")
    for key, value in config.items():
        print(f"  {key}: {value}")
    
    return f"Completed {config['trials']} trials"

# Simple call
result = run_experiment("Test A")

# Complex call with many options
result = run_experiment(
    "Test B",
    temperature=350,
    pressure=200000,
    trials=50,
    catalyst="Platinum"
)
```

:::{admonition} üîç Check Your Understanding #2
:class: question

What's wrong with this function definition, and how would you fix it?

```python
def process_data(values, scale=1.0, *extra, normalize=True):
    # Process data with options
    pass
```

<details>
<summary>Answer</summary>

The parameter order is wrong! Python requires this order:
1. Regular positional parameters
2. *args (if any)
3. Keyword parameters with defaults
4. **kwargs (if any)

Correct version:
```python
def process_data(values, *extra, scale=1.0, normalize=True):
    # Now the order is correct
    pass
```

The original would give a SyntaxError because *extra can't come after a keyword parameter with a default.
</details>
:::

## 5.3 Scope and Namespaces

{margin} Scope
The region of a program where a variable is accessible.

{margin} Namespace  
A container that holds a set of identifiers and their associated objects.

Understanding scope‚Äîwhere variables can be accessed‚Äîis crucial for writing bug-free code. Python's scope rules determine which variables are visible at any point in your program. Without understanding scope, you'll encounter confusing bugs where variables don't have the values you expect, or worse, where changing a variable in one place mysteriously affects code elsewhere.

### The LEGB Rule

{margin} LEGB
Local, Enclosing, Global, Built-in - Python's scope resolution order.

Python resolves variable names using the LEGB rule, searching in this order:
- **L**ocal: Inside the current function
- **E**nclosing: In the enclosing function (for nested functions)  
- **G**lobal: At the top level of the module
- **B**uilt-in: In the built-in namespace (print, len, etc.)

```{code-cell} ipython3
# Demonstrating LEGB with scientific context
speed_of_light = 2.998e10  # Global scope (cm/s in CGS)

def calculate_energy():
    # Enclosing scope
    rest_mass = 9.109e-28  # electron mass in grams
    
    def relativistic_factor(velocity_cms):
        # Local scope
        c = speed_of_light  # Accesses global (cm/s)
        beta = velocity_cms / c
        
        # Local calculation
        import math
        if beta >= 1:
            return float('inf')  # Cannot exceed speed of light
        gamma = 1 / math.sqrt(1 - beta**2)
        
        print(f"  Inside relativistic: Œ≤ = {beta:.3f}, Œ≥ = {gamma:.3f}")
        return gamma
    
    # Use inner function
    v = 0.5 * speed_of_light  # Half light speed (cm/s)
    gamma = relativistic_factor(v)
    
    # Calculate relativistic energy (E = Œ≥mc¬≤)
    energy_ergs = rest_mass * speed_of_light**2 * gamma
    print(f"Inside calculate: E = {energy_ergs:.2e} ergs")
    return energy_ergs

result = calculate_energy()
print(f"Global scope: c = {speed_of_light:.2e} cm/s")
print(f"Final energy: {result:.2e} ergs = {result/1.602e-12:.2f} eV")
```

### The UnboundLocalError Trap

```{code-cell} ipython3
counter = 0  # Global

def increment_wrong():
    """This will crash - Python sees assignment and assumes local!"""
    # counter += 1  # UnboundLocalError!
    print("Uncomment the line above to see the error")

def increment_with_global():
    """Explicitly use global variable."""
    global counter
    counter += 1
    return counter

def increment_better(current_count):
    """Best approach - avoid global state entirely."""
    return current_count + 1

# Test the better approach
count = 0
count = increment_better(count)
count = increment_better(count)
print(f"Count: {count}")

# Global approach (generally avoid)
increment_with_global()
increment_with_global()
print(f"Global counter: {counter}")
```

:::{admonition} ‚ö†Ô∏è Common Bug Alert: UnboundLocalError 
:class: warning

The error happens because Python sees you're assigning to `counter`, assumes it's local, but then can't find a local value to increment.

Symptoms: 
- Variable works fine when reading
- Crashes when trying to modify  
- Error message mentions "referenced before assignment"

Fix: Either use `global` keyword or (better) pass the value explicitly.

Real disaster: A climate model that tried to update global temperature but created local variables instead, producing nonsense results for months before discovery.
:::

### Closures: Functions That Remember

{margin} Closure
A function that remembers variables from its enclosing scope even after that scope has finished.

```{code-cell} ipython3
def create_integrator(method='rectangle'):
    """
    Factory function that creates specialized integrators.
    
    The returned function 'remembers' the method.
    """
    def integrate(func, a, b, n=100):
        """Numerically integrate func from a to b."""
        dx = (b - a) / n
        total = 0
        
        if method == 'rectangle':
            # Left rectangle rule
            for i in range(n):
                x = a + i * dx
                total += func(x) * dx
                
        elif method == 'midpoint':
            # Midpoint rule (more accurate)
            for i in range(n):
                x = a + (i + 0.5) * dx
                total += func(x) * dx
        
        return total
    
    # Return the customized function
    return integrate

# Create specialized integrators
rect_integrate = create_integrator('rectangle')
mid_integrate = create_integrator('midpoint')

# Test with x¬≤ from 0 to 1 (exact answer = 1/3)
def f(x):
    return x**2

rect_result = rect_integrate(f, 0, 1, 100)
mid_result = mid_integrate(f, 0, 1, 100)
exact = 1/3

print(f"Rectangle rule: {rect_result:.6f}")
print(f"Midpoint rule:  {mid_result:.6f}")
print(f"Exact:          {exact:.6f}")
print(f"Rectangle error: {abs(rect_result-exact):.6f}")
print(f"Midpoint error:  {abs(mid_result-exact):.6f}")
```

:::{admonition} üîç Check Your Understanding #3
:class: question

What happens if both inner and outer functions define the same variable name?

```python
def outer():
    x = "outer"
    
    def inner():
        x = "inner"
        print(f"Inner sees: {x}")
    
    inner()
    print(f"Outer sees: {x}")

outer()
```

<details>
<summary>Answer</summary>

Each function sees its own local variable. The inner function's `x` doesn't affect the outer function's `x`. They're in different namespaces!

Output:
```
Inner sees: inner
Outer sees: outer
```

If the inner function didn't define its own `x`, it would see the outer's `x` due to the LEGB rule (finding it in the Enclosing scope).
</details>
:::

:::{admonition} üí° Computational Thinking: Why Global Variables Are Dangerous
:class: tip

Global variables violate the "principle of least surprise" in computing:

```
HIDDEN STATE ANTI-PATTERN:
- Function behavior depends on external state
- Can't understand function in isolation
- Testing requires global setup
- Parallel processing becomes impossible

Real disaster: Mars Climate Orbiter (1999)
- Global unit variable (metric vs imperial)
- One module changed it to imperial
- Another module read it expecting metric
- $327 million spacecraft crashed into Mars

Better pattern: Explicit State Passing
BAD:  current_units = 'metric'; calculate()
GOOD: calculate(units='metric')
```
:::

## 5.4 Functional Programming Elements

{margin} Side Effect
Any state change that occurs beyond returning a value from a function, such as modifying global variables or printing output.

{margin} Pure Function
A function that always returns the same output for the same input with no side effects.

Python supports functional programming‚Äîa style that treats computation as the evaluation of mathematical functions. These concepts are essential for modern scientific frameworks like JAX and lead to cleaner, more testable code.

### Lambda Functions

{margin} Lambda
An anonymous function defined inline using the `lambda` keyword.

Lambda functions are small, anonymous functions defined inline:

```{code-cell} ipython3
# Traditional function
def square(x):
    return x**2

# Equivalent lambda
square_lambda = lambda x: x**2

print(f"Traditional: {square(5)}")
print(f"Lambda: {square_lambda(5)}")

# Lambdas shine for sorting and filtering
data_points = [
    {'x': 1.5, 'y': 2.3, 'error': 0.1},
    {'x': 2.1, 'y': 4.7, 'error': 0.05},
    {'x': 0.8, 'y': 1.2, 'error': 0.2},
]

# Sort by x value
by_x = sorted(data_points, key=lambda p: p['x'])
print("\nSorted by x:")
for p in by_x:
    print(f"  x={p['x']}, y={p['y']}")

# Sort by relative error (error/y)
by_rel_error = sorted(data_points, key=lambda p: p['error']/p['y'])
print("\nSorted by relative error:")
for p in by_rel_error:
    rel_err = p['error']/p['y'] * 100
    print(f"  x={p['x']}, relative error={rel_err:.1f}%")
```

### Map, Filter, and Reduce

These functional tools transform how you process data:

```{code-cell} ipython3
from functools import reduce

# Sample measurements in CGS units
measurements_cms = [981, 979, 983, 9900, 980, 982, 978]  # cm/s¬≤ (g values)

# FILTER: Remove outliers
mean_estimate = 980  # cm/s¬≤
tolerance = 10  # cm/s¬≤

good_data = list(filter(
    lambda x: abs(x - mean_estimate) < tolerance,
    measurements_cms
))

print(f"Original: {measurements_cms}")
print(f"Filtered: {good_data}")
print(f"Removed {len(measurements_cms) - len(good_data)} outliers")

# MAP: Apply calibration factor
calibration = 1.002  # 0.2% correction
calibrated = list(map(lambda x: x * calibration, good_data))
print(f"Calibrated: {[f'{x:.1f}' for x in calibrated]}")

# REDUCE: Calculate mean
mean = reduce(lambda a, b: a + b, calibrated) / len(calibrated)
print(f"Final mean: {mean:.1f} cm/s¬≤")
```

:::{admonition} üîç Check Your Understanding #4
:class: question

Rewrite this loop using functional programming:

```python
# Square all positive values
results = []
for x in data:
    if x > 0:
        results.append(x**2)
```

<details>
<summary>Answer</summary>

Two equivalent functional approaches:

```python
# Using filter and map
results = list(map(
    lambda x: x**2,
    filter(lambda x: x > 0, data)
))

# Using list comprehension (more Pythonic)
results = [x**2 for x in data if x > 0]
```

The list comprehension is generally preferred in Python for readability, but understanding the functional approach prepares you for libraries like JAX.
</details>
:::

### Functions as First-Class Objects

In Python, functions are objects you can pass around:

```{code-cell} ipython3
def apply_operator(data, operator_func):
    """
    Apply any operation to data.
    
    operator_func determines the transformation.
    """
    return [operator_func(x) for x in data]

# Define different operations
def grams_to_kilograms(g):
    """Convert grams to kilograms."""
    return g / 1000.0

def dynes_to_newtons(dynes):
    """Convert dynes to Newtons (1 N = 10‚Åµ dynes)."""
    return dynes / 1e5

def normalize(x, mean=0, std=1):
    """Standardize value."""
    return (x - mean) / std

# Use different operations
masses_g = [100, 200, 300, 400]  # grams
masses_kg = apply_operator(masses_g, grams_to_kilograms)
print(f"Grams: {masses_g}")
print(f"Kilograms: {masses_kg}")

forces_dynes = [1e5, 2e5, 3e5, 4e5]  # dynes
forces_n = apply_operator(forces_dynes, dynes_to_newtons)
print(f"\nDynes: {forces_dynes}")
print(f"Newtons: {forces_n}")

# Create custom normalizer with closure
values = [2, 4, 6, 8, 10]
mean = sum(values) / len(values)
std = (sum((x - mean)**2 for x in values) / len(values)) ** 0.5

normalizer = lambda x: normalize(x, mean, std)
normalized = apply_operator(values, normalizer)
print(f"\nOriginal: {values}")
print(f"Normalized: {[f'{x:.2f}' for x in normalized]}")
```

:::{admonition} üåü Why This Matters: Modern Frameworks Require Functional Thinking
:class: important

Functional programming isn't just academic‚Äîit's essential for modern scientific computing:

1. **JAX** (Google's NumPy replacement) requires pure functions for automatic differentiation
2. **Parallel processing** works best with stateless functions  
3. **Testing** is trivial when functions have no side effects
4. **GPU computing** maps naturally to functional operations

Example: In JAX, you can automatically differentiate through an entire physics simulation if it's written functionally. This enables techniques like physics-informed neural networks that would be impossibly complex to implement manually.
:::

## 5.5 Modules and Packages

{margin} Module
A Python file containing definitions and statements that can be imported.

{margin} Package
A directory containing multiple Python modules and an `__init__.py` file.

As your analysis grows from scripts to projects, organization becomes critical. Modules let you organize related functions together and reuse them across projects.

### Creating Your First Module

:::{admonition} üìù Creating Module Files
:class: note

To create a module, you need to save Python code in a separate `.py` file. You can either:
1. **Create manually**: Open a new file in your editor, paste the code, and save as `statistics_tools.py`
2. **Create programmatically**: Use the code below to generate the file

For this course, we recommend creating files manually in your editor for better understanding.
:::

Save this as `statistics_tools.py` in your current directory:

```{code-cell} ipython3
# Method 1: Show the module contents (you would save this in a file)
module_code = '''
"""
statistics_tools.py - Basic statistical calculations.

This module provides fundamental statistical functions.
"""

import math

def mean(data):
    """Calculate arithmetic mean."""
    return sum(data) / len(data)

def variance(data):
    """Calculate population variance."""
    m = mean(data)
    return sum((x - m)**2 for x in data) / len(data)

def std_dev(data):
    """Calculate population standard deviation."""
    return math.sqrt(variance(data))

def std_error(data):
    """Calculate standard error of the mean."""
    return std_dev(data) / math.sqrt(len(data))

# Module-level code runs on import
print("Loading statistics_tools module...")

# Test code that only runs when script is executed directly
if __name__ == "__main__":
    test_data = [1, 2, 3, 4, 5]
    print(f"Test mean: {mean(test_data)}")
    print(f"Test std dev: {std_dev(test_data):.3f}")
'''

# Method 2: Programmatically create the file
with open('statistics_tools.py', 'w') as f:
    f.write(module_code)

print("Module file 'statistics_tools.py' has been created in your current directory")
```

### Using Your Module

```{code-cell} ipython3
# Different import methods

# Method 1: Import entire module
import statistics_tools

data = [10, 12, 11, 13, 10, 11, 12]
m = statistics_tools.mean(data)
s = statistics_tools.std_dev(data)
print(f"Method 1 - Mean: {m:.2f}, Std: {s:.2f}")

# Method 2: Import specific functions
from statistics_tools import mean, std_error

se = std_error(data)
print(f"Method 2 - Standard error: {se:.3f}")

# Method 3: Import with alias
import statistics_tools as stats

var = stats.variance(data)
print(f"Method 3 - Variance: {var:.3f}")
```

### Building a Physics Module

Let's create a comprehensive module for physics calculations:

```{code-cell} ipython3
# Create physics_cgs.py module with all CGS units
physics_module = '''
"""
physics_cgs.py - Physics calculations in CGS units.

All calculations use CGS (centimeter-gram-second) units.
"""

# Physical constants in CGS
SPEED_OF_LIGHT = 2.998e10      # cm/s
PLANCK_CONSTANT = 6.626e-27    # erg‚ãÖs  
BOLTZMANN = 1.381e-16          # erg/K
ELECTRON_MASS = 9.109e-28      # g
PROTON_MASS = 1.673e-24        # g
GRAVITATIONAL_CONSTANT = 6.674e-8  # cm¬≥/(g‚ãÖs¬≤)

def kinetic_energy(mass_g, velocity_cms):
    """KE = ¬Ωmv¬≤ in ergs."""
    return 0.5 * mass_g * velocity_cms**2

def momentum(mass_g, velocity_cms):
    """p = mv in g‚ãÖcm/s."""
    return mass_g * velocity_cms

def de_broglie_wavelength(mass_g, velocity_cms):
    """Œª = h/p in cm."""
    p = momentum(mass_g, velocity_cms)
    return PLANCK_CONSTANT / p if p != 0 else float('inf')

def thermal_velocity(temp_k, mass_g):
    """RMS thermal velocity in cm/s."""
    import math
    return math.sqrt(3 * BOLTZMANN * temp_k / mass_g)

def photon_energy(wavelength_cm):
    """E = hc/Œª in ergs."""
    return PLANCK_CONSTANT * SPEED_OF_LIGHT / wavelength_cm

def gravitational_force(m1_g, m2_g, distance_cm):
    """F = Gm‚ÇÅm‚ÇÇ/r¬≤ in dynes."""
    if distance_cm == 0:
        return float('inf')
    return GRAVITATIONAL_CONSTANT * m1_g * m2_g / distance_cm**2

class Particle:
    """Simple particle with physics methods."""
    
    def __init__(self, mass_g, velocity_cms):
        self.mass = mass_g
        self.velocity = velocity_cms
    
    def kinetic_energy(self):
        return kinetic_energy(self.mass, self.velocity)
    
    def wavelength(self):
        return de_broglie_wavelength(self.mass, self.velocity)
'''

with open('physics_cgs.py', 'w') as f:
    f.write(physics_module)

# Now use it
import physics_cgs

# Electron at 1% light speed (all CGS units)
v_electron = 0.01 * physics_cgs.SPEED_OF_LIGHT  # cm/s
ke = physics_cgs.kinetic_energy(physics_cgs.ELECTRON_MASS, v_electron)
wavelength = physics_cgs.de_broglie_wavelength(physics_cgs.ELECTRON_MASS, v_electron)

print(f"Electron at 1% c:")
print(f"  Velocity = {v_electron:.2e} cm/s")
print(f"  KE = {ke:.2e} ergs")
print(f"  de Broglie Œª = {wavelength:.2e} cm")

# Test gravitational force (Earth-Moon in CGS)
earth_mass_g = 5.972e27  # grams
moon_mass_g = 7.342e25   # grams  
earth_moon_distance_cm = 3.844e10  # cm

force = physics_cgs.gravitational_force(earth_mass_g, moon_mass_g, earth_moon_distance_cm)
print(f"\nEarth-Moon gravitational force: {force:.2e} dynes")
```

### Import Best Practices

```{code-cell} ipython3
# GOOD: Clear, explicit imports
import numpy as np
import matplotlib.pyplot as plt
from math import sin, cos, pi

# BAD: Wildcard imports pollute namespace
# from numpy import *  # Adds 600+ names!
# from math import *   # Conflicts with numpy!

# Example of namespace collision
import math
import cmath  # Complex math module (introduced in Chapter 2)

# Clear which log we're using
real_log = math.log(10)      # Natural log of real number
complex_log = cmath.log(-1)   # Can handle complex numbers

print(f"math.log(10) = {real_log:.3f}")
print(f"cmath.log(-1) = {complex_log}")  # Returns complex number

# If we had used 'from math import *':
# log(10)  # Which log? math or cmath? Unclear!
```

:::{admonition} üîç Check Your Understanding #5
:class: question

Why is `from module import *` dangerous in scientific code?

<details>
<summary>Answer</summary>

Wildcard imports are dangerous because:

1. **Name collisions**: Multiple libraries have functions with same names (log, sqrt, sin)
2. **Hidden overwrites**: Later imports silently replace earlier ones
3. **Unclear source**: Can't tell where a function comes from
4. **Namespace pollution**: Hundreds of unwanted names

Real example that caused wrong results:
```python
from numpy import *    # Has log (natural log)
from math import *     # Also has log, overwrites numpy's
from sympy import *    # Has symbolic log, overwrites again

# Which log is this?
result = log(data)  # Could be any of the three!
```

Always use explicit imports for clarity and safety!
</details>
:::

:::{admonition} üåü The More You Know: How Modular Code Detected Gravitational Waves
:class: history

On September 14, 2015, at 09:50:45 UTC, the Laser Interferometer Gravitational-Wave Observatory (LIGO) detected humanity's first gravitational wave signal, named GW150914. This discovery, which earned the 2017 Nobel Prize in Physics, was only possible because of meticulously modular software architecture.

The LIGO analysis pipeline consisted of hundreds of independent Python modules, each performing one specific task:

```python
# Simplified LIGO pipeline structure
def calibrate_strain(raw_data, calibration_data):
    """Convert detector output to strain."""
    # Complex calibration involving transfer functions
    
def remove_lines(strain_data, line_frequencies):
    """Remove power line interference and harmonics."""
    # Notch filters at 60Hz, 120Hz, etc.
    
def whiten_data(strain, psd):
    """Normalize frequency response."""
    # Spectral whitening for uniform sensitivity
    
def matched_filter(data, template_bank):
    """Search for gravitational wave patterns."""
    # Cross-correlation with theoretical waveforms
```

When the first signal appeared, this modular design enabled unprecedented rapid verification. Within minutes, automated pipelines had processed the data through dozens of independent checks. The modularity allowed the 1000+ member collaboration to:

- Test each processing step with simulated signals
- Run multiple independent analysis pipelines in parallel
- Swap alternative algorithms to verify robustness
- Complete peer review in record time

The discovery paper lists over 30 independent software modules, each thoroughly tested and documented. This modular approach‚Äîexactly what you're learning in this chapter‚Äîenabled one of physics' greatest discoveries!
:::

:::{admonition} üåü The More You Know: The $370 Million Integer Overflow
:class: history

On June 4, 1996, the maiden flight of Ariane 5 ended in spectacular failure just 37 seconds after liftoff. The rocket veered off course and self-destructed, destroying four Cluster satellites worth $370 million. This disaster teaches us a crucial lesson about function design and validation.

The root cause traced back to a single function in the inertial reference system, originally written for Ariane 4. This function converted a 64-bit floating-point horizontal velocity value to a 16-bit signed integer. In Ariane 4's slower flight profile, this worked perfectly. But Ariane 5 was more powerful. At T+37 seconds, the horizontal velocity exceeded 32,767‚Äîthe maximum value for a 16-bit signed integer.

The conversion function essentially looked like this conceptually:

```python
def convert_velocity(velocity_64bit):
    # Direct conversion without validation
    return int(velocity_64bit)  # Assumes it fits in 16 bits!
```

What they needed was:

```python
def convert_velocity_safe(velocity_64bit):
    MAX_INT16 = 32767
    MIN_INT16 = -32768
    
    if velocity_64bit > MAX_INT16:
        # Handle overflow appropriately
        log_error(f"Velocity {velocity_64bit} exceeds 16-bit range")
        return MAX_INT16  # Or raise exception with proper handling
    elif velocity_64bit < MIN_INT16:
        return MIN_INT16
        
    return int(velocity_64bit)
```

The overflow caused an exception in both the primary and backup systems (which ran identical code). The main flight computer, suddenly receiving diagnostic data instead of attitude information, interpreted it as actual flight data and commanded a violent course correction. The resulting aerodynamic forces exceeded design limits, triggering automatic self-destruct.

The investigation revealed four critical lessons about function design:
1. **Always validate input ranges**, especially when converting between data types
2. **Never assume code reuse is safe** without checking new operating conditions
3. **Design for graceful degradation** rather than complete failure
4. **Test with realistic data ranges** that match actual operating conditions

Every validation function you write, every bounds check you add, every type conversion you verify‚Äîthese aren't bureaucratic overhead. They're the difference between mission success and catastrophic failure. This disaster led directly to modern software engineering practices that require explicit contracts for all functions, comprehensive range checking, and formal verification of critical code paths.
:::


:class: history

In 1990, the Hubble Space Telescope launched with a precisely ground mirror that was perfectly wrong. The primary mirror's shape was off by just 2.2 micrometers‚Äî1/50th the width of a human hair‚Äîbut this tiny error made the $1.5 billion telescope nearly useless for its first three years.

The cause? A spacing error in the reflective null corrector, the device used to test the mirror during grinding. But the real failure was in the testing software's validation functions. The quality control program had a function like this:

```python
def validate_mirror_test(measurement, reference):
    """Check if mirror measurement matches reference."""
    difference = abs(measurement - reference)
    threshold = 0.01  # Arbitrary threshold!
    
    if difference < threshold:
        return "PASS"
    else:
        return "FAIL"
```

The problem? When two independent tests disagreed, engineers trusted the wrong one. The software's validation function didn't:
- Check which test was more reliable
- Validate the validation equipment itself
- Require agreement between multiple independent methods
- Flag when results were suspiciously perfect

A proper validation would have looked like:

```python
def validate_mirror_comprehensive(test1, test2, test3):
    """Require agreement between independent tests."""
    # Check all tests are within tolerance of each other
    if not all_agree([test1, test2, test3], tolerance=0.001):
        raise ValueError("Independent tests disagree!")
    
    # Check results aren't suspiciously perfect
    if any(is_too_perfect(test) for test in [test1, test2, test3]):
        raise ValueError("Measurement suspiciously perfect - check equipment!")
    
    # Validate the validators
    if not validate_test_equipment():
        raise ValueError("Test equipment out of calibration!")
    
    return statistics.mean([test1, test2, test3])
```

The incorrect mirror required a dramatic Space Shuttle servicing mission in 1993 to install corrective optics. The lesson? In scientific computing, your validation functions are as critical as your calculations. A thorough validation function‚Äîchecking multiple independent sources, validating the validators, and flagging suspicious results‚Äîwould have caught this error on the ground instead of in orbit.

Today, Hubble's legacy includes not just stunning images and revolutionary science, but also a fundamental lesson: never trust a single test, always validate your validators, and remember that perfect results are often perfectly wrong.
:::

## 5.6 Documentation and Testing

Good documentation and testing make your functions trustworthy and reusable. Professional scientific code requires both to ensure reproducibility and reliability.

### Documentation Levels

```{code-cell} ipython3
# Level 1: Minimal (but essential)
def quadratic_formula(a, b, c):
    """Solve ax¬≤ + bx + c = 0."""
    discriminant = b**2 - 4*a*c
    if discriminant < 0:
        return None, None
    sqrt_disc = discriminant**0.5
    return (-b + sqrt_disc)/(2*a), (-b - sqrt_disc)/(2*a)

# Level 2: Professional documentation
def integrate_trapezoid(x_values, y_values):
    """
    Integrate using the trapezoidal rule.
    
    Parameters
    ----------
    x_values : list or array
        X coordinates of data points (must be sorted)
    y_values : list or array
        Y coordinates of data points
    
    Returns
    -------
    float
        Approximate integral of y with respect to x
    
    Examples
    --------
    >>> x = [0, 1, 2, 3]
    >>> y = [0, 1, 4, 9]  # y = x¬≤
    >>> integrate_trapezoid(x, y)
    8.0  # Exact answer is 9
    
    Notes
    -----
    Uses the composite trapezoidal rule:
    ‚à´y dx ‚âà Œ£(y[i] + y[i+1])/2 * (x[i+1] - x[i])
    
    More accurate for smooth functions with
    many points.
    """
    if len(x_values) != len(y_values):
        raise ValueError("x and y must have same length")
    
    integral = 0
    for i in range(len(x_values) - 1):
        dx = x_values[i + 1] - x_values[i]
        avg_y = (y_values[i] + y_values[i + 1]) / 2
        integral += avg_y * dx
    
    return integral

# Test the function
x = [0, 0.5, 1.0, 1.5, 2.0]
y = [0, 0.25, 1.0, 2.25, 4.0]  # y = x¬≤
result = integrate_trapezoid(x, y)
print(f"Integral of x¬≤ from 0 to 2: {result:.3f}")
print(f"Exact answer: {8/3:.3f}")
print(f"Error: {abs(result - 8/3):.3f}")
```

### Testing Your Functions

```{code-cell} ipython3
def quadratic_formula(a, b, c):
    """
    Solve quadratic equation ax¬≤ + bx + c = 0.
    
    Returns:
        tuple: (root1, root2) or (None, None) if no real roots
    """
    import math
    
    # Calculate discriminant
    discriminant = b**2 - 4*a*c
    
    # Check if real roots exist
    if discriminant < 0:
        return None, None
    elif discriminant == 0:
        # One repeated root
        root = -b / (2*a)
        return root, root
    else:
        # Two distinct roots
        sqrt_disc = math.sqrt(discriminant)
        root1 = (-b + sqrt_disc) / (2*a)
        root2 = (-b - sqrt_disc) / (2*a)
        return root1, root2

def test_quadratic():
    """Test quadratic formula solver."""
    
    # Test 1: Simple case (x¬≤ - 5x + 6 = 0)
    # Roots should be 2 and 3
    r1, r2 = quadratic_formula(1, -5, 6)
    assert abs(r1 - 3) < 1e-10 or abs(r1 - 2) < 1e-10
    assert abs(r2 - 3) < 1e-10 or abs(r2 - 2) < 1e-10
    print("‚úì Test 1: Simple roots")
    
    # Test 2: No real roots (x¬≤ + 1 = 0)
    r1, r2 = quadratic_formula(1, 0, 1)
    assert r1 is None and r2 is None
    print("‚úì Test 2: No real roots")
    
    # Test 3: Single root (x¬≤ - 2x + 1 = 0)
    # Root should be 1 (twice)
    r1, r2 = quadratic_formula(1, -2, 1)
    assert abs(r1 - 1) < 1e-10
    assert abs(r2 - 1) < 1e-10
    print("‚úì Test 3: Repeated root")
    
    print("All tests passed!")

# Run tests
test_quadratic()
```

### Introduction to Type Hints (Optional Enhancement)

:::{admonition} üìå Type Hints are Optional
:class: info

Type hints are an **optional** Python feature introduced in Python 3.5. They document expected types but **Python does not enforce them** - they're primarily for documentation and IDE support. 

**For this course**: Type hints are not required but can help make your code clearer. Focus on writing correct functions first, then add type hints if desired.
:::

Python supports type hints to document expected types:

```{code-cell} ipython3
def kinetic_energy_typed(mass_g: float, velocity_cms: float) -> float:
    """
    Calculate kinetic energy with type hints.
    
    The ': float' annotations indicate expected types.
    The '-> float' indicates the return type.
    These are documentation only - Python doesn't enforce them!
    """
    return 0.5 * mass_g * velocity_cms**2

# Type hints help IDEs provide better autocomplete and catch potential errors
energy: float = kinetic_energy_typed(9.109e-28, 3e9)
print(f"Energy: {energy:.2e} ergs")

# Python still allows "wrong" types - hints aren't enforced!
# This works even though we pass integers instead of floats:
energy2 = kinetic_energy_typed(1, 100)  # Still works!
print(f"Integer inputs still work: {energy2} ergs")

# For more complex types, import from typing module
from typing import List, Tuple, Optional

def analyze_data_typed(values: List[float]) -> Tuple[float, float, Optional[float]]:
    """
    Analyze data with complex type hints.
    
    List[float] means a list of floats
    Tuple[float, float, Optional[float]] means it returns
    a tuple with two floats and maybe a third float (or None)
    """
    if not values:
        return 0.0, 0.0, None
    
    n = len(values)
    mean = sum(values) / n
    variance = sum((x - mean)**2 for x in values) / n
    std_dev = variance ** 0.5
    
    sorted_vals = sorted(values)
    median = sorted_vals[n // 2] if n % 2 == 1 else (sorted_vals[n//2-1] + sorted_vals[n//2]) / 2
    
    return mean, std_dev, median

# The IDE now knows exactly what types are returned
mean_val, std_val, median_val = analyze_data_typed([1.0, 2.0, 3.0, 4.0, 5.0])
print(f"Mean: {mean_val:.1f}, Std: {std_val:.2f}, Median: {median_val:.1f}")
```

:::{admonition} üõ†Ô∏è Debug This!
:class: challenge

This function has a subtle bug. Can you spot it?

```python
def find_peaks(data, threshold):
    """Find all peaks above threshold - HAS BUG."""
    peaks = []
    peak_indices = []
    
    for i in range(len(data)):
        if data[i] > threshold:
            # Check if it's a local maximum
            if i == 0 or data[i] > data[i-1]:
                if i == len(data)-1 or data[i] > data[i+1]:
                    peaks.append(data[i])
                    peak_indices.append(i)
    
    return peaks, peak_indices
```

<details>
<summary>Answer and Fix</summary>

**Bug**: The condition `data[i] > data[i-1]` should be `data[i] >= data[i-1]` (same for i+1). Otherwise it misses peaks in plateaus where consecutive values are equal.

**Fix**:
```python
def find_peaks_fixed(data, threshold):
    """Find all peaks above threshold."""
    peaks = []
    peak_indices = []
    
    for i in range(len(data)):
        if data[i] > threshold:
            # Check if it's a local maximum (>= for plateaus)
            is_peak = True
            
            if i > 0 and data[i] < data[i-1]:
                is_peak = False
            if i < len(data)-1 and data[i] < data[i+1]:
                is_peak = False
                
            if is_peak:
                peaks.append(data[i])
                peak_indices.append(i)
    
    return peaks, peak_indices
```

This bug appeared in actual spectroscopy software, missing emission lines that had flat tops!
</details>
:::

## 5.7 Performance Considerations

Understanding function overhead helps you write efficient code. Let's measure and optimize!

### Function Call Overhead

```{code-cell} ipython3
import time

def simple_add(a, b):
    """Minimal work - overhead visible."""
    return a + b

def complex_calc(x):
    """Heavy computation - overhead negligible."""
    result = 0
    for i in range(100):
        result += (x + i) ** 0.5
    return result / 100

# Measure overhead
n_calls = 100000

# Simple function
start = time.perf_counter()
for _ in range(n_calls):
    simple_add(1.0, 2.0)
simple_time = time.perf_counter() - start

# Inline equivalent
start = time.perf_counter()
for _ in range(n_calls):
    result = 1.0 + 2.0
inline_time = time.perf_counter() - start

# Complex function
start = time.perf_counter()
for _ in range(n_calls // 100):  # Fewer calls (it's slower)
    complex_calc(1.0)
complex_time = time.perf_counter() - start

print("Function Call Analysis:")
print(f"Simple function: {simple_time*1000:.1f} ms")
print(f"Inline addition: {inline_time*1000:.1f} ms")
print(f"Overhead factor: {simple_time/inline_time:.1f}x")
print(f"\nComplex function: {complex_time*1000:.1f} ms")
print("\nLesson: Overhead only matters for trivial operations!")
```

### Memoization for Expensive Calculations

{margin} Memoization
Caching function results to avoid recomputing expensive operations.

```{code-cell} ipython3
from functools import lru_cache

# Fibonacci without memoization (exponentially slow!)
def fib_slow(n):
    if n < 2:
        return n
    return fib_slow(n-1) + fib_slow(n-2)

# Fibonacci with memoization (linear time!)
@lru_cache(maxsize=128)
def fib_fast(n):
    if n < 2:
        return n
    return fib_fast(n-1) + fib_fast(n-2)

# Compare performance
import time

n = 30

start = time.perf_counter()
result_slow = fib_slow(n)
time_slow = time.perf_counter() - start

start = time.perf_counter()
result_fast = fib_fast(n)
time_fast = time.perf_counter() - start

print(f"Fibonacci({n}) = {result_fast}")
print(f"Without memoization: {time_slow*1000:.1f} ms")
print(f"With memoization: {time_fast*1000:.3f} ms")
print(f"Speedup: {time_slow/time_fast:.0f}x")

# Check cache statistics
print(f"\nCache info: {fib_fast.cache_info()}")
```

## Practice Exercises

### Exercise 5.1: Build Statistical Analysis Functions

Create a suite of analysis functions for experimental data:

```{code-cell} ipython3
def calculate_statistics(data):
    """
    Calculate comprehensive statistics.
    
    TODO: Your implementation should:
    1. Handle empty lists (return None or raise ValueError)
    2. Check for None values in the data
    3. Return a dictionary with: mean, std, min, max, median
    
    Example return value:
    {'mean': 5.2, 'std': 1.3, 'min': 3.1, 'max': 7.8, 'median': 5.0}
    """
    # Your implementation here
    # Start with: if not data: return None
    pass

def remove_outliers(data, n_sigma=3):
    """
    Remove points more than n_sigma standard deviations from mean.
    
    TODO: Your implementation should:
    1. Calculate mean and standard deviation
    2. Keep only values within mean ¬± n_sigma*std
    3. Return filtered list
    
    Hint: Use the statistics from calculate_statistics()
    """
    # Your implementation here
    pass

def bootstrap_error(data, statistic_func=None, n_samples=1000):
    """
    Estimate error using bootstrap resampling.
    
    TODO (Advanced challenge!):
    1. Default to mean if no statistic_func provided
    2. Resample data with replacement n_samples times
    3. Calculate statistic for each resample
    4. Return standard deviation of the statistics
    
    Hint: Use random.choices(data, k=len(data)) for resampling
    """
    # Your implementation here
    pass

# Test with sample data
test_data = [9.8, 9.7, 10.1, 9.9, 50.0, 9.8, 10.0, 9.9]  # Note outlier!
print(f"Original data: {test_data}")
print("Implement the functions above to analyze this data!")

# Once implemented, you should be able to:
# stats = calculate_statistics(test_data)
# clean_data = remove_outliers(test_data, n_sigma=2)
# error = bootstrap_error(clean_data)
```

### Exercise 5.2: Create a Scientific Module

Build `analysis_tools.py` module:

```python
"""
analysis_tools.py - Data analysis utilities

TODO: Create this module with:
1. Constants (confidence levels, etc.)
2. Statistical functions
3. Data cleaning functions
4. Plotting helpers
5. Module testing in __main__
"""

# Your module here
```

### Exercise 5.3: Variable Star Analysis Functions

Continue building our variable star analysis toolkit:

```{code-cell} ipython3
def generate_cepheid_data(period_days=5.4, amplitude_mag=0.3, n_points=50):
    """
    Generate simulated Cepheid variable star data.
    
    Parameters:
        period_days: Period in days
        amplitude_mag: Amplitude in magnitudes
        n_points: Number of observations
    
    Returns:
        times, magnitudes, errors (all as lists)
    """
    import random
    import math
    
    times = []
    mags = []
    errors = []
    
    for i in range(n_points):
        # Irregular sampling
        t = i * period_days / 10 + random.uniform(-0.1, 0.1)
        
        # Cepheid light curve (asymmetric - rises quickly, falls slowly)
        phase = (t % period_days) / period_days
        
        if phase < 0.3:
            # Rising branch (quick brightening)
            mag = 12.0 - amplitude_mag * (phase / 0.3)
        else:
            # Falling branch (slow dimming)
            mag = 12.0 - amplitude_mag * math.exp(-(phase - 0.3) / 0.4)
        
        # Add realistic noise
        mag += random.gauss(0, 0.02)
        error = 0.01 + 0.01 * random.random()
        
        times.append(t)
        mags.append(mag)
        errors.append(error)
    
    return times, mags, errors

# Create analysis functions (for you to implement)
def find_period_simple(times, mags):
    """
    TODO: Estimate period from data.
    Hint: Look for repeating patterns in brightness!
    
    One approach:
    1. Find time between brightness minima
    2. Average these intervals
    3. Return estimated period
    """
    # Your implementation here
    pass

def phase_fold(times, mags, period):
    """
    TODO: Fold data on given period.
    
    Algorithm:
    1. Calculate phase for each time: phase = (time % period) / period
    2. Sort by phase
    3. Return phases and corresponding magnitudes
    """
    # Your implementation here
    pass

# Generate and analyze data
t, m, e = generate_cepheid_data()
print(f"Generated {len(t)} observations")
print(f"Time range: {min(t):.1f} to {max(t):.1f} days")
print(f"Magnitude range: {min(m):.2f} to {max(m):.2f}")
print("\nImplement the analysis functions to find the period!")
```

## Main Takeaways

Functions transform programming from repetitive scripting into modular, maintainable software engineering. When you encapsulate logic in well-designed functions, you create building blocks that can be tested independently, shared with collaborators, and combined into complex analysis pipelines. The progression from simple functions to modules to packages mirrors how scientific software naturally grows‚Äîwhat starts as a quick calculation evolves into a shared tool used by entire research communities.

The distinction between positional, keyword, and default arguments gives you the flexibility to design interfaces that are both powerful and intuitive. Positional arguments work well for obvious parameters like `power(base, exponent)`, while keyword arguments with defaults enable complex functions that remain simple for common cases. Understanding when to use each type‚Äîand the critical danger of mutable default arguments‚Äîprevents the subtle bugs that have plagued major scientific packages.

The scope rules and namespace concepts you've learned explain why variables sometimes behave unexpectedly in complex programs. Understanding the LEGB rule prevents frustrating bugs where variables have unexpected values or modifications in one place affect seemingly unrelated code. The mutable default argument trap demonstrates why understanding Python's evaluation model is crucial for writing reliable code. These aren't just academic concepts‚Äîthey've caused real disasters in production systems.

Functional programming concepts like map, filter, and pure functions prepare you for modern scientific computing frameworks. JAX requires functional style for automatic differentiation, parallel processing works best with stateless functions, and testing becomes trivial when functions have no side effects. The ability to pass functions as arguments and return them from other functions enables powerful patterns like the specialized integrators we created with closures.

The performance measurements showed that function call overhead only matters for trivial operations in tight loops‚Äîexactly where you'll want to use NumPy's vectorized operations (Chapter 7) instead. For complex calculations, overhead is negligible compared to computation time. Memoization can provide dramatic speedups when expensive calculations repeat, as often happens in optimization and parameter searching.

Looking forward, the functions you've learned to write here form the foundation for object-oriented programming in Chapter 6, where functions become methods attached to objects. The module organization skills prepare you for building larger scientific packages, while the documentation practices ensure your code can be understood and maintained by others. Most importantly, thinking in terms of functional contracts and clear interfaces will make you a better computational scientist, capable of building the robust, efficient tools that modern research demands.

## Definitions

**argument** - The actual value passed to a function when calling it (e.g., in `f(5)`, 5 is an argument)

**closure** - A function that remembers variables from its enclosing scope even after that scope has finished executing

**decorator** - A function that modifies another function's behavior without changing its code

**default argument** - A parameter value used when no argument is provided during function call

**docstring** - A string literal that appears as the first statement in a function, module, or class to document its purpose

**function** - A reusable block of code that performs a specific task, taking inputs and optionally returning outputs

**global** - A keyword that allows a function to modify a variable in the global scope

**keyword argument** - An argument passed to a function by explicitly naming the parameter

**lambda** - An anonymous function defined inline using the `lambda` keyword

**LEGB** - The order Python searches for variables: Local, Enclosing, Global, Built-in

**memoization** - Caching function results to avoid recomputing expensive operations

**module** - A Python file containing definitions and statements that can be imported and reused

**namespace** - A container that holds a set of identifiers and their associated objects

**package** - A directory containing multiple Python modules and an `__init__.py` file

**parameter** - A variable in a function definition that receives a value when the function is called

**positional argument** - An argument passed to a function based on its position in the parameter list

**pure function** - A function that always returns the same output for the same input with no side effects

**return value** - The result that a function sends back to the code that called it

**scope** - The region of a program where a variable is accessible

**side effect** - Any state change that occurs beyond returning a value from a function

***args** - Syntax for collecting variable positional arguments into a tuple

***kwargs** - Syntax for collecting variable keyword arguments into a dictionary

## Key Takeaways

- Functions are contracts: they promise specific outputs for given inputs
- Choose positional arguments for obvious parameters, keyword arguments for optional ones
- The mutable default argument trap occurs because defaults are evaluated once at definition time
- Always use `None` as a sentinel for mutable default arguments  
- Python searches for variables using LEGB: Local, Enclosing, Global, Built-in
- Global variables make code hard to test, debug, and parallelize
- Lambda functions are useful for simple operations but limited to single expressions
- Functional programming concepts (map, filter, reduce) prepare you for modern frameworks
- The `if __name__ == "__main__"` pattern makes modules both importable and executable
- Never use `from module import *` in production code‚Äîit causes namespace pollution
- Docstrings are essential for scientific code that others will use and maintain
- Function call overhead matters only in tight loops with trivial operations
- Memoization can dramatically speed up expensive repeated calculations
- Testing and validation functions are as important as calculation functions
- Performance optimization should follow: algorithm ‚Üí vectorization ‚Üí caching ‚Üí parallelization

## Quick Reference Tables

### Function Definition Patterns

| Pattern | Syntax | Use Case |
|---------|--------|----------|
| Basic function | `def func(x, y):` | Simple operations |
| Positional only | `def func(a, b, /):` | Force positional |
| Default arguments | `def func(x, y=10):` | Optional parameters |
| Keyword only | `def func(*, x, y):` | Force keywords |
| Variable args | `def func(*args):` | Unknown number of inputs |
| Keyword args | `def func(**kwargs):` | Flexible options |
| Combined | `def func(a, *args, x=1, **kwargs):` | Maximum flexibility |
| Lambda | `lambda x: x**2` | Simple inline functions |

### When to Use Different Argument Types

| Argument Type | When to Use | Example |
|--------------|-------------|---------|
| Positional | Obvious meaning, 1-3 params | `power(2, 3)` |
| Keyword | Many params, optional | `plot(data, color='red')` |
| Default | Common values | `round(3.14, digits=2)` |
| *args | Variable inputs | `maximum(1, 2, 3, 4)` |
| **kwargs | Configuration options | `setup(debug=True, verbose=False)` |

### Module Import Patterns

| Pattern | Example | When to Use |
|---------|---------|-------------|
| Import module | `import numpy` | Use many functions |
| Import with alias | `import numpy as np` | Standard abbreviations |
| Import specific | `from math import sin, cos` | Few specific functions |
| Import all (avoid!) | `from math import *` | Never in production |
| Relative import | `from . import module` | Within packages |

### Common Function Bugs and Fixes

| Problem | Symptom | Fix |
|---------|---------|-----|
| Mutable default | Data persists between calls | Use `None` sentinel |
| UnboundLocalError | Can't modify global | Use `global` or pass value |
| Missing return | Function returns None | Add `return` statement |
| Namespace pollution | Name conflicts | Avoid wildcard imports |
| Slow recursion | Exponential time | Add memoization |
| Type confusion | Unexpected types | Add type hints/validation |

## Next Chapter Preview

With functions and modules mastered, Chapter 6 introduces Object-Oriented Programming (OOP)‚Äîa paradigm that bundles data and behavior together. You'll learn to create classes that model physical systems naturally: a `Particle` class with position and velocity attributes, methods to calculate energy and momentum, and special methods that make your objects work seamlessly with Python's built-in functions.

The functional programming concepts from this chapter provide essential background for OOP. Methods are just functions attached to objects, and understanding scope prepares you for the `self` parameter that confuses many beginners. The module organization skills you've developed will expand to organizing classes and building object hierarchies. Most importantly, the design thinking you've practiced‚Äîcreating clean interfaces and thinking about contracts‚Äîdirectly applies to designing effective classes that model the complex systems you'll encounter in computational physics.

## References

### Historical Events and Technical Details

1. **Ariane 5 Flight 501 Failure (1996)**
   - Lions, J. L. et al. (1996). "ARIANE 5 Flight 501 Failure: Report by the Inquiry Board." European Space Agency. Paris: ESA.
   - Gleick, J. (1996). "A Bug and A Crash." *The New York Times Magazine*, December 1, 1996.
   - Nuseibeh, B. (1997). "Ariane 5: Who Dunnit?" *IEEE Software*, 14(3), 15-16.

2. **LIGO Gravitational Wave Detection (2015)**
   - Abbott, B. P. et al. (LIGO Scientific Collaboration) (2016). "Observation of Gravitational Waves from a Binary Black Hole Merger." *Physical Review Letters*, 116(6), 061102.
   - Abbott, B. P. et al. (2016). "GW150914: The Advanced LIGO Detectors in the Era of First Discoveries." *Physical Review Letters*, 116(13), 131103.
   - LIGO Scientific Collaboration. (2021). "LIGO Algorithm Library - LALSuite." Available at: https://lscsoft.docs.ligo.org/lalsuite/
   - LIGO Open Science Center. (2024). "LIGO Open Data." Available at: https://www.gw-openscience.org/

3. **Hubble Space Telescope Mirror Error (1990)**
   - Allen, L. et al. (1990). "The Hubble Space Telescope Optical Systems Failure Report." NASA-TM-103443.
   - Chaisson, E. (1994). *The Hubble Wars*. New York: HarperCollins. ISBN 0-06-017114-6.
   - Leckrone, D. S. (1995). "The Hubble Space Telescope Servicing Mission." *Astrophysics and Space Science*, 226(1), 1-24.

4. **Mars Climate Orbiter Loss (1999)**
   - Stephenson, A. G. et al. (1999). "Mars Climate Orbiter Mishap Investigation Board Report." NASA.
   - Oberg, J. (1999). "Why the Mars Probe Went Off Course." *IEEE Spectrum*, 36(12), 34-39.

### Python Documentation

5. **Python Language Reference**
   - Van Rossum, G., & Drake, F. L. (2024). "Python Language Reference, version 3.12." Python Software Foundation. Available at: https://docs.python.org/3/reference/

6. **Scientific Python Resources**
   - Harris, C. R. et al. (2020). "Array programming with NumPy." *Nature*, 585(7825), 357-362.
   - VanderPlas, J. (2016). *Python Data Science Handbook*. O'Reilly Media. ISBN: 978-1491912058.